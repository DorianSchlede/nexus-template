---
paper_id: "19"
title: "Graph of Thoughts: Solving Elaborate Problems with Large Language Models"
authors: ["Maciej Besta", "Nils Blach", "Ales Kubicek", "Robert Gerstenberger", "Michal Podstawski", "Lukas Gianinazzi", "Joanna Gajda", "Tomasz Lehmann", "Hubert Niewiadomski", "Piotr Nyczyk", "Torsten Hoefler"]
institution: "ETH Zurich, Warsaw University of Technology, Cledar"
year: 2023
domain: "LLM Reasoning, Prompt Engineering, Graph-based Computation"
extraction_version: "v2_discovery"
extraction_date: "2025-12-31"

ontological_primitives:
  - term: "Thought"
    definition: "A unit of information generated by an LLM - purposefully not prescribed; can be a paragraph, document, code block, or sequence of numbers depending on use case"
    source: "Chunk 1:157-161"
    unique_aspects: "Domain-agnostic primitive; identity is functional rather than structural - what matters is how it connects, not what it contains"

  - term: "Vertex"
    definition: "A node in the reasoning graph containing a solution to a problem (initial, intermediate, or final)"
    source: "Chunk 1:246-249"
    unique_aspects: "Not just data containers - vertices have states (validity, scores) tracked by GRS"

  - term: "Edge"
    definition: "A directed dependency between thoughts indicating that thought t2 was constructed using t1 as direct input"
    source: "Chunk 1:249-252"
    unique_aspects: "Represents explicit instruction to LLM to use one thought for generating another - not just logical dependency but conversational/prompt dependency"

  - term: "Graph of Operations (GoO)"
    definition: "A static structure specifying the graph decomposition of a task - prescribes transformations to be applied to LLM thoughts with their order and dependencies"
    source: "Chunk 1:391-394"
    unique_aspects: "META-LEVEL primitive - the plan/schema that governs how the actual reasoning graph unfolds; constructed once before execution"

  - term: "Graph Reasoning State (GRS)"
    definition: "A dynamic structure maintaining the continually updated state of the ongoing LLM reasoning process - includes executed operations, thought states, validity, scores"
    source: "Chunk 1:439-443"
    unique_aspects: "Runtime state tracker - bridges the static GoO plan with actual execution; maintains provenance"

  - term: "Transformation"
    definition: "An operation T(G, p_theta) that modifies the reasoning graph G by adding/removing vertices and edges"
    source: "Chunk 1:321-331"
    unique_aspects: "First-class operation type; explicitly allows removal (V-, E-) unlike typical acyclic computation graphs"

  - term: "Volume"
    definition: "For a given thought t, the number of preceding LLM thoughts that could have impacted t - formally, the number of thoughts from which there exists a path to t"
    source: "Chunk 1:738-740"
    unique_aspects: "Novel metric introduced by this paper - measures information integration capacity of a reasoning scheme"

structural_patterns:
  - pattern_name: "Decompose-Solve-Merge DAG"
    structure: "Split -> Parallel Process -> Aggregate -> Refine"
    instances:
      - "Sorting: Split list -> Sort sublists independently -> Merge sorted sublists -> Improve result"
      - "Set intersection: Split set -> Intersect subsets -> Merge intersections"
      - "Keyword counting: Split text -> Count per passage -> Merge counts"
      - "Document merging: Generate multiple merges -> Aggregate best -> Improve"
    source: "Chunk 1:477-479, Chunk 1:667-669"

  - pattern_name: "Generate-Score-KeepBest Selection"
    structure: "Generate k alternatives -> Score each -> Retain top n"
    instances:
      - "k=5 sort attempts, keep best 1"
      - "k=10 merge attempts, keep best 1"
      - "k=3 aggregate attempts with validation"
    source: "Chunk 7:640-650"

  - pattern_name: "Feedback Loop / Refinement"
    structure: "Thought -> Self-edge (v,v) -> Improved thought"
    instances:
      - "improve_prompt for sorting corrections"
      - "improve_merge_prompt for fixing dictionary combinations"
      - "improve_prompt for NDA enhancement"
    source: "Chunk 1:355-357"

  - pattern_name: "Aggregation Fan-in"
    structure: "Multiple thoughts -> Single aggregated thought"
    instances:
      - "Two sorted sublists -> One merged list"
      - "Multiple NDA merge attempts -> One combined NDA"
      - "Partial intersection results -> Complete intersection"
    source: "Chunk 1:344-352"

  - pattern_name: "Heterogeneous Graph with Typed Nodes"
    structure: "Vertices belong to different classes; mapping function c: V -> C"
    instances:
      - "In writing: plans vs paragraphs (C = {plan, par})"
      - "In NDA merging: original docs vs merged docs vs scores"
    source: "Chunk 1:253-259"

novel_concepts:
  - concept: "Graph of Operations (GoO)"
    definition: "A static execution plan that prescribes thought transformations before any LLM interaction begins"
    novelty_claim: "Separates the WHAT (strategy/plan) from the HOW (execution) - enables reusable reasoning templates"
    source: "Chunk 1:434-437"

  - concept: "Volume of a Thought"
    definition: "The number of preceding thoughts that could have contributed to a given thought"
    novelty_claim: "New metric for comparing prompting schemes - captures information integration capacity that latency alone misses"
    source: "Chunk 1:729-740"

  - concept: "Aggregation Transformation"
    definition: "Graph-enabled operation that combines arbitrary thoughts into new ones, reinforcing advantages while eliminating disadvantages"
    novelty_claim: "Not available in CoT (linear) or ToT (tree) - requires graph structure to represent many-to-one dependencies"
    source: "Chunk 1:344-352"

  - concept: "Refining Transformation with Self-Loops"
    definition: "Modifying thought content via edge (v,v) - an iterated thought maintaining original connections"
    novelty_claim: "Graph cycles for iterative improvement - impossible in tree structures"
    source: "Chunk 1:355-357"

  - concept: "Latency-Volume Tradeoff"
    definition: "Fundamental tradeoff between hops to reach a thought (latency) and information integration (volume)"
    novelty_claim: "GoT achieves log_k(N) latency with N volume - best of both worlds vs CoT (N/N) and ToT (log_k(N)/log_k(N))"
    source: "Chunk 1:756-767"

semantic_commitments:
  - commitment: "Thoughts as First-Class Entities"
    position: "Treats LLM outputs as manipulable graph vertices rather than just text"
    implications: "Enables formal operations (merge, refine, aggregate) on reasoning artifacts"
    source: "Chunk 1:17-22"

  - commitment: "Reasoning as Graph Traversal/Construction"
    position: "LLM reasoning is modeled as building and navigating a directed graph"
    implications: "Admits cycles (refinement), DAGs (decomposition), and arbitrary topologies"
    source: "Chunk 1:74-76"

  - commitment: "Task Decomposition Principle"
    position: "Complex tasks should be broken down to sizes where LLM can solve correctly with single prompt"
    implications: "Combinatorial composition of simpler solutions; quality emerges from structure not raw capability"
    source: "Chunk 2:137-142"

  - commitment: "Evaluation as External Function"
    position: "Scoring function E(v,G,p_theta) is general - can use LLM, human, or algorithmic evaluation"
    implications: "No assumption about source of truth; supports hybrid human-AI evaluation"
    source: "Chunk 1:370-377"

  - commitment: "Static Plan, Dynamic Execution"
    position: "GoO (plan) is constructed before execution; GRS (state) evolves during"
    implications: "Reasoning follows pre-specified structure; emergent adaptation limited to which branches to pursue"
    source: "Chunk 1:435-443"

boundary_definitions:
  - entity_type: "Thought"
    identity_criteria: "Functional - determined by use case (paragraph, code block, number sequence)"
    boundary_cases: "When is a 'refined' thought the same vs different thought? Paper uses self-edge notation suggesting same-with-modification"
    source: "Chunk 1:159-161"

  - entity_type: "Transformation"
    identity_criteria: "Defined by effect on graph: which V+, E+, V-, E- sets it produces"
    boundary_cases: "Is a transformation with same effect but different prompt the same transformation?"
    source: "Chunk 1:321-331"

  - entity_type: "Reasoning Process (G)"
    identity_criteria: "The complete graph built during LLM conversation for a task"
    boundary_cases: "Different runs on same task produce different G - identity tied to execution instance"
    source: "Chunk 1:244-246"

temporal_modeling:
  - aspect: "Operation Sequencing"
    approach: "GoO specifies predecessor/successor relationships between operations"
    mechanism: "Each operation knows its predecessor and successor; execution follows this DAG"
    source: "Chunk 1:437-439"

  - aspect: "State Evolution"
    approach: "GRS maintains dynamic, continually-updated state"
    mechanism: "Tracks which operations executed, thought states, scores - grows over time"
    source: "Chunk 1:439-443"

  - aspect: "Iteration/Refinement"
    approach: "Self-loops represent temporal iteration on same entity"
    mechanism: "Edge (v,v) indicates thought refined over time while maintaining identity"
    source: "Chunk 1:355-357"

  - aspect: "Latency"
    approach: "Number of hops to reach final thought"
    mechanism: "Measures sequential depth of reasoning path"
    source: "Chunk 1:730-732"

agency_spectrum:
  - agent_type: "LLM (p_theta)"
    capabilities: "Text generation, reasoning within single prompt, following instructions"
    constraints: "Cannot reliably sort long lists; errors on complex counting; no persistent memory across prompts"
    source: "Chunk 1:157, Chunk 1:475-476"

  - agent_type: "Controller"
    capabilities: "Coordinates reasoning process, selects thoughts from GRS, decides transformations, determines when to finalize"
    constraints: "Follows static GoO plan; limited adaptive decision-making"
    source: "Chunk 1:422-429"

  - agent_type: "Prompter"
    capabilities: "Prepares prompts, encodes graph structure within prompts"
    constraints: "No autonomous decision-making; serves Controller"
    source: "Chunk 1:398-402"

  - agent_type: "Parser"
    capabilities: "Extracts information from LLM outputs, constructs thought states"
    constraints: "Reactive; depends on LLM output format"
    source: "Chunk 1:405-410"

  - agent_type: "Evaluator (Human or LLM)"
    capabilities: "Assigns scores to thoughts"
    constraints: "For LLM: unreliable (e.g., redundancy scores varied 3-8 for same NDA); For human: expensive"
    source: "Chunk 1:415-419, Chunk 6:629-691"

knowledge_representation:
  - mechanism: "Graph of Thoughts (G)"
    formalism: "Directed graph G = (V, E) or heterogeneous G = (V, E, c)"
    reasoning: "Path traversal for dependency tracking; volume calculation for information flow"
    source: "Chunk 1:244-259"

  - mechanism: "Graph of Operations (GoO)"
    formalism: "Execution DAG with operation objects linked by predecessor/successor"
    reasoning: "Dictates transformation application order"
    source: "Chunk 1:434-443"

  - mechanism: "Graph Reasoning State (GRS)"
    formalism: "Dynamic state object containing thought states, scores, execution history"
    reasoning: "Enables selection of best thoughts, tracking of validity"
    source: "Chunk 1:439-443"

  - mechanism: "Prompt Templates"
    formalism: "Structured text with parameter substitution (e.g., {input_list})"
    reasoning: "Encodes task instructions, few-shot examples, approach descriptions"
    source: "Chunk 3:586-689"

emergence_indicators:
  - phenomenon: "Quality Improvement from Structure"
    mechanism: "Decomposition + aggregation produces better results than single-shot prompting"
    evidence: "62% improvement over ToT in sorting quality; 31% cost reduction"
    source: "Chunk 1:104-105"

  - phenomenon: "Volume-Enabled Information Integration"
    mechanism: "Graph aggregation allows final thought to benefit from ALL intermediate thoughts"
    evidence: "GoT achieves volume N with latency log_k(N); ToT only achieves log_k(N) volume"
    source: "Chunk 1:756-767"

  - phenomenon: "Error Correction through Redundancy"
    mechanism: "Multiple solution attempts + scoring + selection corrects LLM errors"
    evidence: "Sorting example: only 1 of 5 sort attempts fully correct; selection recovers quality"
    source: "Chunk 3:890-898"

integration_surfaces:
  - surface: "Prompting Schemes Generalization"
    connects_to: ["Chain-of-Thought (CoT)", "Self-Consistency (CoT-SC)", "Tree of Thoughts (ToT)"]
    alignment_quality: "Full subsumption - GoT graph model can express chain, multiple chains, and tree as special cases"
    source: "Chunk 1:81-82, Table 1"

  - surface: "Graph Databases/Computing"
    connects_to: ["Property Graphs", "Graph Neural Networks", "Graph Pattern Matching"]
    alignment_quality: "Conceptual alignment - borrows graph abstraction but no direct data model mapping"
    source: "Chunk 2:553-565"

  - surface: "Planning/Search Algorithms"
    connects_to: ["BFS", "DFS", "Divide-and-Conquer"]
    alignment_quality: "GoT execution resembles search with branching factor k and depth L"
    source: "Chunk 1:196-197, Chunk 2:545-547"

  - surface: "Self-Reflection/Evaluation Schemes"
    connects_to: ["Self-Refine", "Reflexion", "REFINER"]
    alignment_quality: "Partial - GoT incorporates self-evaluation for scoring; could integrate deeper reflection"
    source: "Chunk 2:528-538"

gaps_and_tensions:
  - gap_type: "Omission"
    description: "No formal semantics for thought content - thoughts are opaque strings"
    implications: "Cannot reason ABOUT thought content programmatically; limited to pattern matching in Parser"
    source: "Implicit - Chunk 1:159-161"

  - gap_type: "Omission"
    description: "No mechanism for dynamic GoO modification during execution"
    implications: "Cannot adaptively restructure reasoning based on intermediate results"
    source: "Chunk 1:435-436"

  - gap_type: "Tension"
    description: "Static GoO vs need for adaptive reasoning"
    implications: "Must pre-specify decomposition strategy; cannot discover better structures mid-execution"
    source: "Chunk 1:427-429"

  - gap_type: "Underspecified"
    description: "Scoring function E and ranking function R are abstract placeholders"
    implications: "Evaluation quality depends heavily on implementation; paper shows LLM-based scoring has high variance"
    source: "Chunk 1:370-377, Chunk 6:623-697"

  - gap_type: "Omission"
    description: "No treatment of thought provenance beyond graph edges"
    implications: "Cannot trace WHY a thought was generated, only WHAT contributed to it"
    source: "Implicit - no provenance ontology mentioned"

  - gap_type: "Tension"
    description: "Graph model assumes discrete thoughts, but LLM reasoning is continuous/autoregressive"
    implications: "Abstraction boundary may hide important reasoning dynamics"
    source: "Implicit - Chunk 1:17-22"

  - gap_type: "Omission"
    description: "No treatment of multi-agent scenarios"
    implications: "Framework assumes single LLM; coordination of multiple LLMs not addressed"
    source: "Implicit - single p_theta throughout"

empirical_grounding:
  - type: "Algorithmic Task Benchmarks"
    domain: "Sorting, Set Intersection"
    scale: "100 input samples per task; lists of 32, 64, 128 elements"
    findings: "GoT reduces median error 62% vs ToT on 128-element sorting while reducing cost 31%"
    source: "Chunk 1:780-782, Chunk 2:87-97"

  - type: "Text Processing Task"
    domain: "Keyword Counting"
    scale: "Text passages with country name counting"
    findings: "GoT4, GoT8, GoTx variants show tradeoff between decomposition granularity and cost"
    source: "Chunk 2:495-497"

  - type: "Document Generation Task"
    domain: "NDA Document Merging"
    scale: "4 input NDAs, 50 samples, 16k context"
    findings: "GoT achieves higher retention-redundancy scores through merge-aggregate-improve pipeline"
    source: "Chunk 2:513-516"

  - type: "LLM Comparison"
    domain: "Model capabilities"
    scale: "GPT-3.5 (primary), Llama-2 (limited)"
    findings: "Llama-2 slower and worse than GPT-3.5; budget constraints limited exploration"
    source: "Chunk 2:73-76"

surprises:
  - "VOLUME as novel metric: The paper introduces 'volume of a thought' as a fundamental measure of prompting scheme power - this connects graph-theoretic reachability to reasoning capacity in a way not seen in prior prompting literature"
  - "STATIC GoO design choice: Despite advocating for graph flexibility, the execution plan (GoO) is explicitly static/frozen before execution - this is more rigid than expected for 'complex networks'"
  - "SELF-LOOPS for refinement: The use of (v,v) edges to represent iteration is unusual - most graph formalisms would use v->v' where v' is a new version; here identity is preserved through edge type"
  - "NO AGENT ONTOLOGY: Despite enabling complex reasoning structures, paper has no formal notion of 'agent' beyond the opaque LLM (p_theta) - the Controller, Prompter, Parser are implementation modules not agents"
  - "LLM SCORING UNRELIABILITY: The paper's own data shows LLM self-evaluation varies wildly (redundancy scores 3-8 for same document) - yet this is the default scoring mechanism"
---

# Graph of Thoughts: V2 Discovery Analysis

## Executive Summary

This paper introduces Graph of Thoughts (GoT), a prompting framework that models LLM reasoning as an arbitrary directed graph rather than chains (CoT) or trees (ToT). The key insight is that allowing thoughts to aggregate (many-to-one) and refine (self-loops) enables superior problem decomposition and solution synthesis.

## Key Ontological Contributions

### 1. Thought as First-Class Entity

The paper treats LLM outputs ("thoughts") as manipulable graph vertices with:
- **No prescribed structure** - a thought can be a paragraph, code block, number sequence
- **States tracked by GRS** - validity, scores, relationships
- **Functional identity** - what matters is how thoughts connect, not their internal form

This is notably LESS ontologically committed than frameworks like PROV-O (which distinguishes Entity/Activity/Agent).

### 2. The GoO/GRS Split

A fundamental distinction between:
- **GoO (Graph of Operations)**: Static execution plan, constructed before any LLM interaction
- **GRS (Graph Reasoning State)**: Dynamic state tracker that evolves during execution

This separates STRATEGY from EXECUTION - the plan vs its realization.

### 3. Volume as Information Integration Metric

Novel contribution: measuring how much information CAN reach a final thought.

| Scheme | Latency | Volume |
|--------|---------|--------|
| CoT | N | N |
| CoT-SC | N/k | N/k |
| ToT | log_k(N) | O(log_k(N)) |
| **GoT** | log_k(N) | **N** |

GoT achieves logarithmic latency with linear volume - best tradeoff.

## Structural Patterns Discovered

### The Decompose-Solve-Merge Pattern

Recurring across all use cases:
```
Split Input -> Process Parts in Parallel -> Aggregate Results -> Refine
```

This is a divide-and-conquer pattern but with LLM as the processing unit and explicit merging.

### Generate-Score-KeepBest Selection

Every decision point uses:
```
Generate k alternatives -> Score each -> Retain top n
```

This converts LLM's probabilistic outputs into deterministic selections.

## Gaps Identified

### No Dynamic Plan Adaptation

The GoO is frozen before execution. The framework cannot:
- Restructure reasoning based on intermediate results
- Discover better decomposition strategies during execution
- Adapt branching factor or depth dynamically

### Evaluation Quality Problem

The paper's own data shows LLM self-evaluation is highly variable:
- Same NDA merger scored 3-8 on redundancy by LLM
- Yet this is the default scoring mechanism

### No Provenance Semantics

Beyond graph edges showing "A contributed to B", there is no:
- Explanation of WHY a contribution matters
- Attribution of specific content elements
- Reasoning trace beyond structural dependency

## Integration Opportunities

### With Process Mining

The GoO structure (static execution plan with typed operations) parallels:
- BPMN process definitions
- Workflow nets
- OCEL event logs

GoT could be instrumented to produce OCEL-compatible traces.

### With Multi-Agent Systems

Currently single-LLM only. Natural extensions:
- Different LLMs for different operation types
- Human-in-the-loop for scoring
- Distributed thought generation

### With Foundational Ontologies

The Thought/Transformation primitives could be grounded in:
- **UFO**: Thoughts as Situations, Transformations as Events
- **PROV-O**: Thoughts as Entities, Transformations as Activities (but loses GoT's aggregation semantics)
- **BFO**: Thoughts as Information Content Entities

## Key Quotes

> "The key idea and primary advantage of GoT is the ability to model the information generated by an LLM as an arbitrary graph, where units of information ('LLM thoughts') are vertices, and edges correspond to dependencies between these vertices." (Chunk 1:18-22)

> "When working on a novel idea, a human would not only follow a chain of thoughts (as in CoT) or try different separate ones (as in ToT), but would actually form a more complex network of thoughts." (Chunk 1:63-67)

> "The overall goal when conducting graph decomposition is to break down a task to the point where the LLM can solve it correctly for the majority of time using a single prompt." (Chunk 2:137-140)

## Quality Checklist

- [x] Used paper's own terminology (Thought, GoO, GRS, Volume, Transformation)
- [x] Captured multiple novel concepts (Volume metric, GoO/GRS split, Aggregation transformation)
- [x] Found gaps and tensions (Static GoO, evaluation variance, no provenance)
- [x] Noted surprises (Volume metric, self-loop refinement, no agent ontology)
- [x] All extractions have chunk:line references
- [x] Did NOT force-fit to predefined categories
- [x] Preserved nuance (e.g., thoughts are intentionally undefined)
