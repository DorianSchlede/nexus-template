---
paper_id: "21"
title: "On LLM-Assisted Generation of Smart Contracts from Business Processes"
authors: ["Fabian Stiehle", "Hans Weytjens", "Ingo Weber"]
institution: "Technical University of Munich, Fraunhofer Gesellschaft"
year: 2025
extraction_version: "2.0"
extraction_date: "2025-12-31"

ontological_primitives:
  - term: "Conforming Trace"
    definition: "The ordered sequence of events (activities) that represents a valid execution path through a process model"
    source: "Chunk 1:247-250"
    unique_aspects: "Ground truth for behavioral verification; contrasted with non-conforming traces for benchmarking"

  - term: "Non-Conforming Trace"
    definition: "A manipulated conforming trace that the smart contract must reject"
    source: "Chunk 1:311-312"
    unique_aspects: "Generated by random manipulation; serves as negative test case"

  - term: "Choreography Task"
    definition: "An interaction between participants in a BPMN Choreography, representing a unit of collaboration"
    source: "Chunk 1:219, 414"
    unique_aspects: "Multi-party interaction primitive, not single-actor task"

  - term: "Interaction Net"
    definition: "A special type of labelled Petri net suitable to represent choreographies"
    source: "Chunk 1:349-350"
    unique_aspects: "Intermediate formal representation bridging BPMN and execution semantics"

  - term: "Case Identifier"
    definition: "Groups events into a case; each case represents one process instance"
    source: "Chunk 1:248-249"
    unique_aspects: "Single-case-centric view (contrast with object-centric approaches)"

structural_patterns:
  - pattern_name: "Model-to-Code Transformation Pipeline"
    structure: "Process Model -> Intermediate Representation -> Executable Code"
    instances:
      - "BPMN Choreography -> Interaction Net -> Solidity Smart Contract"
      - "Process Description -> LLM Prompt -> Generated Code"
    source: "Chunk 1:54-55, 346-350"

  - pattern_name: "Conformance Testing Dyad"
    structure: "Positive Cases (accept) vs Negative Cases (reject)"
    instances:
      - "Conforming traces -> True Positive/False Negative"
      - "Non-conforming traces -> True Negative/False Positive"
    source: "Chunk 1:546-555"

  - pattern_name: "Prompt Engineering Spectrum"
    structure: "Zero-shot -> One-shot -> Few-shot"
    instances:
      - "Instructions only vs instructions with examples"
      - "Trade-off between context size and task performance"
    source: "Chunk 1:145-147, 464-470"

  - pattern_name: "Centralized vs Decentralized Deployment"
    structure: "Cloud Platform <-> Blockchain Network (competing paradigms)"
    instances:
      - "AWS/Azure/GCP hosting vs on-chain execution"
      - "Proprietary API models vs self-hosted open-source"
    source: "Chunk 1:72-75, 167-170"

novel_concepts:
  - concept: "Automated Benchmarking Framework for Process-to-Contract Generation"
    definition: "A configurable system that generates conforming/non-conforming traces from process models, deploys LLM-generated contracts, and measures correctness via replay"
    novelty_claim: "First open-source framework for automated functional correctness assessment of LLM-generated smart contracts from process models"
    source: "Chunk 1:221-227, 293-332"

  - concept: "Transactional Logic in Choreography Execution"
    definition: "Smart contract makes as much progress as possible after a task is executed; data-based decisions are made autonomously once gateway is enabled"
    novelty_claim: "Autonomous decision enforcement at XOR gateways without external triggers"
    source: "Chunk 1:370-373"

  - concept: "Bitmasking State Encoding"
    definition: "Efficient encoding technique for token-based execution where contract state is represented as bitmask"
    novelty_claim: "Most efficient encoding for representing process state in smart contracts"
    source: "Chunk 1:482-485"

semantic_commitments:
  - commitment: "Determinism Requirement"
    position: "Smart contracts demand perfect reliability; stochastic LLM outputs are fundamentally unsuitable for autonomous code generation"
    implications: "Even 98% F1 score is inadequate; LLMs should augment rather than replace rule-based tools"
    source: "Chunk 1:637-649"

  - commitment: "Decentralization as Goal"
    position: "Blockchain-based processes seek trust minimization and operational independence"
    implications: "Reliance on centralized LLM providers conflicts with blockchain ethos"
    source: "Chunk 1:74-75, 168-170"

  - commitment: "Behavioral Equivalence"
    position: "Generated code is correct if it accepts all conforming traces and rejects all non-conforming traces"
    implications: "Correctness is behavioral, not syntactic; compilation is necessary but not sufficient"
    source: "Chunk 1:86-87, 238-241"

boundary_definitions:
  - entity_type: "Correct Smart Contract"
    identity_criteria: "Accepts all valid execution paths (conforming traces); rejects all invalid paths (non-conforming traces); compiles successfully"
    boundary_cases: "What about contracts that are functionally correct but gas-inefficient? What about partial correctness?"
    source: "Chunk 1:546-555"

  - entity_type: "Process Case"
    identity_criteria: "Grouped by case identifier; starts with start event, ends with end event"
    boundary_cases: "Loops create potentially infinite traces; threshold of 2,500 traces per process used"
    source: "Chunk 1:248-250, 479, 561"

temporal_modeling:
  - aspect: "Event Ordering"
    approach: "Traces as ordered sequences"
    mechanism: "Each event has position in trace; control flow enforces ordering constraints"
    source: "Chunk 1:249-250, 467"

  - aspect: "Autonomous Progression"
    approach: "Eager execution"
    mechanism: "Contract advances state as far as possible after each task; decisions made when gateway enabled"
    source: "Chunk 1:370-373"

  - aspect: "Immutability"
    approach: "Write-once blockchain semantics"
    mechanism: "Transactions cannot be reversed; errors are permanent and costly"
    source: "Chunk 1:645-648"

agency_spectrum:
  - agent_type: "Human Process Participant"
    capabilities: "Execute choreography tasks; provide data for decisions; hold blockchain addresses"
    constraints: "Must follow process flow; cannot execute out-of-order"
    source: "Chunk 1:313-314, 467"

  - agent_type: "LLM as Code Generator"
    capabilities: "Transform process models to smart contracts; understand natural language and code; generalize from examples"
    constraints: "Non-deterministic output; potential for hallucination/confabulation; no guarantee of correctness"
    source: "Chunk 1:65-69, 110-111"

  - agent_type: "Smart Contract"
    capabilities: "Enforce control flow; verify participant identity; evaluate data conditions; autonomous gateway decisions"
    constraints: "Cannot correct errors post-deployment; gas costs; deterministic execution only"
    source: "Chunk 1:467-469"

knowledge_representation:
  - mechanism: "Process Model"
    formalism: "BPMN 2.0 Choreography XML"
    reasoning: "Structural analysis; trace generation via Petri net playout"
    source: "Chunk 1:338-339, 351-354"

  - mechanism: "Prompt Engineering"
    formalism: "Natural language instructions + code examples + model encoding"
    reasoning: "In-context learning; zero/one/few-shot patterns"
    source: "Chunk 1:145-147, 458-470"

  - mechanism: "State Encoding"
    formalism: "Bitmask (most efficient for token-based execution)"
    reasoning: "Boolean operations on state bits; efficient gas usage"
    source: "Chunk 1:482-485"

emergence_indicators:
  - phenomenon: "Prompt Sensitivity"
    mechanism: "Same model produces different quality output based on prompt design"
    evidence: "Two-shot did not consistently outperform one-shot; pre-runs required iterative refinement"
    source: "Chunk 1:458-465, 618-623"

  - phenomenon: "Model Size vs Performance Non-linearity"
    mechanism: "Larger models generally better but relationship is not strictly linear"
    evidence: "Llama-3.1-405b outperformed by smaller Claude Sonnet; Qwen3-235b performance varied"
    source: "Chunk 1:575-608"

integration_surfaces:
  - surface: "BPMN Choreography"
    connects_to: ["BPMN 2.0 Standard", "Chorpiler tool", "pm4py library"]
    alignment_quality: "Good - standard format with established tooling"
    source: "Chunk 1:338-339, 346-354"

  - surface: "Ethereum Virtual Machine"
    connects_to: ["Solidity language", "Hardhat framework", "Blockchain networks"]
    alignment_quality: "Good - standard execution target with mature tooling"
    source: "Chunk 1:341-343, 362-364"

  - surface: "LLM Provider API"
    connects_to: ["OpenRouter", "OpenAI GPT", "Anthropic Claude", "Open-source models"]
    alignment_quality: "Unified API interface; but centralization conflicts with blockchain values"
    source: "Chunk 1:364-366, 72-75"

gaps_and_tensions:
  - gap_type: "Fundamental Limitation"
    description: "LLMs are inherently stochastic; smart contracts require deterministic correctness"
    implications: "Cannot achieve 100% reliability with current LLM architectures; paradigm mismatch"
    source: "Chunk 1:67, 637-649"

  - gap_type: "Centralization Paradox"
    description: "Using centralized LLM APIs to generate code for decentralized execution"
    implications: "Data sovereignty, privacy, and autonomy concerns conflict with blockchain goals"
    source: "Chunk 1:71-75, 168-170"

  - gap_type: "Evaluation Gap"
    description: "Prior work relied on compilability (syntax) not functional correctness (behavior)"
    implications: "Misleading assessment of LLM capabilities for smart contract generation"
    source: "Chunk 1:86-87, 198-213"

  - gap_type: "Dataset Limitations"
    description: "SAP-SAM models created by students/researchers, not production processes; many non-standard compliant"
    implications: "Results may not generalize to real-world enterprise choreographies"
    source: "Chunk 1:388-394, 400-416"

  - gap_type: "Omission - Security Testing"
    description: "Framework tests functional correctness but not security vulnerabilities"
    implications: "Contracts could be correct but exploitable; GitHub Copilot shown to introduce vulnerabilities"
    source: "Chunk 1:66-67, 673-676"

empirical_grounding:
  - type: "Large-scale benchmarking study"
    domain: "Business process execution on blockchain"
    scale: "165 process models sampled from 1,427 filtered choreographies (from 4,096 original SAP-SAM models); 7 LLMs; 2,500 conforming + 50 non-conforming traces per model"
    findings: "Best models (Grok, Claude) achieved F1 0.85-0.92; compilability near 100%; open-source models 10-40% cheaper but lower F1"
    source: "Chunk 1:99-101, 417-421, 575-619"

  - type: "Cost analysis"
    domain: "LLM API economics"
    scale: "Per-model per-process cost ranging from $0.001 to $0.064"
    findings: "Open-source models significantly cheaper (0.1-1 cent) vs proprietary (3-6 cents)"
    source: "Chunk 1:575-608"

surprises:
  - "Two-shot prompting did NOT consistently outperform one-shot - contradicts typical few-shot learning expectations"
  - "Compilability was near 100% across models, but functional correctness varied dramatically - syntax is easy, semantics is hard"
  - "The paper explicitly argues LLMs CANNOT solve this problem with current architectures - unusually honest about fundamental limitations"
  - "Confabulation preferred over hallucination as terminology - neuroanatomy metaphor critique"
  - "Temperature=0 still produces non-deterministic output due to floating point variability and token probability ties"

quality_indicators:
  - used_paper_terminology: true
  - novel_concepts_found: 3
  - gaps_found: 5
  - surprises_noted: 5
  - chunk_references: "All extractions include Chunk 1:line references"
---

# Paper Summary

## Core Contribution

This paper presents the first open-source benchmarking framework for evaluating LLM-generated smart contracts from BPMN process models. Unlike prior work that only tested compilability, this framework tests functional correctness by replaying conforming and non-conforming traces against deployed contracts.

## Key Insight: Paradigm Mismatch

The paper's most significant contribution is the explicit articulation of a **fundamental paradigm mismatch**:

> "LLMs are inherently non-deterministic... We do not see a way in which this fundamental issue could be resolved with current LLM architectures." (Lines 67, 648-649)

This is surprising because most LLM papers emphasize potential and promise. Here, the authors argue that smart contracts require 100% reliability, and even 98% F1 is unacceptable given the immutable, financial nature of blockchain transactions.

## Ontological Primitives Discovered

The paper operates with a **trace-centric ontology**:
- **Conforming Trace**: Valid execution path (positive test case)
- **Non-Conforming Trace**: Invalid path (negative test case)
- **Choreography Task**: Multi-party interaction (not single-actor task)
- **Interaction Net**: Petri net intermediate representation

Notably, this is **case-centric** (single case identifier grouping), contrasting with the object-centric paradigm in other process mining work.

## Structural Patterns

The **Model-to-Code Transformation Pipeline** (Process Model -> IR -> Executable) is the dominant pattern, with LLMs attempting to shortcut this by going directly from model to code without explicit intermediate representations.

The **Conformance Testing Dyad** (accept valid, reject invalid) provides the semantic grounding for correctness assessment.

## Agency Model

Three distinct agent types emerge:
1. **Human Participants**: Execute tasks, constrained by process flow
2. **LLM as Generator**: Powerful but unreliable, non-deterministic
3. **Smart Contract**: Deterministic executor, autonomous at gateways, but cannot self-correct

## Critical Gaps Identified

1. **Stochasticity vs Determinism**: Fundamental incompatibility between LLM architecture and smart contract requirements
2. **Centralization Paradox**: Using centralized APIs to generate decentralized code
3. **Security Blind Spot**: Functional correctness tested, not vulnerability detection
4. **Dataset Representativeness**: Academic models may not reflect production complexity

## Implications for Synthesis

This paper suggests that **AI agents cannot autonomously produce safety-critical artifacts** with current architectures. The recommended approach is **augmentation over replacement**: LLMs assist humans with code generation tools rather than generating final artifacts directly.

This has implications for any ontology of AI agency: there may be an **reliability boundary** that separates tasks suitable for autonomous AI execution from those requiring human verification loops.
