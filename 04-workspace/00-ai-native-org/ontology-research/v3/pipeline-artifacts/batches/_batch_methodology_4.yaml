---
batch_id: "methodology_4"
field: methodology
extracted_at: "2025-12-31T12:00:00Z"
chunks_read: 7
patterns_found: 24
---

patterns:
  - name: "Semi-Automated Ontology Mapping Methodology"
    chunk_ref: "04-PROV-O_to_BFO (Chunk 1:123-131)"
    quote: "Our approach to mapping is based on the semi-automated curation of ontologies leveraging conceptual analysis techniques and semantic web technologies."
    description: "The PROV-O to BFO mapping paper describes a hybrid methodology combining manual conceptual analysis with automated semantic web tools. This involves carefully evaluating necessary and sufficient conditions for class/relation instances, prioritizing semantic accuracy over fully-automated matching despite higher effort. The methodology balances theoretical criteria with practical engineering techniques."

  - name: "Theoretical Criteria-Driven Alignment"
    chunk_ref: "04-PROV-O_to_BFO (Chunk 1:128-131)"
    quote: "Our methodology can be described first by the theoretical criteria we chose for a successful alignment, and second by the engineering techniques used for rigorously evaluating an alignment"
    description: "The mapping methodology separates theoretical criteria (what constitutes a successful alignment) from engineering techniques (how to verify and implement). This dual-track approach ensures both semantic correctness and practical implementability. The criteria include coherence, consistency, conservativity, and totality."

  - name: "Equivalence and Subsumption Mapping Relations"
    chunk_ref: "04-PROV-O_to_BFO (Chunk 1:146-164)"
    quote: "Equivalence relations represented by OWL equivalentClass and OWL equivalentProperty give necessary and sufficient conditions... Subsumption relations represented by RDFS subClassOf or RDFS subPropertyOf give sufficient conditions"
    description: "The methodology prioritizes two types of mapping predicates: equivalence (bidirectional, full semantic interoperability) and subsumption (one-way bridge). Equivalence provides strongest interoperability while subsumption allows non-injective mappings where multiple source terms map to a single target. Complex mappings use OWL unionOf, intersectionOf, property restrictions, and SWRL rules."

  - name: "Coherence and Consistency Testing"
    chunk_ref: "04-PROV-O_to_BFO (Chunk 1:201-221)"
    quote: "An ontology alignment is coherent if and only if all formulae in the aligned ontologies are satisfiable... A consistent alignment is free of entailed or derivable contradictions."
    description: "The methodology requires testing alignments against model-theoretic semantics. Coherence ensures all classes can have instances (satisfiability). Consistency ensures no contradictions are derivable. These are verified using semantic reasoners like HermiT against canonical example instances (312 individuals tested in PROV-O case)."

  - name: "Conservativity Principle for Alignments"
    chunk_ref: "04-PROV-O_to_BFO (Chunk 1:241-267)"
    quote: "The conservativity principle states that an ontology alignment should not change the semantic relationships between terms within each ontology."
    description: "Based on the concept of conservative extensions in logic, this methodology criterion prevents alignments from introducing new subsumption or equivalence relationships within either source ontology. The approximate deductive difference between aligned and unaligned ontologies is computed to verify no hierarchy changes occur."

  - name: "Totality-Synonymy Spectrum"
    chunk_ref: "04-PROV-O_to_BFO (Chunk 1:273-347)"
    quote: "An alignment is a total alignment of an ontology O1 to ontology O2... if and only if for any term in O1, there exists some term in O2... Two ontologies are synonymous if and only if there exist two sets of translation definitions"
    description: "The methodology defines a spectrum of alignment completeness: total alignment (syntactic, every term mapped), interpretability (semantic, implications preserved), and synonymy (bidirectional full translation). Synonymy represents maximum interoperability where each ontology can be fully understood in terms of the other."

  - name: "SPARQL-Based Verification Queries"
    chunk_ref: "04-PROV-O_to_BFO (Chunk 1:398-421)"
    quote: "A SPARQL query was developed for automatically verifying the Totality of the combined alignments... The query finds any PROV-O class or object property term such that it... is not transitively related"
    description: "The methodology uses SPARQL queries in continuous development pipelines to automatically verify alignment completeness. Queries check for unmapped terms, transitively entailed mappings, and property chain axioms. This enables automated progress tracking during the mapping project."

  - name: "FAIR-Compliant Technical Artifacts"
    chunk_ref: "04-PROV-O_to_BFO (Chunk 1:489-508)"
    quote: "The technical artifacts produced by our work comply with FAIR (Findable, Accessible, Interoperable, Reusable) principles. All artifacts, data, and code are maintained in a public GitHub repository"
    description: "The methodology mandates FAIR compliance for all ontology mapping outputs. Artifacts are published in OWL 2 DL, serialized in RDF Turtle, maintained in versioned files separate from source ontologies, and archived on Zenodo with DOIs. This ensures reproducibility and reusability of mappings."

  - name: "Agentic Provenance Methodology"
    chunk_ref: "03-PROV-AGENT (Chunk 1:114-127)"
    quote: "Our contributions are threefold: (1) PROV-AGENT, a provenance model that extends the W3C PROV standard and incorporates concepts from the Model Context Protocol (MCP) to represent agent actions"
    description: "PROV-AGENT employs a methodology of extending existing standards (W3C PROV) with domain-specific concepts (MCP for AI agents). The approach integrates agent interactions as first-class provenance elements alongside traditional workflow tasks, enabling unified traceability graphs."

  - name: "Decorator-Based Provenance Capture"
    chunk_ref: "03-PROV-AGENT (Chunk 1:343-351)"
    quote: "In generic Python functions, applying the @flowcept_task decorator ensures that, upon execution, the function's inputs, outputs, and any generated telemetry or scheduling data are automatically captured."
    description: "The methodology uses Python decorators (@flowcept_agent_tool) for non-invasive instrumentation of MCP tools. This enables automatic provenance capture without modifying core logic. Each tool execution creates an AgentTool activity linked to the executing agent via PROV relationships."

  - name: "Cross-Facility Evaluation Methodology"
    chunk_ref: "03-PROV-AGENT (Chunk 1:36-37)"
    quote: "a cross-facility evaluation spanning edge, cloud, and HPC environments, demonstrating support for critical provenance queries and agent reliability analysis"
    description: "The PROV-AGENT methodology validates through real-world deployment across heterogeneous computing environments (edge devices, cloud services, HPC systems). Evaluation focuses on enabling specific provenance queries (lineage tracing, error propagation, decision influence) rather than just technical metrics."

  - name: "Philosophically-Grounded Ontology Development"
    chunk_ref: "05-DOLCE (Chunk 1:34-44)"
    quote: "In order to rely on well-established modeling principles and theoretical bases, it is a common practice for the categories and relations of foundational ontologies to be philosophically grounded."
    description: "DOLCE methodology grounds ontology categories in philosophical analysis, specifically adopting descriptive (rather than referentialist) metaphysics. Categories are influenced by natural language, human cognition, and social practices. This top-down approach from philosophy yields stable foundations used for 20+ years."

  - name: "OntoClean Methodology"
    chunk_ref: "05-DOLCE (Chunk 1:73-75)"
    quote: "The analysis underlying the formalization of DOLCE leverages the techniques of ontological engineering and the study of classes' meta-properties of the OntoClean methodology"
    description: "OntoClean is a methodology developed by Guarino and Welty for analyzing ontological categories through meta-properties like rigidity, identity, and unity. DOLCE uses OntoClean to distinguish essential from accidental properties, guiding decisions about entity classification and persistence conditions."

  - name: "Modal Logic Formalization"
    chunk_ref: "05-DOLCE (Chunk 1:227-234)"
    quote: "The formal theory of DOLCE is written in the first-order quantified modal logic QS5, including the Barcan and the converse Barcan formula... These assumptions entail a possibilistic view of the entities"
    description: "DOLCE employs first-order modal logic (QS5) for maximum expressivity over computational tractability. This allows representing possible entities, counterfactuals, and necessary relationships. The possibilistic domain includes all possible entities regardless of actual existence, enabling robust philosophical grounding."

  - name: "Artifact-Based vs Role-Based Modeling"
    chunk_ref: "05-DOLCE (Chunk 1:453-472)"
    quote: "DOLCE provides two ways to model this and similar examples. The first option, which we call artifact-based... The second option, called role-based, considers table and leg as roles of objects."
    description: "DOLCE methodology offers alternative modeling strategies based on whether properties are considered essential (artifact-based) or accidental (role-based). The choice depends on modeling purposes and persistence conditions. This flexibility allows DOLCE to accommodate different ontological commitments within a single framework."

  - name: "Constitution vs Composition Distinction"
    chunk_ref: "05-DOLCE (Chunk 1:486-497)"
    quote: "The constitution and composition relations in DOLCE capture distinct forms of dependence: the former is the dependence holding between entities with different essential properties (intercategorical)"
    description: "DOLCE methodology distinguishes constitution (cross-category dependence, e.g., statue and matter) from composition (same-category parthood, e.g., table and legs). Constitution relates entities with different persistence conditions while composition relates entities within the same category. This enables precise modeling of material change."

  - name: "BFO Realist Methodology"
    chunk_ref: "06-BFO_Function_Role (Chunk 1:47-52)"
    quote: "BFO is an upper-level ontology developed to support integration of data obtained through scientific research. It is deliberately designed to be very small, so that it may represent in a consistent fashion those upper-level categories"
    description: "BFO methodology prioritizes minimalism and domain-neutrality. The ontology is kept small (34 terms) to enable modularity, division of expertise, and consistent representation across domains. Domain-specific terms belong in narrower ontology modules, not the top-level. This supports integration across 100+ ontology development groups."

  - name: "Inherence-Based Dependence"
    chunk_ref: "06-BFO_Function_Role (Chunk 1:186-191)"
    quote: "Dependent continuants are related to their bearers by inherence. Inherence is defined as a one-sided, existential dependence relation."
    description: "BFO methodology uses inherence as the primitive relation for specific dependence. Qualities, functions, roles, and dispositions can only exist as properties of specific independent continuants. This constrains what kinds of entities can bear what kinds of dependent entities, providing clear ontological commitments."

  - name: "Realizable Entity Pattern"
    chunk_ref: "06-BFO_Function_Role (Chunk 1:241-243)"
    quote: "A realizable entity is defined as a specifically dependent continuant that has an independent continuant entity as its bearer, and whose instances can be realized (manifested, actualized, executed) in associated processes"
    description: "BFO methodology introduces realizable entities (roles, dispositions, functions, capabilities) as dependent continuants that can be manifested in processes. This captures the distinction between having a capacity and exercising it. Functions, for example, may exist without ever being realized."

  - name: "Internally vs Externally Grounded Distinction"
    chunk_ref: "06-BFO_Function_Role (Chunk 1:269-270, 333-334)"
    quote: "A role is a realizable entity which exists because the bearer is in some special physical, social, or institutional set of circumstances... A disposition is a realizable entity which is such that, if it ceases to exist, then its bearer is physically changed"
    description: "BFO methodology distinguishes roles (externally grounded, optional, context-dependent) from dispositions (internally grounded, reflecting physical makeup). This classification enables precise modeling of why an entity has certain capabilities - whether from its nature or its circumstances."

  - name: "Bicategorial Ontology Approach"
    chunk_ref: "07-Classifying_Processes (Chunk 1:296-342)"
    quote: "BFO... is founded on a bicategorial approach which seeks to combine elements of both the three-dimensionalist and four-dimensionalist perspectives. Thus it incorporates an ontology of continuants and an ontology of occurrents within a single framework"
    description: "BFO methodology reconciles 3D (continuant/thing) and 4D (occurrent/process) ontological perspectives rather than choosing one. This adapted from Zemach's 'Four Ontologies' enables natural treatment of both objects (things with spatial parts) and processes (entities with temporal parts) without forced reduction."

  - name: "Applied Ontology for Scientific Data"
    chunk_ref: "07-Classifying_Processes (Chunk 1:66-77)"
    quote: "ontologies that are being developed on the basis of the assumption that, to create an ontology that brings benefits to scientists working with data in a given domain, the ontology should employ classifications that are based on the established scientific understanding"
    description: "Applied ontology methodology grounds classifications in established scientific understanding rather than purely philosophical analysis. Ontologies extend the advantages of standardized units (like SI) to theoretical terminology. Terms represent types/universals that are instantiated in experiments."

  - name: "Process Classification via Instantiation"
    chunk_ref: "07-Classifying_Processes (Chunk 1:770-801)"
    quote: "an assertion to the effect that motion p has speed v... should be interpreted as being of the form: motion p instance_of universal motion with speed v"
    description: "BFO methodology treats process measurements as instantiation relationships rather than quality attribution. Since processes cannot change (they ARE changes), speed/duration/rate are captured as determinate universals that processes instantiate. This avoids the need for qualities of occurrents while enabling rich process classification."

  - name: "Hypothesis Mining Methodology"
    chunk_ref: "02-Knowledge_Graphs (Chunk 15:537-548)"
    quote: "The task of hypothesis induction assumes a particular graph entailment relation... Given background knowledge in the form of a knowledge graph, a set of positive edges, the task is to find a set of hypotheses"
    description: "Knowledge graph methodology includes hypothesis induction and mining approaches. Given background knowledge, positive examples, and negative examples, the task is to find hypotheses (rules/axioms) that entail positive edges without entailing negative edges. Scoring uses support and confidence measures under various completeness assumptions (CWA, OWA, PCA)."
