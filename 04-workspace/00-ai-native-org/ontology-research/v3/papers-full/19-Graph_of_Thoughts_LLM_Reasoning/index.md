---
paper_id: "19-Graph_of_Thoughts_LLM_Reasoning"
title: "Graph of Thoughts: Solving Elaborate Problems with Large Language Models"
authors:
  - "Maciej Besta"
  - "Nils Blach"
  - "Ales Kubicek"
  - "Robert Gerstenberger"
  - "Michal Podstawski"
  - "Lukas Gianinazzi"
  - "Joanna Gajda"
  - "Tomasz Lehmann"
  - "Hubert Niewiadomski"
  - "Piotr Nyczyk"
  - "Torsten Hoefler"
year: 2023
chunks_expected: 7
chunks_read: 7
analysis_complete: true
schema_version: "2.3"

chunk_index:
  1:
    token_count: 8212
    fields_found:
      entity_types: true
      entity_definitions: true
      entity_relationships: true
      entity_count: false
      abstraction_level: partial
      framework_comparison: true
      methodology: partial
      ai_integration: true
      agent_modeling: partial
      agentic_workflows: true
      generative_ai_patterns: true
      agent_ontology_integration: partial
      empirical_evidence: partial
      limitations: partial
      tools_standards: partial
  2:
    token_count: 7085
    fields_found:
      entity_types: false
      entity_definitions: false
      entity_relationships: false
      entity_count: false
      abstraction_level: false
      framework_comparison: true
      methodology: false
      ai_integration: true
      agent_modeling: false
      agentic_workflows: true
      generative_ai_patterns: true
      agent_ontology_integration: false
      empirical_evidence: true
      limitations: partial
      tools_standards: false
  3:
    token_count: 5314
    fields_found:
      entity_types: false
      entity_definitions: false
      entity_relationships: false
      entity_count: false
      abstraction_level: false
      framework_comparison: true
      methodology: false
      ai_integration: partial
      agent_modeling: false
      agentic_workflows: partial
      generative_ai_patterns: true
      agent_ontology_integration: false
      empirical_evidence: true
      limitations: false
      tools_standards: false
  4:
    token_count: 6950
    fields_found:
      entity_types: false
      entity_definitions: false
      entity_relationships: false
      entity_count: false
      abstraction_level: false
      framework_comparison: false
      methodology: false
      ai_integration: true
      agent_modeling: false
      agentic_workflows: true
      generative_ai_patterns: true
      agent_ontology_integration: false
      empirical_evidence: true
      limitations: false
      tools_standards: false
  5:
    token_count: 7808
    fields_found:
      entity_types: false
      entity_definitions: false
      entity_relationships: false
      entity_count: false
      abstraction_level: false
      framework_comparison: false
      methodology: false
      ai_integration: true
      agent_modeling: false
      agentic_workflows: true
      generative_ai_patterns: true
      agent_ontology_integration: false
      empirical_evidence: true
      limitations: false
      tools_standards: false
  6:
    token_count: 4789
    fields_found:
      entity_types: false
      entity_definitions: false
      entity_relationships: false
      entity_count: false
      abstraction_level: false
      framework_comparison: false
      methodology: false
      ai_integration: true
      agent_modeling: false
      agentic_workflows: true
      generative_ai_patterns: true
      agent_ontology_integration: false
      empirical_evidence: true
      limitations: false
      tools_standards: false
  7:
    token_count: 5784
    fields_found:
      entity_types: false
      entity_definitions: false
      entity_relationships: false
      entity_count: false
      abstraction_level: false
      framework_comparison: false
      methodology: false
      ai_integration: true
      agent_modeling: false
      agentic_workflows: true
      generative_ai_patterns: true
      agent_ontology_integration: false
      empirical_evidence: true
      limitations: false
      tools_standards: true

entity_types:
  - item: "Thought"
    chunk: 1
    lines: "17-21"
    quote: "The key idea and primary advantage of GoT is the ability to model the information generated by an LLM as an arbitrary graph, where units of information ('LLM thoughts') are vertices, and edges correspond to dependencies between these vertices."
  - item: "Vertex"
    chunk: 1
    lines: "244-252"
    quote: "We model the reasoning process as a directed graph G = (V, E); V is a set of vertices... A vertex contains a solution to a problem at hand (be it an initial, intermediate, or a final one)."
  - item: "Edge"
    chunk: 1
    lines: "249-252"
    quote: "A directed edge (t1, t2) indicates that thought t2 has been constructed using t1 as 'direct input', i.e., by explicitly instructing the LLM to use t1 for generating t2."
  - item: "Graph of Operations (GoO)"
    chunk: 1
    lines: "390-395"
    quote: "GoO is a static structure that specifies the graph decomposition of a given task, i.e., it prescribes transformations to be applied to LLM thoughts, together with their order & dependencies."
  - item: "Graph Reasoning State (GRS)"
    chunk: 1
    lines: "394-396"
    quote: "GRS is a dynamic structure that maintains the state of the ongoing LLM reasoning process (the history of its thoughts and their states)."
  - item: "Prompter"
    chunk: 1
    lines: "398-402"
    quote: "The Prompter prepares the prompts to be sent to the LLM. This module is responsible for the specifics of encoding the graph structure within the prompt."
  - item: "Parser"
    chunk: 1
    lines: "405-411"
    quote: "The Parser extracts information from LLM thoughts. For each such thought, the Parser constructs the thought state, which contains this extracted information."
  - item: "Controller"
    chunk: 1
    lines: "422-429"
    quote: "The Controller implements a specific strategy for selecting thoughts from its GRS structure. It also selects what transformations should be applied to which thoughts."
  - item: "Transformation"
    chunk: 1
    lines: "321-341"
    quote: "Each such transformation can be modeled as T(G, p_theta) where G = (V, E) is the graph reflecting the current state of the reasoning, and p_theta is the used LLM. T modifies G usually by adding new vertices and their incoming edges."
  - item: "Operation"
    chunk: 1
    lines: "434-443"
    quote: "Each operation object knows its predecessor and successor operations. Then, during the execution, an instance of the GRS maintains the continually updated information about the LLM reasoning process."

entity_definitions:
  Thought: "A unit of information generated by an LLM, modeled as a vertex in a graph. Can be a paragraph (in writing tasks), a sequence of numbers (in sorting), a block of code, or other problem-specific representations. Not prescribed to a single form - use-case specific. (Chunk 1:158-161)"
  Graph_of_Thoughts: "A framework modeled as a tuple (G, T, E, R), where G is the LLM reasoning process (thoughts with relationships), T are potential thought transformations, E is an evaluator function for scoring thoughts, and R is a ranking function for selecting relevant thoughts. (Chunk 1:234-238)"
  Aggregation_Transformation: "A graph-enabled transformation that combines arbitrary thoughts into new ones, reinforcing advantages while eliminating disadvantages. Enables aggregating reasoning paths by adding outgoing edges from multiple chain endpoints into a single combining thought. (Chunk 1:344-352)"
  Refining_Transformation: "A thought transformation that modifies the content of a current thought through a loop in the graph (self-edge), indicating an iterated thought with the same connections as the original. (Chunk 1:355-358)"
  Generation_Transformation: "A transformation that generates one or more new thoughts based on an existing single thought, embracing analogous reasoning steps from earlier schemes like ToT or CoT-SC. (Chunk 1:360-364)"
  Volume_of_Thought: "A metric for evaluating prompting strategies defined as the number of LLM thoughts from which one can reach a given thought v using directed edges. Represents all thoughts that had the potential to contribute to v. (Chunk 1:129-134)"
  Scorer: "Module that verifies whether a given LLM thought satisfies potential correctness conditions, and then assigns it a score. May consult the LLM or use local scoring functions. (Chunk 1:413-419)"

entity_relationships:
  - relationship: "Vertex contains-a Solution"
    chunk: 1
    lines: "246-248"
    quote: "A vertex contains a solution to a problem at hand (be it an initial, intermediate, or a final one)."
  - relationship: "Edge represents Dependency-between Thoughts"
    chunk: 1
    lines: "249-252"
    quote: "A directed edge (t1, t2) indicates that thought t2 has been constructed using t1 as 'direct input'."
  - relationship: "Controller uses GRS"
    chunk: 1
    lines: "422-425"
    quote: "The Controller implements a specific strategy for selecting thoughts from its GRS structure."
  - relationship: "Controller uses GoO"
    chunk: 1
    lines: "427-429"
    quote: "In our current design, this is dictated by the execution plan specified in the GoO."
  - relationship: "GoT generalizes CoT and ToT"
    chunk: 1
    lines: "79-82"
    quote: "Overall, the graph abstraction harnessed by GoT seamlessly generalizes CoT and ToT to more complex thought patterns, without resorting to any model updates."
  - relationship: "Prompter prepares-for LLM"
    chunk: 1
    lines: "398-402"
    quote: "The Prompter prepares the prompts to be sent to the LLM."
  - relationship: "Parser extracts-from LLM_thoughts"
    chunk: 1
    lines: "405-408"
    quote: "The Parser extracts information from LLM thoughts. For each such thought, the Parser constructs the thought state."
  - relationship: "Transformation modifies Graph"
    chunk: 1
    lines: "322-324"
    quote: "T modifies G usually by adding new vertices and their incoming edges."

abstraction_level: "Application-level framework for LLM prompting. GoT operates at the prompt engineering level, providing a meta-framework for structuring LLM reasoning without model updates. It abstracts over lower-level prompting paradigms (CoT, ToT) and provides a unified graph-based model. (Chunk 1:17-28)"

framework_comparison:
  - comparison: "GoT vs Chain-of-Thought (CoT)"
    chunk: 1
    lines: "47-49, 104-105"
    quote: "CoT was shown to significantly improve the capability of LLMs to solve problems... GoT outperforms other schemes, for example improving upon CoT by approximately 70% in terms of the quality of sorting."
  - comparison: "GoT vs Tree of Thoughts (ToT)"
    chunk: 1
    lines: "58-61, 104-105"
    quote: "Unfortunately, the ToT approaches still fundamentally limit the reasoning abilities within a prompt by imposing the rigid tree structure on the thought process... GoT improves upon ToT by approximately 62% in terms of the quality of sorting, while simultaneously reducing costs by >31%."
  - comparison: "GoT vs CoT-SC (Self-Consistency)"
    chunk: 1
    lines: "55-57, 176-183"
    quote: "Self-Consistency with CoT (CoT-SC), is a scheme where multiple CoTs are generated, and then the best one is selected... This approach enhances CoT because it offers an opportunity to explore different reasoning paths. However, it does not offer 'local exploration' within a path, such as backtracking."
  - comparison: "Latency-Volume Tradeoff"
    chunk: 1
    lines: "756-767"
    quote: "GoT is the only scheme to come with both a low latency of log_k N and a high volume N. This is enabled by the fact that GoT harnesses aggregations of thoughts."

ai_integration:
  - pattern: "Graph-based LLM reasoning"
    chunk: 1
    lines: "17-28"
    quote: "The key idea and primary advantage of GoT is the ability to model the information generated by an LLM as an arbitrary graph, where units of information ('LLM thoughts') are vertices, and edges correspond to dependencies between these vertices. This approach enables combining arbitrary LLM thoughts into synergistic outcomes."
  - pattern: "Modular architecture for prompting"
    chunk: 1
    lines: "383-396"
    quote: "The GoT architecture consists of a set of interacting modules: the Prompter (prepares the messages for the LLM), the Parser (extracts information from LLM thoughts), the Scoring module (verifies and scores the LLM thoughts), and the Controller (coordinates the entire reasoning process)."
  - pattern: "LLM-based scoring and self-evaluation"
    chunk: 1
    lines: "413-419"
    quote: "Here, we verify whether a given LLM thought satisfies potential correctness conditions, and then we assign it a score. Depending on how the score is derived, the module may consult the LLM."
  - pattern: "Few-shot prompting with structured examples"
    chunk: 3
    lines: "562-580"
    quote: "First, we present the prompt stubs (Table 3), serving as templates to dynamically generate appropriate prompts at runtime. For clarity, we display their corresponding few-shot examples separately in Table 4."
  - pattern: "Task decomposition for subtask solving"
    chunk: 2
    lines: "127-142"
    quote: "The overall goal when conducting graph decomposition is to break down a task to the point, where the LLM can solve it correctly for the majority of time using a single prompt."

agent_modeling:
  - pattern: "Controller as orchestrator"
    chunk: 1
    lines: "422-429"
    quote: "The Controller implements a specific strategy for selecting thoughts from its GRS structure. It also selects what transformations should be applied to which thoughts, and then passes this information to the Prompter. It also decides whether the whole process should be finalized."
  - pattern: "Modular agent components"
    chunk: 1
    lines: "383-389"
    quote: "These modules are the Prompter (prepares the messages for the LLM), the Parser (extracts information from LLM thoughts), the Scoring module (verifies and scores the LLM thoughts), and the Controller (coordinates the entire reasoning process)."
  - pattern: "State management via GRS"
    chunk: 1
    lines: "439-443"
    quote: "During the execution, an instance of the GRS maintains the continually updated information about the LLM reasoning process. This includes which operation has been executed so far, the states of all the generated LLM thoughts, their validity and scores."

agentic_workflows:
  - workflow: "Graph-based task decomposition"
    chunk: 1
    lines: "97-103"
    quote: "GoT is particularly well-suited for tasks that can be naturally decomposed into smaller subtasks that are solved individually and then merged for a final solution."
  - workflow: "Merge-based sorting workflow"
    chunk: 1
    lines: "477-479"
    quote: "In GoT, we employ merge-based sorting: First, one decomposes the input sequence of numbers into subarrays. Then, one sorts these subarrays individually, and then respectively merges them into a final solution."
  - workflow: "Graph of Operations (GoO) execution"
    chunk: 3
    lines: "804-816"
    quote: "1. Split the input list into two sub-lists of equal size (split_prompt) 2. For each sub-list: Sort the sub-list (sort prompt) five times; score each sort attempt; keep the best 3. Merge the sorted sub-lists into one fully sorted list (merge_prompt) 10 times; score each merge attempt; keep the best"
  - workflow: "Generate-Score-Keep pattern"
    chunk: 7
    lines: "640-650"
    quote: "Generate(k=1) # Split second set into two halves... Generate(k=5) # Determine intersected subset... Score(k=1) # Score locally... KeepBestN(1) # Keep the best..."

generative_ai_patterns:
  - pattern: "Thought aggregation for synergistic outcomes"
    chunk: 1
    lines: "20-22"
    quote: "This approach enables combining arbitrary LLM thoughts into synergistic outcomes, distilling the essence of whole networks of thoughts, or enhancing thoughts using feedback loops."
  - pattern: "Iterative refinement through loops"
    chunk: 1
    lines: "355-358"
    quote: "Another thought transformation is the refining of a current thought v by modifying its content: V+ = {} and E+ = {(v, v)}. This loop in the graph indicates an iterated thought with the same connections as the original thought."
  - pattern: "Multi-generation with best selection"
    chunk: 1
    lines: "360-364"
    quote: "Finally, one can generate one or more new thoughts based on an existing single thought v. This class embraces analogous reasoning steps from earlier schemes, such as ToT or CoT-SC."
  - pattern: "Structured prompt templates"
    chunk: 3
    lines: "586-630"
    quote: "sort_prompt: <Instruction> Sort the following list of numbers in ascending order. Output only the sorted list of numbers, no additional text. </Instruction>"
  - pattern: "Volume metric for reasoning depth"
    chunk: 1
    lines: "729-754"
    quote: "We define volume - for a given thought t - as the number of preceding LLM thoughts that could have impacted t... GoT is the only scheme to come with both a low latency of log_k N and a high volume N."

agent_ontology_integration:
  - mechanism: "Graph structure as reasoning scaffold"
    chunk: 1
    lines: "244-260"
    quote: "We model the reasoning process as a directed graph G = (V, E); V is a set of vertices and E is a set of edges."
  - mechanism: "Typed vertices (heterogeneous graph)"
    chunk: 1
    lines: "253-259"
    quote: "In certain use cases, graph nodes belong to different classes... GoT embraces a heterogeneous graph G = (V, E, c) to model the LLM reasoning, where c maps vertices V into their respective classes C."
  - mechanism: "Operation-based planning via GoO"
    chunk: 1
    lines: "434-436"
    quote: "The user constructs a GoO instance, which prescribes the execution plan of thought operations."

entity_count: 10

methodology: "Hybrid (top-down and bottom-up). Top-down theoretical framework design based on graph abstraction from human reasoning and algorithmic execution patterns. Bottom-up empirical validation on concrete tasks. Uses 100 input samples for each task and comparison baseline, temperature 1.0, 4k context size. Experiments conducted primarily with GPT-3.5. (Chunk 1:778-792)"

empirical_evidence:
  - evidence: "Sorting performance improvement"
    chunk: 1
    lines: "104-105"
    quote: "GoT outperforms other schemes, for example improving upon CoT and ToT by, respectively, approximately 70% and approximately 62%, in terms of the quality of sorting, while simultaneously reducing costs by >31% over ToT."
  - evidence: "Performance scaling with problem complexity"
    chunk: 2
    lines: "109-122"
    quote: "Most importantly, the advantages of GoT in the quality increase for all the baselines with the growing size of the problem P. For example, in sorting, while for P = 32 GoT only negligibly improves upon ToT2, its median error count becomes lower by approximately 61% for P = 64 and approximately 69% for P = 128."
  - evidence: "Set intersection results"
    chunk: 4
    lines: "447-484"
    quote: "Step 2a - 5 Responses: 1. [11, 14, 46, 14, 19] (1 Error - Duplicated 14) 2. Output: [11, 14, 46, 19] (Fully Correct)"
  - evidence: "Document merging scores"
    chunk: 6
    lines: "232-253"
    quote: "Response (2/5): Score: 6.87... Response (3/5): Score: 6.60"
  - evidence: "100 samples per task evaluation"
    chunk: 1
    lines: "779-781"
    quote: "We use 100 input samples for each task and comparison baseline. We set the temperature to 1.0 and use a 4k context size unless stated otherwise."

limitations:
  - limitation: "Manual design of Graph of Operations required"
    chunk: 1
    lines: "83-85"
    quote: "Putting GoT to practice requires solving several design challenges. For example, what is the best graph structure for different tasks? How to best aggregate thoughts to maximize accuracy and minimize cost?"
  - limitation: "Budget restrictions limited LLM experimentation"
    chunk: 2
    lines: "74-76"
    quote: "Due to budget restrictions, we focus on GPT-3.5. We also experimented with Llama-2, but it was usually worse than GPT-3.5 and also much slower to run, making it infeasible to obtain enough samples."
  - limitation: "Higher costs than IO/CoT"
    chunk: 2
    lines: "103-108"
    quote: "The higher costs of GoT and ToT are driven by k new thoughts built for each Generate operation; these multiple thoughts are one of the reasons for GoT's superiority in quality."
  - limitation: "Not a fine-tuning approach"
    chunk: 1
    lines: "136-141"
    quote: "We do not include a recent scheme called Graph-of-Thought [79] because it is not a prompting scheme. While its name suggests close connections to ToT and CoT, as a fine-tuning scheme, it resorts to model updates, and is thus outside the focus of this work."

tools_standards:
  - tool: "Python implementation"
    chunk: 1
    lines: "31"
    quote: "Website & code: https://github.com/spcl/graph-of-thoughts"
  - tool: "GPT-3.5 / GPT-4 / Llama-2"
    chunk: 1
    lines: "95-96"
    quote: "This enables rapid prototyping of novel prompting ideas using GoT, while experimenting with different models such as GPT-3.5, GPT-4, or Llama-2."
  - tool: "GoT configuration DSL"
    chunk: 7
    lines: "636-651"
    quote: "Generate(k=1) # Split second set into two halves of 16 elements... foreach subset: Generate(k=5) # Determine intersected subset... Score(k=1) # Score locally... KeepBestN(1) # Keep the best..."
  - tool: "Directed Acyclic Graphs (DAGs)"
    chunk: 1
    lines: "69-70"
    quote: "Executing algorithms also expose networked patterns, often represented by Directed Acyclic Graphs."
---

# Graph of Thoughts: Solving Elaborate Problems with Large Language Models

## Summary

This paper introduces Graph of Thoughts (GoT), a framework that advances LLM prompting capabilities beyond Chain-of-Thought (CoT) and Tree of Thoughts (ToT) by modeling LLM reasoning as an arbitrary directed graph. The key innovation is enabling thought aggregation, where multiple reasoning paths can be combined into synergistic outcomes, and feedback loops for iterative refinement.

## Key Contributions

1. **Graph-based reasoning model**: LLM thoughts as vertices, dependencies as edges
2. **Novel thought transformations**: Aggregation, refinement, and generation operations
3. **Modular architecture**: Prompter, Parser, Scoring module, and Controller components
4. **Graph of Operations (GoO)**: Static execution plans for task decomposition
5. **Volume of thought metric**: New evaluation measure for prompting strategies

## Relevance to Ontologies Research

### Agent-Activity-Entity Pattern
The GoT framework implicitly models an agent-activity-entity pattern:
- **Agent**: The Controller orchestrates reasoning
- **Activity**: Thought transformations (aggregation, generation, refinement)
- **Entity**: Thoughts (vertices) containing solutions or intermediate states

### Generative AI Patterns
The paper provides concrete patterns for LLM-based reasoning:
- Structured graph decomposition of complex tasks
- Multi-generation with scoring and selection
- Iterative refinement through graph loops
- Aggregation of parallel reasoning paths

### Workflow Orchestration
GoT demonstrates agentic workflow patterns:
- Task decomposition into parallelizable subtasks
- Merge-based aggregation of partial solutions
- Scoring and ranking for quality control
- Graph of Operations as declarative workflow specification

## Empirical Results

- **Sorting**: 62% quality improvement over ToT, 31% cost reduction
- **Set intersection**: Comparable quality with reduced complexity
- **Keyword counting**: Effective task decomposition
- **Document merging**: Successful aggregation of multiple documents

## Limitations for Ontology Integration

- No formal ontological grounding of entity types
- Graph structure is application-specific, not derived from foundational ontology
- Requires manual design of Graph of Operations for each task
- No explicit connection to UFO, PROV-O, or other foundational ontologies

## Citation

```bibtex
@article{besta2023graph,
  title={Graph of Thoughts: Solving Elaborate Problems with Large Language Models},
  author={Besta, Maciej and Blach, Nils and Kubicek, Ales and Gerstenberger, Robert and Podstawski, Micha{\l} and Gianinazzi, Lukas and Gajda, Joanna and Lehmann, Tomasz and Niewiadomski, Hubert and Nyczyk, Piotr and Hoefler, Torsten},
  journal={arXiv preprint arXiv:2308.09687},
  year={2023}
}
```

## Resources

- **Code Repository**: https://github.com/spcl/graph-of-thoughts
