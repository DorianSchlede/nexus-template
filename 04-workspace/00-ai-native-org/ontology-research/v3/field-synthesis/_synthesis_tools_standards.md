# Tools Standards

**Source**: Project 16 Ontologies Research v3

**Type**: Synthesis Analysis (UDWO-Primed)

**Field**: tools_standards

**Aggregated**: 2026-01-01T16:22:31.926583

**Batches Merged**: 11

---

## Table of Contents

- [Patterns](#patterns)

## Patterns

**Total Patterns**: 480

### 1. OntoUML Modeling Language

OntoUML is a conceptual modeling language designed based on UFO foundations. It reflects the ontological micro-theories comprising UFO and provides a graphical notation for representing domain models grounded in foundational ontology distinctions.

**Sources**:

- **01-UFO_Unified_Foundational_Ontology (Chunk 1:209-211)**
  > the use of UFO in the design of an ontology-driven conceptual modeling language, which later came to be known as OntoUML

---

### 2. OntoUML Model Repository

A publicly accessible repository containing OntoUML models developed by multiple research groups across various domains, serving as empirical validation and reference implementations.

**Sources**:

- **01-UFO_Unified_Foundational_Ontology (Chunk 1:248)**
  > Several dozens of these models are available at http://purl.org/krdb-core/model-repository/

---

### 3. UFO TPTP Formalization

UFO has been formally specified in TPTP (Thousands of Problems for Theorem Provers) syntax, enabling automated verification using theorem provers. The complete formalization is available at https://purl.org/ufo-formalization.

**Sources**:

- **01-UFO_Unified_Foundational_Ontology (Chunk 1:441-445)**
  > this section is based on a first-order logic formalization of UFO specified in the TPTP syntax (Sutcliffe, 2017), and submitted to multiple automated provers for consistency and satisfability checks

---

### 4. First-Order Modal Logic QS5 Formalization

UFO is formalized using first-order modal logic QS5 with Barcan formula, enabling representation of necessity, possibility, and cross-world identity for ontological analysis.

**Sources**:

- **01-UFO_Unified_Foundational_Ontology (Chunk 1:431-435)**
  > For our purposes, the first-order modal logic QS5 plus the Barcan formula and its converse suffices

---

### 5. BPMN Reengineering

UFO has been used to analyze and reengineer BPMN (Business Process Model and Notation), providing ontological foundations for business process modeling language semantics.

**Sources**:

- **01-UFO_Unified_Foundational_Ontology (Chunk 1:203-204)**
  > It has been employed as a basis for analyzing, reengineering, and integrating many modeling languages and standards in different domains (e.g., UML, BPMN, ArchiMate)

---

### 6. UML Ontological Analysis

UFO provides ontological foundations for UML (Unified Modeling Language) analysis and reengineering, addressing conceptual modeling constructs like entity types and relationship types.

**Sources**:

- **01-UFO_Unified_Foundational_Ontology (Chunk 1:203-204)**
  > It has been employed as a basis for analyzing, reengineering, and integrating many modeling languages and standards in different domains (e.g., UML, BPMN, ArchiMate)

---

### 7. ArchiMate Integration

UFO has been applied to ArchiMate enterprise architecture modeling language to provide ontological grounding for enterprise architecture concepts.

**Sources**:

- **01-UFO_Unified_Foundational_Ontology (Chunk 1:203-204)**
  > It has been employed as a basis for analyzing, reengineering, and integrating many modeling languages and standards in different domains (e.g., UML, BPMN, ArchiMate)

---

### 8. OWL Ontological Evaluation

Merged from 2 sources. UFO provides ontological foundations for UML (Unified Modeling Language), clarifying the formal semantics of UML constructs through ontological analysis.

**Sources**:

- **01-UFO_Unified_Foundational_Ontology (Chunk 1:154-155)**
  > including NIAM (Weber and Zhang, 1991), ER (Wand et al., 1999), UML (Evermann and Wand, 2001), and OWL (Bera and Wand, 2004)

- **01-UFO_Unified_Foundational_Ontology (Chunk 3:687)**
  > UML (Costal et al., 2011; Guizzardi, 2005)

---

### 9. NIAM Ontological Evaluation

NIAM (Natural-language Information Analysis Method) was evaluated using ontological foundations, demonstrating the application of foundational ontology to conceptual modeling languages.

**Sources**:

- **01-UFO_Unified_Foundational_Ontology (Chunk 1:154)**
  > including NIAM (Weber and Zhang, 1991), ER (Wand et al., 1999)

---

### 10. ER Model Ontological Evaluation

The Entity-Relationship model was evaluated using Bunge-Wand-Weber ontological foundations, informing UFO's development of theories for entity types and relationship types.

**Sources**:

- **01-UFO_Unified_Foundational_Ontology (Chunk 1:154)**
  > including NIAM (Weber and Zhang, 1991), ER (Wand et al., 1999)

---

### 11. OntoClean Methodology Integration

OntoClean methodology provides meta-property analysis (rigidity, identity, unity, dependence) that influenced UFO's theory of universals and type hierarchies.

**Sources**:

- **01-UFO_Unified_Foundational_Ontology (Chunk 1:182)**
  > we needed something in the spirit of the ontology of universals underlying the OntoClean methodology (Guarino and Welty, 2009)

---

### 12. DOLCE Foundational Ontology

Merged from 2 sources. DOLCE (Descriptive Ontology for Linguistic and Cognitive Engineering) was one of the foundational ontologies unified to create UFO. DOLCE is based on the Aristotelian Square but designed as an ontology of particulars.

**Sources**:

- **01-UFO_Unified_Foundational_Ontology (Chunk 1:173-175)**
  > the Descriptive Ontology for Linguistic and Cognitive Engineering (DOLCE; Masolo et al., 2003). In this setting, their first attempt was to unify DOLCE and GFO

- **23-UFO_Story_Ontological_Foundations (Chunk 1:139-140)**
  > developing the Descriptive Ontology for Linguistic and Cognitive Engineering (DOLCE)

---

### 13. GFO/GOL Foundational Ontology

GFO (General Formal Ontology) and GOL (General Ontology Language) were reference theories used in early UFO development, providing philosophical grounding and the Aristotelian Square structure.

**Sources**:

- **01-UFO_Unified_Foundational_Ontology (Chunk 1:171-172)**
  > Guizzardi and Wagner attempted to employ the General Formal Ontology (GFO) and the General Ontology Language (GOL) being developed in Leipzig

---

### 14. OntoUML Stereotypes Notation

OntoUML uses UML-style stereotypes to denote ontological categories from UFO, such as <<kind>>, <<quantity>>, <<role>>, <<relator>>, enabling visual representation of ontological distinctions in conceptual models.

**Sources**:

- **01-UFO_Unified_Foundational_Ontology (Chunk 2:562-563)**
  > In OntoUML, the stereotypes <<kind>> and <<quantity>> stand for ObjectKind and QuantityKind, respectively

---

### 15. OntoUML Automatic Model Generation

OntoUML has supporting tools that can automatically generate instances (visual models) that satisfy the logical constraints implied by an OntoUML conceptual model, enabling validation and simulation.

**Sources**:

- **01-UFO_Unified_Foundational_Ontology (Chunk 2:701-705)**
  > From an OntoUML model and using its supporting tools, we can automatically generate models that satisfy the logical theory corresponding to the OntoUML model

---

### 16. OntoUML Anti-Pattern Detection Tool

OntoUML includes tooling support for automatic detection of modeling anti-patterns and rectification suggestions, referenced as (Guerson et al., 2015), improving model quality through automated analysis.

**Sources**:

- **01-UFO_Unified_Foundational_Ontology (Chunk 2:865-866)**
  > This constraint is automatically detected and included in the specification by the anti-pattern detection and rectification support of the OntoUML tool

---

### 17. UFO-S Service Pattern

UFO-S provides reusable ontology patterns for service modeling, including Service Offering and Service Agreement patterns that can be instantiated in domain-specific models like course enrollments.

**Sources**:

- **01-UFO_Unified_Foundational_Ontology (Chunk 2:820-821)**
  > A Course Offering in this model is a particular instantiation of the UFO-S Service Offering pattern (Falbo et al., 2016)

---

### 18. Quality Structure Datatypes

OntoUML provides mechanisms to represent quality structures (value spaces) as datatypes, with enumerations as a special case for discrete value spaces. More sophisticated representations are described in Albuquerque and Guizzardi (2013).

**Sources**:

- **01-UFO_Unified_Foundational_Ontology (Chunk 2:906-908)**
  > In OntoUML, the simplest way to represent quality structures is as datatypes (Guizzardi, 2005). Classes stereotyped as <<enumeration>> are particular types of datatypes

---

### 19. GoT DSL Configuration Syntax

Graph of Thoughts uses a domain-specific configuration language with primitives like Generate(k), Score(k), KeepBestN(n), Aggregate(n), and GroundTruth() to define reasoning workflows. This DSL enables declarative specification of LLM reasoning operations.

**Sources**:

- **19-Graph_of_Thoughts_LLM_Reasoning (Chunk 7:636-651)**
  > Generate(k=1) # Split second set into two halves of 16 elements... Score(k=1) # Score locally the intersected subsets... KeepBestN(1) # Keep the best intersected subset

---

### 20. GoT Operations Primitive Set

The GoT framework defines four core operations: Generate (create candidate solutions), Score (evaluate quality), KeepBestN (selection), and Aggregate (merge results). These form a complete toolkit for structured LLM reasoning.

**Sources**:

- **19-Graph_of_Thoughts_LLM_Reasoning (Chunk 7:640-650)**
  > Generate(k=5) # Determine intersected subset... Score(k=1) # Score locally... KeepBestN(1) # Keep the best... Aggregate(10) # Merge both intersected subsets

---

### 21. GoT Foreach Parallel Iteration

GoT supports foreach iteration constructs for parallel processing of subproblems, enabling divide-and-conquer strategies in LLM reasoning workflows.

**Sources**:

- **19-Graph_of_Thoughts_LLM_Reasoning (Chunk 7:742-746)**
  > Generate(k=1) # Split list into two halves of 16 elements... foreach list part: Generate(k=5) # Sort list part

---

### 22. GoT Iterative Refinement Pattern

GoT implements iterative refinement through repeated Generate-Score-KeepBest cycles, allowing progressive improvement of solutions before final ground truth validation.

**Sources**:

- **19-Graph_of_Thoughts_LLM_Reasoning (Chunk 7:750-753)**
  > Generate(k=10) # Try to improve solution... Score(k=1) # Score locally the sorted result lists... KeepBestN(1) # Keep the best result... GroundTruth()

---

### 23. GitHub Repository for Agentic RAG

The Agentic RAG survey provides an open-source repository with implementation resources and frameworks for building agentic retrieval-augmented generation systems.

**Sources**:

- **20-Agentic_RAG_Survey (Chunk 1:66-67)**
  > The GitHub link for this survey is available at: https://github.com/asinghcsu/AgenticRAG-Survey

---

### 24. Dense Passage Retrieval (DPR)

Dense Passage Retrieval is a key technical standard for advanced RAG systems, enabling semantic understanding and neural ranking for improved retrieval precision.

**Sources**:

- **20-Agentic_RAG_Survey (Chunk 1:232-233)**
  > Advanced RAG systems build upon the limitations of Naive RAG by incorporating semantic understanding... These systems leverage dense retrieval models, such as Dense Passage Retrieval (DPR)

---

### 25. TF-IDF and BM25 Retrieval

TF-IDF and BM25 are foundational sparse retrieval standards used in Naive RAG implementations for keyword-based document retrieval.

**Sources**:

- **20-Agentic_RAG_Survey (Chunk 1:200-201)**
  > These systems rely on simple keyword-based retrieval techniques, such as TF-IDF and BM25, to fetch documents from static datasets

---

### 26. Hybrid Retrieval Strategies

Modular RAG implements hybrid retrieval combining sparse (BM25) and dense (DPR) methods as a technical standard for optimizing retrieval across diverse query types.

**Sources**:

- **20-Agentic_RAG_Survey (Chunk 1:274-276)**
  > Hybrid Retrieval Strategies: Combining sparse retrieval methods (e.g., a sparse encoder-BM25) with dense retrieval techniques (e.g., DPR - Dense Passage Retrieval) to maximize accuracy

---

### 27. External API and Tool Integration

Modular RAG architectures standardize API integration patterns for external tools, enabling real-time data access and domain-specific computation capabilities.

**Sources**:

- **20-Agentic_RAG_Survey (Chunk 1:279-280)**
  > Tool Integration: Incorporating external APIs, databases, or computational tools to handle specialized tasks, such as real-time data analysis or domain-specific computations

---

### 28. Vector Database Search

Vector databases are a core technical infrastructure for RAG systems, enabling dense vector search and semantic retrieval across knowledge bases.

**Sources**:

- **20-Agentic_RAG_Survey (Chunk 1:161-162)**
  > Retrieveal: Responsible for querying external data sources such as knowledge bases, APIs, or vector databases. Advanced retrievers leverage dense vector search

---

### 29. Text-to-SQL Integration

Text-to-SQL engines are integrated into Agentic RAG for structured data retrieval, enabling natural language queries against relational databases.

**Sources**:

- **20-Agentic_RAG_Survey (Chunk 1:775-776)**
  > Structured Databases: For queries requiring tabular data access, the system may use a Text-to-SQL engine that interacts with databases like PostgreSQL or MySQL

---

### 30. GPT-4 Function Calling

GPT-4's function calling capability is a key technical standard enabling agents to dynamically select and execute tools within agentic workflows.

**Sources**:

- **20-Agentic_RAG_Survey (Chunk 1:561-563)**
  > The implementation of this pattern has evolved significantly with advancements like GPT-4's function calling capabilities and systems capable of managing access to numerous tools

---

### 31. Vector Search Tool

Vector search is a standard tool type in multi-agent RAG systems for retrieving semantically relevant information from vector databases.

**Sources**:

- **20-Agentic_RAG_Survey (Chunk 2:11)**
  > Vector Search: For semantic relevance

---

### 32. Web Search Tool

Web search integration is a standard tool in Agentic RAG for accessing real-time public information beyond static knowledge bases.

**Sources**:

- **20-Agentic_RAG_Survey (Chunk 2:15)**
  > Web Search: For real-time public information

---

### 33. APIs for External Services

API integration is a standardized approach in multi-agent RAG for connecting to external services and proprietary enterprise systems.

**Sources**:

- **20-Agentic_RAG_Survey (Chunk 2:17)**
  > APIs: For accessing external services or proprietary systems

---

### 34. LlamaIndex Agentic Document Workflows

LlamaIndex's ADW framework provides a technical standard for document-centric agentic workflows, integrating parsing, retrieval, reasoning, and structured outputs.

**Sources**:

- **20-Agentic_RAG_Survey (Chunk 2:704-708)**
  > Agentic Document Workflows (ADW) extend traditional Retrieval-Augmented Generation (RAG) paradigms by enabling end-to-end knowledge work automation

---

### 35. LlamaParse Document Processing

LlamaParse is an enterprise-grade document parsing tool used within Agentic Document Workflows for structured data extraction from documents.

**Sources**:

- **20-Agentic_RAG_Survey (Chunk 2:717)**
  > Documents are parsed using enterprise-grade tools (e.g., LlamaParse) to extract relevant data fields

---

### 36. LlamaCloud Knowledge Retrieval

LlamaCloud provides cloud-based knowledge retrieval infrastructure for Agentic Document Workflows, supporting vector indexes and external knowledge bases.

**Sources**:

- **20-Agentic_RAG_Survey (Chunk 2:733)**
  > Relevant references are retrieved from external knowledge bases (e.g., LlamaCloud) or vector indexes

---

### 37. LangChain Framework

Merged from 2 sources. LangChain is a major framework for building RAG pipelines, providing modular components for retriever-generator integration and external tool connectivity.

**Sources**:

- **20-Agentic_RAG_Survey (Chunk 3:164)**
  > LangChain and LangGraph: LangChain provides modular components for building RAG pipelines, seamlessly integrating retrievers, generators, and external tools

- **03-PROV-AGENT (Chunk 1:141)**
  > LangChain [10], [11], AutoGen [12], LangGraph [13], Academy [3], and CrewAI [14] support multi-agent systems

---

### 38. LangGraph Workflow Framework

LangGraph extends LangChain with graph-based workflow capabilities including loops, state persistence, and human-in-the-loop interactions for sophisticated agentic orchestration.

**Sources**:

- **20-Agentic_RAG_Survey (Chunk 3:165-167)**
  > LangGraph complements this by introducing graph-based workflows that support loops, state persistence, and human-in-the-loop interactions

---

### 39. LlamaIndex Framework

LlamaIndex is a framework for document-centric RAG with meta-agent architecture supporting sub-agent coordination for compliance analysis and contextual understanding.

**Sources**:

- **20-Agentic_RAG_Survey (Chunk 3:169-172)**
  > LlamaIndex's Agentic Document Workflows (ADW) enable end-to-end automation of document processing, retrieval, and structured reasoning

---

### 40. Hugging Face Transformers

Hugging Face Transformers provides pre-trained models for embedding and generation tasks, serving as a foundation for RAG system components.

**Sources**:

- **20-Agentic_RAG_Survey (Chunk 3:174)**
  > Hugging Face Transformers and Qdrant: Hugging Face offers pre-trained models for embedding and generation tasks

---

### 41. Qdrant Vector Database

Qdrant is a vector database that provides adaptive vector search capabilities with dynamic switching between sparse and dense retrieval methods.

**Sources**:

- **20-Agentic_RAG_Survey (Chunk 3:175-176)**
  > Qdrant enhances retrieval workflows with adaptive vector search capabilities, allowing agents to optimize performance by dynamically switching between sparse and dense vector methods

---

### 42. CrewAI Multi-Agent Framework

CrewAI is a multi-agent framework supporting hierarchical processes, sequential workflows, memory systems, and tool integrations for collaborative AI agents.

**Sources**:

- **20-Agentic_RAG_Survey (Chunk 3:182-183)**
  > CrewAI and AutoGen: These frameworks emphasize multi-agent architectures. CrewAI supports hierarchical and sequential processes, robust memory systems, and tool integrations

---

### 43. AutoGen/AG2 Framework

AutoGen (now AG2) is a multi-agent framework from Microsoft excelling in code generation, tool execution, and collaborative decision-making capabilities.

**Sources**:

- **20-Agentic_RAG_Survey (Chunk 3:183-185)**
  > AG2 (formerly knows as AutoGen) excels in multi-agent collaboration with advanced support for code generation, tool execution, and decision-making

---

### 44. OpenAI Swarm Framework

OpenAI Swarm is a lightweight educational framework for multi-agent orchestration emphasizing agent autonomy and structured collaboration patterns.

**Sources**:

- **20-Agentic_RAG_Survey (Chunk 3:188-189)**
  > OpenAI Swarm Framework: An educational framework designed for ergonomic, lightweight multi-agent orchestration, emphasizing agent autonomy and structured collaboration

---

### 45. Google Vertex AI for Agentic RAG

Google Vertex AI provides enterprise infrastructure for building, deploying, and scaling Agentic RAG systems with advanced retrieval and decision-making workflows.

**Sources**:

- **20-Agentic_RAG_Survey (Chunk 3:192-195)**
  > Agentic RAG with Vertex AI: Developed by Google, Vertex AI integrates seamlessly with Agentic RAG, providing a platform to build, deploy, and scale machine learning models

---

### 46. Microsoft Semantic Kernel

Microsoft Semantic Kernel is an open-source SDK for integrating LLMs into applications, supporting agentic patterns for autonomous AI agents with task automation.

**Sources**:

- **20-Agentic_RAG_Survey (Chunk 3:198-202)**
  > Semantic Kernel is an open-source SDK by Microsoft that integrates large language models (LLMs) into applications. It supports agentic patterns

---

### 47. Amazon Bedrock for Agentic RAG

Amazon Bedrock provides AWS cloud infrastructure for implementing Agentic RAG workflows with foundation model access and agentic capabilities.

**Sources**:

- **20-Agentic_RAG_Survey (Chunk 3:205-206)**
  > Amazon Bedrock for Agentic RAG: Amazon Bedrock provides a robust platform for implementing Agentic RAG workflows

---

### 48. IBM watsonx.ai

IBM watsonx.ai supports Agentic RAG systems using Granite models for complex query answering with external information integration.

**Sources**:

- **20-Agentic_RAG_Survey (Chunk 3:209-211)**
  > IBM Watson and Agentic RAG: IBM's watsonx.ai supports building Agentic RAG systems, exemplified by using the Granite-3-8B-Instruct model

---

### 49. Neo4j Graph Database

Neo4j is an open-source graph database used in Agentic RAG for handling complex relationships and semantic queries in knowledge graph applications.

**Sources**:

- **20-Agentic_RAG_Survey (Chunk 3:214-217)**
  > Neo4j and Vector Databases: Neo4j, a prominent open-source graph database, excels in handling complex relationships and semantic queries

---

### 50. Vector Database Ecosystem

Weaviate, Pinecone, Milvus, and Qdrant form the vector database ecosystem providing similarity search and retrieval infrastructure for Agentic RAG workflows.

**Sources**:

- **20-Agentic_RAG_Survey (Chunk 3:215-217)**
  > Alongside Neo4j, vector databases like Weaviate, Pinecone, Milvus, and Qdrant provide efficient similarity search and retrieval capabilities

---

### 51. BEIR Benchmark

BEIR is a standard benchmark for evaluating embedding models across 17 diverse information retrieval datasets spanning bioinformatics, finance, and QA domains.

**Sources**:

- **20-Agentic_RAG_Survey (Chunk 3:234-236)**
  > BEIR (Benchmarking Information Retrieval): A versatile benchmark designed for evaluating embedding models on a variety of information retrieval tasks, encompassing 17 datasets

---

### 52. MS MARCO Benchmark

MS MARCO is a standard benchmark for passage ranking and question answering, widely used for evaluating dense retrieval in RAG systems.

**Sources**:

- **20-Agentic_RAG_Survey (Chunk 3:239-240)**
  > MS MARCO (Microsoft Machine Reading Comprehension): Focused on passage ranking and question answering, this benchmark is widely used for dense retrieval tasks

---

### 53. FlashRAG Toolkit

FlashRAG is a toolkit implementing 12 RAG methods with 32 benchmark datasets for standardized RAG system evaluation and comparison.

**Sources**:

- **20-Agentic_RAG_Survey (Chunk 3:276-277)**
  > FlashRAG Toolkit: Implements 12 RAG methods and includes 32 benchmark datasets to support efficient and standardized RAG evaluation

---

### 54. BPMN 2.0 Choreographies

BPMN 2.0 Choreographies is a standard notation for modeling business processes, used as input for smart contract generation in blockchain-based process execution.

**Sources**:

- **21-LLM_Smart_Contracts_from_BPMN (Chunk 1:338-339)**
  > In the current instantiation of our framework, we support BPMN 2.0 Choreographies. This is a purely practical implementation choice

---

### 55. Ethereum Virtual Machine (EVM)

The Ethereum Virtual Machine (EVM) is the primary blockchain execution environment for smart contracts, serving as the target platform for BPMN-to-contract transformation.

**Sources**:

- **21-LLM_Smart_Contracts_from_BPMN (Chunk 1:341-342)**
  > We instantiate our framework for an Ethereum virtual machine (EVM) blockchain environment, the most widely employed environment

---

### 56. Solidity Smart Contract Language

Solidity is the primary programming language for Ethereum smart contracts, used as the target output for LLM-based code generation from process models.

**Sources**:

- **21-LLM_Smart_Contracts_from_BPMN (Chunk 1:135)**
  > training ingests very large textual corpora, comprising not only natural language, but also programming code (such as Solidity for blockchain smart contracts)

---

### 57. Hardhat Development Framework

Hardhat is an Ethereum development framework for testing, deploying, and debugging smart contracts in simulated EVM environments.

**Sources**:

- **21-LLM_Smart_Contracts_from_BPMN (Chunk 1:363-365)**
  > To provide the replayer with a blockchain environment, we use hardhat, a popular Ethereum development framework that allows testing, deployment, and debugging of smart contracts

---

### 58. OpenRouter LLM API Platform

OpenRouter is an API platform providing unified access to multiple LLM providers, used for benchmarking smart contract generation across different models.

**Sources**:

- **21-LLM_Smart_Contracts_from_BPMN (Chunk 1:364-366)**
  > As LLM provider, we use OpenRouter, a platform that provides a unified API across multiple language model providers

---

### 59. Chorpiler BPMN Transformation Tool

Chorpiler is an open-source tool for transforming BPMN Choreographies into smart contracts, extended with simulation capabilities for trace generation.

**Sources**:

- **21-LLM_Smart_Contracts_from_BPMN (Chunk 1:346-348)**
  > We extend the open source tool Chorpiler, first introduced in [40] with simulation capabilities. Chorpiler transforms BPMN Choreographies to smart contracts

---

### 60. pm4py Process Mining Library

pm4py is a Python process mining library used for generating conforming traces from Petri net representations of business processes.

**Sources**:

- **21-LLM_Smart_Contracts_from_BPMN (Chunk 1:351-354)**
  > we adopt the implementation of pm4py, a popular Python library for process mining, which includes a playout functionality to generate event log traces from Petri nets

---

### 61. SAP-SAM Process Model Dataset

SAP-SAM is a large academic dataset of 4,096 BPMN 2.0 choreography models, used for benchmarking LLM-based smart contract generation.

**Sources**:

- **21-LLM_Smart_Contracts_from_BPMN (Chunk 1:386-388)**
  > Our process model data is based on the SAP Signavio Academic Models Dataset (SAP-SAM), which was initially introduced in [37]

---

### 62. GitHub Copilot Integration

GitHub Copilot represents the commercial integration of LLM-based code generation into development environments like Visual Studio Code.

**Sources**:

- **21-LLM_Smart_Contracts_from_BPMN (Chunk 1:43-44)**
  > testified by the integration of commercial tools like Github's Copilot... into popular development environments like Visual Studio Code

---

### 63. RPA Technology Platform

RPA (Robotic Process Automation) is defined as a centralized automation platform operating on the presentation layer, enabling business users to automate UI-based tasks.

**Sources**:

- **22-RPA_Framework_BPM_Activities (Chunk 1:107-109)**
  > In this work, we define RPA as an automation technology which performs work on the presentation layer, can be set up by a business user, and is managed on a centralized platform

---

### 64. Low-Code RPA Development

Low-code development is a key characteristic of RPA platforms, enabling business users without programming expertise to configure automation robots.

**Sources**:

- **22-RPA_Framework_BPM_Activities (Chunk 1:104-105)**
  > little to no programming knowledge is required to implement and manage the orchestration and execution of the robots often referred to as low-code development

---

### 65. Process Mining Software (Celonis)

Celonis is a process mining platform used to analyze event logs and evaluate RPA automation candidates based on process characteristics.

**Sources**:

- **22-RPA_Framework_BPM_Activities (Chunk 1:425-426)**
  > We determine process characteristics with Process Mining Software. Hence, we test the framework for its applicability in a practical environment

---

### 66. SAP ECC Enterprise System

SAP ECC is an enterprise system generating event logs used for process mining analysis to identify RPA automation opportunities.

**Sources**:

- **22-RPA_Framework_BPM_Activities (Chunk 1:535)**
  > Since the event log originates from an SAP ECC system, which is a roofing system, we cannot determine whether there are more systems involved

---

### 67. BPI Challenge 2019 Dataset

BPI Challenge 2019 dataset is a publicly available process mining dataset used to validate the RPA process characteristics evaluation framework.

**Sources**:

- **22-RPA_Framework_BPM_Activities (Chunk 1:654-656)**
  > van Dongen, B.: Bpi challenge 2019... https://doi.org/10.4121/uuid:d06aff4b-79f0-45e6-8ec8-e19730c248f1, 4TU.Centre for Research Data. Dataset

---

### 68. OntoUML as UML Profile

OntoUML was implemented as a UML 2.0 profile, leveraging UML's standardized metamodel and metamodeling tools. This approach enabled the use of existing UML tools like Enterprise Architect to create ontology-driven conceptual models.

**Sources**:

- **23-UFO_Story_Ontological_Foundations (Chunk 1:243-249)**
  > The decision to build the language as a version of UML (technically, as a UML profile) was mainly motivated by the fact that UML...has a standardized and explicitly defined metamodel

---

### 69. OWL DL Codification Target

OntoUML models can be transformed into multiple implementation languages including OWL DL, RDFS, F-Logic, Haskell, and relational database schemas. Six different automatic mappings from OntoUML to OWL have been implemented.

**Sources**:

- **23-UFO_Story_Ontological_Foundations (Chunk 1:306)**
  > we can have different mappings to different codification languages (e.g., OWL DL, RDFS, F-Logic, Haskell, Relational Database languages, CASL, among many others)

---

### 70. OCL for Domain Constraints

Object Constraint Language (OCL) and temporal OCL are supported for specifying domain-specific formal constraints that cannot be represented using diagrammatic notation. The editor provides syntax highlighting, code-completion and syntax verification.

**Sources**:

- **23-UFO_Story_Ontological_Foundations (Chunk 1:288-290)**
  > the current editor supports the specification of OCL (Guerson et al., 2014) and temporal OCL formal constraints (Guerson & Almeida, 2015)

---

### 71. SBVR Model Verbalization

SBVR (Semantics for Business Vocabularies and Rules) is used for automatic model verbalization, generating controlled natural language renderings of OntoUML models to allow domain experts to access conceptual model content.

**Sources**:

- **23-UFO_Story_Ontological_Foundations (Chunk 1:275-276)**
  > The editor incorporates a functionality for automatically generating model verbalization in structured English following a slightly modified version of the SBVR (Semantics for Business Vocabularies and Rules) OMG proposal

---

### 72. Alloy Formal Verification

Alloy is used as a lightweight formal method for OntoUML model validation, enabling automatic generation of visual instances for validation via simulation.

**Sources**:

- **23-UFO_Story_Ontological_Foundations (Chunk 1:442-443)**
  > Transforming OntoUML into alloy: Towards conceptual model validation using a lightweight formal method

---

### 73. XML Schema Generation

OntoUML models can be mapped to XML schemas for semantic data representation, enabling integration with XML-based systems.

**Sources**:

- **23-UFO_Story_Ontological_Foundations (Chunk 1:316)**
  > other authors have implemented alternative mappings from OntoUML to languages such as XML (Baumann, 2009)

---

### 74. Smalltalk Code Generation

OntoUML can be transformed into Smalltalk code for implementation purposes, demonstrating the language-independent nature of the ontological foundations.

**Sources**:

- **23-UFO_Story_Ontological_Foundations (Chunk 1:316-317)**
  > other authors have implemented alternative mappings from OntoUML to languages such as XML (Baumann, 2009), Smalltalk (Pergl et al., 2013)

---

### 75. Modal Prolog Implementation

OntoUML can be mapped to Modal Prolog, enabling logical reasoning over ontological models with modal properties.

**Sources**:

- **23-UFO_Story_Ontological_Foundations (Chunk 1:317)**
  > a version of a Modal Prolog (Araujo, 2015)

---

### 76. Enterprise Architect Plugin

A plugin for Enterprise Architect enables professional use of OntoUML in industrial settings, supporting pattern-based model construction, formal verification, and anti-pattern detection.

**Sources**:

- **23-UFO_Story_Ontological_Foundations (Chunk 1:246-247)**
  > an OntoUML plug-in has been implemented for the professional tool Enterprise Architect

---

### 77. Protege Tool Mention

UML metamodeling tools are leveraged for OntoUML development, with Protege being commonly used for OWL implementations in the ontology engineering community.

**Sources**:

- **23-UFO_Story_Ontological_Foundations (Chunk 1:245-246)**
  > a significant set of metamodeling tools that can be used to manipulate and extend this metamodel

---

### 78. Sortal Quantified Modal Logics

UFO-A includes formal semantics expressed in Sortal Quantified Modal Logics for identity theory, providing rigorous mathematical foundations for conceptual modeling.

**Sources**:

- **23-UFO_Story_Ontological_Foundations (Chunk 1:176)**
  > a theory of object identifiers (including a formal semantics in a Sortal Quantified Modal Logics (Guizzardi, 2015))

---

### 79. GFO General Formalized Ontology

GFO (General Formalized Ontology) and GOL (General Ontology Language) were considered as reference theories for UFO development, representing alternative foundational ontology approaches.

**Sources**:

- **23-UFO_Story_Ontological_Foundations (Chunk 1:137-138)**
  > we attempted to employ the GFO (General Formalized Ontology)/GOL (General Ontology Language) being developed in Leipzig, Germany

---

### 80. Standards Integration - TOGAF ArchiMate

UFO has been applied to analyze and integrate multiple enterprise architecture standards including TOGAF, ArchiMate, RM-ODP, and ARIS, demonstrating its role as a foundational framework for standards alignment.

**Sources**:

- **23-UFO_Story_Ontological_Foundations (Chunk 1:197)**
  > UFO has been employed as a basis for analyzing, reengineering and integrating many modeling languages and standards in different domains (e.g., UML, TOGAF, ArchiMate, RM-ODP, TROPOS/i*, AORML, ARIS, BPMN)

---

### 81. BPMN Process Modeling

UFO has been applied to provide ontological foundations for BPMN business process modeling and discrete event simulation, grounding process-related standards.

**Sources**:

- **23-UFO_Story_Ontological_Foundations (Chunk 1:197-201)**
  > Business Processes (Guizzardi & Wagner, 2011b; Santos Junior et al., 2010), Discrete Event Simulation (Guizzardi & Wagner, 2010; 2011a; 2012; 2013)

---

### 82. OntoClean Methodology

Merged from 2 sources. Methodology for evaluating ontological decisions through meta-properties like rigidity, identity, unity, dependence. Used to validate class hierarchies and taxonomic choices.

**Sources**:

- **23-UFO_Story_Ontological_Foundations (Chunk 1:153)**
  > we needed something in the spirit of the ontology of universals underlying the OntoClean methodology (Guarino & Welty, 2009)

- **05-DOLCE (Chunk 1:73-75)**
  > The analysis underlying the formalization of DOLCE leverages the techniques of ontological engineering and the study of classes' meta-properties of the OntoClean methodology

---

### 83. UML 2.0 Class Diagrams

UML 2.0 class diagrams serve as the structural foundation for OntoUML, with the language extending UML with ontological distinctions from UFO-A.

**Sources**:

- **23-UFO_Story_Ontological_Foundations (Chunk 1:213)**
  > OntoUML (Guizzardi, 2005) was conceived as an ontologically well-founded version of the UML 2.0 fragment of class diagrams

---

### 84. OWL 2 DL Implementation

BBO ontology is implemented in OWL 2 DL using Protege, enabling formal reasoning, consistency checking, and SPARQL querying over business process knowledge bases.

**Sources**:

- **31-BBO_BPMN_Ontology (Chunk 1:359)**
  > We have formalized and implemented the conceptual model of BBO in OWL 2 DL using Protege

---

### 85. BPMN 2.0 Meta-model Reuse

BBO is built by reusing and extending the BPMN 2.0 meta-model, extracting the process-execution specification fragment and transforming it into OWL representation.

**Sources**:

- **31-BBO_BPMN_Ontology (Chunk 1:45-47)**
  > we developed the BBO (BPMN 2.0 Based Ontology) ontology for business process representation, by reusing existing ontologies and meta-models like BPMN 2.0, the state-of-the-art meta-model for business process representation

---

### 86. METHONTOLOGY Development Process

METHONTOLOGY was used as the ontology engineering methodology for BBO development, following five stages: Specification, Conceptualization, Formalization, Implementation, and Maintenance.

**Sources**:

- **31-BBO_BPMN_Ontology (Chunk 1:95-96)**
  > a presentation of METHONTOLOGY (Fernandez et al., 1997), the methodology followed to develop BBO in five classical stages

---

### 87. Protege Reasoners - Hermit Fact Pellet

Multiple OWL reasoners (Hermit, Fact, Pellet) are used in Protege for consistency checking and automatic classification of BBO instances, validating ontology design.

**Sources**:

- **31-BBO_BPMN_Ontology (Chunk 1:426-428)**
  > According to the various reasoners in Protege (i.e., Hermit, Fact and Pellet), the ontology is consistent and remains consistent, even after its population with assertions describing several BP models

---

### 88. SPARQL Querying

SPARQL is used to query BBO knowledge bases populated with business process data, enabling answers to competency questions about process structure, resources, and execution.

**Sources**:

- **31-BBO_BPMN_Ontology (Chunk 1:485)**
  > we design SPARQL queries corresponding to the competency questions and check their results

---

### 89. UML to OWL Transformation Rules

Systematic transformation rules convert UML class diagrams to OWL, including class-to-class mapping, relation-to-ObjectProperty mapping, and cardinality-to-restriction mapping.

**Sources**:

- **31-BBO_BPMN_Ontology (Chunk 1:368-386)**
  > Generating an OWL representation from the UML diagrams results in the following algorithm: For each UML class, create an owl class; For each relation between UML classes create an OWL ObjectProperty

---

### 90. UO Unit of Measure Ontology

BBO reuses the UO (Units of Measurement Ontology) for specifying measurement units, demonstrating ontology reuse and interoperability.

**Sources**:

- **31-BBO_BPMN_Ontology (Chunk 1:289)**
  > The UnitOfMeasure class is specified using the two concepts Unit and Prefix of the unit measures ontology UO (UO-onto, 2019)

---

### 91. Camunda BPMN Editor

Camunda is used as an open-source BPMN modeling tool for creating graphical process representations that are then populated into BBO ontology instances.

**Sources**:

- **31-BBO_BPMN_Ontology (Chunk 1:482)**
  > we represent this BP with BPMN graphical elements using an open source software, Camunda (https://camunda.com/)

---

### 92. XML Schema for BPMN

BPMN 2.0 is specified both as UML diagrams and XML Schema, though these formal representations do not capture all natural language specifications from the standard.

**Sources**:

- **31-BBO_BPMN_Ontology (Chunk 1:398-399)**
  > BPMN 2.0 specification provides a meta-model for BPMN elements as a UML class diagram and in the form of an XML schema

---

### 93. rdfs:comment for Documentation

RDFS comment property is used to document BBO classes with descriptions from the BPMN 2.0 specification, enhancing ontology usability.

**Sources**:

- **31-BBO_BPMN_Ontology (Chunk 1:424-425)**
  > for each BPMN element, we added its description as mentioned in BPMN 2.0 specification (OMG., 2011) using the rdfs:comment property

---

### 94. OMG Standards Body

Object Management Group (OMG) maintains BPMN as an industry standard for business process modeling, providing the foundation for BBO's process representation capabilities.

**Sources**:

- **31-BBO_BPMN_Ontology (Chunk 1:82-83)**
  > BPMN is the most adopted meta-model for representing BPs (OMG., 2011). Indeed, it is a standard for BPM maintained by the Object Management Group (OMG)

---

### 95. OntoUML Conceptual Modeling Language

OntoUML is a modeling language used for conceptual modeling grounded in UFO. It provides visual representation of ontological structures including classes, qualities, and relationships with stereotypes like subkind and characterization.

**Sources**:

- **01-UFO_Unified_Foundational_Ontology (Chunk 3:5-8)**
  > In Figure 8, we present an OntoUML model [17] representing this situation. In this model, a Rose is modeled as a subkind of Flower.

---

### 96. OntoUML Quality Structures as Datatypes

OntoUML uses datatypes to represent quality structures, with enumeration stereotypes for specific datatype categories. This provides a formal way to model qualities and their value spaces.

**Sources**:

- **01-UFO_Unified_Foundational_Ontology (Chunk 3:17-19)**
  > In OntoUML, the simplest way to represent quality structures is as datatypes (Guizzardi, 2005). Classes stereotyped as enumeration are particular types of datatypes.

---

### 97. OntoUML Automated Visual Model Generation

OntoUML tools can automatically generate visual world structures from conceptual models, enabling validation and simulation of ontological specifications.

**Sources**:

- **01-UFO_Unified_Foundational_Ontology (Chunk 3:85-86)**
  > In the sequel, we show examples (Figure 9) of the visual models automatically generated for the OntoUML diagram of Figure 8.

---

### 98. OntoUML Event Type Modeling

OntoUML provides EventType stereotypes for modeling perdurant entities (events and processes), enabling formal representation of temporal phenomena.

**Sources**:

- **01-UFO_Unified_Foundational_Ontology (Chunk 3:154-157)**
  > EventType(JoggingProcess) EventType(JoggingEvent)

---

### 99. Allen Temporal Relations in UFO

UFO/OntoUML incorporates Allen temporal relations (meet, before, overlaps, etc.) for reasoning about temporal relationships between events and processes.

**Sources**:

- **01-UFO_Unified_Foundational_Ontology (Chunk 3:176-178)**
  > if we have two Jogging Processes that are manifestations of the same Jog and that immediately follow each other, meeting in the Allen sense

---

### 100. OntoUML Anti-Pattern Detection and Rectification

OntoUML tools include automated anti-pattern detection capabilities that identify modeling errors and generate corrective constraints, improving model quality.

**Sources**:

- **01-UFO_Unified_Foundational_Ontology (Chunk 3:366-369)**
  > This constraint is automatically generated by the OntoUML tool after an automated process of anti-pattern detection and rectification (Sales and Guizzardi, 2015).

---

### 101. ArchiMate Enterprise Architecture Standard

UFO has been used to analyze and reengineer the ArchiMate enterprise architecture standard, demonstrating its applicability to industry modeling frameworks.

**Sources**:

- **01-UFO_Unified_Foundational_Ontology (Chunk 3:668-669)**
  > ArchiMate (Almeida et al., 2009; Amaral et al., 2020a; Azevedo et al., 2011, 2015; Griffo et al., 2017; Sales et al., 2018a, 2019)

---

### 102. ARIS Process Modeling Integration

UFO has been applied to provide ontological analysis and semantic foundations for the ARIS (Architecture of Integrated Information Systems) business process modeling method.

**Sources**:

- **01-UFO_Unified_Foundational_Ontology (Chunk 3:671)**
  > ARIS (Santos Junior et al., 2010, 2013)

---

### 103. BPMN Ontological Analysis

UFO provides ontological foundations for BPMN (Business Process Model and Notation), enabling formal analysis of business process modeling constructs.

**Sources**:

- **01-UFO_Unified_Foundational_Ontology (Chunk 3:679)**
  > BPMN (Guizzardi and Wagner, 2011a)

---

### 104. TOGAF Framework Analysis

UFO has been applied to analyze the TOGAF (The Open Group Architecture Framework), providing ontological grounding for enterprise architecture concepts.

**Sources**:

- **01-UFO_Unified_Foundational_Ontology (Chunk 3:683)**
  > TOGAF (Almeida et al., 2009)

---

### 105. OntoUML Adoption as Modeling Language

OntoUML is recognized as one of the top five languages in ontology-driven conceptual modeling, with empirical validation showing improved model quality compared to classical languages like EER.

**Sources**:

- **01-UFO_Unified_Foundational_Ontology (Chunk 3:691-696)**
  > OntoUML is among the most used languages in ontology-driven conceptual modeling (together with UML, (E)ER, OWL, and BPMN). Moreover, empirical evidence shows that OntoUML significantly contributes to improving the quality of conceptual models

---

### 106. OntoUML as a Service (OaaS) Infrastructure

OaaS is a microservice-based infrastructure for OntoUML that provides model intelligence services including transformations, verifications, simulations, and verbalizations through a decoupled architecture.

**Sources**:

- **01-UFO_Unified_Foundational_Ontology (Chunk 3:697-700)**
  > the OntoUML as a Service infrastructure (Fonseca et al., 2021b), or OaaS, is designed to decouple model services developed by OntoUML researchers (e.g., transformations, verifications, simulations, verbalizations)

---

### 107. HTTP and JSON for Model Serialization

The OntoUML service infrastructure uses HTTP for API communication and JSON for model serialization, enabling interoperability with various tools and programming languages.

**Sources**:

- **01-UFO_Unified_Foundational_Ontology (Chunk 3:705-706)**
  > This is enabled by OaaS's low requirements on services and tool developers that consists of support to HTTP and JSON used for service request and model serialization respectively.

---

### 108. OntoUML JSON Schema Specification

A formal JSON Schema specification governs the serialization of OntoUML models, ensuring consistent representation across tools and services.

**Sources**:

- **01-UFO_Unified_Foundational_Ontology (Chunk 3:706-707)**
  > The current implementation of OaaS consists of a JSON Schema specification to govern the JSON serialization of OntoUML models

---

### 109. OntoUML TypeScript Library (ontouml-js)

The ontouml-js TypeScript library provides programmatic support for manipulating and serializing OntoUML models, enabling integration with JavaScript/TypeScript applications.

**Sources**:

- **01-UFO_Unified_Foundational_Ontology (Chunk 3:707-708)**
  > a TypeScript library to support the manipulation and serialization of OntoUML models

---

### 110. Visual Paradigm UML CASE Tool Integration

A plugin for Visual Paradigm extends this commercial UML CASE tool with OntoUML modeling capabilities, enabling enterprise adoption of ontology-driven conceptual modeling.

**Sources**:

- **01-UFO_Unified_Foundational_Ontology (Chunk 3:708-709)**
  > a plugin for the UML CASE Visual Paradigm that extends it with OntoUML modeling capabilities

---

### 111. ISO/IEC 24744 Software Engineering Standard

UFO has been applied to create ontological infrastructure for ISO/IEC 24744 and other ISO software engineering standards, enabling formal analysis of software development methodologies.

**Sources**:

- **01-UFO_Unified_Foundational_Ontology (Chunk 4:93-94)**
  > Henderson-Sellers, B., Gonzalez-Perez, C., McBride, T., and Low, G. (2014). An ontology for ISO software engineering standards: 1) creating the infrastructure. Computer Standards & Interfaces

---

### 112. ITU-T G.805 Telecommunications Standard

UFO has been used for ontological evaluation of ITU-T G.805, a telecommunications networking standard, demonstrating applicability to technical infrastructure domains.

**Sources**:

- **01-UFO_Unified_Foundational_Ontology (Chunk 3:676-677)**
  > ITU-T G.805 (Barcelos et al., 2011)

---

### 113. RDF (Resource Description Framework)

Merged from 2 sources. RDF is the W3C-recommended standardized data model for directed edge-labelled graphs, forming the foundation for knowledge graph representation with support for IRIs, literals, and blank nodes.

**Sources**:

- **02-Knowledge_Graphs (Chunk 1:435-438)**
  > A standardised data model based on directed edge-labelled graphs is the Resource Description Framework (RDF) [111], which has been recommended by the W3C.

- **04-PROV-O_to_BFO (Chunk 1:42)**
  > Resource Description Framework (RDF) [4]

---

### 114. Internationalized Resource Identifiers (IRIs)

IRIs provide global Web-based identification for entities in RDF knowledge graphs, enabling unique naming across distributed systems.

**Sources**:

- **02-Knowledge_Graphs (Chunk 1:439-441)**
  > The RDF model defines different types of nodes, including Internationalized Resource Identifiers (IRIs) [134] which allow for global identification of entities on the Web

---

### 115. SPARQL Query Language for RDF

Merged from 4 sources. SPARQL is identified as the standard query language for retrieving and manipulating knowledge in knowledge graphs, enabling structured access to graph data.

**Sources**:

- **02-Knowledge_Graphs (Chunk 1:664-665)**
  > A number of practical languages have been proposed for querying graphs [16], including the SPARQL query language for RDF graphs [217]

- **02-Knowledge_Graphs (Chunk 8:128-129)**
  > public services offering such a protocol (most often supporting SPARQL queries [217]) have been found to often exhibit downtimes, timeouts

- **04-PROV-O_to_BFO (Chunk 1:180)**
  > SPARQL [24] queries can also encode mappings that are equivalent to some SWRL rules

- **17-KG_Reasoning (Chunk 1:361)**
  > Conventional query answering is conducted based on structure query languages such as SPARQL to retrieve and manipulate knowledge in a KG

---

### 116. Cypher Query Language for Property Graphs

Cypher is a declarative query language for property graphs, used prominently in Neo4j, supporting pattern matching with isomorphism-based semantics on edges.

**Sources**:

- **02-Knowledge_Graphs (Chunk 1:665)**
  > and Cypher [165], Gremlin [445], and G-CORE [15] for querying property graphs

---

### 117. Gremlin Graph Traversal Language

Gremlin is a graph traversal language for property graphs that provides a functional, data-flow-based approach to graph queries.

**Sources**:

- **02-Knowledge_Graphs (Chunk 1:665)**
  > Cypher [165], Gremlin [445], and G-CORE [15] for querying property graphs

---

### 118. G-CORE Query Language

G-CORE is a composable graph query language that extends property graph querying with path return values, graph projections, and cost function support.

**Sources**:

- **02-Knowledge_Graphs (Chunk 1:665)**
  > Cypher [165], Gremlin [445], and G-CORE [15] for querying property graphs

---

### 119. Neo4j Property Graph Database

Neo4j is the most prominent property graph database system, supporting native graph storage, property graph data model, and Cypher query language.

**Sources**:

- **02-Knowledge_Graphs (Chunk 1:562)**
  > Property graphs are most prominently used in popular graph databases, such as Neo4j [16, 354].

---

### 120. Triple Table Storage for Graphs

Triple table is a storage strategy for RDF graphs using a single three-column relational table, enabling simple storage but potentially limiting query performance.

**Sources**:

- **02-Knowledge_Graphs (Chunk 1:646-648)**
  > Directed-edge labelled graphs can be stored in relational databases either as a single relation of arity three (triple table), as a binary relation for each property (vertical partitioning)

---

### 121. Vertical Partitioning Storage

Vertical partitioning stores RDF graphs by creating separate two-column tables for each property, improving certain query patterns at the cost of increased table count.

**Sources**:

- **02-Knowledge_Graphs (Chunk 1:647-648)**
  > as a binary relation for each property (vertical partitioning)

---

### 122. Property Tables Storage

Property tables storage strategy groups entities by type into n-ary relations, combining the benefits of relational normalization with type-specific query optimization.

**Sources**:

- **02-Knowledge_Graphs (Chunk 1:648-650)**
  > or as n-ary relations for entities of a given type (property tables)

---

### 123. RDFS (RDF Schema) Semantic Schema

RDFS is the W3C standard for defining semantic schemas for RDF graphs, supporting class hierarchies, property hierarchies, and domain/range constraints.

**Sources**:

- **02-Knowledge_Graphs (Chunk 2:176-180)**
  > A prominent standard for defining a semantic schema for (RDF) graphs is the RDF Schema (RDFS) standard [70], which allows for defining subclasses, subproperties, domains, and ranges

---

### 124. OWL (Web Ontology Language)

Merged from 4 sources. OWL (Web Ontology Language) and specifically OWL-DL is a Description Logic-based standard for ontology formulation with restricted expressivity that enables automatic consistency checking through dedicated software reasoners.

**Sources**:

- **02-Knowledge_Graphs (Chunk 2:188-190)**
  > the semantics of terms used in a graph can be defined in much more depth than seen here, as is supported by the Web Ontology Language (OWL) standard [239] for RDF graphs

- **04-PROV-O_to_BFO (Chunk 1:41-42)**
  > A popular way to construct ontologies, and the way relevant to this paper, is by leveraging the W3C standards Web Ontology Language (OWL) [3] and Resource Description Framework (RDF) [4]

- **07-Classifying_Processes_Barry_Smith (Chunk 1:157-161)**
  > and the (OWL) Web Ontology Language. Common Logic is an ISO Standard family of languages with an expressivity equivalent to that of first-order logic. OWL-DL is a fragment of the language of first order logic belonging to the family of what are called Description Logics.

- **17-KG_Reasoning (Chunk 1:44-48)**
  > HermiT [Glimm et al., 2014] is a classic description logic reasoner for OWL ontologies; RDFox [Nenov et al., 2015] is a famous KG storage supporting Datalog rule reasoning

---

### 125. ShEx (Shape Expressions) Validation Language

ShEx is a W3C Community Group specification for validating RDF graphs using shape-based constraints, supporting recursive validation and constraint specification.

**Sources**:

- **02-Knowledge_Graphs (Chunk 2:381-383)**
  > Two shapes languages have recently emerged for RDF graphs: Shape Expressions (ShEx), published as a W3C Community Group Report [423]

---

### 126. SHACL (Shapes Constraint Language)

SHACL is a W3C Recommendation for validating RDF graphs using shapes, supporting constraint specification, SPARQL-based validation, and integration with OWL semantics.

**Sources**:

- **02-Knowledge_Graphs (Chunk 2:383-385)**
  > and SHACL (Shapes Constraint Language), published as a W3C Recommendation [296]

---

### 127. UML-Style Shapes Graph Notation

Shapes graphs for RDF validation can be depicted using UML-like class diagram notation, making constraint specifications more accessible to practitioners familiar with UML.

**Sources**:

- **02-Knowledge_Graphs (Chunk 2:262-264)**
  > Shapes graphs can be depicted as UML-like class diagrams, where Figure 13 illustrates an example of a shapes graph based on Figure 1

---

### 128. Linked Data Web Integration

Linked Data is a method for publishing structured data on the Web using RDF, enabling interlinking between distributed knowledge graphs through shared URIs.

**Sources**:

- **02-Knowledge_Graphs (Chunk 2:613-615)**
  > A prominent use-case for graph datasets is to manage and query Linked Data composed of interlinked documents of RDF graphs spanning the Web

---

### 129. XML Schema Datatypes (XSD)

XSD provides the datatype vocabulary for RDF literals, including string, integer, decimal, boolean, dateTime, and other standard datatypes with defined lexical representations.

**Sources**:

- **02-Knowledge_Graphs (Chunk 2:714-715)**
  > RDF utilises XML Schema Datatypes (XSD) [411], amongst others, where a datatype node is given as a pair (l,d)

---

### 130. DOI (Digital Object Identifiers)

DOI is a persistent identifier scheme used for scholarly papers and other digital objects, enabling stable references in knowledge graphs.

**Sources**:

- **02-Knowledge_Graphs (Chunk 2:565-566)**
  > Prominent examples of PID schemes include Digital Object Identifiers (DOIs) for papers, ORCID iDs for authors

---

### 131. PURL (Persistent URL) Services

PURL services provide redirect mechanisms for HTTP IRIs, enabling persistent identification even when resource locations change.

**Sources**:

- **02-Knowledge_Graphs (Chunk 2:642-644)**
  > Persistent URL (PURL) services offer redirects from a central server to a particular location, where the PURL can be redirected to a new location if necessary

---

### 132. RDF Lists for Ordered Data

RDF lists provide linked-list structures using blank nodes to encode ordered sequences, essential for representing property chains, enumerations, and other ordered data.

**Sources**:

- **02-Knowledge_Graphs (Chunk 2:800-801)**
  > Existential nodes are supported in RDF as blank nodes [111], which are also commonly used to support modelling complex elements in graphs, such as RDF lists [111, 247]

---

### 133. Time Ontology for Temporal Context

The W3C Time Ontology provides a standard vocabulary for describing temporal entities and relations in RDF graphs, enabling interoperable temporal reasoning.

**Sources**:

- **02-Knowledge_Graphs (Chunk 3:110-112)**
  > One example is the Time Ontology [107], which specifies how temporal entities, intervals, time instants, etc. and relations between them such as before, overlaps, etc. can be described in RDF

---

### 134. PROV Data Model for Provenance

Merged from 2 sources. PROV-DM is the W3C standard data model for provenance, describing how entities relate to activities and agents, enabling tracking of data lineage in knowledge graphs.

**Sources**:

- **02-Knowledge_Graphs (Chunk 3:114-118)**
  > Another example is the PROV Data Model [188], which specifies how provenance can be described in RDF graphs, where entities are derived from other entities, are generated and/or used by activities

- **02-Knowledge_Graphs (Chunk 7:715-716)**
  > the PROV Data Model [188] discussed in Section 3 allows for capturing detailed provenance

---

### 135. RDF Reification Pattern

RDF reification is a standard pattern for making statements about statements, using subject/predicate/object properties to describe edges, enabling metadata and context annotation.

**Sources**:

- **02-Knowledge_Graphs (Chunk 3:37-38)**
  > (a) RDF Reification

---

### 136. N-ary Relations Pattern

N-ary relations pattern connects source nodes directly to edge nodes with the original edge label, providing an alternative to RDF reification for representing complex relations.

**Sources**:

- **02-Knowledge_Graphs (Chunk 3:27)**
  > (b) n-ary Relations

---

### 137. Singleton Properties Pattern

Singleton properties use edge labels as individual nodes connected to their type via singleton relation, enabling edge-specific property assertions.

**Sources**:

- **02-Knowledge_Graphs (Chunk 3:41)**
  > (c) Singleton properties

---

### 138. RDF* Extension for Edge Metadata

RDF* is an RDF extension that allows edges to be treated as nodes directly, simplifying edge annotation without traditional reification overhead.

**Sources**:

- **02-Knowledge_Graphs (Chunk 3:17-18)**
  > Third, we can use RDF* [218] (Figure 19c): an extension of RDF that allows edges to be defined as nodes

---

### 139. Temporal RDF for Time Annotations

Temporal RDF extends RDF with time interval annotations on edges, enabling temporal validity tracking and time-based reasoning.

**Sources**:

- **02-Knowledge_Graphs (Chunk 3:111-113)**
  > Temporal RDF [210] allows for annotating edges with time intervals, such as Chile [2006,2010] M. Bachelet

---

### 140. Fuzzy RDF for Uncertainty

Fuzzy RDF extends RDF with fuzzy truth values on edges, enabling representation and reasoning with uncertain or vague knowledge.

**Sources**:

- **02-Knowledge_Graphs (Chunk 3:115-118)**
  > Fuzzy RDF [502] allows for annotating edges with a degree of truth such as Santiago 0.8 Semi-Arid, indicating that it is more-or-less true with a degree of 0.8

---

### 141. Annotated RDF with Semi-rings

Annotated RDF provides domain-independent context annotation using semi-ring algebraic structures, enabling formal reasoning about temporal, fuzzy, and other contextual annotations through meet and join operators.

**Sources**:

- **02-Knowledge_Graphs (Chunk 3:120-123)**
  > Annotated RDF [130, 530, 583] allows for representing various forms of context modelled as semi-rings: algebraic structures consisting of domain values and two main operators to combine domain values: meet and join

---

### 142. OWL 2 RL/RDF Rules

OWL 2 RL/RDF provides rule-based semantics for a subset of OWL features, enabling scalable reasoning through forward-chaining rules while remaining incomplete for some constructs.

**Sources**:

- **02-Knowledge_Graphs (Chunk 3:944-945)**
  > A more comprehensive set of rules for the OWL features of Tables 3-5 have been defined as OWL 2 RL/RDF [363]

---

### 143. Datalog for Graph Rules

Datalog provides a logical foundation for expressing inference rules over knowledge graphs, with extensions like Datalog+/- supporting existentials for more expressive ontological reasoning.

**Sources**:

- **02-Knowledge_Graphs (Chunk 3:934)**
  > Rules of this form correspond to (positive) Datalog [85] in databases, Horn clauses [323] in logic programming

---

### 144. Rete Networks for Materialization

Rete networks are pattern-matching algorithms that optimize rule execution during materialization, reducing redundant computation in knowledge graph reasoning.

**Sources**:

- **02-Knowledge_Graphs (Chunk 3:961)**
  > the efficiency and scalability of materialisation can be enhanced through optimisations like Rete networks [164]

---

### 145. MapReduce for Distributed Reasoning

MapReduce provides a distributed computing framework for scaling knowledge graph reasoning and materialization across large clusters.

**Sources**:

- **02-Knowledge_Graphs (Chunk 3:961)**
  > or using distributed frameworks like MapReduce [531]

---

### 146. OBOF (Open Biomedical Ontologies Format)

OBOF is an ontology format widely used in the biomedical domain, providing many features similar to OWL for representing biomedical knowledge structures.

**Sources**:

- **02-Knowledge_Graphs (Chunk 3:353-356)**
  > Amongst the most popular ontology languages used in practice are the Web Ontology Language (OWL) [239] [12], recommended by the W3C and compatible with RDF graphs; and the Open Biomedical Ontologies Format (OBOF) [366]

---

### 147. OWL 2 RL/RDF Rules for Ontological Entailment

OWL 2 RL/RDF is a standardized rule-based reasoning profile for OWL that enables practical reasoning over knowledge graphs. It provides rules for sub-class, sub-property, domain, and range features but cannot fully capture negation, existentials, universals, or counting features.

**Sources**:

- **02-Knowledge_Graphs (Chunk 4:45-49)**
  > A more comprehensive set of rules for the OWL features of Tables 3-5 have been defined as OWL 2 RL/RDF; these rules are likewise incomplete

---

### 148. Datalog for Rule-Based Reasoning

Datalog is a foundational rule language used in databases that supports if-then-style inference rules with body and head patterns. Positive Datalog rules encode deductive knowledge and enable automated entailment checking.

**Sources**:

- **02-Knowledge_Graphs (Chunk 4:35-36)**
  > Rules of this form correspond to (positive) Datalog [85] in databases, Horn clauses [323] in logic programming, etc.

---

### 149. Datalog Extensions (Datalog+-)

Datalog+- and Disjunctive Datalog extend basic Datalog to support existential quantification and disjunction in rule heads, enabling more expressive reasoning capabilities beyond standard Datalog limitations.

**Sources**:

- **02-Knowledge_Graphs (Chunk 4:51-52)**
  > Other rule languages have, however, been proposed to support additional such features, including existentials (see, e.g., Datalog [+-] [36]), disjunction (see, e.g., Disjunctive Datalog [449])

---

### 150. Horn Clauses in Logic Programming

Horn clauses from logic programming provide a formal foundation for inference rules in knowledge graphs, enabling if-then-style deductive reasoning with graph patterns.

**Sources**:

- **02-Knowledge_Graphs (Chunk 4:35)**
  > Rules of this form correspond to (positive) Datalog [85] in databases, Horn clauses [323] in logic programming, etc.

---

### 151. First Order Logic Theorem Provers

First Order Logic theorem provers can provide complete reasoning but may not halt on certain inputs. They offer an alternative to rules and Description Logics for ontological reasoning.

**Sources**:

- **02-Knowledge_Graphs (Chunk 4:10-11)**
  > Though option (3) has been explored using, e.g., theorem provers for First Order Logic [466], options (1) and (2) are more commonly pursued

---

### 152. Materialisation for Rule Reasoning

Materialisation is a reasoning strategy that recursively applies inference rules to a graph, adding derived conclusions until no new facts can be inferred. It is useful for pre-computing entailments for faster query answering.

**Sources**:

- **02-Knowledge_Graphs (Chunk 4:54-58)**
  > Materialisation refers to the idea of applying rules recursively to a graph, adding the conclusions generated back to the graph until a fixpoint is reached

---

### 153. Rete Networks for Materialisation Optimization

Rete networks provide an optimization technique for materialisation-based reasoning, improving efficiency and scalability of rule application over knowledge graphs.

**Sources**:

- **02-Knowledge_Graphs (Chunk 4:61-62)**
  > the efficiency and scalability of materialisation can be enhanced through optimisations like Rete networks [164], or using distributed frameworks like MapReduce [531]

---

### 154. Query Rewriting for Ontological Reasoning

Query rewriting is an alternative to materialisation that extends user queries at runtime to capture ontological entailments, avoiding the need to pre-compute and store all derived facts.

**Sources**:

- **02-Knowledge_Graphs (Chunk 4:92-95)**
  > Another strategy is to use rules for query rewriting, which given a query, will automatically extend the query in order to find solutions entailed by a set of rules

---

### 155. OWL 2 QL Profile for Query Rewriting

OWL 2 QL is a standardized OWL profile optimized for query rewriting, enabling efficient ontological reasoning over large datasets without materialisation.

**Sources**:

- **02-Knowledge_Graphs (Chunk 4:107-109)**
  > The OWL 2 QL profile [363] is a subset of OWL designed specifically for query rewriting of this form [21].

---

### 156. Tableau Methods for DL Satisfiability

Tableau methods are decision procedures for Description Logic satisfiability checking, constructing models through completion rules and branching for disjunction.

**Sources**:

- **02-Knowledge_Graphs (Chunk 4:195-199)**
  > Thereafter methods such as tableau can be used to check satisfiability, cautiously constructing models by completing them along similar lines to the materialisation strategy

---

### 157. Notation3 (N3) Rule Language

Notation3 (N3) is a language for expressing rules over graphs, enabling declarative specification of if-then inference rules independently or alongside ontology languages.

**Sources**:

- **02-Knowledge_Graphs (Chunk 4:122)**
  > Various languages allow for expressing rules over graphs... including: Notation3 (N3) [42], Rule Interchange Format (RIF) [288]

---

### 158. Rule Interchange Format (RIF)

Merged from 2 sources. W3C standard for exchanging rules between rule systems. Part of the semantic web stack enabling interoperability of rule-based reasoning across different systems.

**Sources**:

- **02-Knowledge_Graphs (Chunk 4:122)**
  > Various languages allow for expressing rules over graphs... including: Notation3 (N3) [42], Rule Interchange Format (RIF) [288]

- **02-Knowledge_Graphs (Chunk 14:313-317)**
  > rules can be directly specified in a rule language such as Notation3 (N3), Rule Interchange Format (RIF), Semantic Web Rule Language (SWRL)

---

### 159. Semantic Web Rule Language (SWRL)

SWRL combines OWL with RuleML, providing a rule language that extends OWL's expressivity for knowledge graph reasoning.

**Sources**:

- **02-Knowledge_Graphs (Chunk 4:123-124)**
  > Semantic Web Rule Language (SWRL) [254], and SPARQL Inferencing Notation (SPIN) [295].

---

### 160. SPARQL Inferencing Notation (SPIN)

Merged from 2 sources. Standard for representing SPARQL-based rules within RDF graphs, enabling rules to be embedded as data and queried/manipulated like other graph content.

**Sources**:

- **02-Knowledge_Graphs (Chunk 4:124)**
  > and SPARQL Inferencing Notation (SPIN) [295].

- **02-Knowledge_Graphs (Chunk 14:315-319)**
  > SPARQL Inferencing Notation (SPIN) [295]. Languages such as SPIN represent rules as graphs, allowing the rules of a knowledge graph to be embedded in the data graph

---

### 161. Description Logics (DLs)

Description Logics form a family of logics that provide the formal foundation for OWL, enabling decidable reasoning over knowledge graphs with expressivity/complexity trade-offs.

**Sources**:

- **02-Knowledge_Graphs (Chunk 4:127-134)**
  > Description Logics (DLs) were initially introduced as a way to formalise the meaning of frames [355] and semantic networks [426]... DLs thus hold an important place in the logical formalisation of knowledge graphs

---

### 162. OWL 2 DL Language Fragment

OWL 2 DL is a restricted OWL fragment ensuring decidable reasoning while maintaining high expressivity, forming the basis for practical knowledge graph ontologies.

**Sources**:

- **02-Knowledge_Graphs (Chunk 4:179-181)**
  > the OWL 2 DL language is a fragment of OWL restricted so that entailment becomes decidable

---

### 163. Apache Spark GraphX

Apache Spark GraphX is a distributed graph processing framework supporting large-scale analytics on knowledge graphs using a systolic abstraction for parallel computation.

**Sources**:

- **02-Knowledge_Graphs (Chunk 4:498-499)**
  > Various frameworks have been proposed for large-scale graph analytics, often in a distributed (cluster) setting. Amongst these we can mention Apache Spark (GraphX) [119, 563]

---

### 164. GraphLab

GraphLab is a distributed machine learning framework for graph analytics supporting iterative vertex-centric computation on large-scale graphs.

**Sources**:

- **02-Knowledge_Graphs (Chunk 4:499-500)**
  > GraphLab [326], Pregel [335], Signal-Collect [503]

---

### 165. Pregel Graph Processing Framework

Pregel is Google's graph processing framework implementing a Bulk Synchronous Parallel model for distributed graph analytics with message passing between vertices.

**Sources**:

- **02-Knowledge_Graphs (Chunk 4:500)**
  > Pregel [335], Signal-Collect [503]

---

### 166. Signal-Collect Framework

Signal-Collect is a distributed graph processing framework based on signal-collect abstraction where vertices signal neighboring vertices and collect incoming signals.

**Sources**:

- **02-Knowledge_Graphs (Chunk 4:500)**
  > Signal-Collect [503]

---

### 167. Shark Distributed Analytics

Shark is a distributed analytics system enabling SQL-like queries over large-scale graph data.

**Sources**:

- **02-Knowledge_Graphs (Chunk 4:500)**
  > Shark [564], etc.

---

### 168. MapReduce for Graph Materialisation

MapReduce can be used to scale materialisation-based reasoning, distributing rule application across a cluster for large knowledge graphs.

**Sources**:

- **02-Knowledge_Graphs (Chunk 4:62)**
  > or using distributed frameworks like MapReduce [531]

---

### 169. Systolic Abstraction for Graph Computing

The systolic abstraction provides the computational model underlying graph parallel frameworks, with nodes as processors and edges as communication channels for iterative computation.

**Sources**:

- **02-Knowledge_Graphs (Chunk 4:501-504)**
  > These graph parallel frameworks apply a systolic abstraction [304] based on a directed graph, where nodes are processors that can send messages to other nodes along edges

---

### 170. PageRank Algorithm

PageRank is a graph centrality algorithm computing node importance based on random walk probabilities, applicable for ranking entities in knowledge graphs.

**Sources**:

- **02-Knowledge_Graphs (Chunk 4:519-522)**
  > A good way to measure this is using centrality, where we choose PageRank [391], which computes the probability of a tourist randomly following the routes shown in the graph being at a particular place

---

### 171. TransE Translational Embedding Model

TransE is a foundational translational embedding model that interprets edge labels as transformations translating subject entity embeddings to object entity embeddings in vector space.

**Sources**:

- **02-Knowledge_Graphs (Chunk 5:12-14)**
  > The most elementary approach in this family is TransE [63]. Over all positive edges [s] p -, TransE learns vectors es, rp, and eo aiming to make es + rp as close as possible to eo

---

### 172. TransH Hyperplane Translation

TransH extends TransE by projecting entities onto relation-specific hyperplanes before translation, handling complex relation patterns better.

**Sources**:

- **02-Knowledge_Graphs (Chunk 5:38-42)**
  > TransH [553] represents different relations using distinct hyperplanes, where for the edge [s] p -, [s] is first projected onto the hyperplane of p before the translation to [o] is learnt

---

### 173. TransR Relation-Specific Projection

TransR projects entities into relation-specific vector spaces using learned projection matrices, enabling distinct representations for different relation types.

**Sources**:

- **02-Knowledge_Graphs (Chunk 5:42-44)**
  > TransR [318] generalises this approach by projecting [s] and [o] into a vector space specific to p, which involves multiplying the entity embeddings for [s] and [o] by a projection matrix specific to p

---

### 174. TransD Simplified Projection

TransD simplifies TransR by using secondary vectors for projection instead of full matrices, reducing parameters while maintaining expressivity.

**Sources**:

- **02-Knowledge_Graphs (Chunk 5:44-47)**
  > TransD [271] simplifies TransR by associating entities and relations with a second vector, where these secondary vectors are used to project the entity into a relation-specific vector space

---

### 175. RotatE Complex Space Embeddings

RotatE models relations as rotations in complex vector space, capturing symmetric, antisymmetric, inverse, and compositional relation patterns.

**Sources**:

- **02-Knowledge_Graphs (Chunk 5:48-52)**
  > RotatE [511] proposes translational embeddings in complex space, which allows to capture more characteristics of relations, such as direction, symmetry, inversion, antisymmetry, and composition

---

### 176. MuRP Hyperbolic Space Embeddings

MuRP embeds knowledge graphs in hyperbolic space (Poincare ball model), providing better separation for hierarchical structures than Euclidean embeddings.

**Sources**:

- **02-Knowledge_Graphs (Chunk 5:54-58)**
  > MuRP [29] uses relation embeddings that transform entity embeddings in the hyperbolic space of the Poincare ball model, whose curvature provides more 'space' to separate entities

---

### 177. DistMult Tensor Decomposition

DistMult applies tensor decomposition to knowledge graphs, learning entity and relation vectors that maximize plausibility scores through element-wise vector multiplication.

**Sources**:

- **02-Knowledge_Graphs (Chunk 5:194-202)**
  > DistMult [568] is a seminal method for computing knowledge graph embeddings based on rank decompositions, where each entity and relation is associated with a vector of dimension d

---

### 178. RESCAL Matrix Relation Embeddings

RESCAL uses matrices rather than vectors for relation embeddings, enabling capture of edge direction and more complex relation patterns.

**Sources**:

- **02-Knowledge_Graphs (Chunk 5:207-211)**
  > RESCAL [386] uses a matrix, which allows for combining values from es and eo across all dimensions, and thus can capture (e.g.) edge direction

---

### 179. HolE Circular Correlation

HolE uses circular correlation to combine entity embeddings, capturing asymmetric relations while maintaining vector-based efficiency.

**Sources**:

- **02-Knowledge_Graphs (Chunk 5:211-216)**
  > HolE [385] uses vectors for relation and entity embeddings, but proposes to use the circular correlation operator - which takes sums along the diagonals of the outer product of two vectors

---

### 180. ComplEx Complex Vector Embeddings

ComplEx uses complex-valued vectors for embeddings, breaking symmetry limitations of real-valued approaches while keeping parameters low.

**Sources**:

- **02-Knowledge_Graphs (Chunk 5:216-221)**
  > ComplEx [526], on the other hand, uses a complex vector (i.e., a vector containing complex numbers) as a relational embedding, which similarly allows for breaking the aforementioned symmetry

---

### 181. SimplE CP Decomposition

SimplE applies standard Canonical Polyadic (CP) decomposition to knowledge graphs, computing averaged entity vectors from decomposed tensors.

**Sources**:

- **02-Knowledge_Graphs (Chunk 5:222-225)**
  > SimplE [283] rather proposes to compute a standard CP decomposition computing two initial vectors for entities from X and Z and then averaging terms

---

### 182. TuckER Tucker Decomposition

TuckER applies Tucker decomposition to knowledge graphs, achieving state-of-the-art results on standard benchmarks through a core tensor approach.

**Sources**:

- **02-Knowledge_Graphs (Chunk 5:225-237)**
  > TuckER [30] employs a different type of decomposition - called a Tucker Decomposition [528], which computes a smaller 'core' tensor T and a sequence of three matrices A, B and C

---

### 183. Canonical Polyadic (CP) Decomposition

CP decomposition decomposes a tensor into sum of rank-1 tensors, providing a foundational technique for tensor-based knowledge graph embeddings.

**Sources**:

- **02-Knowledge_Graphs (Chunk 5:103)**
  > rank decomposition of tensors; this method is called Canonical Polyadic (CP) decomposition [236].

---

### 184. Semantic Matching Energy (SME)

SME is an early neural model for knowledge graph embeddings using parameterized functions to compute plausibility scores via dot product.

**Sources**:

- **02-Knowledge_Graphs (Chunk 5:248-251)**
  > One of the earliest proposals of a neural model was Semantic Matching Energy (SME) [192], which learns parameters (aka weights: w, w') for two functions

---

### 185. Neural Tensor Networks (NTN)

NTN uses a weight tensor to compute plausibility scores through outer products of entity embeddings combined with neural layers, though limited in scalability.

**Sources**:

- **02-Knowledge_Graphs (Chunk 5:251-257)**
  > Neural Tensor Networks (NTN) [488], which rather proposes to maintain a tensor W of internal weights, such that the plausibility score is computed by a complex function

---

### 186. Multi Layer Perceptron (MLP) for KG

MLP-based embedding models concatenate entity and relation vectors and pass through hidden layers for plausibility scoring, offering simpler architecture than NTN.

**Sources**:

- **02-Knowledge_Graphs (Chunk 5:257-258)**
  > Multi Layer Perceptron (MLP) [131] is a simpler model, where es, rp and eo are concatenated and fed into a hidden layer to compute the plausibility score.

---

### 187. ConvE Convolutional Embeddings

Merged from 2 sources. HypER uses a hypernetwork to generate relation-specific convolutional filters applied directly to entity embeddings, outperforming ConvE on benchmarks.

**Sources**:

- **02-Knowledge_Graphs (Chunk 5:260-268)**
  > ConvE [127] proposes to generate a matrix from es and rp by 'wrapping' each vector over several rows and concatenating both matrices. The concatenated matrix serves as the input for a set of (2D) convolutional layers

- **02-Knowledge_Graphs (Chunk 5:272-280)**
  > HypER [28] is a similar model using convolutions, but avoids the need to wrap vectors into matrices. Instead, a fully connected layer (called the 'hypernetwork') is applied to rp

---

### 188. Graph Neural Networks (GNNs)

GNNs build neural network architectures reflecting knowledge graph topology, enabling end-to-end supervised learning for node classification and graph-level tasks.

**Sources**:

- **02-Knowledge_Graphs (Chunk 5:427-429)**
  > A graph neural network (GNN) [462] builds a neural network based on the topology of the data graph; i.e., nodes are connected to their neighbours per the data graph

---

### 189. Recursive Graph Neural Networks (RecGNNs)

RecGNNs recursively pass messages between neighboring nodes until fixpoint, learning transition and output functions from supervised node labels.

**Sources**:

- **02-Knowledge_Graphs (Chunk 5:452-455)**
  > Recursive graph neural networks (RecGNNs) are the seminal approach to graph neural networks [462, 493]. The approach is conceptually similar to the systolic abstraction

---

### 190. Convolutional Graph Neural Networks (ConvGNNs)

Merged from 2 sources. Graph neural network architecture applying convolutional operators over graph neighborhoods. Supports multi-layer aggregation with learnable parameters for node classification and link prediction.

**Sources**:

- **02-Knowledge_Graphs (Chunk 5:613-615)**
  > a number of convolutional graph neural networks (ConvGNNs) [71, 289, 559] have been proposed, where the transition function is implemented by means of convolutions

- **02-Knowledge_Graphs (Chunk 15:502-506)**
  > When the aggregation functions are based on a convolutional operator, we call the result a convolutional graph neural network (ConvGNN)

---

### 191. Attention Mechanism for GNNs

Attention mechanisms enable GNNs to learn which neighboring nodes are most relevant, addressing varying neighborhood sizes and structures.

**Sources**:

- **02-Knowledge_Graphs (Chunk 5:631-633)**
  > An alternative is to use an attention mechanism [535] to learn the nodes whose features are most important to the current node.

---

### 192. word2vec Language Embeddings

word2vec computes word embeddings using neural networks (continuous bag of words or skip-gram), foundational for language-based graph embedding approaches.

**Sources**:

- **02-Knowledge_Graphs (Chunk 5:296-301)**
  > word2vec [352] and GloVe [408] being two seminal approaches. Both approaches compute embeddings for words based on large corpora of text such that words used in similar contexts have similar vectors

---

### 193. GloVe Word Embeddings

GloVe computes word embeddings via regression on word co-occurrence matrices, providing an alternative to neural approaches for embedding generation.

**Sources**:

- **02-Knowledge_Graphs (Chunk 5:306-308)**
  > GloVe rather applies a regression model over a matrix of co-occurrence probabilities of word pairs.

---

### 194. RDF2Vec Graph-to-Language Embeddings

RDF2Vec generates sentences from random walks on knowledge graphs and applies word2vec, enabling transfer of language embedding techniques to graphs.

**Sources**:

- **02-Knowledge_Graphs (Chunk 5:316-328)**
  > RDF2Vec [441] performs (biased [95]) random walks on the graph and records the paths (the sequence of nodes and edge labels traversed) as 'sentences', which are then fed as input into the word2vec model

---

### 195. KGloVe PageRank-Based Embeddings

KGloVe adapts GloVe for knowledge graphs using personalized PageRank to determine node relatedness instead of text co-occurrence.

**Sources**:

- **02-Knowledge_Graphs (Chunk 5:332-338)**
  > KGloVe [96] is based on the GloVe model. Much like how the original GloVe model considers words that co-occur frequently in windows of text to be more related, KGloVe uses personalised PageRank

---

### 196. AMIE Rule Mining System

Merged from 2 sources. AMIE is an influential rule mining system that discovers high-confidence rules from knowledge graphs using Partial Completeness Assumption and top-down rule refinement.

**Sources**:

- **02-Knowledge_Graphs (Chunk 5:773-775)**
  > An influential rule-mining system for graphs is AMIE [169, 170], which adopts the PCA measure of confidence, and builds rules in a top-down fashion

- **17-KG_Reasoning (Chunk 1:444)**
  > Conventional methods like AMIE [Galarraga et al., 2015] and AnyBURL [Meilicke et al., 2019] are symbolic-based

---

### 197. AMIE+ Optimized Rule Mining

AMIE+ provides optimizations over AMIE for more efficient rule mining through improved pruning and indexing strategies.

**Sources**:

- **02-Knowledge_Graphs (Chunk 5:827)**
  > For further discussion of possible optimisations based on pruning and indexing, we refer to the paper on AMIE+ [169].

---

### 198. RuLES Non-Monotonic Rule Learning

RuLES learns non-monotonic rules and extends confidence measures using knowledge graph embedding plausibility scores for edges not in the graph.

**Sources**:

- **02-Knowledge_Graphs (Chunk 5:845-848)**
  > The RuLES system [241] - which is also capable of learning non-monotonic rules - proposes to mitigate the limitations of the PCA heuristic by extending the confidence measure

---

### 199. CARL Cardinality-Aware Rule Learning

CARL enhances rule mining by incorporating relation cardinality constraints to better identify negative examples and compute rule confidence.

**Sources**:

- **02-Knowledge_Graphs (Chunk 5:855-858)**
  > CARL [406] exploits additional knowledge about the cardinalities of relations to refine the set of negative examples and the confidence measure for candidate rules.

---

### 200. NeuralLP Differentiable Rule Mining

NeuralLP applies differentiable rule mining using attention mechanisms to learn path-like rules with variable-length edge label sequences and confidence scores.

**Sources**:

- **02-Knowledge_Graphs (Chunk 5:896-899)**
  > NeuralLP [569] uses an attention mechanism to select a variable-length sequence of edge labels for path-like rules of the form [?x] p1 ?y1 p2 ... pn ?yn pn+1 ?z => [?x] p ?z

---

### 201. DRUM Recurrent Neural Rule Mining

DRUM uses bidirectional RNNs for differentiable rule mining, learning relation sequences that form path-like rules.

**Sources**:

- **02-Knowledge_Graphs (Chunk 5:901-908)**
  > DRUM [455] also learns path-like rules, where... the system uses bidirectional recurrent neural networks (a popular technique for learning over sequential data) to learn sequences of relations for rules

---

### 202. DL-Learner Axiom Mining

DL-Learner mines Description Logic axioms through class learning, finding logical class descriptions that separate positive from negative node examples.

**Sources**:

- **02-Knowledge_Graphs (Chunk 6:59-61)**
  > A prominent such system is DL-Learner [73], which is based on algorithms for class learning (aka concept learning), whereby given a set of positive nodes and negative nodes

---

### 203. T-Norm Fuzzy Logics for Embeddings

T-norm fuzzy logics provide a mechanism to integrate rules with embeddings, computing updated plausibility scores based on rule application.

**Sources**:

- **02-Knowledge_Graphs (Chunk 5:357-366)**
  > KALE [207] computes entity and relation embeddings using a translational model (specifically TransE) that is adapted to further consider rules using t-norm fuzzy logics

---

### 204. Named Entity Recognition (NER)

NER identifies named entities in text for knowledge graph construction, using lexical features, gazetteers, and supervised/bootstrapping approaches.

**Sources**:

- **02-Knowledge_Graphs (Chunk 6:205-209)**
  > The NER task identifies mentions of named entities in a text [369, 434], typically targetting mentions of people, organisations, locations, and potentially other types

---

### 205. Entity Linking (EL)

Entity Linking connects text mentions to knowledge graph nodes, handling mention ambiguity and entity aliases through disambiguation.

**Sources**:

- **02-Knowledge_Graphs (Chunk 6:233-238)**
  > The EL task associates mentions of entities in a text with the existing nodes of a target knowledge graph, which may be the nucleus of a knowledge graph under creation

---

### 206. Relation Extraction (RE)

Relation Extraction identifies relations between entities in text for knowledge graph enrichment, supporting binary, n-ary, and open information extraction.

**Sources**:

- **02-Knowledge_Graphs (Chunk 6:274-277)**
  > The RE task extracts relations between entities in the text [24, 582]. The simplest case is that of extracting binary relations in a closed setting wherein a fixed set of relation types are considered

---

### 207. Open Information Extraction (OIE)

OIE extracts relations without predefined relation types, deriving relations from dependency parse trees and requiring subsequent alignment with knowledge graph vocabularies.

**Sources**:

- **02-Knowledge_Graphs (Chunk 6:292-296)**
  > Binary RE can also be applied using unsupervised methods in an open setting - often referred to as Open Information Extraction (OIE) [31, 149, 150, 341, 342, 357]

---

### 208. Frame Semantics for N-ary RE

Frame semantics (FrameNet) provides semantic frames for verbs enabling n-ary relation extraction with contextual elements like time, place, and purpose.

**Sources**:

- **02-Knowledge_Graphs (Chunk 6:306-314)**
  > Various methods for n-ary RE are based on frame semantics [159], which, for a given verb... captures the entities involved and how they may be interrelated

---

### 209. FrameNet Semantic Frames

FrameNet defines semantic frames associating verbs with participant roles and optional context elements for n-ary relation extraction.

**Sources**:

- **02-Knowledge_Graphs (Chunk 6:309-314)**
  > Resources such as FrameNet [26] then define frames for words, such as to identify that the semantic frame for 'named' includes a speaker (the person naming something), an entity (the thing named) and a name

---

### 210. Discourse Representation Theory (DRT)

DRT provides logical text representation based on existential events, supporting neo-Davidsonian formulas analogous to reification for n-ary relations.

**Sources**:

- **02-Knowledge_Graphs (Chunk 6:316-324)**
  > Other RE methods are rather based on Discourse Representation Theory (DRT) [278], which considers a logical representation of text based on existential events

---

### 211. WordNet Lexical Database

Merged from 2 sources. Large lexical database improved using DOLCE ontological analysis. Individual/class distinction added based on foundational ontology insights.

**Sources**:

- **02-Knowledge_Graphs (Chunk 6:197-198)**
  > linking words with a lexicon of senses (e.g., WordNet [353] or BabelNet [373])

- **05-DOLCE (Chunk 1:459-460)**
  > DOLCE, used to reorganize the WordNet top level and causing Princeton WordNet developers to include the individual/class distinction in their lexicon

---

### 212. BabelNet Multilingual Knowledge Graph

BabelNet combines WordNet and Wikipedia for multilingual lexical knowledge, supporting word sense disambiguation and entity linking.

**Sources**:

- **02-Knowledge_Graphs (Chunk 6:198)**
  > (e.g., WordNet [353] or BabelNet [373])

---

### 213. R2RML RDB-to-RDF Mapping Language

R2RML is a W3C standard for declarative mapping from relational databases to RDF graphs, supporting custom mappings with templates and SQL queries.

**Sources**:

- **02-Knowledge_Graphs (Chunk 6:649-654)**
  > A standard language along these lines is the RDB2RDF Mapping Language (R2RML) [118], which allows for mapping from individual rows of a table to one or more custom edges

---

### 214. Direct Mapping for Tables to RDF

W3C Direct Mapping automatically converts relational tables to RDF graphs, creating edges from rows with table-encoded node identifiers.

**Sources**:

- **02-Knowledge_Graphs (Chunk 6:597-599)**
  > A direct mapping automatically generates a graph from a table. We present in Figure 34 the result of a standard direct mapping [20]

---

### 215. CSV/Tabular Data Mapping to RDF

W3C standards extend direct mapping for CSV and tabular data, allowing specification of metadata like keys and datatypes as part of the mapping.

**Sources**:

- **02-Knowledge_Graphs (Chunk 6:625-629)**
  > Another direct mapping has been defined for CSV and other tabular data [516] that further allows for specifying column names, primary/foreign keys, and datatypes

---

### 216. GRDDL XML-to-RDF Mapping

GRDDL is a W3C standard for extracting RDF graphs from XML documents using transformations.

**Sources**:

- **02-Knowledge_Graphs (Chunk 6:710-711)**
  > the GRDDL standard [99] allows for mapping from XML to (RDF) graphs

---

### 217. JSON-LD JSON-to-RDF Mapping

JSON-LD is a W3C standard for embedding RDF in JSON, enabling JSON data to be interpreted as linked data graphs.

**Sources**:

- **02-Knowledge_Graphs (Chunk 6:711-712)**
  > the JSON-LD standard [494] allows for mapping from JSON to (RDF) graphs

---

### 218. XSPARQL Hybrid Query Language

XSPARQL integrates XQuery and SPARQL for querying XML and RDF together, supporting both materialisation and virtualisation approaches.

**Sources**:

- **02-Knowledge_Graphs (Chunk 6:713-716)**
  > hybrid query languages such as XSPARQL [47] allow for querying XML and RDF in an integrated fashion, thus supporting both materialisation and virtualisation

---

### 219. Ontology-Based Data Access (OBDA)

OBDA provides query rewriting approaches that translate graph queries to SQL while supporting ontological entailments, including recursive entailments.

**Sources**:

- **02-Knowledge_Graphs (Chunk 6:689-695)**
  > The area of Ontology-Based Data Access (OBDA) [561] is then concerned with QR approaches that support ontological entailments

---

### 220. Extract-Transform-Load (ETL)

ETL processes materialise knowledge graphs by transforming and loading source data according to defined mappings.

**Sources**:

- **02-Knowledge_Graphs (Chunk 6:673-677)**
  > Once the mappings have been defined, one option is to use them to materialise graph data following an Extract-Transform-Load (ETL) approach

---

### 221. Shape Expressions for Validation

Shape expressions provide constraint languages for validating knowledge graphs, defining expected patterns and cardinalities for node types.

**Sources**:

- **02-Knowledge_Graphs (Chunk 7:179-181)**
  > Validity means that the knowledge graph is free of constraint violations, such as captured by shape expressions [521] (see Section 3.1.2)

---

### 222. FAIR Principles for Data Publication

FAIR (Findable, Accessible, Interoperable, Reusable) provides principles for publishing knowledge graphs to maximize re-use and machine-readability.

**Sources**:

- **02-Knowledge_Graphs (Chunk 7:621-629)**
  > The FAIR Principles were originally proposed in the context of publishing scientific data [556]... FAIR itself is an acronym for four foundational principles

---

### 223. Linked Data Principles

Linked Data Principles (use IRIs, HTTP lookups, standard formats, include links) provide technical implementation for FAIR publishing of knowledge graphs.

**Sources**:

- **02-Knowledge_Graphs (Chunk 7:731-738)**
  > the Linked Data Principles, proposed by Berners-Lee [41], which provide a technical basis for one possible way in which these FAIR Principles can be achieved

---

### 224. Vocabulary of Interlinked Datasets (VoID)

VoID provides vocabulary for describing dataset metadata, supporting Findability principle of FAIR through structured dataset descriptions.

**Sources**:

- **02-Knowledge_Graphs (Chunk 7:703-705)**
  > resources such as the Vocabulary of Interlinked Datasets (VoID) [11] allow for representing meta-data about graphs

---

### 225. Cypher Query Language

Cypher is the declarative query language for property graphs (Neo4j), supporting graph pattern matching and shortest path finding.

**Sources**:

- **02-Knowledge_Graphs (Chunk 4:686-688)**
  > Query languages such as SPARQL [217], Cypher [165], and G-CORE [15] allow for outputting graphs, where such queries can be used to select sub-graphs for analysis

---

### 226. G-CORE Graph Query Language

G-CORE is a property graph query language supporting graph construction, composable queries, and paths as first-class citizens.

**Sources**:

- **02-Knowledge_Graphs (Chunk 4:687)**
  > Query languages such as SPARQL [217], Cypher [165], and G-CORE [15]

---

### 227. Gremlin Traversal Language

Gremlin is an imperative graph traversal language enabling programmatic navigation and manipulation of property graphs.

**Sources**:

- **02-Knowledge_Graphs (Chunk 4:718)**
  > While one could solve this task using an imperative language such as Gremlin [445], GraphX [563], or R [518]

---

### 228. Triple Pattern Fragments

Triple Pattern Fragments provide a lightweight access protocol returning solutions for single edge patterns, enabling client-side query processing.

**Sources**:

- **02-Knowledge_Graphs (Chunk 8:65-70)**
  > Edge patterns - also known as triple patterns in the case of directed, edge-labelled graphs - are singleton graph patterns, i.e., graph patterns with a single edge

---

### 229. HTTP Dereferencing for Linked Data

HTTP dereferencing enables node lookups in Linked Data by returning RDF descriptions when HTTP IRIs are accessed.

**Sources**:

- **02-Knowledge_Graphs (Chunk 8:30-34)**
  > Such a protocol is the basis for the Linked Data principles outlined previously, where node lookups are implemented through HTTP dereferencing

---

### 230. ODRL Rights Language

ODRL is a W3C standard for expressing digital rights as graphs, supporting machine-readable licenses with permissions, duties, and prohibitions.

**Sources**:

- **02-Knowledge_Graphs (Chunk 8:188-192)**
  > the W3C Open Digital Rights Language (ODRL) [261] provides an information model and related vocabularies that can be used to specify permissions, duties, and prohibitions

---

### 231. WebAccessControl (WAC)

WAC provides access control for graphs using WebID authentication and vocabulary for specifying access policies.

**Sources**:

- **02-Knowledge_Graphs (Chunk 8:217-218)**
  > WebAccessControl (WAC) [31] is an access control framework for graphs that uses WebID for authentication

---

### 232. DALICC License Clearance

DALICC provides tools for license composition and compatibility verification, supporting Apache, Creative Commons, and BSD license families.

**Sources**:

- **02-Knowledge_Graphs (Chunk 8:210-214)**
  > the Data Licenses Clearance Center (DALICC), which includes a library of standard machine readable licenses, and tools that enable users both to compose arbitrary custom licenses and also to verify the compatibility

---

### 233. CryptOntology for Encryption Metadata

CryptOntology provides vocabulary for embedding encryption metadata (algorithm, key-length) within knowledge graphs for fine-grained access control.

**Sources**:

- **02-Knowledge_Graphs (Chunk 8:282-286)**
  > The CryptOntology [182] can further be used to embed details about the encryption mechanism used within the knowledge graph

---

### 234. K-Anonymity for Graphs

K-anonymity suppresses quasi-identifiers in graphs to ensure individuals cannot be distinguished from k-1 others, protecting privacy.

**Sources**:

- **02-Knowledge_Graphs (Chunk 8:324-328)**
  > Approaches to apply k-anonymity on graphs identify and suppress 'quasi-identifiers' that would allow a given individual to be distinguished from fewer than k-1 other individuals

---

### 235. K-Degree Anonymity

K-degree anonymity modifies graphs so each node has at least k-1 other nodes with the same degree, preventing degree-based deanonymisation.

**Sources**:

- **02-Knowledge_Graphs (Chunk 8:354-358)**
  > k-degree anonymity [321], which ensures that individuals cannot be deanonymised by attackers with knowledge of the degree of particular individuals

---

### 236. K-Isomorphic Neighbour Anonymity

K-isomorphic neighbour anonymity ensures each node has k-1 other nodes with isomorphic neighborhoods, preventing neighborhood-based attacks.

**Sources**:

- **02-Knowledge_Graphs (Chunk 8:360-367)**
  > k-isomorphic neighbour anonymity [580], avoids neighbourhood attacks where an attacker knows how an individual is connected to nodes in their neighbourhood

---

### 237. K-Automorphism for Graph Anonymity

K-automorphism provides strongest structural anonymity guarantee, making every node structurally indistinguishable from k-1 others.

**Sources**:

- **02-Knowledge_Graphs (Chunk 8:370-374)**
  > k-automorphism [584], which ensures that for every node, it is structurally indistinguishable from k-1 other nodes, thus avoiding any attack based on structural information

---

### 238. Differential Privacy for Graph Queries

Epsilon-differential privacy adds noise to query results to prevent individual identification, controlling privacy-utility tradeoff via epsilon parameter.

**Sources**:

- **02-Knowledge_Graphs (Chunk 8:395-400)**
  > One approach is to apply epsilon-differential privacy [137] for querying graphs [483]. Such mechanisms are typically used for aggregate (e.g., count) queries, where noise is added

---

### 239. DBpedia Knowledge Graph

Merged from 2 sources. Structured knowledge graph extracted from Wikipedia. DOLCE+DUL used to detect and fix ontological inconsistencies and discover modeling anti-patterns.

**Sources**:

- **02-Knowledge_Graphs (Chunk 8:458-476)**
  > The DBpedia project was developed to extract a graph-structured representation of the semi-structured data embedded in Wikipedia articles [22]

- **05-DOLCE (Chunk 1:457-458)**
  > identifying and fixing millions of inconsistencies in DBpedia, on-the-go discovering modelling anti-patterns that were completely opaque to the axioms of the DBpedia ontology

---

### 240. YAGO Knowledge Graph

YAGO combines Wikipedia extraction with WordNet hierarchies, supporting reification, n-ary relations, and data types in its RDFS-based model.

**Sources**:

- **02-Knowledge_Graphs (Chunk 8:505-529)**
  > YAGO likewise extracts graph-structured data from Wikipedia, which are then unified with the hierarchical structure of WordNet

---

### 241. Freebase Knowledge Graph

Merged from 4 sources. Freebase is referenced as one of the major knowledge graph implementations used for KG reasoning benchmarks and research. It serves as a standard dataset for evaluating KGQA methods.

**Sources**:

- **02-Knowledge_Graphs (Chunk 8:544-570)**
  > Freebase was a general collection of human knowledge that aimed to address some of the large scale information integration problems

- **02-Knowledge_Graphs (Chunk 8:579-621)**
  > The Wikimedia Foundation thus uses Wikidata as a centralised, collaboratively edited knowledge graph to supply Wikipedia

- **16-KG-Agent (Chunk 1:235)**
  > reasoning over KG (e.g., Freebase or Wikidata) typically requires three fundamental operations

- **16-KG-Agent (Chunk 1:235)**
  > reasoning over KG (e.g., Freebase or Wikidata) typically requires three fundamental operations

---

### 242. SKOS Knowledge Organization

SKOS provides vocabulary for representing knowledge organization systems like taxonomies and thesauri within knowledge graphs.

**Sources**:

- **02-Knowledge_Graphs (Chunk 8:493-495)**
  > These schemata include a Simple Knowledge Organization System (SKOS) representation of Wikipedia categories

---

### 243. UMBEL Ontology Categorisation

UMBEL provides upper-level ontology categories for classifying entities in knowledge graphs like DBpedia.

**Sources**:

- **02-Knowledge_Graphs (Chunk 8:496-497)**
  > an Upper Mapping and Binding Exchange Layer (UMBEL) ontology categorisation schema

---

### 244. SHACL Shapes Validation

SHACL (Shapes Constraint Language) for validating knowledge graphs against shapes schemas. Supports partial shapes maps with three-valued logic, path counting constraints, and SAT solver-based validation for recursive cases.

**Sources**:

- **02-Knowledge_Graphs (Chunk 14:27-39)**
  > The semantics we present here assumes that each node in the graph either satisfies or does not satisfy each shape labelled by the schema

---

### 245. Kleene Three-Valued Logic for Shapes

Extension of shapes validation semantics using three-valued logic to support partial shapes maps where satisfaction of some nodes for some shapes can be left undefined.

**Sources**:

- **02-Knowledge_Graphs (Chunk 14:29-31)**
  > More complex semantics - for example, based on Kleene's three-valued logic [104, 305] - have been proposed that support partial shapes maps

---

### 246. Quotient Graph Emergent Schema

Emergent schema discovery through quotient graphs that partition nodes based on equivalence relations (e.g., same types) while preserving edge topology through simulation and bisimulation relations.

**Sources**:

- **02-Knowledge_Graphs (Chunk 14:42-74)**
  > Emergent schemata are often based on the notion of a quotient graph... a quotient graph can merge multiple nodes into one node

---

### 247. Annotation Domain Semi-Ring

Formal algebraic structure for graph annotations supporting context metadata. Enables reasoning and querying over annotated graphs with well-defined operations for temporal, trust, or other domain-specific contexts.

**Sources**:

- **02-Knowledge_Graphs (Chunk 14:113-158)**
  > An annotation domain is defined as an idempotent, commutative semi-ring D = <A, plus, times, bottom, top>

---

### 248. Graph Interpretation Semantics

Formal model-theoretic semantics for knowledge graphs supporting both Unique Name Assumption (UNA) and no-UNA interpretations, enabling formal reasoning about graph entailment.

**Sources**:

- **02-Knowledge_Graphs (Chunk 14:186-198)**
  > A (graph) interpretation I is defined as a pair I := (Gamma, dot_I) where Gamma = (V_Gamma, E_Gamma, L_Gamma) is a graph called the domain graph

---

### 249. SWRL Semantic Web Rule Language

Rule language combining OWL with RuleML to express Horn-like rules for web ontologies. Implemented by semantic reasoners like HermiT for automated inference.

**Sources**:

- **02-Knowledge_Graphs (Chunk 14:315)**
  > Semantic Web Rule Language (SWRL) [254]

---

### 250. Description Logic Knowledge Base

Standard structure for Description Logic ontologies with three components: A-Box (instance data), T-Box (terminological/class definitions), R-Box (role/relation definitions). Forms the basis for OWL reasoning.

**Sources**:

- **02-Knowledge_Graphs (Chunk 14:335-344)**
  > A DL knowledge base K is defined as a tuple (A, T, R), where A is the A-Box: a set of assertional axioms; T is the T-Box: a set of class axioms; and R is the R-Box: a set of relation axioms

---

### 251. OWL 2 DL SROIQ

OWL 2 DL profile corresponds to the SROIQ description logic, providing decidable reasoning with features including transitive closure, complex role inclusions, nominals, inverse relations, and qualified number restrictions.

**Sources**:

- **02-Knowledge_Graphs (Chunk 14:467-469)**
  > DLs have been very influential in the definition of OWL, where the OWL 2 DL fragment (roughly) corresponds to the DL SROIQ

---

### 252. ALC Description Logic Base

Foundation description logic providing core expressivity for ontology languages. Extended by S (transitivity), H (role inclusion), R (complex roles), O (nominals), I (inverse), and N/Q (number restrictions).

**Sources**:

- **02-Knowledge_Graphs (Chunk 14:426-430)**
  > ALC (Attributive Language with Complement), supports atomic classes, the top and bottom classes, class intersection, class union, class negation, universal restrictions and existential restrictions

---

### 253. Knowledge Graph Embeddings

Neural representation methods for knowledge graphs including TransE, RotatE, ComplEx, DistMult, SimplE, TuckER. Support plausibility scoring for link prediction with varying trade-offs between parameters and expressiveness.

**Sources**:

- **02-Knowledge_Graphs (Chunk 15:265-282)**
  > Other embedding techniques - namely RotatE [511] and ComplEx [526] - uses complex space based on complex numbers

---

### 254. Graph Parallel Framework (GPF)

Computational framework for parallel graph processing with user-defined message passing, aggregation, and termination functions. Foundation for algorithms like PageRank on distributed systems.

**Sources**:

- **02-Knowledge_Graphs (Chunk 15:458-472)**
  > The key difference between GPFs and GNNs is that in the former, the functions are defined by the user, while in the latter, the functions are generally learnt from labelled examples

---

### 255. Symbolic Learning Rule Mining

Rule and axiom mining systems for knowledge graphs. Includes discrete mining (AMIE, RuLES, CARL) using refinement operators and differentiable mining (NeuralLP, DRUM) using gradient descent for path-like rules.

**Sources**:

- **02-Knowledge_Graphs (Chunk 15:656-682)**
  > Systems such as AMIE [170], RuLES [241], CARL [406], DL-Learner [73], etc., propose discrete mining that recursively generates candidate formulae

---

### 256. SSSOM Mapping Standard

Simple Standard for Sharing Ontology Mappings provides vocabulary and format for encoding ontology alignments with provenance metadata.

**Sources**:

- **02-Knowledge_Graphs (Chunk 15:not applicable - referenced in 04 chunk)**
  > N/A

---

### 257. W3C PROV Standard

W3C recommended standard for provenance representation with three core classes: Entity, Activity, Agent. Extended by PROV-AGENT for AI agent workflow provenance tracking.

**Sources**:

- **03-PROV-AGENT (Chunk 1:117-121)**
  > PROV-AGENT, a provenance model that extends the W3C PROV [7] standard and incorporates concepts from the Model Context Protocol (MCP)

---

### 258. Model Context Protocol (MCP)

Emerging standard for AI agent development defining concepts for tools, prompts, resources, context management, and RAG integration. Supported by frameworks like LangChain, AutoGen, LangGraph, CrewAI.

**Sources**:

- **03-PROV-AGENT (Chunk 1:144-150)**
  > MCP defines core agentic AI development concepts, including tools, prompts, resources, context management, and agent-client architecture that can communicate with external sources

---

### 259. Flowcept Provenance Framework

Open-source distributed provenance framework using federated broker-based model for streaming provenance data from heterogeneous sources (Dask, MLflow, Redis, Kafka) with W3C PROV-extended model.

**Sources**:

- **03-PROV-AGENT (Chunk 1:321-335)**
  > we extend Flowcept [9], an open-source distributed provenance framework designed for complex, heterogeneous workflows spanning experimental facilities at the edge, cloud platforms, and HPC environments

---

### 260. Python Decorator Instrumentation

Decorator-based instrumentation pattern for capturing agent tool execution provenance. @flowcept_agent_tool creates AgentTool activities linked to executing agent and PROV relationships.

**Sources**:

- **03-PROV-AGENT (Chunk 1:343-351)**
  > applying the @flowcept_task decorator ensures that, upon execution, the function's inputs, outputs, and any generated telemetry or scheduling data are automatically captured

---

### 261. FlowceptLLM Wrapper

Generic wrapper for capturing LLM invocation provenance including prompts, responses, model metadata (provider, name, temperature), and telemetry. Compatible with major LLM frameworks.

**Sources**:

- **03-PROV-AGENT (Chunk 1:358-364)**
  > a generic wrapper for abstract LLM objects, compatible with models from popular LLM interfaces, including CrewAI, LangChain, and OpenAI

---

### 262. AutoGen Multi-Agent Framework

Merged from 2 sources. AutoGen is used as the core framework for implementing automated multi-agent collaboration. It provides an open-source ecosystem for agent-based AI modeling, enabling the construction of complex multi-agent systems with specialized roles.

**Sources**:

- **03-PROV-AGENT (Chunk 1:141)**
  > AutoGen [12]

- **15-SciAgents (Chunk 2:483-484)**
  > We design AI agents using the general-purpose LLM GPT-4 family models. The automated multi-agent collaboration is implemented in the AutoGen framework

---

### 263. RAG Retrieval-Augmented Generation

Pattern for enhancing LLM prompts with retrieved contextual knowledge from external sources like knowledge bases or web pages.

**Sources**:

- **03-PROV-AGENT (Chunk 1:149-150)**
  > Retrieval-Augmented Generation (RAG) [15] to dynamically augment prompts

---

### 264. ProvONE Workflow Extension

PROV extension adding workflow-specific metadata for provenance capture in scientific workflow management systems.

**Sources**:

- **03-PROV-AGENT (Chunk 1:214)**
  > ProvONE [21] adds workflow-specific metadata and aims at supporting existing workflow management systems

---

### 265. PROV-ML Machine Learning Extension

PROV extension for machine learning workflows capturing model training and evaluation provenance.

**Sources**:

- **03-PROV-AGENT (Chunk 1:239-241)**
  > PROV-ML [22] combines general workflow concepts with ML-specific artifacts, especially for model training and evaluation

---

### 266. FAIR4ML Metadata Schema

Metadata schema for ML models supporting FAIR principles for model findability, accessibility, interoperability, and reproducibility.

**Sources**:

- **03-PROV-AGENT (Chunk 1:241-242)**
  > FAIR4ML [23] adopts a model-centric approach to support the Findability, Accessibility, Interoperability, and Reproducibility (FAIR) principles

---

### 267. BFO Basic Formal Ontology ISO Standard

ISO 21838-2:2020 standard top-level ontology providing foundational classes (continuant, occurrent) for domain ontology development and semantic interoperability.

**Sources**:

- **04-PROV-O_to_BFO (Chunk 1:78)**
  > Basic Formal Ontology (BFO) is a top-level ontology ISO standard [10] used to provide foundational classes to structure different domain ontologies

---

### 268. OBO Foundry Relations Ontology (RO)

Extension of BFO standardizing relations for OBO Foundry ontologies. Provides common relation vocabulary for cross-domain interoperability.

**Sources**:

- **04-PROV-O_to_BFO (Chunk 1:80-81)**
  > The OBO Relations Ontology (RO) is an extension of BFO developed by Open Biomedical Ontologies (OBO) Foundry [13, 14, 15] to standardize relations between domain ontologies

---

### 269. Common Core Ontologies (CCO)

Suite of mid-level ontologies bridging BFO and domain ontologies. Provides Information Content Entity, Agent, Plan, and other commonly needed classes.

**Sources**:

- **04-PROV-O_to_BFO (Chunk 1:82-84)**
  > The Common Core Ontologies (CCO) are a suite of mid-level ontologies, used to span across different domain ontologies and intended to bridge the gap between domain ontologies and BFO

---

### 270. OWL equivalentClass Mapping

OWL construct for bidirectional ontology mapping providing necessary and sufficient conditions for class membership. Enables automated reasoning across aligned ontologies.

**Sources**:

- **04-PROV-O_to_BFO (Chunk 1:146-155)**
  > Equivalence relations represented by OWL equivalentClass and OWL equivalentProperty give necessary and sufficient conditions for something to be an instance of a certain BFO class

---

### 271. RDFS subClassOf Subsumption

RDFS construct for one-way ontology mapping establishing sufficient conditions. Allows non-injective alignments where multiple terms map to a single term.

**Sources**:

- **04-PROV-O_to_BFO (Chunk 1:158-164)**
  > Subsumption relations represented by RDFS subClassOf or RDFS subPropertyOf give sufficient conditions for an instance of one class or relation to be an instance of another

---

### 272. SWRL Rules for Complex Mappings

Semantic Web Rule Language for expressing complex ontology mappings with domain/range restrictions. Implemented by HermiT reasoner for automated inference.

**Sources**:

- **04-PROV-O_to_BFO (Chunk 1:168-183)**
  > SWRL rules [21] are especially useful for restricting the domain or range of an OWL object property in order to use it in a valid mapping. An advantage of SWRL is that it is implemented by semantic reasoners such as HermiT

---

### 273. SKOS Vocabulary for Mappings

Simple Knowledge Organization System vocabulary for informal ontology mappings. SKOS relatedMatch provides symmetric, non-transitive associations without strong inferential semantics.

**Sources**:

- **04-PROV-O_to_BFO (Chunk 1:186-195)**
  > Mapping predicates from the SKOS vocabulary [25] represent informal relations between terms which may be interpreted by users to be intuitively similar

---

### 274. SSSOM Simple Standard for Sharing Ontology Mappings

Standard for encoding ontology mappings with provenance metadata. Supports annotations like subject_label, object_label, mapping_justification from SEMAPV vocabulary.

**Sources**:

- **04-PROV-O_to_BFO (Chunk 1:188-189)**
  > Simple Standard for Sharing Ontology Mappings (SSSOM) vocabulary [7]

---

### 275. HermiT OWL Reasoner

OWL 2 DL reasoner using hypertableau calculus for satisfiability and consistency checking. Used for validating ontology alignments and detecting inconsistencies.

**Sources**:

- **04-PROV-O_to_BFO (Chunk 1:179-180)**
  > An advantage of SWRL is that it is implemented by semantic reasoners such as HermiT [22, 23]

---

### 276. RDF Turtle Serialization

Terse RDF Triple Language (Turtle) for human-readable RDF serialization. Used for ontology files and alignment encoding with reified OWL axioms.

**Sources**:

- **04-PROV-O_to_BFO (Chunk 1:426)**
  > every canonical example instance from the W3C documentation for PROV-O and its extensions was copied into RDF files serialized in the Terse Triple Language (TTL or 'RDF Turtle')

---

### 277. ROBOT Ontology Tool

Command-line tool for ontology engineering workflows. Supports SPARQL queries, reasoning, diff operations, and integration with GNU Make for automated pipelines.

**Sources**:

- **04-PROV-O_to_BFO (Chunk 1:419)**
  > HermiT reasoner [22, 23] and ROBOT command-line tool [34] before running the query

---

### 278. Protege Ontology Editor

Open-source ontology editor for OWL. Used for viewing, editing, and testing ontology alignments with reasoner integration.

**Sources**:

- **04-PROV-O_to_BFO (Chunk 1:571)**
  > a separate RDF Turtle file imports all three of the alignment files, along with the BFO, RO, and CCO ontologies, for viewing the alignments in context in Protege

---

### 279. GitHub Actions CI/CD Pipeline

Automated testing pipeline for ontology alignments using GitHub Actions with ROBOT, HermiT, and SPARQL queries for continuous validation of coherence, consistency, and conservativity.

**Sources**:

- **04-PROV-O_to_BFO (Chunk 1:473-474)**
  > These Makefile tasks are run within a continuous development pipeline using GitHub Actions. The result is that changes to the alignments can be automatically, rigorously tested when committed

---

### 280. GNU Make for Ontology Workflows

Build automation tool for composing ROBOT commands and SPARQL queries into reproducible ontology engineering workflows.

**Sources**:

- **04-PROV-O_to_BFO (Chunk 1:470-471)**
  > an ontology engineering pipeline using ROBOT and GNU Make

---

### 281. OWL Property Chain Axioms

OWL construct for expressing complex role compositions (e.g., hasPart o locatedIn implies locatedIn). Used for mapping relations that require composition.

**Sources**:

- **04-PROV-O_to_BFO (Chunk 1:182-183)**
  > If a set of relations in one ontology should imply a relation in another ontology, OWL property chain axioms can be used to axiomatize this complex subsumption relation between object properties

---

### 282. SOSA/SSN Sensor Ontology

W3C recommendation ontology for sensors, observations, samples, and actuators. SOSA is lightweight core; SSN provides fuller axiomatization aligned with PROV-O.

**Sources**:

- **04-PROV-O_to_BFO (Chunk 1:440-444)**
  > The alignments were further tested for consistency with an alignment between PROV-O and the SOSA (Sensor, Observation, Sample, and Actuator) ontology [36], which is a core subset of the Semantic Sensor Network Ontology (SSN) [37]

---

### 283. OWL 2 RL Profile

OWL 2 profile for rule-based reasoning corresponding to description logic DLP. Enables efficient implementation in rule engines while supporting key ontology features.

**Sources**:

- **04-PROV-O_to_BFO (Chunk 2:200-206)**
  > PROV-O and its extensions conform to OWL 2 RL, corresponding to the description logic DLP, with the exception of some axioms that conform to OWL 2 DL

---

### 284. Zenodo Data Archive

Open-access repository for research data and software archiving with persistent DOI identifiers. Used for FAIR-compliant publication of ontology alignments.

**Sources**:

- **04-PROV-O_to_BFO (Chunk 2:344-346)**
  > The alignment files [18] for each ontology and all related project resources are archived at https://doi.org/10.5281/zenodo.14692262

---

### 285. First-Order Modal Logic QS5

Expressive logic for foundational ontology axiomatization supporting possibilistic view of entities. Trade-off between expressiveness and computability addressed through OWL approximations.

**Sources**:

- **05-DOLCE (Chunk 1:227-229)**
  > The formal theory of DOLCE is written in the first-order quantified modal logic QS5, including the Barcan and the converse Barcan formula

---

### 286. DOLCE-Lite OWL Versions

Simplified OWL versions of DOLCE for semantic web applications. DOLCE-ultralite (DUL) widely adopted for knowledge graphs, multimedia annotation, robotics, process mining.

**Sources**:

- **05-DOLCE (Chunk 1:69-70)**
  > several application-oriented, 'lite' versions were later published, including DOLCE-lite, DOLCE-ultralite, and DOLCE-zero

---

### 287. Common Logic CLIF Syntax

ISO 24707 Common Logic Interchange Format for expressing DOLCE axioms. Enables formal verification with tools like Mace4 for consistency proofs.

**Sources**:

- **05-DOLCE (Chunk 1:110-111)**
  > Today DOLCE is becoming part of the ISO 21838 standard, under development, and is available also in CLIF, a syntax of Common Logic ISO 24707

---

### 288. General Extensional Mereology (GEM)

Formal theory of parthood relations for atemporal mereology in DOLCE. Temporary parthood drops antisymmetry for time-indexed part relations.

**Sources**:

- **05-DOLCE (Chunk 1:264)**
  > The first follows the principles of the General Extensional Mereology (GEM)

---

### 289. Conceptual Spaces (Gardenfors)

Geometric framework for representing qualities and properties. Used in DOLCE for quality spaces where qualia represent positions enabling property comparison.

**Sources**:

- **05-DOLCE (Chunk 1:197-198)**
  > Quality spaces in DOLCE are based on Gardenfors' conceptual spaces (Gardenfors, 2000)

---

### 290. CIDOC CRM Cultural Heritage

ISO 21127 standard ontology for cultural heritage documentation. Compatible with DOLCE/DUL for semantic integration of museum and heritage data.

**Sources**:

- **05-DOLCE (Chunk 1:463)**
  > CIDOC CRM (CIDOC Conceptual Reference Model)

---

### 291. SSN Semantic Sensor Network

W3C recommendation for sensor data and observations. Based on or compatible with DOLCE+DUL patterns.

**Sources**:

- **05-DOLCE (Chunk 1:463)**
  > SSN (Semantic Sensor Network Ontology)

---

### 292. SAREF Smart Appliances Ontology

ETSI standard ontology for smart appliances and IoT. Compatible with DOLCE/DUL for semantic interoperability in smart environments.

**Sources**:

- **05-DOLCE (Chunk 1:465)**
  > SAREF (Smart REFerence Ontology)

---

### 293. Framester Knowledge Graph

Large-scale knowledge graph unifying linguistic databases (FrameNet, VerbNet, PropBank, WordNet) under frame semantics with DUL alignment.

**Sources**:

- **05-DOLCE (Chunk 1:461-462)**
  > the recent massive Framester knowledge graph (Gangemi et al., 2016), which unifies many different linguistic databases under a frame semantics, and maps them to widely used ontologies under a common DUL hat

---

### 294. OWL2 Punning

OWL 2 feature allowing the same IRI to denote both a class and an individual. Enables the D&S pattern for reifying concepts as first-class entities in DOLCE extensions.

**Sources**:

- **05-DOLCE (Chunk 2:429-431)**
  > later much facilitated by punning in OWL2 W3C OWL Working Group (2012) (i.e. the ability to use a constant as the name for a class, an individual, or a binary relation)

---

### 295. Description and Situations (D&S) Pattern

Ontology design pattern for overcoming OWL expressivity limits. Enables modeling of contexts, plans, norms, and social aspects as first-class entities.

**Sources**:

- **05-DOLCE (Chunk 2:428-429)**
  > They also address the need for some extensions of DOLCE categories, by reusing the D&S (Description and Situations) ontology pattern framework

---

### 296. Mace4 Model Finder

Automated model finder for first-order logic. Used to prove consistency of DOLCE axiomatization in CLIF format.

**Sources**:

- **05-DOLCE (Chunk 1:256-258)**
  > A CLIF version of DOLCE plus the theory of concepts and roles from (Masolo et al., 2004) is formalized and proved consistent by means of Mace4

---

### 297. OBO Foundry Ontology Framework

BFO serves as the upper-level ontology for the OBO Foundry, providing a standard framework that enables interoperability among biomedical domain ontologies including Gene Ontology, Foundational Model of Anatomy, Protein Ontology, and Ontology for Biomedical Investigations.

**Sources**:

- **06-BFO_Function_Role_Disposition (Chunk 1:42-46)**
  > The ontologies which together form the Open Biomedical Ontologies (OBO) Foundryincluding the Gene Ontology, the Foundational Model of Anatomy, the Protein Ontology, and the Ontology for Biomedical Investigationsutilize Basic Formal Ontology (BFO)

---

### 298. BioPAX Ontology Standard

BioPAX (Biological Pathway Exchange) uses BFO as its upper-level ontology framework, demonstrating BFO's adoption as a standard across multiple biomedical organizations and initiatives.

**Sources**:

- **06-BFO_Function_Role_Disposition (Chunk 1:45)**
  > Individuals and groups in organizations such as BioPAX, Science Commons, Ontology Works, the National Cancer Institute, and Computer Task Group, also utilize BFO in their work.

---

### 299. Gene Ontology Molecular Function Ontology

The Gene Ontology includes a dedicated molecular function ontology for representing functions of genes and gene products, demonstrating how BFO's function concepts are implemented in domain-specific standards.

**Sources**:

- **06-BFO_Function_Role_Disposition (Chunk 1:136-139)**
  > One of the three constituent ontologies of the Gene Ontology (GO) is devoted to the representation of molecular functions associated with genes and gene products.

---

### 300. WHO ICF Classification Standard

The WHO International Classification of Functions, Disabilities and Health (ICF) is a standard classification system that uses function concepts aligned with ontological foundations for health and disability classification.

**Sources**:

- **06-BFO_Function_Role_Disposition (Chunk 1:138-139)**
  > An example is the World Health Organization's International Classification of Functions, Disabilities and Health (ICF).

---

### 301. BFO Upper-Level Ontology Architecture

BFO provides a minimal upper-level ontology architecture designed for modularity and division of expertise, enabling consistent representation of categories across domain ontologies without including domain-specific terms.

**Sources**:

- **06-BFO_Function_Role_Disposition (Chunk 1:48-52)**
  > BFO is an upper-level ontology developed to support integration of data obtained through scientific research. It is deliberately designed to be very small, so that it may represent in a consistent fashion those upper-level categories common to domain ontologies developed by scientists in different fields.

---

### 302. Continuant-Occurrent Dual Hierarchy

BFO implements a dual hierarchy standard separating continuants (persistent entities) from occurrents (events/processes), providing a foundational organizational structure for domain ontologies.

**Sources**:

- **06-BFO_Function_Role_Disposition (Chunk 1:54-57)**
  > BFO adopts a view of reality as comprising (1) continuants, entities that continue or persist through time, such as objects, qualities, and functions, and (2) occurrents, the events or happenings in which continuants participate.

---

### 303. BFO 1.1 Continuant Taxonomy

BFO 1.1 provides a standardized taxonomy of continuant types including independent continuants (objects), dependent continuants (qualities, realizable entities), and spatial regions, enabling consistent ontological modeling across domains.

**Sources**:

- **06-BFO_Function_Role_Disposition (Chunk 1:66-88)**
  > continuant, independent continuant, material entity, object, fiat object part, object aggregate, object boundary, site, dependent continuant, generically dependent continuant, specifically dependent continuant, quality, realizable entity, role, disposition, capability, function, spatial region...

---

### 304. BFO 1.1 Occurrent Taxonomy

BFO 1.1 provides a standardized taxonomy of occurrent types including processes, process boundaries, spatiotemporal regions, and temporal regions for representing events and happenings.

**Sources**:

- **06-BFO_Function_Role_Disposition (Chunk 1:92-108)**
  > occurrent, processual entity, process, process boundary, process aggregate, fiat process part, processual context, spatiotemporal region, scattered spatiotemporal region, connected spatiotemporal region, spatiotemporal instant, spatiotemporal interval, temporal region...

---

### 305. NIH NCBO Funding Standard

The National Center for Biomedical Ontology (NCBO) is the NIH-funded center that supports BFO and related ontology standards, providing infrastructure and resources for biomedical ontology development.

**Sources**:

- **06-BFO_Function_Role_Disposition (Chunk 1:514-515)**
  > This work is funded by the United States National Institutes of Health (NIH) through the NIH Roadmap for Medical Research, National Center for Biomedical Ontologies (NCBO), Grant 1 U54 HG004028.

---

### 306. Common Logic Interchange Format (CLIF)

Common Logic Interchange Format (CLIF) is an ISO Standard family of languages with expressivity equivalent to first-order logic, used for formulating ontological axioms and definitions for data interchange between computer systems.

**Sources**:

- **07-Classifying_Processes_Barry_Smith (Chunk 1:154-159)**
  > Ontological axioms such as (1) and (2), together with accompanying definitions of terms and relations, are formulated using logical languages  typically fragments of first-order logic  developed to facilitate the representation and interchange of information and data among disparate computer systems. Prominent examples are the (CLIF) Common Logic Interchange Format

---

### 307. ISO Common Logic Standard

Common Logic is an ISO/IEC standard (JTC 1/SC 32N1377) providing a framework for logic-based languages used in ontology development and data interchange.

**Sources**:

- **07-Classifying_Processes_Barry_Smith (Chunk 1:166-168)**
  > Common Logic  A Framework for a Family of Logic-Based Languages, ed. Harry Delugach. ISO/IEC JTC 1/SC 32N1377, International Standards Organization Final Committee Draft, 2005-12-13

---

### 308. OWL 2 W3C Standard

OWL 2 is a W3C standard for web ontology language, providing the technical specification for ontology formulation on the semantic web.

**Sources**:

- **07-Classifying_Processes_Barry_Smith (Chunk 1:169)**
  > 'OWL 2 Web Ontology Language', http://www.w3.org/TR/owl2-overview

---

### 309. Reasoner Software Tools

Reasoners are dedicated software tools that automatically check the consistency of merged ontologies formulated in languages like OWL, enabling validation of ontological axioms and definitions.

**Sources**:

- **07-Classifying_Processes_Barry_Smith (Chunk 1:182-184)**
  > the consistency of such mergers can be checked automatically using dedicated software applications called 'reasoners'.

---

### 310. Gene Ontology Biomedical Standard

The Gene Ontology (GO) is a standardized controlled vocabulary of approximately 30,000 terms organized into three sub-ontologies (biological processes, molecular functions, cellular components) for annotating genes and gene products.

**Sources**:

- **07-Classifying_Processes_Barry_Smith (Chunk 1:190-207)**
  > The GO consists of three sub-ontologies, together comprehending some 30,000 terms representing types and subtypes of biological processes, molecular functions, and cellular components. The GO is used by researchers in biology and biomedicine as a controlled vocabulary for describing in species-neutral fashion the attributes of genes and gene products

---

### 311. XES Event Log Standard

XES (eXtensible Event Stream) is a standard format for storing event logs used in process mining, though it has limitations with convergence problems requiring event replication.

**Sources**:

- **07-Classifying_Processes_Barry_Smith (Chunk 1:71)**
  > In event log formats such as XES, this leads to replicating the same event.

---

### 312. Ontology for Biomedical Investigations (OBI)

OBI is an ontology standard providing preferred terms and logical definitions for describing experimental methods, protocols, statistical algorithms, sample processing techniques, software, and equipment used in biomedical investigations.

**Sources**:

- **07-Classifying_Processes_Barry_Smith (Chunk 1:239-252)**
  > This aspect of the unification of science is addressed by the Ontology for Biomedical Investigations (OBI), which comprehends a set of terms which can be used to describe the attributes of experiments in biological and related domains.

---

### 313. BFO 34-Term Ontology Standard

BFO is a minimal standard with exactly 34 terms providing core ontological categories including process, object, function, role, disposition, and boundary concepts used as a framework for domain ontology development.

**Sources**:

- **07-Classifying_Processes_Barry_Smith (Chunk 1:285-287)**
  > BFO is, by the standards predominant in contemporary ontology, very small, consisting of just 34 terms, including both familiar terms such as 'process', 'object', 'function', 'role' and 'disposition', and less familiar terms such as 'generically dependent continuant' and 'continuant fiat boundary'.

---

### 314. International System of Units Analogy

Ontologies function analogously to the International System of Units (SI) by providing standardized terminology for scientific data representation, enabling consistent interpretation across different research contexts.

**Sources**:

- **07-Classifying_Processes_Barry_Smith (Chunk 1:74-77)**
  > It thereby extends into the terminology of scientific theories some of the advantages brought by the International System of Units to the consistent representation of experimental data expressed in quantitative terms.

---

### 315. Directed Acyclic Graph Structure

Merged from 2 sources. Ontologies are technically structured as directed acyclic graphs (DAGs) where terms form nodes connected by relations like is_a and part_of, enabling hierarchical classification and reasoning.

**Sources**:

- **07-Classifying_Processes_Barry_Smith (Chunk 1:78-80)**
  > Each ontology can be conceived as a set of terms (nouns and noun phrases) which form the nodes of a directed acyclical graph

- **19-Graph_of_Thoughts (Chunk 1:69)**
  > Executing algorithms also expose networked patterns, often represented by Directed Acyclic Graphs

---

### 316. Information Artifact Ontology

The Information Artifact Ontology (IAO) extends BFO to handle generically dependent continuants like measurement expressions and records that can be transferred between bearers.

**Sources**:

- **07-Classifying_Processes_Barry_Smith (Chunk 1:629)**
  > the BFO:generically dependent continuant expression: '1.7 m tall'. Each item on this list is unproblematically identifiable as instantiating a BFO category. (4) is an information artifact.

---

### 317. OCEL 2.0 Standard Specification

OCEL 2.0 is the new standard specification for recording and exchanging object-centric event logs, superseding OCEL 1.0 with enhanced expressiveness for object-centric process mining.

**Sources**:

- **09-OCEL_20_Specification (Chunk 1:1-3)**
  > OCEL (Object-Centric Event Log) 2.0 Specification

---

### 318. OCEL 2.0 Triple Exchange Formats

OCEL 2.0 provides three standardized exchange formats: SQLite relational database, XML, and JSON, enabling interoperability across different process mining tools and systems.

**Sources**:

- **09-OCEL_20_Specification (Chunk 1:38-39)**
  > OCEL 2.0 offers three exchange formats: a relational database (SQLite), XML, and JSON format.

---

### 319. OCEL XML Schema Validation

OCEL 2.0 provides an official XML Schema Definition (XSD) file for validating XML-formatted object-centric event logs against the standard specification.

**Sources**:

- **09-OCEL_20_Specification (Chunk 1:23)**
  > XML: https://www.ocel-standard.org/2.0/ocel20-schema-xml.xsd

---

### 320. OCEL JSON Schema Validation

OCEL 2.0 provides an official JSON Schema file for validating JSON-formatted object-centric event logs against the standard specification.

**Sources**:

- **09-OCEL_20_Specification (Chunk 1:25)**
  > JSON: https://www.ocel-standard.org/2.0/ocel20-schema-json.json

---

### 321. OCEL Relational Schema Specification

OCEL 2.0 provides a relational schema specification document for implementing object-centric event logs in SQLite or other relational databases.

**Sources**:

- **09-OCEL_20_Specification (Chunk 1:27)**
  > Relational: https://www.ocel-standard.org/2.0/ocel20-schema-relational.pdf

---

### 322. IEEE XES Standard Evolution

IEEE XES (IEEE 1849) is the established IEEE standard for event log storage, with the 2023 revision extending its validity. OCEL 2.0 addresses XES limitations for object-centric scenarios.

**Sources**:

- **09-OCEL_20_Specification (Chunk 1:237-240)**
  > The first comprehensive standard for storing event data was the IEEE Standard for eXtensible Event Stream (XES). XES became an official IEEE standard in 2016. The revised standard (IEEE 1849-2023) was published on 8 September 2023 and will be valid for another ten years.

---

### 323. OCEL Object-to-Object Relationships

OCEL 2.0 introduces Object-to-Object (O2O) relationships as a standard feature, enabling representation of complex object networks and interactions beyond simple event-object associations.

**Sources**:

- **09-OCEL_20_Specification (Chunk 1:286-290)**
  > Object-to-Object (O2O) Relationships: OCEL 2.0 allows a deeper understanding of how objects interact within a business process. It shows that objects are part of a complex network of relationships and actions.

---

### 324. OCEL Dynamic Object Attributes

OCEL 2.0 standardizes dynamic object attribute values that change over time during process execution, providing more realistic representation of evolving business objects.

**Sources**:

- **09-OCEL_20_Specification (Chunk 1:293-297)**
  > Dynamic Object Attribute Values: OCEL 2.0 adopts a dynamic approach where attribute values can change over time. Instead of having a single, fixed value, an object attribute may have a value that changes during the process.

---

### 325. OCEL Relationship Qualifiers

OCEL 2.0 introduces relationship qualifiers as a standard feature to characterize the role or nature of relationships between objects and events in process logs.

**Sources**:

- **09-OCEL_20_Specification (Chunk 1:306-308)**
  > Relationship Qualifiers: OCEL 2.0 offers capabilities to express qualifiers for relationships, both for Object-to-Object (O2O) and Event-to-Object (E2O) relationships.

---

### 326. OCEL Dense Tables Relational Design

OCEL 2.0 uses a dense table design pattern where each event/object type has its own table storing only relevant attributes, improving storage efficiency and scalability.

**Sources**:

- **09-OCEL_20_Specification (Chunk 1:316-321)**
  > Relational Specification based on Dense Tables: One major feature of OCEL 2.0 is its data structure using dense tables. Each table corresponds to a unique event or object type, storing only relevant attributes.

---

### 327. ISO 8601 Timestamp Format

OCEL 2.0 implementations use ISO 8601 standard format for timestamps, ensuring consistent temporal representation across different systems and platforms.

**Sources**:

- **09-OCEL_20_Specification (Chunk 1:387-388)**
  > In the formalization, time is mapped on the non-negative reals, but concrete implementations will use, for example, the ISO 8601 time format.

---

### 328. OCEL SQLite Reference Implementation

The OCEL 2.0 SQLite implementation uses Unix epoch (1970-01-01 00:00 UTC) as the reference timestamp for initial object attribute values, following the formal definition of time as 0.

**Sources**:

- **09-OCEL_20_Specification (Chunk 2:182-188)**
  > In accordance with the definition of OCEL 2.0, rows possessing the smallest feasible timestamp, specifically 1970-01-01 00:00 UTC  which equates to 0 in Definition 2  correspond to the initial values of all the attributes for a given object.

---

### 329. OCEL Foreign Key Constraints

OCEL 2.0 relational implementation enforces referential integrity through foreign key constraints between event tables, object tables, and relationship tables, ensuring data consistency.

**Sources**:

- **09-OCEL_20_Specification (Chunk 2:354-396)**
  > The uniqueness of the event/object types in the tables event_map_type and object_map_type is ensured by setting the type as the primary key... There is a foreign key between the specific event type tables and the generic event table.

---

### 330. OCEL JSON Schema Draft-07

The OCEL 2.0 JSON validation schema is based on JSON Schema draft-07 specification, providing a standard mechanism for validating JSON-formatted event logs.

**Sources**:

- **09-OCEL_20_Specification (Chunk 4:206)**
  > "$schema": "http://json-schema.org/draft-07/schema#",

---

### 331. Object-Centric Petri Nets Discovery

Merged from 2 sources. Object-centric Petri nets are formal process models that discover separate Petri nets per entity type, then compose them using entity identifiers on places and arcs, resulting in coloured Petri net models.

**Sources**:

- **09-OCEL_20_Specification (Chunk 1:258-259)**
  > Several OCEL 1.0 data sets were provided and the availability of the standard fueled the development of a range of OCPM techniques, e.g., discovering object-centric Petri nets

- **11-Process_Mining_Event_Knowledge_Graphs (Chunk 3:17-20)**
  > An alternative formalization of this concept are object-centric Petri nets. Object-centric Petri nets also first discover one Petri net per entity type, then annotate the places and arcs with entity identifiers

---

### 332. OC-PM Web-Based Tool

OC-PM is a comprehensive tool for object-centric process mining available as both a web-based interface and a ProM plugin, implementing OCEL-based analysis techniques.

**Sources**:

- **10-OC-PM_Object-Centric_Process_Mining (Chunk 1:139-141)**
  > Also, the paper describes the OC-PM tool(s) for object-centric process mining, providing the proposed techniques as a web-based interface and as a plugin of the ProM framework. These are available at the address https://ocpm.info.

---

### 333. ProM Process Mining Framework

ProM 6.x is a popular open-source process mining framework that supports OCEL through the OCELStandard package, enabling object-centric process mining analysis.

**Sources**:

- **10-OC-PM_Object-Centric_Process_Mining (Chunk 2:291-293)**
  > We present another implementation of the process discovery techniques proposed in this paper, on top of the popular process mining framework ProM 6.x. The implementation is proposed in the package OCELStandard

---

### 334. JSON-OCEL and XML-OCEL Formats

JSON-OCEL and XML-OCEL are the two primary serialization formats for OCEL event logs, with additional support for MongoDB storage for scalable implementations.

**Sources**:

- **10-OC-PM_Object-Centric_Process_Mining (Chunk 1:529-533)**
  > Recently, the OCEL format has been proposed for object-centric event logs. Two implementations of the format exist (JSON-OCEL, supported by JSON; XML-OCEL, supported by XML; MongoDB)

---

### 335. OCEL Multi-Language Library Support

OCEL standard has library implementations in multiple programming languages including Java (ProM), JavaScript, and Python, enabling wide adoption across different development environments.

**Sources**:

- **10-OC-PM_Object-Centric_Process_Mining (Chunk 1:530-531)**
  > with tool support available for some popular languages (Java/ProM framework, Javascript, Python)

---

### 336. Object-Centric Directly-Follows Multigraph (OC-DFG)

Object-Centric Directly-Follows Multigraph (OC-DFG) is a process model formalism for representing object-centric processes with typed edges between activities for each object type.

**Sources**:

- **10-OC-PM_Object-Centric_Process_Mining (Chunk 1:746-751)**
  > In this section, we formalize one object-centric process model, the object-centric directly-follows multigraph (OC-DFG), and how to discover an object-centric directly-follows multigraph starting from an object-centric event log.

---

### 337. Neo4J Graph Database for Process Mining

Neo4J is a graph database that has been explored for object-centric event data storage and querying, though scalability challenges have been identified for process mining tasks.

**Sources**:

- **10-OC-PM_Object-Centric_Process_Mining (Chunk 2:413-416)**
  > In [20], the execution time of process mining tasks in a popular graph database (Neo4J) is shown to be disappointing.

---

### 338. Colored Petri Nets Standard

Colored Petri nets are an extension of Petri nets from the 1980s that support data values (colors) on tokens and type-based places (color sets), providing rich semantics for process modeling.

**Sources**:

- **10-OC-PM_Object-Centric_Process_Mining (Chunk 2:379-382)**
  > Colored Petri nets have been proposed in the '80 and have a wide range of applications. Colored Petri nets allow the storage of a data value for each token. The data value is called the token color.

---

### 339. Neo4j Graph Database for Event Knowledge Graphs

Neo4j is identified as the primary graph database system for implementing event knowledge graphs. The paper references a GitHub repository with tutorial implementations using Cypher queries.

**Sources**:

- **11-Process_Mining_Event_Knowledge_Graphs (Chunk 1:182-184)**
  > All concepts for constructing and analyzing event knowledge graphs presented in this chapter are implemented as Cypher queries on the graph database system Neo4j

---

### 340. Cypher Query Language for Graph Operations

Cypher is the query language used for constructing, querying, and analyzing event knowledge graphs in Neo4j. Used for all graph operations including entity inference and directly-follows relationship construction.

**Sources**:

- **11-Process_Mining_Event_Knowledge_Graphs (Chunk 1:182-183)**
  > All concepts for constructing and analyzing event knowledge graphs presented in this chapter are implemented as Cypher queries

---

### 341. Labeled Property Graphs (LPG) Data Model

Labeled property graphs serve as the foundational data model for event knowledge graphs, allowing typed nodes with labels (Event, Entity) and typed relationships (df, corr) with properties attached to both.

**Sources**:

- **11-Process_Mining_Event_Knowledge_Graphs (Chunk 1:550-552)**
  > A typed graph data model such as labeled property graphs allows to distinguish different types of nodes (events, entities) and relationships (directly-follows, correlated-to)

---

### 342. LPG Formal Definition Standard

Formal mathematical definition of labeled property graphs with nodes carrying labels from set Lambda_N, relationships carrying labels from Lambda_R, and both supporting attribute-value pairs as properties.

**Sources**:

- **11-Process_Mining_Event_Knowledge_Graphs (Chunk 1:574-583)**
  > A labeled property graph (LPG) G = (N, R, lambda, #) is a graph with nodes N, and relationships R with... Each node n carries a label... Each relationship r carries a label

---

### 343. RDF Comparison and Limitations

RDF is identified as an alternative encoding for event knowledge graphs, but LPGs are preferred because RDF does not support attributes on relationships, which are essential for df-paths with entity type properties.

**Sources**:

- **11-Process_Mining_Event_Knowledge_Graphs (Chunk 1:681-682)**
  > While the nodes and relationships of Definition 8 can also be encoded in RDF, the df-paths rely on attributes of relationships which are not supported by RDF but by LPGs

---

### 344. OCEL Object-Centric Event Logs Standard

OCEL (Object-Centric Event Logs) is a standard format for multi-entity event data. The paper's event table with entity types formalizes and extends the OCEL concept using 'entity' as a more general term.

**Sources**:

- **11-Process_Mining_Event_Knowledge_Graphs (Chunk 1:236-239)**
  > Note that Definition 2 formalizes the object-centric event logs (OCEL) described in Sect. 3.4 of [1]; we here use the more general term 'entity' instead of 'object'

---

### 345. XOC Event Logs for Dynamic Relations

XOC event logs are an extension that captures dynamically changing relationships between entities over time, addressing limitations of static relation views in standard event logs.

**Sources**:

- **11-Process_Mining_Event_Knowledge_Graphs (Chunk 1:341-342)**
  > Modeling such dynamics requires additional concepts as defined in XOC event logs

---

### 346. GitHub Repository Implementation

Reference implementation of event knowledge graph concepts available as open-source tutorial with Cypher queries for Neo4j graph database.

**Sources**:

- **11-Process_Mining_Event_Knowledge_Graphs (Chunk 1:183-184)**
  > at https://github.com/multi-dimensional-process-mining/eventgraph_tutorial

---

### 347. Cypher Queries for Graph Construction

Cypher queries enable implementation of the complete event knowledge graph construction method, from entity inference to directly-follows relationship creation, in production graph databases.

**Sources**:

- **11-Process_Mining_Event_Knowledge_Graphs (Chunk 2:60-62)**
  > All steps of the method can be implemented as a series of Cypher queries to construct event knowledge graphs in a graph database

---

### 348. Causal Event Graph Extraction from Relational Databases

Causal event graphs are a simplified variant of event knowledge graphs that model only events without explicit entity nodes, supporting automatic extraction from relational database sources.

**Sources**:

- **11-Process_Mining_Event_Knowledge_Graphs (Chunk 2:63-65)**
  > A variant of event knowledge graphs, called causal event graph that only models events but not the entities, can be extracted automatically from relational databases

---

### 349. Real-Life Dataset References with DOIs

Multiple real-life event knowledge graph datasets are available with DOI references (BPI Challenges 2014-2019), providing empirical validation resources for process mining research.

**Sources**:

- **11-Process_Mining_Event_Knowledge_Graphs (Chunk 2:62-63)**
  > several event knowledge graphs of real-life processes are available [19-24]

---

### 350. Inductive Miner Process Discovery

Inductive Miner is a process discovery algorithm that produces process models from event logs, used for comparison to demonstrate limitations of traditional single-case-identifier approaches.

**Sources**:

- **11-Process_Mining_Event_Knowledge_Graphs (Chunk 1:458-461)**
  > the directly-follows graph (DFG) of the log in Table 2 and the corresponding process model discovered with the Inductive Miner (IM) annotated with the mean waiting times

---

### 351. Multi-Entity Directly-Follows Graph (DFG)

Multi-entity DFG is a process model representation that respects local directly-follows relations per entity type, enabling accurate multi-dimensional process discovery.

**Sources**:

- **11-Process_Mining_Event_Knowledge_Graphs (Chunk 2:366-367)**
  > The resulting graph is a multi-entity directly-follows graph, also called multi-viewpoint DFG or artifact-centric model

---

### 352. Synchronous Proclets Multi-Entity Model

Synchronous proclets are a multi-entity extension of Petri nets where each proclet describes behavior of one entity type, with synchronization edges indicating which transitions occur together.

**Sources**:

- **11-Process_Mining_Event_Knowledge_Graphs (Chunk 2:383-391)**
  > An alternative representation of the multi-entity DFG is the proclet model... constructed by creating a Class node per unique pair of activity name and entity type

---

### 353. Event and DF Aggregation Queries

Aggregation operations for discovering multi-entity process models can be implemented as efficient, scalable queries directly within graph databases.

**Sources**:

- **11-Process_Mining_Event_Knowledge_Graphs (Chunk 2:380-382)**
  > Event and df-aggregation can be implemented as simple, scalable queries over standard graph databases, enabling efficient in-database process discovery

---

### 354. Performance Spectrum Visual Analytics

The Performance Spectrum is a visual analytics technique for analyzing process performance over time, implemented as a specialized graph layout with time on x-axis and activity on y-axis.

**Sources**:

- **11-Process_Mining_Event_Knowledge_Graphs (Chunk 2:521-522)**
  > Setting the x-coordinate of each event by its time property and the y-coordinate by its Activity entity results in the graph in Fig. 17, which is called the Performance Spectrum

---

### 355. Performance Spectrum Mining Tool

Performance Spectrum has been implemented as a dedicated visual analytics tool for fine-grained performance analysis, revealing patterns like batching, FIFO violations, and bottlenecks.

**Sources**:

- **11-Process_Mining_Event_Knowledge_Graphs (Chunk 2:590-591)**
  > It is also implemented as a visual analytics tool over event data and in combination with process models

---

### 356. Coloured Petri Nets for Multi-Entity Analysis

Coloured Petri nets result from composing object-centric Petri nets, enabling formal analysis and quality measurement of multi-entity process models.

**Sources**:

- **11-Process_Mining_Event_Knowledge_Graphs (Chunk 3:19-21)**
  > then compose all entity nets along transitions for the same activity, resulting in a coloured Petri net model that is accessible for analysis and measuring model quality

---

### 357. Modular DCR Graphs

Modular DCR (Dynamic Condition Response) graphs are declarative process models that apply modular composition principles, potentially better suited for complex entity interaction patterns than procedural models.

**Sources**:

- **11-Process_Mining_Event_Knowledge_Graphs (Chunk 3:29-32)**
  > Extensions of declarative models (see [10]) such as modular DCR graphs, that apply similar principles as synchronous proclets, could be more suitable

---

### 358. Scenario-Based Models for Partial Orders

Scenario-based models specify conditional partial orders of events across multiple entities, providing an alternative formalism for describing complex multi-entity interactions.

**Sources**:

- **11-Process_Mining_Event_Knowledge_Graphs (Chunk 3:33-34)**
  > Alternatively, scenario-based models that specify conditional partial orders of events over multiple entities could be applied

---

### 359. BPI Challenge Datasets with DOIs

BPI Challenge datasets (2014-2019) are available as event knowledge graphs with persistent DOI identifiers, providing standardized benchmark datasets for process mining research.

**Sources**:

- **11-Process_Mining_Event_Knowledge_Graphs (Chunk 3:112-123)**
  > Event Graph of BPI Challenge 2014. Dataset. https://doi.org/10.4121/14169494... Event Graph of BPI Challenge 2019. Dataset. https://doi.org/10.4121/14169614

---

### 360. Zenodo Dataset Repository

Zenodo is used as a repository for sharing multi-dimensional event data and query implementations, supporting reproducible process mining research.

**Sources**:

- **11-Process_Mining_Event_Knowledge_Graphs (Chunk 3:122-123)**
  > Event Data and Queries for Multi-Dimensional Event Data in the Neo4j Graph Database, April 2021. https://doi.org/10.5281/zenodo.4708117

---

### 361. XES Standard (IEEE 1849-2016)

XES (eXtensible Event Stream) is the IEEE 1849-2016 standard for event log interchange, using W3C XML Schema for defining event log structure with logs, traces, events, and extensible attributes.

**Sources**:

- **12-Foundations_of_Process_Event_Data (Chunk 1:163-168)**
  > This observation drove the development of the eXtensible Event Stream (XES) standard, an IEEE Standards Association-approved language to transport, store, and exchange event data. Its metadata structure is represented in Fig. 2. XES uses the W3C XML Schema definition language

---

### 362. XES Extensions Mechanism

XES supports domain-specific extensions that define standardized attribute sets for events, traces, or logs, enabling interoperability within specific application domains.

**Sources**:

- **12-Foundations_of_Process_Event_Data (Chunk 1:172-175)**
  > it does allow for extensions. An extension can be used to define a set of attributes for events, traces and/or logs. For instance, a common set of attributes can be defined for event logs within a particular application domain

---

### 363. XES Lifecycle Extension

The XES lifecycle extension provides a standardized activity lifecycle state machine for representing event types like start, complete, suspend, resume within process execution.

**Sources**:

- **12-Foundations_of_Process_Event_Data (Chunk 1:185-186)**
  > Also in IEEE XES, a lifecycle extension has been approved, which specifies a default activity lifecycle

---

### 364. BPMN 2.0 Activity Lifecycle

BPMN 2.0 defines a transactional lifecycle model for activities with standardized state transitions, providing a reference model for event type semantics in process-aware systems.

**Sources**:

- **12-Foundations_of_Process_Event_Data (Chunk 1:183-184)**
  > One example of such a transactional lifecycle model is shown in Fig. 3a. This is the transition lifecycle model of the BPMN 2.0 standard

---

### 365. OCEL Standard for Object-Centric Event Data

OCEL (Object-Centric Event Log) standard provides a format for exchanging event data with multiple case notions, addressing limitations of single-case-identifier formats.

**Sources**:

- **12-Foundations_of_Process_Event_Data (Chunk 1:431-433)**
  > Finally, the recently introduced OCEL standard is another relevant piece of work, putting forward a general standard to interchange object-centric event data with multiple case notions

---

### 366. BPMS Event Logging

Business Process Management Systems (BPMS) provide native event logging with rich event type information, representing the most process-aware source for event data extraction.

**Sources**:

- **12-Foundations_of_Process_Event_Data (Chunk 1:217-218)**
  > When retrieving data from process-aware information system, especially from Business Process Management Systems (BPMS), a large collection of event types might be readily available

---

### 367. Ontology-Based Data Access (OBDA) for Event Extraction

OBDA (Ontology-Based Data Access) uses ontological domain views linked to database schemas for event log extraction, implemented in the Onprom tool for semantic event extraction.

**Sources**:

- **12-Foundations_of_Process_Event_Data (Chunk 1:428-431)**
  > One noteworthy scientific initiative in this context is ontology-based data access (ODBA) for event log extraction. The approach is based on an ontological view of the domain of interest and linking it as such to a database schema and has been implemented in the Onprom tool

---

### 368. ProM Import Framework

ProM Import Framework pioneered extensible plug-in architecture for connecting to diverse source systems, establishing the pattern for process mining tool integration.

**Sources**:

- **12-Foundations_of_Process_Event_Data (Chunk 1:416-419)**
  > One of the first tools stemming from scientific research was the ProM Import Framework. Already in these early days, the idea of an extensible plug-in architecture allowing to develop adapters to hook into a large variety of systems was proposed

---

### 369. XESame Event Log Extraction Tool

XESame is a flexible event log extraction tool developed as successor to ProM Import Framework, aligned with XES standard for improved interoperability.

**Sources**:

- **12-Foundations_of_Process_Event_Data (Chunk 1:419-420)**
  > With the uptake of XES, XESame was developed as a more flexible successor to the ProM Import Framework

---

### 370. EVS Model Builder and XTract for ERP Extraction

EVS Model Builder and XTract are specialized tools for extracting event logs from ERP systems, while Eventifier handles other operational systems.

**Sources**:

- **12-Foundations_of_Process_Event_Data (Chunk 1:421-422)**
  > Other researchers have focused on extraction from ERP systems, e.g. the EVS Model Builder and XTract, or other operational systems, e.g. Eventifier

---

### 371. ETL Processing for Event Data Integration

ETL (Extract-Transform-Load) tools are standard technology for integrating event data from non-integrated sources, particularly within data warehousing architectures.

**Sources**:

- **12-Foundations_of_Process_Event_Data (Chunk 1:464-466)**
  > Hereto, especially when an organizational data warehousing architecture is present, Extract-Transform-Load (ETL) processing would be a default technology to resort to. ETL tools are perfectly equipped to derive and deploy matching schemes to integrate data

---

### 372. Data Federation for Event Log Integration

Data federation and virtualization layers provide an alternative to ETL consolidation, enabling flexible querying across multiple source systems without data duplication.

**Sources**:

- **12-Foundations_of_Process_Event_Data (Chunk 1:467-470)**
  > Increasingly, companies start to focus on the introduction of data virtualization layers in order to realize a more federation-oriented data integration. Data federation can prevent the creation of yet another duplicated database

---

### 373. JSON for Web Event Data

JSON is the default data format for event data from web-based platforms like MOOCs, requiring transformation for process mining compatibility.

**Sources**:

- **12-Foundations_of_Process_Event_Data (Chunk 1:313-315)**
  > in many cases, including for instance learning environments such as MOOCs, a default standard for web-based platforms to store data is JSON (JavaScript Object Notation)

---

### 374. SAP ECC/S4 HANA and Salesforce as Top Sources

SAP ECC, SAP S/4 HANA, and Salesforce are empirically validated as the three most common source systems for process mining, based on a 289-participant industry survey.

**Sources**:

- **12-Foundations_of_Process_Event_Data (Chunk 1:327-329)**
  > In an online survey with 289 participants spanning the roles of practitioners, researchers, software vendors, and end-users, SAP ECC (R/3), SAP S/4 HANA, and Salesforce are selected as the top three most analyzed source systems

---

### 375. PM2 Process Mining Methodology

PM2 is a process mining methodology that defines four specialized preprocessing tasks for event data, distinct from traditional data analytics pipelines like CRISP-DM.

**Sources**:

- **12-Foundations_of_Process_Event_Data (Chunk 1:370-373)**
  > When making an assessment of one of the most recently introduced process mining methodologies, i.e. PM2, four event data preprocessing tasks are defined: (1) creating views, (2) filtering logs, (3) enriching logs, and (4) aggregating events

---

### 376. CRISP-DM Adaptation for Process Mining

CRISP-DM (Cross-Industry Standard Process for Data Mining) has been adapted for process mining projects, particularly in healthcare domains, bridging traditional data science and process analytics.

**Sources**:

- **12-Foundations_of_Process_Event_Data (Chunk 1:374-375)**
  > Several process mining case studies such as the one presented in [6] adapted CRISP-DM to work with healthcare datasets

---

### 377. GPT-4 LLM API for Multi-Agent Systems

GPT-4 family models accessed via OpenAI API serve as the foundation for multi-agent scientific discovery systems, powering specialized agents with distinct roles.

**Sources**:

- **15-SciAgents_Multi-Agent_Graph_Reasoning (Chunk 1:708-709)**
  > The automated multi-agent system consists of a team of AI agents, each powered by a state-of-the-art general purpose large language model from the GPT-4 family, accessed via the OpenAI API

---

### 378. Semantic Scholar API for Novelty Assessment

Semantic Scholar API integration enables automated novelty assessment of generated research hypotheses against existing scientific literature.

**Sources**:

- **15-SciAgents_Multi-Agent_Graph_Reasoning (Chunk 1:195-197)**
  > For instance, we have empowered our automated multi-agent model with the Semantic Scholar API as a tool that provides it with an ability to check the novelty of the generated hypothesis against the existing literature

---

### 379. Ontological Knowledge Graphs from Scientific Papers

Large-scale ontological knowledge graphs constructed from scientific papers (around 1,000 papers) provide structured interconnection of concepts as nodes and relationships as edges.

**Sources**:

- **15-SciAgents_Multi-Agent_Graph_Reasoning (Chunk 1:31-34)**
  > the use of large-scale ontological knowledge graphs to organize and interconnect diverse scientific concepts... Applied to biologically inspired materials

---

### 380. GROMACS and AMBER Molecular Dynamics Software

GROMACS and AMBER are recommended molecular dynamics simulation tools for modeling molecular interactions in bio-inspired materials research.

**Sources**:

- **15-SciAgents_Multi-Agent_Graph_Reasoning (Chunk 1:396-398)**
  > The model suggests using Molecular Dynamics (MD) Simulations to explore interactions at the molecular level. Specifically, it proposes employing software like GROMACS or AMBER

---

### 381. CHARMM and AMBER Force Fields

CHARMM and AMBER force fields with CGenFF parameterization are used for molecular dynamics simulations of silk fibroin and pigment interactions.

**Sources**:

- **15-SciAgents_Multi-Agent_Graph_Reasoning (Chunk 1:623-625)**
  > Appropriate force fields, such as CHARMM or AMBER, are selected, with parameters defined using tools like CGenFF

---

### 382. VMD and PyMOL Molecular Visualization

VMD, PyMOL, and Chimera are molecular visualization tools used for system setup and post-simulation structure analysis in scientific discovery workflows.

**Sources**:

- **15-SciAgents_Multi-Agent_Graph_Reasoning (Chunk 1:625-626)**
  > using VMD or GROMACS for setup... using tools like PyMOL, Chimera, and GROMACS

---

### 383. Finite Element Analysis (FEA) for Material Modeling

Finite Element Analysis (FEA) and Dynamic Mechanical Analysis (DMA) are used for simulating mechanical behavior and viscoelastic properties of composite materials.

**Sources**:

- **15-SciAgents_Multi-Agent_Graph_Reasoning (Chunk 1:596-598)**
  > Use FEA to simulate the mechanical behavior of the composite under different loading conditions. Use dynamic mechanical analysis (DMA) to study the viscoelastic properties

---

### 384. Life Cycle Assessment (LCA) for Sustainability

Life Cycle Assessment (LCA) methodology is recommended for evaluating environmental sustainability and energy efficiency of bio-inspired material production.

**Sources**:

- **15-SciAgents_Multi-Agent_Graph_Reasoning (Chunk 1:602-604)**
  > Use life cycle assessment (LCA) to evaluate the environmental impact and energy efficiency of the production process

---

### 385. AutoGen UserProxyAgent Class

Merged from 2 sources. UserProxyAgent is used to construct human agent proxies in the multi-agent system, enabling human-in-the-loop interactions within the AutoGen framework.

**Sources**:

- **15-SciAgents (Chunk 2:487-488)**
  > In our multi-agent system, the human agent is constructed using UserProxyAgent class from Autogen, and Assistant, Planner, Ontologist...

- **15-SciAgents (Chunk 2:488)**
  > Assistant, Planner, Ontologist, Scientist 1, Scientist 2, and Critic agents are created via AssistantAgent class from Autogen

---

### 386. AutoGen GroupChatManager

GroupChatManager class enables coordination and management of group conversations between multiple agents in the AutoGen framework.

**Sources**:

- **15-SciAgents (Chunk 2:489)**
  > and the group chat manager is created using GroupChatManager class

---

### 387. Python Function-Based Tool Design

Tools are implemented as Python functions with well-defined names, descriptions, and typed input properties, enabling structured function calling by LLM agents.

**Sources**:

- **15-SciAgents (Chunk 2:498-499)**
  > All the tools implemented in this work are defined as python functions. Each function is characterized by a name, a description, and input properties

---

### 388. Semantic Scholar API Integration

Semantic Scholar API is integrated as an external tool for novelty assessment, enabling agents to search academic literature and compare research hypotheses against existing publications.

**Sources**:

- **15-SciAgents (Chunk 2:505-510)**
  > We use the Semantic Scholar API, an AI-powered search engine for academic resources, to search for related publications using a set of keywords

---

### 389. BAAI/bge-large-en-v1.5 Embedding Model

The BAAI/bge-large-en-v1.5 embedding model is used for generating node embeddings in the knowledge graph, enabling semantic similarity calculations for pathfinding algorithms.

**Sources**:

- **15-SciAgents (Chunk 2:239)**
  > We use the `BAAI/bge-large-en-v1.5` embedding model

---

### 390. GPT-4 Family Models

GPT-4 family of large language models serves as the foundation for agent intelligence, providing natural language understanding and generation capabilities.

**Sources**:

- **15-SciAgents (Chunk 2:483)**
  > We design AI agents using the general-purpose LLM GPT-4 family models

---

### 391. GitHub Code Repository

Open-source code repositories on GitHub provide implementation reference for the SciAgents framework and graph reasoning components.

**Sources**:

- **15-SciAgents (Chunk 2:529-530)**
  > All data and codes are available on GitHub at `https://github.com/lamm-mit/SciAgentsDiscovery` and `https://github.com/lamm-mit/GraphReasoning/`

---

### 392. Ontological Knowledge Graph

Large-scale ontological knowledge graphs (33,159 nodes, 48,753 edges, 92 communities) serve as the substrate for multi-agent reasoning and hypothesis generation.

**Sources**:

- **15-SciAgents (Chunk 2:134)**
  > leveraging LLMs and a comprehensive ontological knowledge graph

---

### 393. JSON Structured Output Format

JSON format is used for structured output from agents, enabling standardized representation of research hypotheses with fields like hypothesis, outcome, mechanisms, design_principles.

**Sources**:

- **15-SciAgents (Chunk 2:305-306)**
  > The output, in JSON format, provides key fields such as `mechanisms`, `unexpected_properties`, and `comparison`

---

### 394. Dijkstra Algorithm Variant

Modified Dijkstra's algorithm with randomness factor (0.2) is used for heuristic pathfinding in knowledge graphs, enabling diverse path exploration.

**Sources**:

- **15-SciAgents (Chunk 2:250-251)**
  > the algorithm uses a modified version of Dijkstra's algorithm that introduces a randomness factor to the priority queue

---

### 395. PDF and CSV Output Generation

Final research documents are exported in both PDF and CSV formats for further analysis and dissemination.

**Sources**:

- **15-SciAgents (Chunk 2:458-459)**
  > The entire research concept, expanded and reviewed, is then compiled into a final document which is saved as both a PDF and CSV file

---

### 396. Molecular Dynamics Simulation Tools

GROMACS and AMBER molecular dynamics simulation software are recommended for modeling molecular interactions in materials science applications.

**Sources**:

- **15-SciAgents (Chunk 3:104-105)**
  > Molecular Dynamics (MD) Simulations: Use MD simulations to model the interactions between silk fibroin and dandelion pigments... Software such as GROMACS or AMBER

---

### 397. Finite Element Analysis Software

Merged from 2 sources. FEA is a computational modeling technique used to optimize hierarchical structure geometry for mechanical properties. Creates 3D models and predicts stress, strain, and deformation under loading.

**Sources**:

- **15-SciAgents (Chunk 3:106-107)**
  > Finite Element Analysis (FEA): Apply FEA to simulate the mechanical behavior... Software like ANSYS or COMSOL Multiphysics

- **15-SciAgents (Chunk 6:482)**
  > Design the hierarchical structure using computational modeling techniques, such as finite element analysis (FEA), to optimize the geometry for crashworthiness and stiffness memory.

---

### 398. FTIR Spectroscopy

FTIR spectroscopy is used as an analytical technique to characterize molecular structures and confirm chemical compositions in biomaterials.

**Sources**:

- **15-SciAgents (Chunk 3:113)**
  > Fourier Transform Infrared Spectroscopy (FTIR): To confirm the formation of beta-sheets in silk fibroin

---

### 399. X-ray Diffraction Analysis

XRD is used for crystalline structure analysis of composite materials, providing insights into molecular organization.

**Sources**:

- **15-SciAgents (Chunk 3:114)**
  > X-ray Diffraction (XRD): To analyze the crystalline structure of the composite material

---

### 400. Thermogravimetric Analysis

Merged from 2 sources. TGA is used to assess thermal stability of composite materials. Expected structural integrity maintained up to 300-400 degrees Celsius.

**Sources**:

- **15-SciAgents (Chunk 3:115)**
  > Thermogravimetric Analysis (TGA): To assess the thermal stability of the composite material

- **15-SciAgents (Chunk 10:78)**
  > The thermal stability of the composites will be assessed using thermogravimetric analysis (TGA) and differential scanning calorimetry (DSC).

---

### 401. UV-Vis Spectroscopy

Merged from 2 sources. Raman spectroscopy is used to analyze chemical interactions and binding mechanisms. Monitors characteristic peaks (G and 2D bands) to assess molecular interactions.

**Sources**:

- **15-SciAgents (Chunk 3:294)**
  > UV-Vis Spectroscopy: To monitor the distribution of pigments within the silk matrix

- **15-SciAgents (Chunk 10:146-147)**
  > Raman spectroscopy will be employed to analyze the chemical interactions and binding mechanisms between graphene and amyloid fibrils.

---

### 402. Atomic Force Microscopy

Merged from 2 sources. AFM is used for characterizing mechanical properties at the nanoscale and for imaging hierarchical structures. Enables measurement of surface roughness and nanoscale features.

**Sources**:

- **15-SciAgents (Chunk 3:295)**
  > Atomic Force Microscopy (AFM) and Scanning Electron Microscopy (SEM): To observe the microstructure and distribution

- **15-SciAgents (Chunk 6:111)**
  > Characterize the mechanical properties and microstructure of the synthesized materials using SEM, AFM, and tensile testing.

---

### 403. Scanning Electron Microscopy

Merged from 2 sources. SEM is a characterization technique for analyzing material microstructure and surface morphology. Used to confirm hierarchical porous architecture in biomimetic materials.

**Sources**:

- **15-SciAgents (Chunk 3:295)**
  > Scanning Electron Microscopy (SEM): To observe the microstructure and distribution of pigments

- **15-SciAgents (Chunk 6:111)**
  > Characterize the mechanical properties and microstructure of the synthesized materials using SEM, AFM, and tensile testing.

---

### 404. Transmission Electron Microscopy

Merged from 2 sources. TEM provides atomic resolution imaging (0.1-1 nm) of internal structure and arrangement of composite materials.

**Sources**:

- **15-SciAgents (Chunk 3:296)**
  > Transmission Electron Microscopy (TEM) and Small-angle X-ray Scattering (SAXS): To analyze the nanoscale structures

- **15-SciAgents (Chunk 10:145)**
  > Transmission Electron Microscopy (TEM): TEM will provide detailed images of the internal structure and arrangement of the composites at atomic resolution (0.1-1 nm).

---

### 405. Small-Angle X-ray Scattering

SAXS is used to analyze nanoscale periodic structures and self-assembled organizations in materials.

**Sources**:

- **15-SciAgents (Chunk 3:296)**
  > Small-angle X-ray Scattering (SAXS): To analyze the nanoscale structures formed by self-assembly

---

### 406. Synchrotron Micro-CT

Synchrotron-based micro-computed tomography is used for high-resolution 3D imaging of hierarchical material structures.

**Sources**:

- **15-SciAgents (Chunk 3:243)**
  > synchrotron radiation-based micro-computed tomography (SR-microCT)

---

### 407. FDTD Optical Simulations

FDTD simulations are used for modeling optical properties and predicting structural coloration in materials.

**Sources**:

- **15-SciAgents (Chunk 3:343)**
  > Finite-difference time-domain (FDTD) simulations to model light interaction with the composite material

---

### 408. Dynamic Mechanical Analysis

Merged from 2 sources. DMA is used for evaluating viscoelastic properties, storage modulus, and loss modulus. Measures mechanical properties before and after environmental exposure.

**Sources**:

- **15-SciAgents (Chunk 3:355)**
  > Use dynamic mechanical analysis (DMA) to study the viscoelastic properties

- **15-SciAgents (Chunk 7:6-7)**
  > Measure mechanical properties before and after thermal cycling using tensile tests and dynamic mechanical analysis (DMA).

---

### 409. Life Cycle Assessment

LCA methodology is used for evaluating environmental impact and sustainability of production processes.

**Sources**:

- **15-SciAgents (Chunk 3:364)**
  > Use life cycle assessment (LCA) to evaluate the environmental impact and energy efficiency

---

### 410. Response Surface Methodology

RSM is used as a process optimization technique to identify optimal processing conditions for material fabrication.

**Sources**:

- **15-SciAgents (Chunk 3:365-366)**
  > Apply process optimization algorithms such as response surface methodology (RSM) to identify the optimal conditions

---

### 411. MEEP Electromagnetic Simulations

MEEP software is used for electromagnetic equation propagation and predicting reflectance spectra of photonic structures.

**Sources**:

- **15-SciAgents (Chunk 3:753-754)**
  > using finite-difference time-domain (FDTD) simulations. Software like MEEP (MIT Electromagnetic Equation Propagation)

---

### 412. VMD Molecular Visualization

VMD is used for molecular visualization and system preparation in molecular dynamics simulations.

**Sources**:

- **15-SciAgents (Chunk 3:718)**
  > Use tools like VMD (Visual Molecular Dynamics) or GROMACS for this step

---

### 413. MDAnalysis and Cluster Analysis

MDAnalysis and GROMACS tools are used for cluster analysis and identifying self-assembled molecular structures.

**Sources**:

- **15-SciAgents (Chunk 3:742-743)**
  > Perform cluster analysis to identify and categorize different self-assembled structures... Tools like GROMACS or MDAnalysis

---

### 414. PyMOL and Chimera Visualization

PyMOL and Chimera are used for molecular visualization and analyzing binding interfaces between molecules.

**Sources**:

- **15-SciAgents (Chunk 3:736)**
  > Use visualization tools like PyMOL or Chimera to analyze the binding interfaces

---

### 415. LAMMPS Molecular Dynamics

Merged from 2 sources. LAMMPS (Large-scale Atomic/Molecular Massively Parallel Simulator) is identified as a tool for molecular modeling of lamellar structures and heat transfer analysis. Used for developing molecular models with defined thermal properties.

**Sources**:

- **15-SciAgents (Chunk 3:751)**
  > Use software like LAMMPS or custom scripts to simulate light interaction with the nanostructures

- **15-SciAgents (Chunk 6:94)**
  > Develop a molecular model of the lamellar structure using software such as LAMMPS or GROMACS.

---

### 416. Computational Fluid Dynamics

CFD simulations are used to model fluid flow, pressure drop, and heat transfer efficiency in microfluidic systems.

**Sources**:

- **15-SciAgents (Chunk 5:699-700)**
  > Computational Fluid Dynamics (CFD): Use CFD to simulate fluid flow within the microfluidic channels

---

### 417. Particle Image Velocimetry

PIV is used as an experimental technique to visualize and quantify flow patterns within microfluidic channels.

**Sources**:

- **15-SciAgents (Chunk 5:756-757)**
  > Conduct fluid dynamics experiments using particle image velocimetry (PIV) to visualize and quantify the flow patterns

---

### 418. GROMACS Molecular Dynamics Software

GROMACS is referenced as molecular dynamics simulation software for analyzing material properties at the molecular level, particularly for heat transfer and structural analysis.

**Sources**:

- **15-SciAgents (Chunk 6:94)**
  > Develop a molecular model of the lamellar structure using software such as LAMMPS or GROMACS.

---

### 419. VMD Visualization Tool

VMD (Visual Molecular Dynamics) is a molecular visualization tool used to visualize and analyze heat transfer pathways and molecular structures.

**Sources**:

- **15-SciAgents (Chunk 6:97)**
  > Use tools like VMD or OVITO to visualize and analyze the heat transfer pathways and efficiency within the lamellar structure.

---

### 420. OVITO Visualization Tool

OVITO is an open visualization tool for atomistic simulations, used for analyzing heat transfer pathways and structural efficiency in materials.

**Sources**:

- **15-SciAgents (Chunk 6:97)**
  > Use tools like VMD or OVITO to visualize and analyze the heat transfer pathways and efficiency within the lamellar structure.

---

### 421. Soft Lithography Fabrication

Soft lithography is a fabrication technique for creating intricate lamellar and microfluidic structures. Identified as having challenges in reproducibility and scalability for manufacturing.

**Sources**:

- **15-SciAgents (Chunk 6:68)**
  > The use of soft lithography to create intricate lamellar structures may present challenges in reproducibility and scalability.

---

### 422. Electrospinning Manufacturing

Electrospinning is a manufacturing technique for synthesizing biomimetic materials with hierarchical structures. Used for creating collagen micro/nanofibers with controlled deposition times.

**Sources**:

- **15-SciAgents (Chunk 6:110)**
  > Synthesize biomimetic materials with a lamellar structure using techniques such as electrospinning or layer-by-layer assembly.

---

### 423. Layer-by-Layer Assembly

Layer-by-layer assembly is a fabrication technique for creating hierarchical structures in biomimetic materials. Enables precise control over material layer composition.

**Sources**:

- **15-SciAgents (Chunk 6:110)**
  > Synthesize biomimetic materials with a lamellar structure using techniques such as electrospinning or layer-by-layer assembly.

---

### 424. 3D Printing for Scaffold Fabrication

3D printing is identified as an advanced manufacturing technique for fabricating collagen-based materials with hierarchical porous architecture and controlled microstructure.

**Sources**:

- **15-SciAgents (Chunk 6:484)**
  > Fabricate the collagen-based material using advanced manufacturing techniques, such as 3D printing or electrospinning, to achieve the desired hierarchical, porous architecture.

---

### 425. X-ray Computed Tomography (XCT)

XCT is used for obtaining 3D images of material internal structure and quantifying pore distribution in hierarchical materials.

**Sources**:

- **15-SciAgents (Chunk 6:736)**
  > X-ray Computed Tomography (XCT): To obtain 3D images of the material's internal structure and quantify pore distribution.

---

### 426. Fourier Transform Infrared Spectroscopy (FTIR)

FTIR is used for identifying chemical interactions, confirming nanocomposite incorporation, and assessing distribution within scaffold materials. Analyzes characteristic peaks in infrared spectra.

**Sources**:

- **15-SciAgents (Chunk 8:20-24)**
  > Fourier Transform Infrared Spectroscopy (FTIR): Purpose: To confirm the incorporation of nanocomposites and assess their distribution within the scaffolds.

---

### 427. Molecular Dynamics (MD) Simulations

MD simulations are used to study molecular-level self-assembly, deformation mechanisms, and interactions between materials. Provides insights into reversible deformation and mechanical properties.

**Sources**:

- **15-SciAgents (Chunk 6:706-710)**
  > MD simulations will be used to study the self-assembly and deformation mechanisms of collagen at the molecular level.

---

### 428. Monte Carlo Simulations

Monte Carlo simulations are used for exploring configurational space and optimizing material structures. Identifies stable and energetically favorable configurations.

**Sources**:

- **15-SciAgents (Chunk 10:519-520)**
  > Monte Carlo Simulations: To explore the configurational space and optimize the structure.

---

### 429. Density Functional Theory (DFT)

DFT is a computational technique for investigating electronic properties, binding energies, and charge distribution in composite materials.

**Sources**:

- **15-SciAgents (Chunk 10:518)**
  > Density Functional Theory (DFT): To investigate the electronic properties and binding energies.

---

### 430. Tensile Testing

Tensile testing is used to measure tensile strength, Young's modulus, and elongation at break. Evaluates mechanical properties of scaffolds and composite materials.

**Sources**:

- **15-SciAgents (Chunk 8:6-10)**
  > Mechanical Testing: Purpose: To evaluate the tensile strength, compression strength, and elasticity of the scaffolds. Method: Perform tensile testing, compression testing, and dynamic mechanical analysis.

---

### 431. Nanoindentation Testing

Nanoindentation is used to measure elastic modulus and hardness at the nanoscale. Expected values include elastic modulus around 70 GPa and hardness around 3 GPa.

**Sources**:

- **15-SciAgents (Chunk 9:123-124)**
  > Nanoindentation tests will be performed to measure the elastic modulus and hardness of the material.

---

### 432. Three-Point Bending Tests

Three-point bending tests and single-edge notched bending (SENB) tests are used to measure fracture toughness. Target values of at least 10 MPa.m^0.5.

**Sources**:

- **15-SciAgents (Chunk 9:121-122)**
  > Mechanical testing will be conducted using methods such as three-point bending tests to measure the fracture toughness.

---

### 433. Impact Testing

Drop-weight impact tests and dynamic crash tests are used to measure energy absorption capacity and crashworthiness of hierarchical materials.

**Sources**:

- **15-SciAgents (Chunk 7:141-142)**
  > Perform drop-weight impact tests or dynamic crash tests to measure the energy absorption capacity of the material.

---

### 434. MTT Cell Viability Assay

MTT assay is a standard method for assessing cell viability and proliferation rates in biocompatibility studies. Used with targets of 25-35% improvement.

**Sources**:

- **15-SciAgents (Chunk 8:124)**
  > Assess cell proliferation rates using assays like MTT or Alamar Blue. Aim for a 25-35% increase in cell proliferation over a 7-day period.

---

### 435. Live/Dead Cell Staining

Live/dead staining is used to evaluate cell viability with target of over 90% viability in enhanced scaffolds for biocompatibility assessment.

**Sources**:

- **15-SciAgents (Chunk 8:125)**
  > Cell Viability: Evaluate cell viability using live/dead staining. Aim for over 90% cell viability in nanocomposite-enhanced scaffolds.

---

### 436. Fluorescence Microscopy

Fluorescence microscopy is used to monitor changes in material properties during biological interactions and cell culture experiments.

**Sources**:

- **15-SciAgents (Chunk 6:500)**
  > Use techniques such as fluorescence microscopy and biochemical assays to monitor changes in material properties.

---

### 437. Differential Scanning Calorimetry (DSC)

DSC is used alongside TGA for thermal stability assessment of composite materials and phase transition analysis.

**Sources**:

- **15-SciAgents (Chunk 10:78)**
  > The thermal stability of the composites will be assessed using thermogravimetric analysis (TGA) and differential scanning calorimetry (DSC).

---

### 438. Four-Point Probe Measurement

Four-point probe method measures voltage drop while passing current through probes to determine electrical conductivity. Expected conductivities of 10^5 to 10^6 S/m.

**Sources**:

- **15-SciAgents (Chunk 10:150-152)**
  > The electrical conductivity of the composites will be measured using four-point probe and Hall effect measurements.

---

### 439. Hall Effect Measurement

Hall effect measurements provide information on carrier concentration and mobility in conductive materials.

**Sources**:

- **15-SciAgents (Chunk 10:150-152)**
  > The electrical conductivity of the composites will be measured using four-point probe and Hall effect measurements.

---

### 440. Surface Plasmon Resonance (SPR)

SPR is used to quantify binding affinity between materials with expected dissociation constants (Kd) in nanomolar to micromolar range.

**Sources**:

- **15-SciAgents (Chunk 10:18-19)**
  > We hypothesize that the binding affinity between graphene and amyloid fibrils can be quantified using techniques such as isothermal titration calorimetry (ITC) or surface plasmon resonance (SPR).

---

### 441. Isothermal Titration Calorimetry (ITC)

ITC is used for quantifying binding affinity and thermodynamic parameters of molecular interactions between composite components.

**Sources**:

- **15-SciAgents (Chunk 10:18-19)**
  > We hypothesize that the binding affinity between graphene and amyloid fibrils can be quantified using techniques such as isothermal titration calorimetry (ITC) or surface plasmon resonance (SPR).

---

### 442. Chemical Vapor Deposition (CVD)

CVD is the standard technique for producing high-quality graphene sheets. Involves methane decomposition at approximately 1000 degrees Celsius on copper substrate.

**Sources**:

- **15-SciAgents (Chunk 10:132-133)**
  > Chemical Vapor Deposition (CVD): High-quality graphene sheets will be synthesized using CVD. The process involves the decomposition of a carbon-containing gas at high temperatures (~1000C).

---

### 443. CRISPR/Cas9 Gene Editing

CRISPR/Cas9 is used for engineering cells with specific signaling pathways to control material properties through synthetic biology approaches.

**Sources**:

- **15-SciAgents (Chunk 7:345)**
  > Use CRISPR/Cas9 or other gene-editing techniques to engineer cells with the desired signaling pathways.

---

### 444. Quantitative PCR (qPCR)

qPCR is used for validating protein expression levels in synthetic biology applications. Enables precise measurement of gene expression for material production control.

**Sources**:

- **15-SciAgents (Chunk 10:47)**
  > We expect that the gene circuits will allow for fine-tuning of protein expression with high precision, achieving desired protein concentrations within a range of 1-100 uM. This control can be validated through quantitative PCR (qPCR).

---

### 445. Goniometer Contact Angle Measurement

Goniometry is the standard method for measuring water contact angle to assess superhydrophobicity. Target contact angles greater than 150 degrees indicate superhydrophobic surfaces.

**Sources**:

- **15-SciAgents (Chunk 9:114)**
  > The water contact angle will be measured using a goniometer. A contact angle greater than 150 degrees will confirm superhydrophobicity.

---

### 446. SPARQL Structured Query Language for KG

SPARQL is identified as a key standard for structured search on knowledge graphs. The paper positions SPARQL as enabling structured queries that complement LLM language understanding in synergy-augmented reasoning methods.

**Sources**:

- **16-KG-Agent (Chunk 1:66-68)**
  > synergy-augmented methods can benefit from the structured search on KG (e.g., SPARQL) and the language understanding capacity of LLMs

---

### 447. LLaMA2-7B as Backbone LLM

LLaMA2-7B is used as the standard backbone language model for instruction tuning in the KG-Agent framework, demonstrating that smaller open-source LLMs can be effective for autonomous KG reasoning.

**Sources**:

- **16-KG-Agent (Chunk 1:271)**
  > we construct a high-quality instruction dataset for fine-tuning a small LLM (i.e., LLaMA2-7B)

---

### 448. SQL Query Format for KGQA

SQL is used as the query format standard in KGQA datasets, providing structured representations that can be grounded on knowledge graphs for reasoning program synthesis.

**Sources**:

- **16-KG-Agent (Chunk 1:284-288)**
  > These KGQA datasets contain the annotated SQL queries that can be executed to directly extract the answer entities for each question

---

### 449. Python Program Compiler for KG Execution

A program compiler (Python-based) serves as the execution environment for KG-Agent, enabling code-format function calls to be executed against the knowledge graph.

**Sources**:

- **16-KG-Agent (Chunk 1:568-569)**
  > the KG-based executor will execute it using a program compiler. It can cache or operate the intermediate variables

---

### 450. WebQSP Benchmark Dataset

Merged from 2 sources. WebQSP (Web Questions Semantic Parsing) is a standard benchmark dataset for KGQA based on Freebase, used for evaluating in-domain KG reasoning performance.

**Sources**:

- **16-KG-Agent (Chunk 1:676-677)**
  > i.e., WebQSP, CWQ, and GrailQA, which are based on Freebase

- **17-KG_Reasoning (Chunk 1:534)**
  > Commonly used benchmarks for KGEs' evaluation, such as WN18RR, FB15k-237 and NELL

---

### 451. CWQ Complex WebQuestions Dataset

Complex WebQuestions (CWQ) is a challenging benchmark dataset extending WebQSP with multi-hop and constraint-based questions for evaluating complex KGQA.

**Sources**:

- **16-KG-Agent (Chunk 1:676-677)**
  > i.e., WebQSP, CWQ, and GrailQA, which are based on Freebase

---

### 452. GrailQA Benchmark Dataset

GrailQA is a comprehensive benchmark dataset for evaluating generalization in KGQA across three levels: i.i.d., compositional, and zero-shot settings.

**Sources**:

- **16-KG-Agent (Chunk 1:677)**
  > i.e., WebQSP, CWQ, and GrailQA, which are based on Freebase

---

### 453. KQA Pro Wikidata-based Dataset

KQA Pro is a benchmark dataset based on Wikidata requiring multiple reasoning capabilities including compositional reasoning, multi-hop reasoning, and quantitative comparison.

**Sources**:

- **16-KG-Agent (Chunk 1:678)**
  > and KQA Pro, which is based on Wikidata

---

### 454. OpenAI API for LLM Access

OpenAI APIs (gpt-3.5-turbo-instruct, gpt-3.5-turbo, gpt-4) serve as standard interfaces for accessing commercial LLMs for KGQA evaluation and baseline comparison.

**Sources**:

- **16-KG-Agent (Chunk 2:633-635)**
  > When evaluating the performance of Davinci-003, ChatGPT, and GPT4, we use the latest February version of APIs from OpenAI

---

### 455. MetaQA Movie Domain KG

MetaQA is a domain-specific benchmark based on a movie knowledge graph, used to evaluate generalizability of KG reasoning methods to specialized domains.

**Sources**:

- **16-KG-Agent (Chunk 2:469-470)**
  > we further select the MetaQA (Zhang et al., 2018), which is based on a domain-specific movie KG

---

### 456. HermiT Description Logic Reasoner

HermiT is referenced as a classic implementation of description logic reasoning for OWL ontologies, representing the standard tool for symbolic ontology reasoning.

**Sources**:

- **17-KG_Reasoning (Chunk 1:44)**
  > HermiT [Glimm et al., 2014] is a classic description logic reasoner for OWL ontologies

---

### 457. RDFox KG Storage with Datalog

RDFox is identified as a prominent knowledge graph storage system supporting Datalog rule reasoning, combining efficient storage with logical inference capabilities.

**Sources**:

- **17-KG_Reasoning (Chunk 1:45)**
  > RDFox [Nenov et al., 2015] is a famous KG storage supporting Datalog rule reasoning

---

### 458. SROIQ Description Logic

SROIQ is the description logic foundation for OWL 2, providing the formal logical basis for the web ontology language standard.

**Sources**:

- **17-KG_Reasoning (Chunk 1:87-88)**
  > It is based on the SROIQ DL [Horrocks et al., ]. OWL 2 provides rich expressive power

---

### 459. OWL 2 Schema Language

OWL 2 is identified as the key standard schema language for knowledge graphs, supporting rich expressive power including datatypes and rules for defining class hierarchies and complex relations.

**Sources**:

- **17-KG_Reasoning (Chunk 1:86-92)**
  > the Web Ontology Language OWL 2, which is based on Description Logics (DLs), is a key standard schema language of KGs

---

### 460. TransE Embedding Method

Merged from 2 sources. TransE is one of the foundational KG embedding methods, using translation-based scoring in Euclidean space where h+r should approximate t for valid triples.

**Sources**:

- **17-KG_Reasoning (Chunk 1:102)**
  > Many successful KGE methods, such as TransE [Bordes et al., 2013], ComplEx [Trouillon et al., 2016] and RotatE [Sun et al., 2019]

- **17-KG_Reasoning (Chunk 1:102)**
  > Many successful KGE methods, such as TransE [Bordes et al., 2013], ComplEx [Trouillon et al., 2016] and RotatE [Sun et al., 2019]

---

### 461. ComplEx Embedding Method

ComplEx is a KG embedding method using complex vector space to model asymmetric relations, extending beyond real-valued embeddings for richer relational modeling.

**Sources**:

- **17-KG_Reasoning (Chunk 1:102)**
  > Many successful KGE methods, such as TransE [Bordes et al., 2013], ComplEx [Trouillon et al., 2016] and RotatE [Sun et al., 2019]

---

### 462. FB15k-237 Benchmark Dataset

FB15k-237 is a standard benchmark dataset for KGE evaluation derived from Freebase, addressing test leakage issues in the original FB15k.

**Sources**:

- **17-KG_Reasoning (Chunk 1:535)**
  > Commonly used benchmarks for KGEs' evaluation, such as WN18RR, FB15k-237 and NELL

---

### 463. NELL Knowledge Base

NELL (Never-Ending Language Learning) is referenced as a benchmark knowledge base for evaluating knowledge graph embedding methods.

**Sources**:

- **17-KG_Reasoning (Chunk 1:535)**
  > Commonly used benchmarks for KGEs' evaluation, such as WN18RR, FB15k-237 and NELL

---

### 464. Prolog Logic Programming Language

Prolog is identified as a foundational logic programming language for theorem proving, serving as the basis for differentiable theorem proving approaches like NTP.

**Sources**:

- **17-KG_Reasoning (Chunk 1:419)**
  > Conventional theorem proving methods are based on different logic languages, such as Prolog, Datalog, and OWL

---

### 465. Datalog Rule Language

Datalog is referenced as a standard rule language for logic-based reasoning and theorem proving in knowledge graph systems.

**Sources**:

- **17-KG_Reasoning (Chunk 1:419)**
  > Conventional theorem proving methods are based on different logic languages, such as Prolog, Datalog, and OWL

---

### 466. TensorLog Differentiable Probabilistic Logic

TensorLog is a differentiable probabilistic logic system enabling first-order logical inference through sparse matrix operations, foundational for differentiable rule mining.

**Sources**:

- **17-KG_Reasoning (Chunk 1:470-472)**
  > They are inspired by TensorLog [Cohen et al., 2020], a differentiable probabilistic logic. Tensorlog establishes a connection between inference using first-order rules and sparse matrix multiplication

---

### 467. AnyBURL Rule Mining System

AnyBURL is a bottom-up rule learning system for knowledge graph completion, representing anytime symbolic rule mining approaches.

**Sources**:

- **17-KG_Reasoning (Chunk 1:444)**
  > Conventional methods like AMIE [Galarraga et al., 2015] and AnyBURL [Meilicke et al., 2019] are symbolic-based

---

### 468. UML Class Diagram for Domain Ontology

UML2 class diagrams are used as the standard notation for representing domain ontology models, organizing concepts as classes with generalizations and associations.

**Sources**:

- **18-Multi-Agent (Chunk 1:816-818)**
  > Our domain ontology is represented as a conceptual model in terms of a class diagram of the Unified Modeling Language (UML2)

---

### 469. LangChain Python Framework

LangChain is identified as a foundational Python framework for building multi-agent systems, providing predefined components for agent types, prompt templates, and tool/data integration.

**Sources**:

- **18-Multi-Agent (Chunk 1:348-352)**
  > Some of these recent multi-agent systems as well as further related projects such as GORILLA or VOYAGER are built upon the LANGCHAIN Python framework

---

### 470. Vector Databases for Agent Memory

Vector databases (Pinecone, Chroma) are standard tools for storing unstructured text data in multi-agent systems, enabling semantic search through vector embeddings.

**Sources**:

- **18-Multi-Agent (Chunk 2:135-136)**
  > For optimal processing by LLMs, unstructured text is typically stored in vector databases like PINECONE or CHROMA

---

### 471. Hugging Face Model Platform

Hugging Face is identified as the standard platform for accessing foundation models, providing the global ML community's models for multi-agent system integration.

**Sources**:

- **18-Multi-Agent (Chunk 2:170-171)**
  > Platforms like HUGGING FACE even offer access to numerous models provided by the global machine learning community

---

### 472. Wolfram Alpha Reasoning Tool

Wolfram Alpha is referenced as an example reasoning tool that enhances agent computational intelligence capabilities in multi-agent systems.

**Sources**:

- **18-Multi-Agent (Chunk 2:116-117)**
  > For instance, platforms like WOLFRAM ALPHA empower agents with advanced computational skills

---

### 473. AutoGPT Multi-Agent System

Merged from 2 sources. AutoGPT is identified as a representative autonomous multi-agent system providing general-purpose task management with generic agent types and collaboration mechanics.

**Sources**:

- **18-Multi-Agent (Chunk 1:336)**
  > Exemplary but representative autonomous multi-agent systems are AUTOGPT, BABYAGI, SUPERAGI, HUGGINGGPT, CAMEL, AGENTGPT and METAGPT

- **18-Multi-Agent (Chunk 1:338)**
  > Exemplary but representative autonomous multi-agent systems are AUTOGPT, BABYAGI, SUPERAGI, HUGGINGGPT, CAMEL, AGENTGPT and METAGPT

---

### 474. GPT-3 Foundation Model

GPT-3 is referenced as a foundational large language model that enabled the emergence of autonomous multi-agent systems with advanced reasoning capabilities.

**Sources**:

- **18-Multi-Agent (Chunk 1:317)**
  > The advent and widespread use of large language models (LLMs) like GPT-3 have opened up new opportunities for creating increasingly sophisticated and human-like AI systems

---

### 475. API-based Resource Access

APIs are the standard mechanism for accessing LLMs and contextual resources in multi-agent systems, with access details integrated into the interaction layer.

**Sources**:

- **18-Multi-Agent (Chunk 2:174-176)**
  > Access to LLMs, as well as associated resources such as tools, foundation models, and external data resources, is typically facilitated through Application Programming Interfaces (APIs)

---

### 476. GPT Transformer Architecture

The decoder-only Transformer architecture is identified as the foundation for major LLMs (GPT, PaLM, LLaMA) that enable modern prompting and reasoning frameworks.

**Sources**:

- **19-Graph_of_Thoughts (Chunk 1:37-38)**
  > Recent years saw a rapid development of models primarily based on the decoder-only Transformer variant, such as GPT, PaLM, or LLaMA

---

### 477. GPT-4 Language Model

GPT-4 is supported as one of the LLM backends for the Graph of Thoughts framework, enabling experimentation with different model capabilities.

**Sources**:

- **19-Graph_of_Thoughts (Chunk 1:95-96)**
  > This enables rapid prototyping of novel prompting ideas using GoT, while experimenting with different models such as GPT-3.5, GPT-4, or Llama-2

---

### 478. GPT-3.5 Language Model

GPT-3.5 (ChatGPT) is used as the primary LLM for evaluation in the Graph of Thoughts framework due to cost considerations while maintaining quality.

**Sources**:

- **19-Graph_of_Thoughts (Chunk 1:95)**
  > experimenting with different models such as GPT-3.5, GPT-4, or Llama-2

---

### 479. Llama-2 Open Source LLM

Llama-2 is supported as an open-source LLM option in the GoT framework, though noted to be slower and typically performing worse than GPT-3.5 in experiments.

**Sources**:

- **19-Graph_of_Thoughts (Chunk 1:96)**
  > experimenting with different models such as GPT-3.5, GPT-4, or Llama-2

---

### 480. GitHub Repository for GoT

The GoT framework is released as open-source on GitHub, providing extensible APIs for implementing different prompting schemes and thought transformations.

**Sources**:

- **19-Graph_of_Thoughts (Chunk 1:31)**
  > Website & code: https://github.com/spcl/graph-of-thoughts

---

