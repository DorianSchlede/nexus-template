---
# PARTIAL INDEX HEADER
partial: true
part: 1
total_parts: 2
chunks_covered: [1, 2, 3, 4, 5, 6]

# Paper metadata
paper_id: "19-Graph_of_Thoughts_LLM_Reasoning"
title: "Graph of Thoughts: Solving Elaborate Problems with Large Language Models"
authors:
  - "Maciej Besta"
  - "Nils Blach"
  - "Ales Kubicek"
  - "Robert Gerstenberger"
  - "Michal Podstawski"
  - "Lukas Gianinazzi"
  - "Joanna Gajda"
  - "Tomasz Lehmann"
  - "Hubert Niewiadomski"
  - "Piotr Nyczyk"
  - "Torsten Hoefler"
year: 2023
chunks_expected: 7
chunks_read: 6
analysis_complete: false
high_priority_fields_found: 8

# Extraction fields
entity_types:
  - "Thought"
  - "Graph"
  - "Vertex"
  - "Edge"
  - "Transformation"
  - "Operation"
  - "Controller"
  - "Prompter"
  - "Parser"
  - "Scorer"

entity_definitions:
  Thought: "A unit of information generated by an LLM; can be a paragraph, document, block of code, or sequence of numbers depending on use case (Chunk 1:158-161)"
  Graph: "Directed graph G = (V, E) where V is a set of vertices and E is a set of edges representing the LLM reasoning process (Chunk 1:244-246)"
  Vertex: "Contains a solution to a problem at hand (initial, intermediate, or final); concrete form depends on use case (Chunk 1:246-249)"
  Edge: "Directed connection (t1, t2) indicating thought t2 was constructed using t1 as direct input (Chunk 1:249-252)"
  Transformation: "Operation T(G, p_theta) that modifies graph G by adding/removing vertices and edges (Chunk 1:321-327)"
  Volume: "The number of LLM thoughts from which one can reach a given thought v using directed edges (Chunk 1:738-740)"

entity_relationships:
  - from: "Thought"
    to: "Vertex"
    relationship: "modeled_as"
    source: "Chunk 1:77-78"
  - from: "Edge"
    to: "Thought"
    relationship: "represents_dependency_between"
    source: "Chunk 1:78-79"
  - from: "Transformation"
    to: "Graph"
    relationship: "modifies"
    source: "Chunk 1:321-324"
  - from: "Controller"
    to: "Graph of Operations"
    relationship: "contains"
    source: "Chunk 1:390-395"
  - from: "Controller"
    to: "Graph Reasoning State"
    relationship: "maintains"
    source: "Chunk 1:394-395"

abstraction_level: "application"

framework_comparison:
  - compared_to: "Chain-of-Thought (CoT)"
    relationship: "generalizes"
    details: "GoT generalizes CoT from linear chain to arbitrary graph, enabling aggregation and more complex reasoning patterns"
    source: "Chunk 1:81-82"
  - compared_to: "Tree of Thoughts (ToT)"
    relationship: "generalizes"
    details: "GoT generalizes ToT from tree to arbitrary graph, enabling thought aggregation and feedback loops"
    source: "Chunk 1:58-61, 81-82"
  - compared_to: "CoT-SC (Self-Consistency with CoT)"
    relationship: "extends"
    details: "GoT encompasses CoT-SC's multiple chains but adds ability to aggregate and combine thoughts"
    source: "Chunk 1:55-57, 109"

ai_integration:
  - pattern: "Graph-based prompt engineering"
    description: "Models LLM reasoning as arbitrary graph where thoughts are vertices and dependencies are edges, enabling novel transformations"
    source: "Chunk 1:73-79"
  - pattern: "Modular architecture for prompting"
    description: "Controller, Prompter, Parser, Scorer modules enable extensible prompt engineering with different LLMs (GPT-3.5, GPT-4, Llama-2)"
    source: "Chunk 1:383-396"
  - pattern: "Task decomposition via graph"
    description: "Complex tasks decomposed into subtasks solved independently, then merged via graph aggregation"
    source: "Chunk 2:94-97"

agent_modeling:
  - aspect: "Modular reasoning agents"
    description: "LLM acts as reasoning agent through Controller that coordinates thought generation, scoring, and transformation"
    source: "Chunk 1:422-429"
  - aspect: "Multi-operation orchestration"
    description: "Graph of Operations (GoO) prescribes execution plan of thought operations, Controller decides next actions"
    source: "Chunk 1:433-443"

agentic_workflows:
  - pattern: "Graph of Operations (GoO)"
    description: "Static structure prescribing execution plan of thought operations with predecessor/successor relationships"
    source: "Chunk 1:433-438"
  - pattern: "Graph Reasoning State (GRS)"
    description: "Dynamic structure maintaining state of ongoing LLM reasoning process including executed operations and thought states"
    source: "Chunk 1:438-443"
  - pattern: "Generate-Score-Aggregate workflow"
    description: "Workflow pattern: generate multiple thoughts, score each, keep best, then aggregate for final solution"
    source: "Chunk 3:810-817"

generative_ai_patterns:
  - pattern: "Graph of Thoughts (GoT)"
    description: "Framework modeling LLM reasoning as arbitrary graph, enabling aggregation, refinement loops, and complex thought transformations"
    source: "Chunk 1:17-28"
  - pattern: "Thought Aggregation"
    description: "Combining arbitrary thoughts into new ones by creating vertices with multiple incoming edges, reinforcing advantages while eliminating disadvantages"
    source: "Chunk 1:344-352"
  - pattern: "Thought Refinement"
    description: "Iterative improvement of a thought via self-loop in graph: V+ = {} and E+ = {(v, v)}"
    source: "Chunk 1:355-357"
  - pattern: "Thought Generation"
    description: "Generating one or more new thoughts from existing thought, embracing ToT/CoT-SC generation patterns"
    source: "Chunk 1:360-364"
  - pattern: "Latency-Volume Tradeoff"
    description: "GoT achieves both low latency (log_k N) and high volume (N) through aggregation, unlike ToT which has low volume"
    source: "Chunk 1:743-753"
  - pattern: "Decompose-Solve-Merge"
    description: "Tasks decomposed into subtasks, solved individually, then incrementally merged for final result"
    source: "Chunk 2:94-97"

agent_ontology_integration:
  - mechanism: "Graph abstraction for reasoning"
    description: "Graph structure provides ontological framework for organizing and relating LLM thoughts and their dependencies"
    source: "Chunk 1:73-82"
  - mechanism: "Heterogeneous graph modeling"
    description: "Supports heterogeneous graphs G = (V, E, c) where c maps vertices to different classes (e.g., plans vs paragraphs)"
    source: "Chunk 1:253-259"
  - mechanism: "Scoring and ranking functions"
    description: "Evaluation function E(v, G, p_theta) and ranking function R(G, p_theta, h) for thought quality assessment"
    source: "Chunk 1:370-377"

entity_count:
  count: 10
  rationale: "Core entities: Thought, Graph, Vertex, Edge, Transformation, Operation, Controller, Prompter, Parser, Scorer"
  source: "Chunk 1:383-419"

methodology: "top-down"

empirical_evidence:
  - type: "Quantitative evaluation"
    description: "100 input samples per task comparing GoT vs IO, CoT, ToT on sorting, set intersection, keyword counting, document merging"
    source: "Chunk 1:780-782"
  - type: "Sorting benchmark"
    description: "GoT reduces median error by ~62% over ToT for P=128, while reducing costs by >31%"
    source: "Chunk 2:92-94"
  - type: "Quality improvement"
    description: "GoT median error ~65% and ~83% lower than CoT and IO respectively for sorting P=64"
    source: "Chunk 2:100-101"
  - type: "LLM tested"
    description: "Primary evaluation on GPT-3.5; also experimented with Llama-2 (slower, worse performance)"
    source: "Chunk 2:74-76"

limitations:
  - "Higher computational cost than IO/CoT due to generating k new thoughts per Generate operation (Chunk 2:103-108)"
  - "Requires careful design of graph structure for different task types (Chunk 1:83-84)"
  - "Static few-shot examples can become significant overhead when splitting tasks (Chunk 2:130-134)"

tools_standards:
  - "Python implementation"
  - "GPT-3.5 API"
  - "GPT-4 API"
  - "Llama-2"
  - "GitHub repository: https://github.com/spcl/graph-of-thoughts"
---

# Graph of Thoughts: Solving Elaborate Problems with Large Language Models - Partial Index (Part 1 of 2)

## Chunks Covered: 1-6

## Paper Overview

- **Source**: 19-Graph_of_Thoughts_LLM_Reasoning.pdf
- **Chunks**: 6 of 7 chunks analyzed in this part (~160,643 estimated tokens)
- **Analyzed**: 2025-12-28 (Partial - Part 1 of 2)
- **Institution**: ETH Zurich, Warsaw University of Technology, Cledar

## Key Extractions

This paper introduces Graph of Thoughts (GoT), a framework that advances LLM prompting capabilities beyond Chain-of-Thought (CoT) and Tree of Thoughts (ToT). The key innovation is modeling LLM reasoning as an arbitrary directed graph where thoughts are vertices and dependencies are edges. This enables novel transformations like aggregation (combining multiple thoughts), refinement (iterative improvement via loops), and generation (creating new thoughts from existing ones).

### Generative AI Patterns (HIGH)

| Pattern | Source | Quote |
|---------|--------|-------|
| Graph of Thoughts | Chunk 1:17-21 | "ability to model the information generated by an LLM as an arbitrary graph" |
| Thought Aggregation | Chunk 1:344-352 | "aggregate arbitrary thoughts into new ones, combining advantages" |
| Thought Refinement | Chunk 1:355-357 | "refining of a current thought v by modifying its content" |
| Latency-Volume Tradeoff | Chunk 1:743-753 | "GoT is the only scheme to come with both low latency and high volume" |
| Decompose-Solve-Merge | Chunk 2:94-97 | "decompose complex tasks into simpler subtasks, solve independently, merge" |

### Agentic Workflows (HIGH)

| Pattern | Source | Quote |
|---------|--------|-------|
| Graph of Operations (GoO) | Chunk 1:433-438 | "static structure that prescribes the execution plan of thought operations" |
| Graph Reasoning State (GRS) | Chunk 1:438-443 | "dynamic structure that maintains the continually updated information" |
| Generate-Score-Aggregate | Chunk 3:810-817 | "generate, score each attempt, keep best, aggregate for final solution" |

### Framework Comparison (HIGH)

| Compared To | Relationship | Source |
|-------------|--------------|--------|
| Chain-of-Thought | Generalizes | Chunk 1:81-82 |
| Tree of Thoughts | Generalizes | Chunk 1:58-61 |
| CoT-SC | Extends | Chunk 1:55-57 |

### Entity Types and Definitions

The paper defines a formal ontology for LLM reasoning:

| Entity | Definition | Source |
|--------|------------|--------|
| Thought | Unit of information (paragraph, code, etc.) | Chunk 1:158-161 |
| Graph | Directed graph G = (V, E) for reasoning | Chunk 1:244-246 |
| Vertex | Contains a solution (initial/intermediate/final) | Chunk 1:246-249 |
| Edge | Dependency between thoughts | Chunk 1:249-252 |
| Volume | Number of thoughts that can reach a given thought | Chunk 1:738-740 |

### System Architecture

| Component | Function | Source |
|-----------|----------|--------|
| Controller | Coordinates reasoning, decides next actions | Chunk 1:422-429 |
| Prompter | Prepares prompts, encodes graph structure | Chunk 1:400-402 |
| Parser | Extracts information from LLM thoughts | Chunk 1:406-410 |
| Scorer | Verifies correctness, assigns scores | Chunk 1:413-419 |

### Key Findings (with evidence)

- **GoT outperforms ToT by ~62%** (Chunk 2:92-94): "reduces median error by approximately 62%, achieving higher quality sorting for P=128 while ensuring >31% cost reductions"
- **GoT outperforms CoT and IO** (Chunk 2:100-101): "GoT median error is approximately 65% and approximately 83% lower than CoT and IO respectively"
- **Volume metric advantage** (Chunk 1:750-753): "GoT is the only scheme to come with both a low latency of log_k N and a high volume N"
- **Task decomposition key** (Chunk 2:137-142): "break down task to the point where LLM can solve it correctly, combining subresults is usually easier"

## Chunk Navigation

### Chunk 1: Abstract, Introduction, GoT Framework, Architecture
- **Summary**: Introduces GoT framework that models LLM reasoning as arbitrary graphs. Defines core concepts (thoughts as vertices, edges as dependencies) and thought transformations (aggregation, refinement, generation). Describes modular architecture with Controller, Prompter, Parser, Scorer components. Introduces "volume of a thought" metric.
- **Key concepts**: [Graph of Thoughts, Thought Aggregation, Thought Refinement, Volume metric, Modular architecture]
- **Key quotes**:
  - Line 17-21: "ability to model the information generated by an LLM as an arbitrary graph"
  - Line 77-79: "LLM thought is modeled as a vertex, while an edge is a dependency"
  - Line 344-352: "aggregate arbitrary thoughts into new ones"
- **Load when**: "User asks about GoT framework", "Query about graph-based prompting", "Comparing prompting strategies"

### Chunk 2: Evaluation Results, Related Work, Conclusion
- **Summary**: Presents experimental results showing GoT advantages over baselines. Discusses task decomposition benefits and cost tradeoffs. Reviews related work on prompting paradigms, self-reflection, planning. Concludes that graph abstraction enables novel thought transformations.
- **Key concepts**: [Evaluation results, Task decomposition, Cost-quality tradeoff, Related work, Self-reflection]
- **Key quotes**:
  - Line 92-94: "reduces median error by approximately 62% for P=128 while ensuring >31% cost reductions"
  - Line 137-142: "break down task to the point where LLM can solve correctly for majority of time"
- **Load when**: "User asks about GoT performance", "Query about cost efficiency", "Comparing to related work"

### Chunk 3: References, Appendix A (Positive Score Evaluation)
- **Summary**: Contains bibliography references and Appendix A showing positive score plots for sorting and set intersection tasks. Most content is references which should be skipped per extraction guide.
- **Key concepts**: [Positive score evaluation, Sorting accuracy, Set intersection accuracy]
- **Key quotes**:
  - Line 380-381: "Accuracy and cost in sorting tasks with ChatGPT-3.5"
- **Load when**: "User asks about specific references", "Query about positive score metrics"

### Chunk 4: Appendix B - Sorting Prompts (continued)
- **Summary**: Continues sorting task prompts and responses. Shows Step 3 (merge) and Step 4 (improve) with multiple responses and error analysis. Introduces Set Intersection appendix with prompt stubs for intersect, split, and merge operations.
- **Key concepts**: [Merge prompts, Improve prompts, Set intersection, Error analysis]
- **Key quotes**:
  - Line 117-125: "Fix the sorted variant so that it is correct"
  - Line 229-230: "Find the intersection of two sets of numbers"
- **Load when**: "User asks about sorting prompts", "Query about merge operations", "Set intersection details"

### Chunk 5: Appendix C-D - Set Intersection and Keyword Counting
- **Summary**: Contains execution examples for set intersection and keyword counting tasks. Shows step-by-step prompts/responses with scoring. Demonstrates Generate, Aggregate, and ValidateAndImprove operations for keyword counting with country frequency detection.
- **Key concepts**: [Set intersection workflow, Keyword counting, Country frequency, ValidateAndImprove operation]
- **Key quotes**:
  - Line 421-426: "Combine the following 2 dictionaries into a single dictionary"
  - Line 516-517: "Valid - No Improvement (Fully Correct)"
- **Load when**: "User asks about keyword counting", "Query about dictionary aggregation", "Set intersection examples"

### Chunk 6: Appendix E - Document Merging
- **Summary**: Contains document merging use case with 4 NDA documents. Shows prompts for merge, score, aggregate, and improve operations. Demonstrates full execution plan (GoO) with 3 steps: initial merge, aggregation, improvement. Includes scoring examples evaluating redundancy and information retention.
- **Key concepts**: [Document merging, NDA merging, Score prompt, Aggregate prompt, Improve prompt, Redundancy evaluation]
- **Key quotes**:
  - Line 103-108: "Merge the 4 NDAs into a single one 5 times; Score each attempt and keep the best 3"
  - Line 523-528: "A score of 10 for redundancy implies absolutely no information is redundant"
- **Load when**: "User asks about document merging", "Query about NDA consolidation", "Aggregation scoring examples"
