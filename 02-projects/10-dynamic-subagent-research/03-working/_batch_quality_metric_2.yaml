---
batch_id: "quality_metric_2"
field: quality_metric
extracted_at: "2025-12-29T00:00:00Z"
chunks_read: 14
patterns_found: 28
---

patterns:
  # === 02-ContextSurvey-2507.13334 ===

  - name: "GAIA Human-AI Performance Gap Metric"
    chunk_ref: "02-ContextSurvey-2507.13334 (Chunk 8:191-193)"
    quote: "The GAIA benchmark demonstrates substantial performance gaps, with human achievement of 92% accuracy compared to advanced models achieving only 15%"
    description: "Quality metric measuring the gap between human and AI performance on complex reasoning tasks. GAIA benchmark shows humans at 92% accuracy vs AI at 15%, highlighting fundamental limitations in current reasoning and planning capabilities for context-engineered systems."

  - name: "Self-Refine Performance Improvement Metric"
    chunk_ref: "02-ContextSurvey-2507.13334 (Chunk 8:258-262)"
    quote: "Self-Refine, Reflexion, and N-CRITICS frameworks achieve significant performance improvements, with GPT-4 demonstrating approximately 20% improvement through iterative refinement"
    description: "Quality metric showing 20% improvement in GPT-4 performance through iterative self-refinement mechanisms. These frameworks use multi-dimensional feedback incorporating correctness, relevance, clarity, and robustness."

  - name: "Post-Fault Answer Discovery Retention Metric"
    chunk_ref: "02-ContextSurvey-2507.13334 (Chunk 9:34-35)"
    quote: "evaluation frameworks like GAIA (human 92% vs AI 15%) highlight the importance of transparent capability communication and appropriate expectation setting"
    description: "Comparative quality metric between human and AI capabilities on general AI assistant tasks. Used to measure capability gaps and set appropriate expectations for deployed context-engineered systems."

  # === 03-ClaudeCode-2508.08322 ===

  - name: "Single-Shot Success Rate Metric"
    chunk_ref: "03-ClaudeCode-2508.08322 (Chunk 1:465-468)"
    quote: "our system achieved a successful outcome on 4 tasks (80%) without any human corrections. The single-agent baseline succeeded on only 2 tasks (40%)"
    description: "Quality metric comparing multi-agent vs single-agent success rates. Multi-agent context-engineered approach achieves 80% first-attempt success vs 40% for baseline, defined by passing all tests and meeting acceptance criteria."

  - name: "Token Efficiency Overhead Metric"
    chunk_ref: "03-ClaudeCode-2508.08322 (Chunk 1:475-481)"
    quote: "Our system exchanged around 30-40 messages across all agents for a single task and consumed roughly 100k tokens in total. The multi-agent method used about 3-5x more tokens on successful tasks"
    description: "Quality metric measuring token consumption overhead for multi-agent systems. While using 3-5x more tokens than baseline, the overhead is justified by achieving correct solutions autonomously with saved developer time."

  - name: "Context Adherence Metric"
    chunk_ref: "03-ClaudeCode-2508.08322 (Chunk 2:57-64)"
    quote: "The multi-agent system was far less prone to hallucinating irrelevant code or inventing functions. Every function or class used by the generated code existed in the repository"
    description: "Quality metric measuring hallucination prevention through context retrieval. Semantic code retrieval providing real definitions leads to 100% valid function/class references vs baseline guessing non-existent functions."

  - name: "SWE-Bench Lite Resolution Rate"
    chunk_ref: "03-ClaudeCode-2508.08322 (Chunk 1:61-62)"
    quote: "MASAI achieving significantly higher success on repository-level challenges (28.3% resolution on the SWE-Bench Lite benchmark) than single-agent baselines"
    description: "Quality metric for repository-level coding task resolution. MASAI modular architecture achieves 28.3% resolution rate on SWE-Bench Lite, demonstrating effectiveness of sub-agent decomposition for complex software engineering tasks."

  - name: "Pass@1 Accuracy Metric"
    chunk_ref: "03-ClaudeCode-2508.08322 (Chunk 1:66-68)"
    quote: "AllianceCoder retrieves relevant API information to guide code generation, yielding up to 20% higher pass@1 accuracy"
    description: "Quality metric measuring first-attempt code generation accuracy. AllianceCoder's API retrieval approach yields up to 20% higher pass@1 accuracy through targeted context injection."

  # === 04-GCC-2508.00031 ===

  - name: "SWE-Bench-Lite SOTA Resolution Rate"
    chunk_ref: "04-GCC-2508.00031 (Chunk 1:25-26)"
    quote: "agents equipped with GCC achieve state-of-the-art performance on the SWE-Bench-Lite benchmark, resolving 48.00% of software bugs—outperforming 26 competitive systems"
    description: "Quality metric showing GCC-augmented agents achieve 48% resolution rate on SWE-Bench-Lite, establishing new SOTA by outperforming 26 existing systems including both open and commercial tools."

  - name: "Self-Replication Task Resolution Metric"
    chunk_ref: "04-GCC-2508.00031 (Chunk 1:27-28)"
    quote: "a GCC-augmented agent builds a new CLI agent from scratch, achieving 40.7% task resolution, compared to only 11.7% without GCC"
    description: "Quality metric comparing agent self-replication capability. GCC-augmented agents achieve 40.7% task resolution vs 11.7% without GCC, demonstrating 3.5x improvement through structured context management."

  - name: "Localization Accuracy Metrics"
    chunk_ref: "04-GCC-2508.00031 (Chunk 2:27-29)"
    quote: "GCC reaches 44.3% line-level, 61.7% function-level, and 78.7% file-level correctness—consistently ranking among the top performers"
    description: "Quality metrics measuring patch localization accuracy at three granularity levels. GCC achieves 44.3% line-level, 61.7% function-level, and 78.7% file-level correctness for identifying correct edit regions."

  - name: "Average Cost Per Task Metric"
    chunk_ref: "04-GCC-2508.00031 (Chunk 1:398-399)"
    quote: "Avg. Cost - the average inference cost per task; and Avg. Tokens - the average number of tokens consumed per query"
    description: "Quality metrics for evaluating efficiency of agent systems including average inference cost per task and average tokens consumed. GCC reports $2.77 average cost with 569,468 tokens."

  # === 07-ProtocolBench-2510.17149 ===

  - name: "Protocol Completion Time Variance Metric"
    chunk_ref: "07-ProtocolBench-2510.17149 (Chunk 1:23)"
    quote: "In the Streaming Queue scenario, overall completion time varies by up to 36.5% across protocols, and mean end-to-end latency differs by 3.48 s"
    description: "Quality metric measuring protocol impact on system performance. Completion time varies 36.5% and latency differs by 3.48s across protocols, demonstrating that protocol choice significantly affects system behavior."

  - name: "Fail-Storm Recovery Retention Rate"
    chunk_ref: "07-ProtocolBench-2510.17149 (Chunk 1:123-124)"
    quote: "Under Fail-Storm, A2A preserves 98.85% of pre-fault answer discovery (post 14.57 vs. pre 14.74), compared with ACP 92.41%, ANP 86.96%, and Agora 81.29%"
    description: "Quality metric measuring protocol resilience under node failures. A2A maintains 98.85% retention vs ACP 92.41%, ANP 86.96%, Agora 81.29%, quantifying fault tolerance across protocols."

  - name: "Quality Average Score Metric (1-5 Scale)"
    chunk_ref: "07-ProtocolBench-2510.17149 (Chunk 1:291)"
    quote: "Primary signals are task success and LLM-judge quality (1-5), together with per-message byte counts"
    description: "Quality metric using LLM-judge scoring on 1-5 scale evaluating factual accuracy, reasoning coherence, and task completion. Primary assessment method for GAIA Document QA scenario."

  - name: "Mean End-to-End Latency Metric"
    chunk_ref: "07-ProtocolBench-2510.17149 (Chunk 2:178-181)"
    quote: "ACP demonstrates superior latency characteristics achieving the lowest mean response time of 9,663ms with the smallest variance of 1,077ms and the most controlled maximum latency of 14,235ms"
    description: "Quality metrics for streaming queue performance including mean latency (9,663ms), variance (1,077ms), and maximum latency (14,235ms). ACP achieves optimal latency with smallest dispersion."

  - name: "Protocol Task Utility Metrics"
    chunk_ref: "07-ProtocolBench-2510.17149 (Chunk 2:161-163)"
    quote: "A2A emerges as the superior protocol for overall task utility, achieving the highest average quality score of 2.51 and success rate of 9.29"
    description: "Quality metrics showing A2A achieves 2.51 quality score and 9.29 success rate on GAIA. Represents 10.57% quality improvement and 76.95% success rate enhancement over ACP."

  - name: "Recovery Time Metric"
    chunk_ref: "07-ProtocolBench-2510.17149 (Chunk 2:103-106)"
    quote: "Recovery Time as the duration from fault injection to system stabilization. We measure Answer Discovery Rate as the percentage of queries successfully resolved in each window"
    description: "Quality metric measuring time from fault injection to system stabilization. Recovery times cluster around 8.0 seconds across protocols, with pre-fault and post-fault discovery rate comparisons."

  - name: "Scenario Accuracy Metric"
    chunk_ref: "07-ProtocolBench-2510.17149 (Chunk 2:143-145)"
    quote: "The main metric is Scenario Accuracy, which measures how often we get all module choices right for a complete scenario. Every module in a scenario must be predicted correctly"
    description: "Quality metric for protocol router evaluation requiring 100% correct predictions per scenario. Module accuracy tracks individual choice correctness, with confusion matrix analysis for protocol disambiguation."

  - name: "Protocol Selection Accuracy Improvement"
    chunk_ref: "07-ProtocolBench-2510.17149 (Chunk 2:373-375)"
    quote: "Adding performance priors lifts accuracy to 63.3% (scenario) and 81.7% (module), i.e., +18.3% and +14.7% respectively, and improves macro-F1 from 0.721 to 0.824"
    description: "Quality metric showing performance-aware selection improves scenario accuracy from 53.5% to 63.3% (+18.3%) and module accuracy from 71.2% to 81.7% (+14.7%) with macro-F1 improving from 0.721 to 0.824."

  - name: "Router vs Single-Protocol Performance"
    chunk_ref: "07-ProtocolBench-2510.17149 (Chunk 3:71-75)"
    quote: "The router achieves lower latency in Streaming Queue, significantly reduces recovery time in Fail-Storm (6.55s vs 8.00s), yields higher success rates in GAIA (9.90 vs 9.29)"
    description: "Quality metrics showing ProtocolRouter advantages: 18.1% recovery time reduction (6.55s vs 8.00s), 6.5% higher GAIA success (9.90 vs 9.29), competitive latency in streaming scenarios."

  - name: "Security Capability Binary Matrix"
    chunk_ref: "07-ProtocolBench-2510.17149 (Chunk 2:131-135)"
    quote: "We evaluate security capabilities using a binary matrix indicating whether each protocol supports specific security features (TLS transport, session hijacking protection, end-to-end encryption)"
    description: "Quality metric evaluating protocol security through binary capability matrix. ANP and Agora achieve full coverage across TLS, session protection, E2E encryption, tunnel resistance, and metadata protection."

  - name: "Fine-Grained Time Accounting"
    chunk_ref: "07-ProtocolBench-2510.17149 (Chunk 4:14)"
    quote: "Fine-Grained Time Accounting: Timestamps are recorded at agent, step, and workflow levels in milliseconds (Unix epoch), enabling latency profiling and straggler detection"
    description: "Quality metric infrastructure recording timestamps at agent, step, and workflow levels in milliseconds. Enables comprehensive latency profiling and identification of performance bottlenecks in multi-agent workflows."

  # === 08-LACP-2510.13821 ===

  - name: "LACP Latency Overhead Metric"
    chunk_ref: "08-LACP-2510.13821 (Chunk 1:469-471)"
    quote: "Baseline Latency 0.89 ms, LACP Latency 0.92 ms, Latency Overhead 2.9% for Large payloads (1,964B)"
    description: "Quality metric measuring LACP protocol overhead. For large payloads (1,964B), latency overhead is only 2.9% (0.92ms vs 0.89ms baseline) while providing full cryptographic security guarantees."

  - name: "LACP Payload Size Overhead Metric"
    chunk_ref: "08-LACP-2510.13821 (Chunk 1:471)"
    quote: "Size Overhead shrinks to a modest and justifiable +30% for realistic payloads. This represents the necessary cost for verifiable, end-to-end cryptographic security"
    description: "Quality metric for protocol payload overhead. Large realistic payloads incur only 30% size overhead (2,560 bytes vs 1,964 bytes baseline) for comprehensive security features including signature verification."

  - name: "Security Feature Comparison Matrix"
    chunk_ref: "08-LACP-2510.13821 (Chunk 1:122-137)"
    quote: "Table 1: Comparison of agent communication protocols - Security Features ranging from API key auth only to E2E crypto, 2PC, Auth (core)"
    description: "Quality metric comparing security capabilities across protocols. LACP provides layered semantics with E2E crypto and 2PC as core features, vs baseline protocols offering only API key or JWT auth."

  - name: "Transactional Integrity Metric"
    chunk_ref: "08-LACP-2510.13821 (Chunk 1:206-209)"
    quote: "Transactional Layer ensures the reliability and integrity of communications. It provides mechanisms for message signing, sequencing, unique transaction IDs for idempotency"
    description: "Quality metric for multi-step operation reliability. LACP transactional layer provides message signing, sequencing, unique transaction IDs, and atomic transaction support (two-phase commit) for complex workflows."
