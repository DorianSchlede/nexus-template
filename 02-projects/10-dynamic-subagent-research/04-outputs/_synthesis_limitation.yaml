field: limitation
aggregated_at: '2025-12-29T10:38:16.599305'
batches_merged: 5
patterns_input: 167
patterns_output: 163
patterns:
- name: Brevity Bias in Prompt Optimization
  sources:
  - chunk_ref: 01-ACE-2510.04618 (Chunk 1:89-94)
    quote: 'brevity bias: many prompt optimizers prioritize concise, broadly applicable
      instructions over comprehensive accumulation'
  description: Context adaptation methods suffer from brevity bias where optimization
    collapses toward short, generic prompts. GEPA highlights brevity as strength,
    but such abstraction omits domain-specific heuristics, tool-use guidelines, or
    common failure modes that matter in practice. This undermines performance in domains
    requiring detailed, context-rich guidance.
- name: Context Collapse via Monolithic Rewriting
  sources:
  - chunk_ref: 01-ACE-2510.04618 (Chunk 1:186-199)
    quote: context collapse, which arises when an LLM is tasked with fully rewriting
      the accumulated context at each adaptation step
  description: 'When LLMs perform monolithic context rewriting, they tend to compress
    large contexts into shorter, less informative summaries. Example: at step 60,
    context contained 18,282 tokens with 66.7% accuracy, but at step 61 collapsed
    to 122 tokens with accuracy dropping to 57.1% - worse than the 63.7% baseline
    without adaptation.'
- name: Reflector Dependency for Context Quality
  sources:
  - chunk_ref: 01-ACE-2510.04618 (Chunk 3:103-106)
    quote: 'A potential limitation of ACE is its reliance on a reasonably strong Reflector:
      if the Reflector fails to extract meaningful insights'
  description: ACE's effectiveness depends critically on having a Reflector capable
    of extracting meaningful insights from generated traces or outcomes. In domain-specific
    tasks where no model can extract useful insights, the resulting context will naturally
    lack them. This dependency is similar to Dynamic Cheatsheet where quality hinges
    on the model's curation ability.
- name: Context Pollution from Unreliable Feedback
  sources:
  - chunk_ref: 01-ACE-2510.04618 (Chunk 2:127-132)
    quote: when ground-truth supervision or reliable execution signals are absent,
      both ACE and DC may degrade in performance
  description: When ground-truth supervision or reliable execution signals are absent,
    context adaptation methods including ACE and Dynamic Cheatsheet may degrade. The
    constructed context can be polluted by spurious or misleading signals. ACE's effectiveness
    depends on availability of signals allowing the Reflector and Curator to make
    sound judgments.
- name: Inapplicability for Simple Tasks
  sources:
  - chunk_ref: 01-ACE-2510.04618 (Chunk 3:107-113)
    quote: not all applications require rich or detailed contexts. Tasks like HotPotQA
      often benefit more from concise, high-level instructions
  description: ACE is not universally beneficial. Tasks like HotPotQA benefit more
    from concise, high-level instructions rather than long contexts. Games with fixed
    strategies like Game of 24 may only need a single reusable rule, rendering additional
    context redundant. ACE is most beneficial for settings demanding detailed domain
    knowledge, complex tool use, or environment-specific strategies.
- name: Quadratic Computational Complexity
  sources:
  - chunk_ref: 02-ContextSurvey-2507.13334 (Chunk 2:169-173)
    quote: self-attention mechanism imposes quadratic computational and memory overhead
      as sequence length increases
  description: The self-attention mechanism creates O(n^2) computational and memory
    overhead as sequence length increases, creating substantial obstacles to processing
    extended contexts. This significantly impacts real-world applications such as
    chatbots and code comprehension models. Commercial deployment compounds through
    repeated context processing introducing latency and token-based pricing costs.
- name: LLM Reliability Issues
  sources:
  - chunk_ref: 02-ContextSurvey-2507.13334 (Chunk 2:176-178)
    quote: LLMs demonstrate concerning reliability issues including frequent hallucinations,
      unfaithfulness to input context
  description: LLMs demonstrate concerning reliability issues including frequent hallucinations,
    unfaithfulness to input context, problematic sensitivity to input variations,
    and responses that appear syntactically correct while lacking semantic depth or
    coherence. These fundamental issues necessitate sophisticated context engineering
    approaches.
- name: Prompt Engineering Methodological Challenges
  sources:
  - chunk_ref: 02-ContextSurvey-2507.13334 (Chunk 2:181-186)
    quote: prompt engineering process presents methodological challenges through approximation-driven
      and subjective approaches
  description: The prompt engineering process suffers from approximation-driven and
    subjective approaches that focus narrowly on task-specific optimization while
    neglecting individual LLM behavior. Despite challenges, prompt engineering remains
    critical through precise and contextually rich prompts that reduce ambiguity and
    enhance response consistency.
- name: Context Window Fundamental Constraints
  sources:
  - chunk_ref: 02-ContextSurvey-2507.13334 (Chunk 4:88-100)
    quote: LLMs face fundamental constraints in context management stemming from finite
      context window sizes inherent in most architectures
  description: Finite context window sizes significantly reduce model efficacy on
    tasks requiring deep understanding of lengthy documents while imposing substantial
    computational demands hindering applications requiring quick responses and high
    throughput. Traditional transformer architectures experience quadratic computational
    complexity growth as sequence length increases.
- name: Lost-in-the-Middle Phenomenon
  sources:
  - chunk_ref: 02-ContextSurvey-2507.13334 (Chunk 4:103-108)
    quote: lost-in-the-middle phenomenon, where LLMs struggle to access information
      positioned in middle sections of long contexts
  description: LLMs perform significantly better when relevant information appears
    at beginning or end of inputs, struggling with middle sections. This positional
    bias severely impacts extended chain-of-thought reasoning tasks where critical
    earlier results become susceptible to forgetting, with performance degrading drastically
    by as much as 73% compared to no prior context.
- name: Fundamental Statelessness
  sources:
  - chunk_ref: 02-ContextSurvey-2507.13334 (Chunk 4:111-115)
    quote: LLMs inherently process each interaction independently, lacking native
      mechanisms to maintain state across sequential exchanges
  description: LLMs lack native mechanisms to maintain state across sequential exchanges
    and robust self-validation mechanisms - constraints stemming from fundamental
    limits identified in Godel's incompleteness theorems. This fundamental statelessness
    necessitates explicit management systems to maintain coherent operation sequences
    and ensure robust failure recovery.
- name: Context Overflow vs Context Collapse Tradeoff
  sources:
  - chunk_ref: 02-ContextSurvey-2507.13334 (Chunk 4:115-118)
    quote: opposing challenges of context window overflow, where models 'forget' prior
      context due to exceeding window limits
  description: 'Context management faces opposing challenges: context window overflow
    where models forget prior context due to exceeding window limits, and context
    collapse where enlarged context windows or conversational memory cause models
    to fail in distinguishing between different conversational contexts.'
- name: Chain-of-Thought Prompting Limitations
  sources:
  - chunk_ref: 02-ContextSurvey-2507.13334 (Chunk 4:118-120)
    quote: claimed benefits of chain-of-thought prompting don't stem from genuine
      algorithmic learning but rather depend on problem-specific prompts
  description: Research demonstrates that claimed benefits of chain-of-thought prompting
    do not stem from genuine algorithmic learning but rather depend on problem-specific
    prompts. Benefits deteriorate as problem complexity increases, challenging assumptions
    about reasoning capabilities.
- name: KV Cache Computational Overhead
  sources:
  - chunk_ref: 02-ContextSurvey-2507.13334 (Chunk 4:120-125)
    quote: computational overhead of long-context processing creates additional challenges
      in managing key-value caches which grow substantially
  description: Long-context processing computational overhead creates challenges in
    managing key-value caches which grow substantially with input length, creating
    bottlenecks in both latency and accuracy. Multi-turn and longitudinal interaction
    challenges further complicate as limited effective context hinders knowledge accumulation
    and token demands constrain space for system/user inputs.
- name: Linearization Failure for Structured Data
  sources:
  - chunk_ref: 02-ContextSurvey-2507.13334 (Chunk 3:360-364)
    quote: Linearization often fails to preserve complex relationships and structural
      properties, with performance degrading when information is dispersed
  description: LLMs face fundamental constraints processing relational and structured
    data including tables, databases, and knowledge graphs due to text-based input
    requirements and sequential architecture limitations. Linearization often fails
    to preserve complex relationships and structural properties, with performance
    degrading when information is dispersed throughout contexts.
- name: Memory System Storage Limitations
  sources:
  - chunk_ref: 02-ContextSurvey-2507.13334 (Chunk 5:144-149)
    quote: Neural memory mechanisms struggle with inadequate structured information
      storage and reliance on approximate vector similarity calculations
  description: Neural memory mechanisms struggle with inadequate structured information
    storage and reliance on approximate vector similarity calculations rather than
    precise symbolic operations. This challenges accurate storage and retrieval for
    multi-hop reasoning, representing critical challenges for AI systems operating
    in complex real-world applications.
- name: Current Memory Implementation Gaps
  sources:
  - chunk_ref: 02-ContextSurvey-2507.13334 (Chunk 5:163-169)
    quote: Current implementations lack sophisticated lifecycle management and multi-modal
      integration, limiting long-term knowledge evolution
  description: Current memory implementations lack sophisticated lifecycle management
    and multi-modal integration, limiting long-term knowledge evolution. Feed-forward
    network layers serve as key-value tables storing memory, functioning as 'inner
    lexicon' for word retrieval and creating mechanisms analogous to human associative
    memory.
- name: Extended Context Reasoning Limitations
  sources:
  - chunk_ref: 02-ContextSurvey-2507.13334 (Chunk 5:183-187)
    quote: Despite advances expanding context windows to millions of tokens, LLMs
      struggle with effective reasoning over extended contexts
  description: Despite advances expanding context windows to millions of tokens, LLMs
    struggle with effective reasoning over extended contexts, particularly when relevant
    information appears in middle positions. Modern LLM short-term memory frequently
    manifests as in-context learning, reflecting ability to acquire and process information
    temporarily within context windows.
- name: Catastrophic Forgetting in Long-Term Memory
  sources:
  - chunk_ref: 02-ContextSurvey-2507.13334 (Chunk 5:189-194)
    quote: LLMs face significant challenges maintaining long-term memory due to context
      window limitations and catastrophic forgetting
  description: LLMs face significant challenges maintaining long-term memory due to
    context window limitations and catastrophic forgetting. External memory-based
    methods address these limitations by utilizing physical storage to cache historical
    information, allowing relevant history retrieval without maintaining all information
    within constrained context windows.
- name: Modality Bias in Multimodal Models
  sources:
  - chunk_ref: 02-ContextSurvey-2507.13334 (Chunk 3:304-309)
    quote: modality bias, where models favor textual inputs, generating plausible
      but multimodally ungrounded responses
  description: A primary obstacle in MLLM development is modality bias where models
    favor textual inputs, generating plausible but multimodally ungrounded responses
    by relying on learned linguistic patterns rather than integrated visual or auditory
    information. VPGs trained on simple image-captioning tasks learn to extract only
    salient features, neglecting other visual details crucial for complex instruction-based
    tasks.
- name: Fine-Grained Spatial/Temporal Reasoning Deficiency
  sources:
  - chunk_ref: 02-ContextSurvey-2507.13334 (Chunk 3:310-314)
    quote: MLLMs frequently struggle with fine-grained spatial or temporal reasoning,
      such as precise object localization
  description: MLLMs frequently struggle with fine-grained spatial or temporal reasoning,
    such as precise object localization or understanding detailed event sequences
    in videos. This is particularly challenging in complex domains like social media
    where interpreting interplay of text and images to understand misinformation or
    sarcasm is difficult.
- name: MLLM Black Box Problem
  sources:
  - chunk_ref: 02-ContextSurvey-2507.13334 (Chunk 3:318-320)
    quote: Compounding these issues is our limited mechanistic understanding of MLLMs
      themselves; their internal workings are largely a black box
  description: Limited mechanistic understanding of MLLMs compounds multimodal challenges;
    their internal workings are largely a black box, hindering development of better
    architectures. Effective multimodal reasoning requires not just comprehending
    each modality but also inferring their combined holistic meaning.
- name: In-Context Learning Context Window Constraint
  sources:
  - chunk_ref: 02-ContextSurvey-2507.13334 (Chunk 3:327-331)
    quote: in-context learning is constrained by fixed context windows, as image tokens
      consume significant space, limiting many-shot learning
  description: In-context learning is constrained by fixed context windows, as image
    tokens consume significant space, limiting many-shot learning. Performance is
    also sensitive to input order and the relative importance of each modality varies
    by task. Processing long multimodal contexts crucial for video analysis remains
    a major research frontier.
- name: Memory Evaluation Methodology Gaps
  sources:
  - chunk_ref: 02-ContextSurvey-2507.13334 (Chunk 6:28-33)
    quote: Fundamental limitations include absence of consistent, rigorous methodologies
      for assessing memory performance
  description: Memory evaluation faces fundamental limitations including absence of
    consistent, rigorous methodologies for assessing memory performance, particularly
    regarding generalization beyond training data. Lack of standardized benchmarks
    specifically designed for long-term memory evaluation is a significant obstacle,
    with existing frameworks failing to capture full spectrum of memory capabilities.
- name: Stateless Agent Architecture Constraint
  sources:
  - chunk_ref: 02-ContextSurvey-2507.13334 (Chunk 6:36-39)
    quote: most contemporary LLM-based agents operate in fundamentally stateless manners,
      treating interactions independently
  description: Architectural constraints significantly complicate evaluation as most
    contemporary LLM-based agents operate in fundamentally stateless manners, treating
    interactions independently without truly accumulating knowledge incrementally
    over time. This prevents genuine lifelong learning assessment - a cornerstone
    of human-level intelligence.
- name: Memory vs Reasoning Isolation Challenge
  sources:
  - chunk_ref: 02-ContextSurvey-2507.13334 (Chunk 6:45-48)
    quote: Methodological issues arise when isolating memory-specific performance
      from other intelligence aspects
  description: Methodological issues arise when isolating memory-specific performance
    from other intelligence aspects, challenging determination of whether failures
    stem from inadequate memory mechanisms or reasoning limitations. Dynamic memory
    usage in real-world applications poses evaluation challenges as controlled laboratory
    tests inadequately capture performance in complex scenarios.
- name: Transaction Integrity in Multi-Agent Systems
  sources:
  - chunk_ref: 02-ContextSurvey-2507.13334 (Chunk 6:434-439)
    quote: contemporary frameworks including LangGraph, AutoGen, and CAMEL demonstrating
      insufficient transaction support
  description: Multi-agent orchestration encounters significant challenges in maintaining
    transactional integrity across complex workflows. LangGraph provides basic state
    management lacking atomicity guarantees. AutoGen prioritizes flexible agent interactions
    without adequate compensatory action management. Many frameworks rely exclusively
    on LLM self-validation without independent validation procedures.
- name: Context Handling Failures in Multi-Agent
  sources:
  - chunk_ref: 02-ContextSurvey-2507.13334 (Chunk 6:442-449)
    quote: Context handling failures compound these challenges as agents struggle
      with long-term context maintenance
  description: Context handling failures compound challenges as agents struggle with
    long-term context maintenance encompassing both episodic and semantic information.
    Central orchestrator topologies introduce non-deterministic, runtime-dependent
    execution paths that enhance adaptability while complicating anomaly detection.
    Environmental misconfigurations and LLM hallucinations can distract agentic systems,
    with goal deviation amplified in multi-agent setups.
- name: Inter-Agent Dependency Opacity
  sources:
  - chunk_ref: 02-ContextSurvey-2507.13334 (Chunk 6:452-458)
    quote: Inter-agent dependency opacity presents additional concerns as agents may
      operate on inconsistent assumptions or conflicting data
  - chunk_ref: 02-ContextSurvey-2507.13334 (Chunk 7:56-58)
    quote: agents may operate on inconsistent assumptions or conflicting data without
      explicit constraints or validation layers
  description: Merged from 2 sources. Inter-agent dependency opacity presents concerns
    as agents may operate on inconsistent assumptions or conflicting data without
    explicit constraints or validation layers. This necessitates anomaly detection
    incorporating reasoning over orchestration intent and planning coherence. Comprehensive
    solutions like SagaLLM framework provide transaction support, independent validation,
    and robust context preservation mechanisms.
- name: Transaction Support Insufficiency
  sources:
  - chunk_ref: 02-ContextSurvey-2507.13334 (Chunk 7:38-43)
    quote: LangGraph provides basic state management while lacking atomicity guarantees
      and systematic compensation mechanisms
  description: Contemporary orchestration frameworks like LangGraph, AutoGen, and
    CAMEL demonstrate insufficient transaction support. LangGraph lacks atomicity
    guarantees, AutoGen prioritizes flexible interactions without adequate compensatory
    action management, leading to inconsistent system states following partial failures.
- name: LLM Self-Validation Dependency
  sources:
  - chunk_ref: 02-ContextSurvey-2507.13334 (Chunk 7:41-43)
    quote: validation limitations emerge as many frameworks rely exclusively on large
      language models' inherent self-validation capabilities without implementing
      independent validation procedures
  description: Multi-agent systems often rely exclusively on LLMs' self-validation
    capabilities without independent validation procedures, exposing systems to reasoning
    errors, hallucinations, and inter-agent inconsistencies.
- name: Long-term Context Maintenance Failure
  sources:
  - chunk_ref: 02-ContextSurvey-2507.13334 (Chunk 7:46-47)
    quote: agents struggle with long-term context maintenance encompassing both episodic
      and semantic information
  description: Context handling failures compound challenges as agents struggle to
    maintain long-term context that encompasses both episodic (event-based) and semantic
    (meaning-based) information across extended interactions.
- name: Non-deterministic Execution Path Complexity
  sources:
  - chunk_ref: 02-ContextSurvey-2507.13334 (Chunk 7:47-49)
    quote: central orchestrator topologies introduce non-deterministic, runtime-dependent
      execution paths that enhance adaptability while complicating anomaly detection
  description: Central orchestrator topologies create non-deterministic, runtime-dependent
    execution paths that while enhancing adaptability, significantly complicate anomaly
    detection and require dynamic graph reconstruction rather than simple path matching.
- name: Goal Deviation Amplification
  sources:
  - chunk_ref: 02-ContextSurvey-2507.13334 (Chunk 7:50-53)
    quote: environmental misconfigurations and LLM hallucinations can distract agentic
      systems, with poor recovery leading to goal deviation that becomes amplified
      in multi-agent setups
  description: Environmental misconfigurations and LLM hallucinations distract agentic
    systems. Poor recovery leads to goal deviation that becomes amplified in multi-agent
    setups with distributed subtasks.
- name: Quadratic Context Scaling
  sources:
  - chunk_ref: 02-ContextSurvey-2507.13334 (Chunk 8:93-97)
    quote: current attention mechanisms scaling quadratically (O(n^2)) with sequence
      length, creating prohibitive memory and computational requirements for ultra-long
      sequences
  description: Context scaling efficiency faces fundamental computational challenges
    with current attention mechanisms scaling O(n^2) with sequence length, creating
    prohibitive memory and computational requirements for ultra-long sequences.
- name: Comprehension-Generation Gap
  sources:
  - chunk_ref: 02-ContextSurvey-2507.13334 (Chunk 8:71-78)
    quote: fundamental asymmetry between LLMs' remarkable comprehension capabilities
      and their pronounced generation limitations
  description: There is a fundamental asymmetry between LLMs' remarkable comprehension
    capabilities and their pronounced generation limitations. This comprehension-generation
    gap manifests across dimensions including long-form output coherence, factual
    consistency maintenance, and planning sophistication.
- name: Extended Generation Performance Degradation
  sources:
  - chunk_ref: 02-ContextSurvey-2507.13334 (Chunk 8:83-87)
    quote: Current systems exhibit significant performance degradation in extended
      generation tasks, highlighting the need for architectural innovations beyond
      traditional transformer paradigms
  description: Current systems exhibit significant performance degradation in extended
    generation tasks. State space models like Mamba demonstrate potential for efficient
    long sequence processing but require substantial development to match transformer
    performance across diverse tasks.
- name: Multi-modal Coordination Gap
  sources:
  - chunk_ref: 02-ContextSurvey-2507.13334 (Chunk 8:103-108)
    quote: Current approaches typically employ modality-specific encoders with limited
      cross-modal interaction, failing to capture the rich interdependencies
  description: Multi-modal integration presents fundamental challenges. Current approaches
    employ modality-specific encoders with limited cross-modal interaction, failing
    to capture rich interdependencies. Systems show substantial performance gaps when
    processing video, audio, and text simultaneously.
- name: Absence of Unified Theoretical Foundations
  sources:
  - chunk_ref: 02-ContextSurvey-2507.13334 (Chunk 8:40-47)
    quote: Context Engineering currently operates without unified theoretical foundations
      that connect disparate techniques and provide principled design guidelines
  description: Context Engineering lacks unified theoretical foundations connecting
    disparate techniques. The absence of mathematical frameworks characterizing capabilities,
    limitations, and optimal design principles impedes both fundamental understanding
    and practical optimization.
- name: Tool-integrated Reasoning Performance Gap
  sources:
  - chunk_ref: 02-ContextSurvey-2507.13334 (Chunk 9:33-35)
    quote: substantial performance gaps revealed by evaluation frameworks like GAIA
      (human 92% vs AI 15%) highlight the importance of transparent capability communication
  description: Tool-integrated reasoning systems show substantial performance gaps.
    The GAIA benchmark reveals human achievement of 92% accuracy compared to AI 15%,
    indicating fundamental limitations in current reasoning and planning capabilities.
- name: Memory System Privacy Challenges
  sources:
  - chunk_ref: 02-ContextSurvey-2507.13334 (Chunk 9:27-30)
    quote: Memory systems face particular privacy challenges due to their persistent
      information storage and retrieval capabilities, requiring advanced frameworks
      for secure memory management
  description: Memory systems face particular privacy challenges due to persistent
    information storage and retrieval capabilities, requiring advanced frameworks
    for secure memory management and selective forgetting mechanisms.
- name: Fixed Static Context Limitation
  sources:
  - chunk_ref: 03-ClaudeCode-2508.08322 (Chunk 1:46-48)
    quote: A key limitation of default code assistants is their reliance on a fixed
      static context...a single prompt cannot capture all relevant details for every
      possible task
  description: Default code assistants rely on fixed static context. A single CLAUDE.md
    file cannot capture all relevant details for every possible task, leading to incomplete
    or incorrect solutions for non-trivial features.
- name: Context Window Limitation
  sources:
  - chunk_ref: 03-ClaudeCode-2508.08322 (Chunk 1:44-45)
    quote: Purely prompt-driven single-agent solutions struggle with these challenges
      due to limited context windows and the risk of hallucinations
  - chunk_ref: 15-AgentSurvey-2503.21460 (Chunk 1:388-390)
    quote: due to LLMs' context window limitations, practical implementations require
      active information compression and impose many constraints on multi-turn interaction
      depth
  description: Merged from 2 sources. LLM context window limitations force practical
    implementations to use information compression (summarization, selective retention)
    and constrain multi-turn interaction depth to prevent performance degradation.
- name: Incomplete Multi-file Edits
  sources:
  - chunk_ref: 03-ClaudeCode-2508.08322 (Chunk 1:49-51)
    quote: a default Claude Code agent with a basic CLAUDE.md prompt often produced
      incomplete or incorrect solutions...It might miss necessary edits in distant
      files
  description: Single-agent Claude with basic CLAUDE.md often produces incomplete
    solutions for non-trivial features, missing necessary edits in distant files or
    misusing unfamiliar libraries due to insufficient context comprehension.
- name: Multi-agent Token Cost Overhead
  sources:
  - chunk_ref: 03-ClaudeCode-2508.08322 (Chunk 2:76-85)
    quote: the multi-agent method used about 3-5x more tokens on successful tasks.
      However, the baseline often needed multiple attempts
  description: Multi-agent approaches consume approximately 3-5x more tokens than
    single-agent methods. However, this overhead is justified when single-agent baselines
    require multiple attempts or lengthy debugging, pushing token counts to similar
    levels.
- name: Irrelevant External Knowledge Injection
  sources:
  - chunk_ref: 03-ClaudeCode-2508.08322 (Chunk 2:125-128)
    quote: Elicit returned an irrelevant research paper due to an ambiguous query,
      and although NotebookLM summarized it faithfully, that summary added noise to
      the context
  description: External knowledge retrieval can return irrelevant documents due to
    ambiguous queries. Even faithful summarization of irrelevant content adds noise
    to context and can confuse the Planner agent.
- name: Brittle Orchestrator Logic
  sources:
  - chunk_ref: 03-ClaudeCode-2508.08322 (Chunk 2:129-133)
    quote: the current orchestrator logic is relatively brittle; it follows a predetermined
      sequence...if an unexpected situation arises, the system is not yet equipped
      to dynamically re-plan
  description: Current orchestrator logic is brittle, following predetermined sequences
    (plan->code->test->review). If unexpected situations arise or new requirements
    emerge mid-way, the system cannot dynamically re-plan from scratch.
- name: Computational Cost at Scale
  sources:
  - chunk_ref: 03-ClaudeCode-2508.08322 (Chunk 2:134-136)
    quote: the computational cost, while acceptable for our use, might become problematic
      on very large projects or if many agents run in parallel
  description: Computational cost becomes problematic on very large projects or with
    many parallel agents. Techniques like context compression, caching vector search
    results, or using smaller specialized models could help reduce overhead.
- name: Test Suite Dependency
  sources:
  - chunk_ref: 03-ClaudeCode-2508.08322 (Chunk 2:137-140)
    quote: we relied heavily on the presence of a comprehensive test suite. If tests
      are sparse, the system might incorrectly judge a task as complete
  description: Heavy reliance on comprehensive test suites means if tests are sparse,
    the system might incorrectly judge a task as complete. Static analysis tools and
    spec verification agents could partially address this gap.
- name: Multi-agent Error Tracing Difficulty
  sources:
  - chunk_ref: 03-ClaudeCode-2508.08322 (Chunk 2:140-143)
    quote: error tracing can be challenging in a multi-agent context; if a final result
      is wrong, it takes careful log analysis to pinpoint which agent's action led
      to the mistake
  description: Error tracing is challenging in multi-agent contexts. Pinpointing which
    agent's action or which piece of context led to a mistake requires careful log
    analysis. Visualization tools like SeaView could aid debugging.
- name: Session Context Loss
  sources:
  - chunk_ref: 04-GCC-2508.00031 (Chunk 1:47-49)
    quote: closing a session and starting a new one typically erases the agent's memory
      of prior goals, user preferences, and task-specific instruction
  description: Closing a session and starting a new one erases the agent's memory
    of prior goals, user preferences, and task-specific instructions. Users must repeatedly
    're-teach' the model from scratch across sessions.
- name: Context Truncation Information Loss
  sources:
  - chunk_ref: 04-GCC-2508.00031 (Chunk 1:50-53)
    quote: truncate older context once the token limit is reached...this risks discarding
      important historical details—especially problematic when the agent needs to
      revisit earlier decisions
  description: Context truncation when token limits are reached risks discarding important
    historical details, especially problematic when agents need to revisit earlier
    decisions or maintain consistency across multi-step plans.
- name: Summary Compression Detail Loss
  sources:
  - chunk_ref: 04-GCC-2508.00031 (Chunk 1:54-58)
    quote: relying on a simple compression means removing the fine-grained details,
      weakening the agent's ability to ground its actions in specific prior thoughts
  description: Summary-based compression approaches (e.g., memory.md) remove fine-grained
    details, weakening the agent's ability to ground actions in specific prior thoughts.
    Context becomes either too verbose or too abstract.
- name: Session Latency Increase
  sources:
  - chunk_ref: 04-GCC-2508.00031 (Chunk 1:45-47)
    quote: sessions become increasingly slow and costly as context grows, since longer
      histories are passed as tokens
  description: In CLI-based usage, sessions become increasingly slow and costly as
    context grows, since longer histories are passed as tokens, creating a context
    management bottleneck.
- name: RAG Memory Fragility
  sources:
  - chunk_ref: 04-GCC-2508.00031 (Chunk 2:164-170)
    quote: the RAG-based approach introduced more drawbacks than benefits. It proved
      fragile—minor bugs in serialization or indexing logic led to unpredictable behaviors
  description: RAG-based memory approaches can prove fragile, with minor bugs in serialization
    or indexing leading to unpredictable behaviors. They require non-trivial overhead
    for embedding and storing every intermediate step.
- name: Self-reproduction Performance Gap
  sources:
  - chunk_ref: 04-GCC-2508.00031 (Chunk 2:53-59)
    quote: when the same agent is prompted to reproduce this CLI from scratch without
      GCC, the resulting system performs drastically worse, resolving only 11.7% of
      tasks
  description: Without structured context management (GCC), agent-produced CLI systems
    perform drastically worse (11.7% vs 72.7% original, 40.7% with GCC), demonstrating
    how capability scaffolding matters more than raw model capability.
- name: Protocol Selection Intuition-driven
  sources:
  - chunk_ref: 07-ProtocolBench-2510.17149 (Chunk 1:41-42)
    quote: protocol selection in practice is often intuition-driven and lacks standardized
      guidance
  description: Despite proliferation of multi-agent protocols (A2A, ACP, ANP, Agora),
    selection remains intuition-driven without standardized guidance. Trade-offs remain
    under-characterized in existing benchmarks.
- name: No Single Protocol Dominance
  sources:
  - chunk_ref: 07-ProtocolBench-2510.17149 (Chunk 2:84-88)
    quote: protocol choice significantly impacts system behavior across multiple dimensions—no
      single protocol dominates universally
  description: No single protocol dominates universally across all scenarios. Protocol
    choice significantly impacts system behavior, creating tightly coupled trade-offs
    between task success, latency, overhead, and robustness.
- name: Security-Performance Trade-off
  sources:
  - chunk_ref: 07-ProtocolBench-2510.17149 (Chunk 2:311-321)
    quote: ACP and A2A offer partial security capabilities, lacking TLS transport
      layer protection and tunnel sniffing resistance
  description: Security-performance trade-offs exist between protocol families. ACP
    and A2A lack TLS transport protection and tunnel sniffing resistance, while ANP
    and Agora provide comprehensive security at the cost of increased latency overhead.
- name: Router A2A-ACP Confusion
  sources:
  - chunk_ref: 07-ProtocolBench-2510.17149 (Chunk 2:373-375)
    quote: the spec-only baseline attains 53.5% scenario accuracy...with errors dominated
      by A2A/ACP confusions
  description: Protocol router selection shows systematic confusions, particularly
    between A2A and ACP protocols. Spec-only baseline achieves only 53.5% scenario
    accuracy with errors dominated by A2A-ACP confusion.
- name: ProtocolRouter Workload Assumption
  sources:
  - chunk_ref: 07-ProtocolBench-2510.17149 (Chunk 3:302-304)
    quote: ProtocolRouter's learning approach assumes stationary or slowly-changing
      workload distributions. Rapid context switches or rare events may not provide
      sufficient signal for adaptation
  description: ProtocolRouter assumes stationary or slowly-changing workload distributions.
    Rapid context switches or rare events may not provide sufficient signal for adaptation.
    Computational overhead of maintaining multiple protocol states could become prohibitive
    at very large scales.
- name: A2A Inbox Not Universally Implemented
  sources:
  - chunk_ref: 07-ProtocolBench-2510.17149 (Chunk 6:310-311)
    quote: /inbox is not universally implemented (PAL keeps a negative cache); receive_message()
      is a compatibility stub
  description: A2A protocol limitation where the /inbox endpoint is not consistently
    implemented across systems, requiring PAL to maintain a negative cache. The receive_message
    function serves only as a compatibility stub rather than full functionality.
- name: ACP Streaming Depends on Server SSE
  sources:
  - chunk_ref: 07-ProtocolBench-2510.17149 (Chunk 6:314-315)
    quote: Streaming depends on server SSE; long-running flows require /acp/status
  description: ACP protocol limitation where streaming functionality is contingent
    on server-side SSE (Server-Sent Events) support. Long-running workflows must rely
    on polling the /acp/status endpoint.
- name: ANP DID Verification Bypass in Test Configs
  sources:
  - chunk_ref: 07-ProtocolBench-2510.17149 (Chunk 6:318-320)
    quote: Test configs may enable DID verification bypass; if no DID service is available,
      use HTTP fallback POST /anp/message
  description: ANP adapter limitation where test configurations may bypass DID verification
    for interoperability, and HTTP fallback is required when no DID service is available.
    The local resolver caches target DIDs but is not a general-purpose resolver.
- name: Agora Simplified HTTP Without Official Toolformer
  sources:
  - chunk_ref: 07-ProtocolBench-2510.17149 (Chunk 6:323)
    quote: Without official toolformer, uses simplified HTTP with keyword classification;
      semantics and performance are limited
  description: AgoraClientAdapter limitation where absence of official toolformer
    forces use of simplified HTTP with keyword classification, resulting in limited
    semantics and performance capabilities.
- name: HTTP Transport Not Ordered
  sources:
  - chunk_ref: 07-ProtocolBench-2510.17149 (Chunk 6:331)
    quote: HTTP is not ordered; ANP is near-ordered per session; cross-session requires
      merge logic
  description: Transport layer limitation where HTTP does not guarantee message ordering.
    ANP provides only near-ordering within a single session, and cross-session scenarios
    require application-level merge logic.
- name: Client Adapters No Deduplication Persistence
  sources:
  - chunk_ref: 07-ProtocolBench-2510.17149 (Chunk 6:335-336)
    quote: Client adapters do not persist deduplication; implement on the server or
      one layer up
  description: Idempotency limitation where client adapters do not maintain persistent
    deduplication state. Deduplication must be implemented at the server level or
    in a higher layer of the architecture.
- name: Local Loopback Requires Explicit Default Adapter
  sources:
  - chunk_ref: 07-ProtocolBench-2510.17149 (Chunk 6:326-328)
    quote: IntelligentAgentNetwork._execute_single_agent_task() may use agent.send(agent_id,
      ...) for self-delivery; the network must bind an explicit default adapter
  description: Local loopback limitation requiring explicit binding of a default adapter
    for agent self-delivery when using IntelligentAgentNetwork._execute_single_agent_task()
    for internal routing.
- name: Crippling Interoperability Gaps in Current Protocols
  sources:
  - chunk_ref: 08-LACP-2510.13821 (Chunk 1:100-103)
    quote: The absence of a universal standard necessitates bespoke, often brittle,
      integrations between different agent systems
  description: Current agent communication landscape limitation where lack of universal
    standards creates fragmented ecosystem requiring custom integrations that are
    fragile and impede scalable multi-agent system development.
- name: Security as an Afterthought in Existing Protocols
  sources:
  - chunk_ref: 08-LACP-2510.13821 (Chunk 1:106-109)
    quote: Security is often not a core, mandatory component of existing protocols.
      This design choice exposes systems to significant risks
  description: Fundamental limitation of existing agent protocols where security is
    treated as optional add-on rather than foundational requirement, exposing systems
    to data tampering, agent spoofing, and adversarial attacks.
- name: Monolithic Design Lacks Transactional Integrity
  sources:
  - chunk_ref: 08-LACP-2510.13821 (Chunk 1:112-115)
    quote: Current approaches often tightly couple communication logic with core agent
      implementation. This leads to monolithic systems
  description: Architectural limitation where tight coupling of communication and
    agent logic creates monolithic systems that are difficult to maintain, debug,
    and extend, with no built-in atomic transaction support.
- name: LACP Payload Size Overhead for Small Messages
  sources:
  - chunk_ref: 08-LACP-2510.13821 (Chunk 1:469-471)
    quote: Small (51B) 51 bytes 306 bytes +500%... Medium (151B) 151 bytes 442 bytes
      +191%
  description: LACP protocol limitation where cryptographic overhead causes significant
    payload size increase for small messages (+500% for 51-byte messages), though
    overhead decreases to +30% for larger payloads.
- name: LACP Latency Overhead from Cryptographic Verification
  sources:
  - chunk_ref: 08-LACP-2510.13821 (Chunk 2:78-82)
    quote: The latency overhead is minimal across all scenarios, with an absolute
      increase of only 0.03ms for large, complex tasks
  description: LACP performance limitation with ~3% latency overhead due to cryptographic
    signature verification, though absolute increase remains minimal at 0.03ms for
    realistic payloads.
- name: Under-specification in Multi-Agent LLMs
  sources:
  - chunk_ref: 09-SEMAP-2510.12120 (Chunk 1:18-21)
    quote: 'these systems often fail due to three core deficiencies: under-specification,
      coordination misalignment, and inappropriate verification'
  description: Multi-agent LLM limitation where agent responsibilities and role boundaries
    are poorly defined, leading to ambiguity and system failures. SEMAP addresses
    this through explicit behavioral contracts.
- name: Coordination Misalignment Between Agents
  sources:
  - chunk_ref: 09-SEMAP-2510.12120 (Chunk 1:74-75)
    quote: insufficient interface specification, where inter-agent communication lacks
      semantic structure or typed formats
  description: Multi-agent system limitation where inter-agent communication lacks
    semantic clarity and typed message formats, causing coordination failures. Requires
    structured messaging protocols to resolve.
- name: Inappropriate Transition Logic Without Gating
  sources:
  - chunk_ref: 09-SEMAP-2510.12120 (Chunk 1:76-77)
    quote: inappropriate transition logic, where the system progresses between stages
      without formal gating or validation
  description: Multi-agent workflow limitation where state transitions occur without
    proper verification gates, leading to premature or invalid termination. Lifecycle-guided
    execution with verification is needed.
- name: Loosely Coupled Prompts in Current Frameworks
  sources:
  - chunk_ref: 09-SEMAP-2510.12120 (Chunk 1:83-84)
    quote: these SE principles remain largely absent from current multi-agent LLM
      frameworks, which are often built on loosely coupled prompts
  description: Current multi-agent framework limitation where systems rely on informal,
    ad hoc role specifications and loosely coupled prompts rather than explicit behavioral
    contracts and structured protocols.
- name: 'SEMAP Future Work: Resource Overhead Measurement'
  sources:
  - chunk_ref: 09-SEMAP-2510.12120 (Chunk 2:18-19)
    quote: Future work also includes measuring resource overhead, enabling cross-agent
      tool use, adding formal protocol correctness verification
  description: Current SEMAP limitation acknowledging need for resource overhead measurement,
    cross-agent tool use capabilities, and formal protocol correctness verification
    not yet implemented.
- name: Vague Quality Threshold Definition
  sources:
  - chunk_ref: 10-TalkHier-2502.11098 (Chunk 2:54-56)
    quote: The main Supervisor evaluates whether the summarized feedback meets the
      quality threshold (M_threshold), defined vaguely as 'ensuring correctness'
  description: TalkHier limitation where quality threshold for hierarchical refinement
    is defined vaguely as 'ensuring correctness' or 'achieving high relevance' without
    precise quantitative criteria.
- name: OpenAI o1 Limited API Support
  sources:
  - chunk_ref: 10-TalkHier-2502.11098 (Chunk 2:103-105)
    quote: OpenAI-o1-preview... a beta model using advanced inference techniques,
      though limited by API support
  description: Baseline comparison limitation where OpenAI o1-preview model has restricted
    API support, limiting direct comparisons with TalkHier and other baselines.
- name: Baseline Implementation Challenges for Ad Text
  sources:
  - chunk_ref: 10-TalkHier-2502.11098 (Chunk 2:303-306)
    quote: Setting up baselines like AutoGPT, AgentVerse, and GPTSwarm for this task
      was challenging, as their implementations focus on general benchmarks
  description: Multi-agent framework limitation where existing systems like AutoGPT,
    AgentVerse, and GPTSwarm require significant customization for domain-specific
    tasks like ad text generation.
- name: High API Cost from Hierarchical Communication
  sources:
  - chunk_ref: 10-TalkHier-2502.11098 (Chunk 3:6-15)
    quote: One of the main limitations of TalkHier is the relatively high API cost
      associated with the experiments... This is a trade-off due to the design of
      TalkHier
  description: TalkHier limitation where hierarchical multi-agent collaboration with
    structured communication protocol significantly increases computational expenses
    and API costs, posing barriers for resource-limited researchers. Total experiment
    cost approximately $2,100 USD.
- name: LLM Research Accessibility and Democratization Concerns
  sources:
  - chunk_ref: 10-TalkHier-2502.11098 (Chunk 3:11-15)
    quote: This raises broader concerns about the accessibility and democratization
      of LLM research, as such costs may pose barriers for researchers with limited
      resources
  description: Systemic limitation where high computational costs of multi-agent LLM
    systems create accessibility barriers, limiting research democratization and participation
    by resource-constrained researchers.
- name: LLM Intrinsic Hallucination Limitation
  sources:
  - chunk_ref: 12-CollabSurvey-2501.06322 (Chunk 1:69-70)
    quote: While these models have shown remarkable capabilities individually, they
      still suffer from intrinsic limitations such as hallucination
  description: Fundamental LLM limitation where models generate false or fabricated
    information (hallucination), which can propagate and amplify in multi-agent collaborative
    systems.
- name: Auto-regressive Nature Limits Slow-Thinking
  sources:
  - chunk_ref: 12-CollabSurvey-2501.06322 (Chunk 1:70)
    quote: auto-regressive nature (e.g., incapable of slow-thinking)
  description: Inherent LLM architectural limitation where auto-regressive generation
    prevents deliberate slow-thinking capabilities, requiring multi-agent approaches
    to address complex reasoning tasks.
- name: LLMs Not Trained for Inter-Agent Communication
  sources:
  - chunk_ref: 12-CollabSurvey-2501.06322 (Chunk 1:390-391)
    quote: LLMs are not inherently designed and trained to communicate with one another,
      leaving a wide array of potential applications and open problems
  description: Fundamental limitation where LLMs lack native training for multi-agent
    communication, creating challenges in collaborative task execution and requiring
    additional protocols and frameworks.
- name: Context Window Token Constraint
  sources:
  - chunk_ref: 12-CollabSurvey-2501.06322 (Chunk 2:31-33)
    quote: 'Environment e: the environment or context... In LLM, usually the context
      window is constrained by the number of tokens'
  description: LLM operational limitation where context window size constrains the
    amount of environmental and task information agents can process, limiting collaborative
    task complexity.
- name: Cooperation Communication and Computational Overhead
  sources:
  - chunk_ref: 12-CollabSurvey-2501.06322 (Chunk 2:342-345)
    quote: Frequent communication and multiple collaboration channels in C between
      agents can lead to increased computational cost and complexity
  description: Multi-agent cooperation limitation where frequent inter-agent communication
    increases computational costs and system complexity, particularly challenging
    in dynamic environments.
- name: Agent Failure Amplification in Cooperative MAS
  sources:
  - chunk_ref: 12-CollabSurvey-2501.06322 (Chunk 2:354-357)
    quote: the failure of one agent or more agents (e.g., infinite conversation loop,
      amplified hallucinations) can negatively impact the entire system
  description: Cooperative MAS limitation where single agent failures such as infinite
    loops or hallucinations can cascade and negatively impact the entire multi-agent
    system performance.
- name: Unpredictable Agent Behavior in Cooperation
  sources:
  - chunk_ref: 12-CollabSurvey-2501.06322 (Chunk 2:346-348)
    quote: agents may act unpredictably by sending messages to themselves, pretending
      to be clients
  description: Multi-agent cooperation limitation where agents may exhibit unpredictable
    behaviors including self-messaging and role impersonation, requiring failure handling
    and trustworthiness mechanisms.
- name: Competition Conflict Resolution Requirements
  sources:
  - chunk_ref: 12-CollabSurvey-2501.06322 (Chunk 2:391-393)
    quote: competition can also introduce challenges, including potential conflicts
      that require mechanisms to ensure that competition remains constructive
  description: Competitive MAS limitation requiring additional mechanisms to resolve
    conflicts and ensure competition remains beneficial to overall system goals rather
    than destructive.
- name: Suboptimal MAS Design Underperforms Single-Agent
  sources:
  - chunk_ref: 12-CollabSurvey-2501.06322 (Chunk 2:394-396)
    quote: a MAS approach with suboptimal design for their competitive collaboration
      channels can be overtaken by single-agent counterparts with strong prompts
  description: Critical MAS limitation where poorly designed competitive collaboration
    channels result in worse performance than single-agent systems with well-crafted
    prompts on reasoning tasks.
- name: Rule-based Systems Low Adaptability
  sources:
  - chunk_ref: 12-CollabSurvey-2501.06322 (Chunk 3:97-101)
    quote: rule-based systems suffer from a lack of adaptability. When confronted
      with unexpected situations or dynamic environments... these systems may fail
  description: Rule-based protocol limitation where predefined rules cannot handle
    unexpected situations or dynamic environments, requiring manual intervention.
    Rule complexity grows exponentially with task complexity.
- name: Role-based Systems Rigidity and Interdependencies
  sources:
  - chunk_ref: 12-CollabSurvey-2501.06322 (Chunk 3:134-138)
    quote: role-based systems can show rigidity, which might result in disputes or
      functional deficiencies... interdependencies between agent jobs are intrinsically
      linked
  description: Role-based protocol limitation where improperly specified roles cause
    system rigidity, disputes, and functional gaps. Performance depends heavily on
    effective inter-role communication.
- name: Model-based Systems Complex and Computationally Expensive
  sources:
  - chunk_ref: 12-CollabSurvey-2501.06322 (Chunk 3:173-176)
    quote: the greater complexity of model-based solutions is a trade-off for their
      flexibility. These systems can be difficult to design and deploy
  description: Model-based protocol limitation where probabilistic decision-making
    increases system complexity, implementation difficulty, and computational costs,
    potentially restricting real-time applications.
- name: Centralized Structure Single Point of Failure
  sources:
  - chunk_ref: 12-CollabSurvey-2501.06322 (Chunk 4:193-197)
    quote: If the central node fails the entire system might collapse... System is
      less resilient to disruptions
  description: Centralized communication structure limitation where failure of the
    central coordinating agent causes complete system collapse, reducing overall resilience
    and fault tolerance.
- name: Decentralized Structure Inefficient Resource Allocation
  sources:
  - chunk_ref: 12-CollabSurvey-2501.06322 (Chunk 4:198-200)
    quote: Inefficient resource allocation... High communication overheads
  description: Decentralized structure limitation causing inefficient resource distribution
    and high communication overhead as agents must coordinate directly without central
    management.
- name: Hierarchical Structure Edge Device Criticality
  sources:
  - chunk_ref: 12-CollabSurvey-2501.06322 (Chunk 4:203-206)
    quote: Edge devices become critical as a failure in edge devices lead to system
      failure... High complexity and latency
  description: Hierarchical structure limitation where edge device failures cascade
    to system-wide failures, combined with increased complexity and communication
    latency across layers.
- name: Static Architecture Scalability and Flexibility Issues
  sources:
  - chunk_ref: 12-CollabSurvey-2501.06322 (Chunk 4:14-19)
    quote: Relies on accurate initial design and domain knowledge... Fixed channels
      may deal with scalability and flexibility
  description: Static coordination architecture limitation requiring accurate initial
    design and domain expertise, with fixed collaboration channels limiting scalability
    and adaptability to new requirements.
- name: Dynamic Architecture Resource Usage and Failure Risk
  sources:
  - chunk_ref: 12-CollabSurvey-2501.06322 (Chunk 4:20-26)
    quote: Higher resource usage due to real-time adjustments... Potential failures
      in dynamic adjustments
  description: Dynamic coordination architecture limitation with increased resource
    consumption for real-time adjustments and risk of failures during dynamic channel
    reconfiguration.
- name: LLMs Struggle with Long Scenario Coherence
  sources:
  - chunk_ref: 12-CollabSurvey-2501.06322 (Chunk 5:6-7)
    quote: LLMs struggle to maintain coherence and relevance in long scenarios
  description: Multi-agent debate limitation where LLMs have difficulty maintaining
    coherent and relevant arguments across extended debate scenarios, affecting quality
    of collaborative reasoning.
- name: ChatDev Requires Clear Detailed Requirements
  sources:
  - chunk_ref: 12-CollabSurvey-2501.06322 (Chunk 5:41-47)
    quote: Without clear, detailed requirements, agents struggle to grasp task ideas...
      Automating the evaluation of general-purpose software is highly complex
  description: ChatDev framework limitation where agents require explicit, detailed
    requirements to understand tasks. Multiple agents increase token usage and computational
    demands. Automated software evaluation remains highly complex.
- name: Swarm Framework Not Production-Ready
  sources:
  - chunk_ref: 12-CollabSurvey-2501.06322 (Chunk 5:94-96)
    quote: Concern mainly with role-based protocol & centralised/decentralized structure...
      Not yet production-ready
  description: OpenAI Swarm limitation where framework focuses primarily on role-based
    protocols and specific structures, and is not yet suitable for production deployment.
- name: Training Data Bias Limitation
  sources:
  - chunk_ref: 12-CollabSurvey-2501.06322 (Chunk 6:28-30)
    quote: biases in training data that lack global psychological diversity, cautioning
      against treating stand-alone LLMs as universal solutions
  description: LLMs are limited by biases in training data that lack global psychological
    diversity. This constrains their ability to represent diverse perspectives and
    cultural contexts, making them unsuitable as universal solutions for social science
    applications.
- name: Information Asymmetry Limitation
  sources:
  - chunk_ref: 12-CollabSurvey-2501.06322 (Chunk 6:87-90)
    quote: limitations of using LLMs as human replacements in social science experiments,
      particularly in scenarios involving information asymmetry
  description: LLMs face fundamental limitations in scenarios requiring information
    asymmetry where agents have unequal access to private mental states or goals.
    This restricts their effectiveness in social science experiments and competitive/conflict
    resolution tasks.
- name: Hallucination Propagation in MAS
  sources:
  - chunk_ref: 12-CollabSurvey-2501.06322 (Chunk 6:125-128)
    quote: A single agent's hallucination can be spread and reinforced by other agents,
      leading to minor inaccuracies into critical and cascading effects
  - chunk_ref: 18-HallucinationSurvey-2509.18970 (Chunk 1:196-201)
    quote: agent hallucinations often span multiple steps and involve multi-state
      transitions...may also arise during intermediate processes such as perception
      and reasoning, where they can propagate and accumulate
  - chunk_ref: 22-PROV-AGENT-2508.02866 (Chunk 1:21-23)
    quote: agents can hallucinate or reason incorrectly, propagating errors when one
      agent's output becomes another's input
  description: Merged from 3 sources. A critical limitation of agentic workflows is
    error propagation - when an agent hallucinates or reasons incorrectly, these errors
    cascade through the workflow as outputs become inputs to downstream agents, compounding
    inaccuracies.
- name: LLM Overconfidence Problem
  sources:
  - chunk_ref: 12-CollabSurvey-2501.06322 (Chunk 6:173-177)
    quote: LLM overconfidence problem, where LLMs persistently assert the correctness
      of their outputs despite inaccuracies
  description: LLMs exhibit an overconfidence problem where they persistently assert
    correctness despite inaccuracies. This limitation, combined with misunderstandings
    between agents during collaboration, amplifies safety risks in multi-agent systems.
- name: Scalability and Resource Bottlenecks
  sources:
  - chunk_ref: 12-CollabSurvey-2501.06322 (Chunk 6:131-135)
    quote: Managing resources (memory, processing time), coordination and collaboration
      channels among a growing number of agents introduces additional complexities
  description: Increasing agent population creates significant resource management
    challenges including memory, processing time, and coordination channel overhead.
    This introduces complexities like maintaining efficiency and preventing bottlenecks.
- name: Limited Decision-Making Methods
  sources:
  - chunk_ref: 12-CollabSurvey-2501.06322 (Chunk 6:119-122)
    quote: Current LLM-based MASs commonly utilize limited decision-making methods,
      such as dictatorial or popular voting, which may not capture different aspects
      of agent preferences
  description: Current MAS implementations rely on limited decision-making approaches
    (dictatorial/voting) that fail to capture diverse agent preferences and may aggregate
    overconfidence of LLMs.
- name: Inconsistent Evaluation Results
  sources:
  - chunk_ref: 12-CollabSurvey-2501.06322 (Chunk 6:163-168)
    quote: evaluations of MASs are often conducted in narrow scenarios with different
      configurations, leading to inconsistent and incomparable results
  description: MAS evaluations suffer from inconsistency due to narrow scenarios and
    varying configurations, preventing objective comparison between systems and tracking
    progress across the field.
- name: Short-term Memory Transience
  sources:
  - chunk_ref: 15-AgentSurvey-2503.21460 (Chunk 1:377-382)
    quote: its transient nature limits knowledge retention beyond immediate contexts—intermediate
      reasoning traces often dissipate after task completion and cannot be directly
      transferred
  description: Short-term memory is inherently transient, causing intermediate reasoning
    traces to dissipate after task completion. This prevents direct knowledge transfer
    to new scenarios.
- name: Single-path Chaining Inflexibility
  sources:
  - chunk_ref: 15-AgentSurvey-2503.21460 (Chunk 1:452-456)
    quote: it may suffer from a lack of flexibility and error accumulation during
      chaining, as the agent is required to follow the pre-defined plan without any
      deviation
  description: Single-path planning chaining lacks flexibility and causes error accumulation
    because agents must follow pre-defined plans without deviation during problem-solving.
- name: Centralized Control Bottleneck
  sources:
  - chunk_ref: 15-AgentSurvey-2503.21460 (Chunk 2:244-246)
    quote: centralized architectures where a single control node often becomes a bottleneck
      due to handling all inter-agent communication, task scheduling, and contention
      resolution
  description: Centralized architectures create bottlenecks at the single control
    node which must handle all inter-agent communication, task scheduling, and contention
    resolution.
- name: Weak Discriminator Controller Limitation
  sources:
  - chunk_ref: 15-AgentSurvey-2503.21460 (Chunk 2:237-240)
    quote: WJudge demonstrates that even controllers with limited discriminative power
      can also significantly enhance the overall performance of agent systems
  description: While weak discriminators can still improve performance, controllers
    with limited discriminative power represent a constraint on optimal task allocation
    and decision aggregation.
- name: Adversarial Attack Vulnerability
  sources:
  - chunk_ref: 15-AgentSurvey-2503.21460 (Chunk 4:82-84)
    quote: Adversarial attacks aim to compromise the reliability of the agents, rendering
      them ineffective in specific tasks
  description: LLM agents are vulnerable to adversarial attacks targeting Perception,
    Brain, and Action components, which can compromise reliability and render agents
    ineffective in specific tasks.
- name: Backdoor Attack Vulnerability
  sources:
  - chunk_ref: 15-AgentSurvey-2503.21460 (Chunk 4:139-141)
    quote: Backdoor attacks implant specific triggers to cause the model to produce
      preset errors when encountering these triggers while performing normally under
      normal inputs
  description: LLM agents are vulnerable to backdoor attacks that implant specific
    triggers causing preset errors upon activation while appearing normal under regular
    inputs.
- name: Model Collaboration Attack Vulnerability
  sources:
  - chunk_ref: 15-AgentSurvey-2503.21460 (Chunk 4:163-167)
    quote: attackers manipulate the interaction or collaboration mechanisms between
      multiple models to disrupt the overall functionality of the system
  description: Multi-agent systems face collaboration attacks where adversaries exploit
    contagion and recursion in agent interactions, disrupting communications and system
    functionality in ways hard to mitigate via alignment.
- name: Privacy Leakage from Memorization
  sources:
  - chunk_ref: 15-AgentSurvey-2503.21460 (Chunk 4:408-412)
    quote: privacy concerns...mainly caused by the memory capacity of LLMs, which
      may lead to the leakage of private information during conversations or when
      completing tasks
  description: LLM memory capacity creates privacy vulnerabilities where private information
    may leak during conversations or task completion, particularly severe in multi-agent
    systems with multiple sensitive data sources.
- name: Data Extraction Attack Risk
  sources:
  - chunk_ref: 15-AgentSurvey-2503.21460 (Chunk 5:43-51)
    quote: The risk of data extraction increases with model size, frequency of repeated
      data, and context length
  description: Data extraction attacks exploit LLM memory to extract sensitive PII.
    Risk increases with model size, data repetition frequency, and context length,
    making larger models more vulnerable.
- name: Membership Inference Vulnerability
  sources:
  - chunk_ref: 15-AgentSurvey-2503.21460 (Chunk 5:54-66)
    quote: fine-tuning the model head makes it more vulnerable to such attacks...particularly
      dangerous in multi-agent systems, as the training data may originate from multiple
      sources
  description: Fine-tuned LLMs are more vulnerable to membership inference attacks.
    This is especially dangerous in MAS where training data comes from multiple sensitive
    sources.
- name: Bias and Discrimination in LLMs
  sources:
  - chunk_ref: 15-AgentSurvey-2503.21460 (Chunk 5:237-248)
    quote: LLM agents inherently inherit biases present in their training datasets
      and may even amplify them during the learning process, leading to skewed outputs
  description: LLM agents inherit and potentially amplify biases from training data,
    leading to skewed outputs and reinforced stereotypes. This limits fairness and
    ethical deployment.
- name: Lack of Semantic Understanding
  sources:
  - chunk_ref: 15-AgentSurvey-2503.21460 (Chunk 5:283-288)
    quote: LLM agents lack true semantic and contextual understanding, relying purely
      on statistical word associations. This limitation is often misinterpreted and
      overestimated
  description: LLM agents lack true semantic understanding, relying on statistical
    word associations. This fundamental limitation is frequently overestimated, leading
    to undue reliance on models.
- name: Carbon Footprint and Computational Costs
  sources:
  - chunk_ref: 15-AgentSurvey-2503.21460 (Chunk 5:288-291)
    quote: concerns have been raised about the significant carbon footprint of LLM
      agents, posing environmental challenges, alongside the high computational costs
  description: LLM agents pose environmental challenges due to significant carbon
    footprint and high computational costs associated with training large models.
- name: Medical AI Validation Complexity
  sources:
  - chunk_ref: 15-AgentSurvey-2503.21460 (Chunk 6:205-208)
    quote: the multi-agent paradigm in medicine holds promise for improving AI reliability
      by introducing redundancy, specialization, and oversight. However, it also complicates
      the system, requiring rigorous validation
  description: While multi-agent medical systems improve reliability through redundancy
    and specialization, they introduce system complexity that demands rigorous validation
    before deployment.
- name: Static Benchmark Inadequacy
  sources:
  - chunk_ref: 15-AgentSurvey-2503.21460 (Chunk 7:3-11)
    quote: Traditional AI evaluation frameworks, designed for static datasets and
      single-turn tasks, fail to capture the complexities of LLM agents in dynamic,
      multi-turn, and multi-agent environments
  description: Static benchmarks and single-turn evaluation frameworks are inadequate
    for assessing LLM agents in dynamic multi-turn, multi-agent environments, risking
    data contamination and memorization-based performance.
- name: Role-playing Training Data Limitation
  sources:
  - chunk_ref: 15-AgentSurvey-2503.21460 (Chunk 7:41-45)
    quote: LLMs are predominantly trained on web-based corpora, they struggle to emulate
      roles with insufficient representation online and often produce conversations
      lacking diversity
  description: LLMs trained on web corpora struggle with role-playing scenarios lacking
    sufficient online representation, producing conversations that lack diversity.
- name: Scalability and Coordination Challenges
  sources:
  - chunk_ref: 15-AgentSurvey-2503.21460 (Chunk 7:61)
    quote: significant challenges remain, including scalability limitations, memory
      constraints, reliability concerns, and inadequate evaluation frameworks
  description: LLM agent systems face persistent challenges including scalability
    limitations, memory constraints, reliability concerns, and inadequate evaluation
    frameworks.
- name: Agent Hallucination Type Diversity
  sources:
  - chunk_ref: 18-HallucinationSurvey-2509.18970 (Chunk 1:192-196)
    quote: Rather than the straightforward response errors of a single model, agent
      hallucinations are compound behaviors arising from interactions among multiple
      modules
  description: Agent hallucinations are more diverse than single-model errors, arising
    as compound behaviors from multi-module interactions, resulting in broader and
    more varied hallucination types.
- name: Physically Consequential Errors
  sources:
  - chunk_ref: 18-HallucinationSurvey-2509.18970 (Chunk 1:201-205)
    quote: Agent hallucinations involve 'physically consequential' errors, where incorrect
      embodied actions can directly affect task execution, system devices, and user
      experiences in the real world
  description: Agent hallucinations carry higher stakes than text errors because they
    involve physically consequential outcomes - incorrect actions directly affect
    real-world task execution, devices, and user experiences.
- name: Problematic Objective Expression
  sources:
  - chunk_ref: 18-HallucinationSurvey-2509.18970 (Chunk 2:127-132)
    quote: when the expression of goal information carries a certain degree of semantic
      vagueness, it can easily lead to erroneous parsing of user intention and induce
      reasoning hallucinations
  description: Semantic vagueness in goal expressions from incomplete specifications
    or ambiguous content causes erroneous parsing of user intentions, triggering reasoning
    hallucinations.
- name: Instruction-following Deviation
  sources:
  - chunk_ref: 18-HallucinationSurvey-2509.18970 (Chunk 2:141-148)
    quote: the agent struggles to segment critical fields and extract key information
      from instructions, leading to distorted understanding and reasoning hallucinations
  description: Agents with instruction-following deviation struggle to segment critical
    fields and extract key information, leading to distorted understanding and context
    window over-reliance on recent tokens.
- name: Sub-intention Modeling Deficiency
  sources:
  - chunk_ref: 18-HallucinationSurvey-2509.18970 (Chunk 2:154-175)
    quote: 'Inadequate modeling of dependency relationships among these sub-intentions
      can give rise to three types of errors: Sub-intention Omission, Sub-intention
      Redundancy and Sub-intentions Disorder'
  description: Deficient dependency modeling in intention decomposition causes sub-intention
    omission (missing critical steps), redundancy (task-irrelevant additions), and
    disorder (wrong sequencing), compromising reasoning integrity.
- name: Self-knowledge Boundary Overconfidence
  sources:
  - chunk_ref: 18-HallucinationSurvey-2509.18970 (Chunk 2:191-196)
    quote: When confronted with planning problems beyond its knowledge boundary, the
      agent tends to respond with excessive confidence, generating answers that sound
      certain but are actually incorrect
  description: Agents overconfidently generate seemingly certain but incorrect answers
    when faced with problems beyond their knowledge boundaries, causing planning generation
    hallucinations.
- name: Tool Documentation Limitation
  sources:
  - chunk_ref: 18-HallucinationSurvey-2509.18970 (Chunk 2:253-261)
    quote: deficiencies may include redundant information, incomplete or inaccurate
      descriptions, or a lack of standardization, all of which impair the agent's
      ability to properly use tools
  description: Tool documentation limitations including redundancy, incompleteness,
    inaccuracy, and lack of standardization cause execution hallucinations where agents
    believe they correctly use tools despite misleading documentation.
- name: Shallow Tool Pattern Understanding
  sources:
  - chunk_ref: 18-HallucinationSurvey-2509.18970 (Chunk 2:262-276)
    quote: LLM-based agents are typically trained with insufficient exposure to diverse
      and complex tool-use scenarios...prone to hallucinating tool invocations that
      may appear plausible
  description: Insufficient training on diverse tool-use scenarios causes shallow
    pattern understanding, making agents prone to plausible-appearing but invalid
    tool invocations, especially for complex or novel tasks.
- name: Weak Tool Dynamic Adaptability
  sources:
  - chunk_ref: 18-HallucinationSurvey-2509.18970 (Chunk 2:277-287)
    quote: When an agent lacks sufficient adaptability to these Tool Dynamics, its
      tool-use behavior becomes misaligned with the actual environment
  description: Agents trained on static datasets lack adaptability to evolving tool
    functionalities, API modifications, and deprecations, causing misalignment between
    learned behavior and current environment state.
- name: Lack of Tool Solvability Awareness
  sources:
  - chunk_ref: 18-HallucinationSurvey-2509.18970 (Chunk 2:298-312)
    quote: A lack of solvability awareness in LLM-based agents can also lead to execution
      hallucinations, where the agent mistakenly assumes that pt is solvable
  description: Agents lacking solvability awareness mistakenly assume plans are executable,
    leading to retrieval of irrelevant/fabricated tools or parameter hallucination
    when suitable tools are unavailable or plans are unclear.
- name: Limited Multimodal Encoding Capability
  sources:
  - chunk_ref: 18-HallucinationSurvey-2509.18970 (Chunk 2:329-341)
    quote: Agents struggle to extract the key information of individual modality...Agents
      lack an effective mechanism to integrate semantic associations across different
      modalities
  description: Perception hallucinations arise from insufficient unimodal representation
    (failing to extract key modality information) and weak cross-modal collaboration
    (failing to integrate semantic associations across modalities).
- name: Memory Priority Assignment Failures
  sources:
  - chunk_ref: 18-HallucinationSurvey-2509.18970 (Chunk 3:9-20)
    quote: poorly assigned priorities can result in the elimination of important information
      or the retention of irrelevant content...merged memory containing inherent conflicts
  description: Imperfect memory priority assignment causes elimination of important
    information, retention of irrelevant content, and conflicting merged memories
    with implicit semantic differences.
- name: Information Compression Distortion
  sources:
  - chunk_ref: 18-HallucinationSurvey-2509.18970 (Chunk 3:21-30)
    quote: generated summaries may be overly general, omit crucial details, or introduce
      distortions due to imperfect abstraction...Memory Capacity Constraints exacerbate
      these challenges
  description: Memory update processes suffer from information compression issues
    where summaries become overly general, omit details, or introduce abstraction
    distortions, exacerbated by capacity constraints and non-standardized formats.
- name: Erroneous Message Propagation in MAS
  sources:
  - chunk_ref: 18-HallucinationSurvey-2509.18970 (Chunk 3:45-60)
    quote: some agents may produce messages containing inaccurate facts, misinterpretations
      of shared knowledge, or misleading inferences...Content Redundancy is also an
      important cause
  description: Communication hallucinations arise from LLM factuality/faithfulness
    issues propagating through MAS, compounded by content redundancy that obscures
    signals and information asymmetry causing incomplete instructions.
- name: Uncoordinated Communication Protocols
  sources:
  - chunk_ref: 18-HallucinationSurvey-2509.18970 (Chunk 3:61-78)
    quote: Without a unified and effective protocol, agents may 'talk past each other'...Asynchronous
      Scheduling...information loss and information overload
  description: Lack of unified communication protocols causes agents to 'talk past
    each other'. Asynchronous scheduling leads to information loss/overload, while
    natural language formats introduce ambiguity requiring structured alternatives.
- name: Deep Layer Detection Difficulty
  sources:
  - chunk_ref: 18-HallucinationSurvey-2509.18970 (Chunk 4:128-142)
    quote: memory and communication are part of the deeper layers of the agent, where
      the final outputs are coupled with computations from numerous intermediate modules.
      This makes hallucination detection...more challenging
  description: Hallucination detection for memory and communication is more challenging
    than perception because these deeper layers couple final outputs with numerous
    intermediate module computations, complicating localization.
- name: Hallucinatory Accumulation Challenge
  sources:
  - chunk_ref: 18-HallucinationSurvey-2509.18970 (Chunk 4:153-161)
    quote: hallucinations can accumulate and amplify over time. In such cases, hallucinations
      may initially appear as minor issues, but their iterative accumulation can ultimately
      lead to severe consequences
  description: Multi-step agent decision-making allows hallucinations to accumulate
    and amplify over time, transforming initially minor issues into severe consequences
    through iterative accumulation.
- name: Full-chain Error Propagation Complexity
  sources:
  - chunk_ref: 18-HallucinationSurvey-2509.18970 (Chunk 4:170-182)
    quote: agent hallucinations are far more complex, involving full-chain error propagation
      across multiple interdependent components...may arise at any stage...and often
      exhibit complex characteristics
  description: Agent hallucinations involve full-chain error propagation across interdependent
    components, arising at any pipeline stage with characteristics like hallucinatory
    accumulation and inter-module dependency.
- name: Transformer Architecture Limitations
  sources:
  - chunk_ref: 18-HallucinationSurvey-2509.18970 (Chunk 4:232-241)
    quote: this architecture faces challenges in handling long-context information
      and suffers from high computational complexity, which have gradually revealed
      performance bottlenecks
  description: Transformer architecture limitations in handling long-context information
    and high computational complexity create performance bottlenecks contributing
    to hallucination issues.
- name: Black-Box Access Limitation
  sources:
  - chunk_ref: 19-HalMit-2507.15903 (Chunk 1:23-24)
    quote: the related approaches either depend on white-box access to LLMs or fail
      to accurately identify hallucinations
  description: Existing hallucination detection approaches have a fundamental limitation
    requiring white-box access to LLM internal states, which is not feasible for commercial,
    closed-source models. This prevents their application to large-scale commercial
    systems.
- name: Vacuous Bounds at Billion-Parameter Scale
  sources:
  - chunk_ref: 19-HalMit-2507.15903 (Chunk 1:60-61)
    quote: these bounds tend to become vacuous at the scale of billion-parameter models
  description: Theoretical generalization bounds derived for deep learning models
    become vacuous and non-applicable when applied to billion-parameter LLMs, limiting
    their utility for hallucination detection in modern large-scale agents.
- name: Semantic Space Complexity Limitation
  sources:
  - chunk_ref: 19-HalMit-2507.15903 (Chunk 1:63-64)
    quote: Given the vastness of the semantic space, the tightness of the existing
      generalization bounds remains difficult to establish
  description: The vast semantic space of natural language makes it extremely challenging
    to establish tight generalization bounds, limiting the precision of hallucination
    detection methods.
- name: Single Metric Threshold Insufficiency
  sources:
  - chunk_ref: 19-HalMit-2507.15903 (Chunk 1:318-322)
    quote: relying solely on a fixed threshold is insufficient. The presence of high-entropy
      yet potentially non-hallucinatory responses
  description: Semantic entropy alone with a fixed threshold is insufficient for hallucination
    detection due to outliers - high-entropy responses that are not hallucinations
    and vice versa, requiring more nuanced detection methods.
- name: Domain-Specific Bound Requirement
  sources:
  - chunk_ref: 19-HalMit-2507.15903 (Chunk 1:87-88)
    quote: Due to the complexity of the semantic space, deriving a universal generalization
      bound across all domains is extremely challenging
  description: No universal generalization bound can be established across all domains
    - hallucination patterns vary significantly across application domains, necessitating
    per-domain and per-agent boundary modeling.
- name: Local Loop Trap in Bound Exploration
  sources:
  - chunk_ref: 19-HalMit-2507.15903 (Chunk 1:92-94)
    quote: The bound exploration process may deviate from the true boundary and become
      trapped in local loops
  description: The generalization bound exploration can get trapped in local loops
    rather than converging to the true boundary, requiring sophisticated fractal sampling
    and reinforcement learning to accelerate convergence.
- name: Poorly Calibrated Confidence Estimates
  sources:
  - chunk_ref: 19-HalMit-2507.15903 (Chunk 1:71-76)
    quote: these black-box/gray-box approaches allow hallucination monitoring through
      output text or associated confidence scores, they are degraded by limited knowledge
      of LLMs and often poorly calibrated confidence estimates
  description: Black-box and gray-box approaches relying on output confidence scores
    suffer from poor calibration, leading to inaccurate hallucination monitoring and
    reduced effectiveness.
- name: White-Box Method Complexity and Computational Demand
  sources:
  - chunk_ref: 19-HalMit-2507.15903 (Chunk 4:50-53)
    quote: Requiring the access to internal states of LLMs to detect hallucinations,
      these methods not only suffer from high complexity and computational demand
      but also may not be feasible for commercial LLM software
  description: White-box hallucination detection methods analyzing internal model
    states have prohibitive computational complexity and are infeasible for commercial
    closed-source LLM systems.
- name: Slangy Dialogue Sensitivity
  sources:
  - chunk_ref: 19-HalMit-2507.15903 (Chunk 3:175-178)
    quote: The only exception is the New York City topic, where SelfCheckGPT outperforms
      our method... This may be due to the miscellaneous slangy dialogues on this
      topic
  description: HalMit performs less well on topics with miscellaneous slangy dialogues
    (e.g., New York City), where alternative methods like SelfCheckGPT may be better
    suited.
- name: Non-Deterministic Workflow Behavior
  sources:
  - chunk_ref: 22-PROV-AGENT-2508.02866 (Chunk 1:80-84)
    quote: agentic workflows are non-deterministic, shaped by near real-time data,
      adaptive decisions, and evolving interactions
  description: Unlike traditional deterministic workflows, agentic workflows exhibit
    non-deterministic behavior with dynamic, cyclic patterns where agent outputs inform
    subsequent decisions, making provenance tracking and reproducibility challenging.
- name: Disconnected Agent Metadata
  sources:
  - chunk_ref: 22-PROV-AGENT-2508.02866 (Chunk 1:161-163)
    quote: these data are typically isolated from the rest of the workflow. This disconnection
      hinders the contextualization of agent interactions or understanding their downstream
      impact
  description: MCP-based agent frameworks record prompts and responses in isolation
    from the broader workflow, limiting the ability to contextualize agent interactions
    and trace their downstream impact.
- name: Traditional Provenance Model Inadequacy
  sources:
  - chunk_ref: 22-PROV-AGENT-2508.02866 (Chunk 1:165-168)
    quote: Existing provenance techniques lack explicit representations of key agent
      artifacts and their integration with the workflow. They typically model workflows
      as static graphs, missing the semantics needed to capture agentic behavior
  description: Existing provenance systems model workflows as static graphs and lack
    semantics for representing agent artifacts, dynamic decisions, and model-driven
    reasoning, limiting their applicability to agentic workflows.
- name: Iterative Error Propagation Risk
  sources:
  - chunk_ref: 22-PROV-AGENT-2508.02866 (Chunk 2:42-45)
    quote: because the agent relies on an LLM, there is a risk of hallucinated or
      incorrect outputs. Since each decision influences the next in this iterative
      loop, a single error may propagate across layers
  description: In iterative agentic workflows with feedback loops, a single LLM hallucination
    or incorrect output can propagate across all subsequent iterations, potentially
    compromising all downstream outputs.
- name: Single-Agent Regression on Complex Tasks
  sources:
  - chunk_ref: 24-EffectiveCollab-2412.05449 (Chunk 2:344-348)
    quote: In the single-agent setting, we observe an absolute regression of up to
      37%. MAC allows each specialist agent to be provided with instructions for the
      specific subset of tasks... This specialized task assignment may not be achievable
      by a single agent
  description: Single agents exhibit significant performance regression (up to 37%)
    on complex tasks compared to multi-agent collaboration, struggling to manage the
    multitude of instructions required for complex tasks and showing more hallucination
    in tool parameters.
- name: Multi-Agent Latency Overhead
  sources:
  - chunk_ref: 24-EffectiveCollab-2412.05449 (Chunk 2:391-396)
    quote: The Software domain consistently demonstrates higher latency metrics across
      all settings, with user-perceived turn latency reaching 168.73s compared to
      31.46s for Travel
  description: Multi-agent collaboration introduces significant latency overhead,
    especially in complex domains like Software Development where user-perceived latency
    can reach 168.73 seconds per session - much higher than simpler domains.
- name: Benchmarking Complexity with Multiple Agents
  sources:
  - chunk_ref: 24-EffectiveCollab-2412.05449 (Chunk 1:123-128)
    quote: Benchmarking single AI agents is already difficult and increasing the number
      of agents to benchmark only complicates the problem
  description: Multi-agent system evaluation is inherently more complex than single-agent
    benchmarking, as success definitions become unclear and multiple correct trajectories
    may exist that are not captured in ground truth.
- name: Static Evaluation Assumption Flaw
  sources:
  - chunk_ref: 24-EffectiveCollab-2412.05449 (Chunk 1:366-371)
    quote: Prior single-agent benchmarking is more static where user inputs and follow-up
      responses are pre-defined... In reality, there may be multiple trajectories
      that enable the agent to fulfill user requests. If those trajectories are not
      captured in the gold-truth, then the agent is incorrectly penalized
  description: Static benchmarking that assumes single correct trajectory incorrectly
    penalizes agents that achieve goals through alternative valid paths, limiting
    evaluation accuracy for dynamic multi-agent systems.
