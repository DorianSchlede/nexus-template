field: failure_mode
aggregated_at: '2025-12-29T10:38:14.578085'
batches_merged: 4
patterns_input: 91
patterns_output: 91
patterns:
- name: Context Collapse Failure
  sources:
  - chunk_ref: 01-ACE-2510.04618 (Chunk 1:182-199)
    quote: at step 60 the context contained 18,282 tokens and achieved an accuracy
      of 66.7, but at the very next step it collapsed to just 122 tokens, with accuracy
      dropping to 57.1
  description: A critical failure mode where monolithic LLM context rewriting causes
    sudden information loss. When an LLM is tasked with fully rewriting accumulated
    context at each adaptation step, it tends to compress it into much shorter, less
    informative summaries. This causes dramatic loss of information - accuracy dropped
    from 66.7% to 57.1% (worse than the 63.7% baseline without adaptation). This is
    a fundamental risk of end-to-end context rewriting where accumulated knowledge
    can be abruptly erased instead of preserved.
- name: Brevity Bias Degradation
  sources:
  - chunk_ref: 01-ACE-2510.04618 (Chunk 1:89-98)
    quote: 'brevity bias: many prompt optimizers prioritize concise, broadly applicable
      instructions over comprehensive accumulation... such abstraction can omit domain-specific
      heuristics'
  description: A failure mode where optimization for conciseness causes loss of critical
    domain-specific details. Prompt optimizers like GEPA highlight brevity as a strength,
    but this can omit tool-use guidelines, common failure modes, and heuristics that
    matter in practice. The brevity objective may align with validation metrics in
    some settings but often fails to capture detailed strategies required by agents
    and knowledge-intensive applications.
- name: Monolithic Rewriting Information Loss
  sources:
  - chunk_ref: 01-ACE-2510.04618 (Chunk 1:94-99)
    quote: methods that rely on monolithic rewriting by an LLM often degrade into
      shorter, less informative summaries over time, causing sharp performance declines
  description: Failure mode in context adaptation where iterative rewriting causes
    progressive degradation. In domains such as interactive agents, domain-specific
    programming, and financial/legal analysis, strong performance depends on retaining
    detailed, task-specific knowledge rather than compressing it away. The monolithic
    rewriting approach fails to preserve this essential knowledge.
- name: Feedback Quality Dependency Failure
  sources:
  - chunk_ref: 01-ACE-2510.04618 (Chunk 1:469-472)
    quote: in the absence of reliable feedback signals (e.g., ground-truth labels
      or execution outcomes), both ACE and other adaptive methods such as Dynamic
      Cheatsheet may degrade
  description: Context adaptation methods critically depend on feedback quality. When
    reliable feedback signals like ground-truth labels or execution outcomes are unavailable,
    adaptive methods including ACE and Dynamic Cheatsheet can degrade in performance.
    This suggests that context adaptation depends critically on feedback quality -
    a key failure mode when operating in environments without strong supervisory signals.
- name: Lost-in-the-Middle Phenomenon
  sources:
  - chunk_ref: 02-ContextSurvey-2507.13334 (Chunk 4:103-108)
    quote: LLMs struggle to access information positioned in middle sections of long
      contexts, performing significantly better when relevant information appears
      at the beginning or end of inputs
  description: A positional bias failure where LLMs fail to access information in
    middle sections of long contexts. This severely impacts performance in extended
    chain-of-thought reasoning tasks where critical earlier results become susceptible
    to forgetting. Performance can degrade drastically by as much as 73% compared
    to performance with no prior context.
- name: Context Window Overflow vs Collapse
  sources:
  - chunk_ref: 02-ContextSurvey-2507.13334 (Chunk 4:115-117)
    quote: Context management faces opposing challenges of context window overflow,
      where models 'forget' prior context due to exceeding window limits, and context
      collapse
  description: 'A dual failure mode in context management: (1) context window overflow
    causes models to ''forget'' prior context when window limits are exceeded, and
    (2) context collapse occurs when enlarged context windows or conversational memory
    cause models to fail in distinguishing between different conversational contexts.
    These are opposing but related failure modes requiring different mitigation strategies.'
- name: Chain-of-Thought Prompting Degradation
  sources:
  - chunk_ref: 02-ContextSurvey-2507.13334 (Chunk 4:118-120)
    quote: Research demonstrates that claimed benefits of chain-of-thought prompting
      don't stem from genuine algorithmic learning but rather depend on problem-specific
      prompts, with benefits deteriorating as problem complexity increases
  description: Chain-of-thought prompting shows diminishing returns as problem complexity
    increases. The benefits don't stem from genuine algorithmic learning but depend
    on problem-specific prompts. This is a failure mode for complex reasoning tasks
    where CoT was expected to provide robust improvements.
- name: Transactional Integrity Failure in Multi-Agent Systems
  sources:
  - chunk_ref: 02-ContextSurvey-2507.13334 (Chunk 6:435-439)
    quote: LangGraph provides basic state management while lacking atomicity guarantees
      and systematic compensation mechanisms, AutoGen prioritizes flexible agent interactions
      without adequate compensatory action management
  description: Multi-agent orchestration frameworks fail to maintain transactional
    integrity across complex workflows. LangGraph lacks atomicity guarantees and compensation
    mechanisms. AutoGen lacks adequate compensatory action management, potentially
    resulting in inconsistent system states following partial failures. Systems relying
    exclusively on LLM self-validation capabilities are exposed to reasoning errors,
    hallucinations, and inter-agent inconsistencies.
- name: Context Handling Failures in Agents
  sources:
  - chunk_ref: 02-ContextSurvey-2507.13334 (Chunk 7:46-53)
    quote: agents struggle with long-term context maintenance encompassing both episodic
      and semantic information... environmental misconfigurations and LLM hallucinations
      can distract agentic systems, with poor recovery leading to goal deviation
  description: 'Agents face compound failure modes: (1) struggle with long-term context
    maintenance for both episodic and semantic information, (2) central orchestrator
    topologies introduce non-deterministic execution paths complicating anomaly detection,
    (3) environmental misconfigurations and LLM hallucinations distract agents, and
    (4) poor recovery leads to goal deviation amplified in multi-agent setups with
    distributed subtasks.'
- name: Inter-Agent Dependency Opacity
  sources:
  - chunk_ref: 02-ContextSurvey-2507.13334 (Chunk 7:56-58)
    quote: Inter-agent dependency opacity presents additional concerns as agents may
      operate on inconsistent assumptions or conflicting data without explicit constraints
      or validation layers
  description: In multi-agent systems, agents may operate on inconsistent assumptions
    or conflicting data without explicit constraints or validation layers. This opacity
    in inter-agent dependencies necessitates anomaly detection incorporating reasoning
    over orchestration intent and planning coherence. Without such mechanisms, agents
    can produce inconsistent outputs.
- name: Memory System Statelessness Limitation
  sources:
  - chunk_ref: 02-ContextSurvey-2507.13334 (Chunk 6:36-39)
    quote: most contemporary LLM-based agents operate in fundamentally stateless manners,
      treating interactions independently without truly accumulating knowledge incrementally
      over time
  description: A fundamental architectural failure mode where LLM-based agents treat
    interactions independently without accumulating knowledge incrementally over time.
    This limitation prevents genuine lifelong learning assessment - a cornerstone
    of human-level intelligence involving continuous knowledge acquisition, retention,
    and reuse across diverse contexts and extended time horizons.
- name: Memory Evaluation Isolation Problem
  sources:
  - chunk_ref: 02-ContextSurvey-2507.13334 (Chunk 6:45-48)
    quote: Methodological issues arise when isolating memory-specific performance
      from other intelligence aspects, challenging determination of whether failures
      stem from inadequate memory mechanisms or reasoning limitations
  description: A methodological failure mode in memory system evaluation where it's
    difficult to isolate memory-specific performance from other intelligence aspects.
    This makes it challenging to determine whether failures stem from inadequate memory
    mechanisms or reasoning limitations. Dynamic memory usage in real-world applications
    poses additional evaluation challenges as controlled tests inadequately capture
    performance in complex scenarios.
- name: Commercial Assistant Memory Degradation
  sources:
  - chunk_ref: 02-ContextSurvey-2507.13334 (Chunk 6:4-7)
    quote: demonstrating 30% accuracy degradation in commercial assistants throughout
      prolonged interactions
  description: Commercial AI assistants (including GPT-4, Claude variants, and Llama
    3.1) show 30% accuracy degradation during prolonged interactions. Cutting-edge
    models encounter difficulties with episodic memory challenges involving interconnected
    events or intricate spatio-temporal associations even in comparatively brief contexts.
    This represents a critical production failure mode.
- name: Single-Agent Baseline Hallucination
  sources:
  - chunk_ref: 03-ClaudeCode-2508.08322 (Chunk 2:57-63)
    quote: The baseline often guessed function or variable names (e.g. referring to
      a non-existent getEvents() API) which then caused failures
  description: Single-agent code assistants without proper context retrieval hallucinate
    non-existent functions and APIs. The baseline attempted to use a refreshToken()
    call that did not exist, whereas multi-agent systems with proper context consultation
    correctly utilized existing functions. This failure mode demonstrates the importance
    of semantic code retrieval providing real definitions to agents.
- name: Incomplete Task Execution in Single-Agent
  sources:
  - chunk_ref: 03-ClaudeCode-2508.08322 (Chunk 1:89-90)
    quote: a baseline single-agent Claude often omitted needed steps... might miss
      necessary edits in distant files or misuse an unfamiliar library
  description: Single-agent code assistants with basic context prompts produce incomplete
    solutions for non-trivial features. They miss necessary edits in distant files
    and misuse unfamiliar libraries, reflecting insufficient context comprehension.
    The baseline produced only the React component and forgot to update the registry
    and type definitions, resulting in runtime errors.
- name: Session Context Erasure
  sources:
  - chunk_ref: 04-GCC-2508.00031 (Chunk 1:47-49)
    quote: closing a session and starting a new one typically erases the agent's memory
      of prior goals, user preferences, and task-specific instruction. As a result,
      users are forced to repeatedly 'teach' the model from scratch
  description: A critical failure mode where closing an agent session erases memory
    of prior goals, user preferences, and task-specific instructions. This forces
    users to repeatedly 're-teach' the model from scratch across sessions, wasting
    time and potentially losing important accumulated context. Current implementations
    lack mechanisms for cross-session memory persistence.
- name: Context Verbosity vs Abstraction Trade-off
  sources:
  - chunk_ref: 04-GCC-2508.00031 (Chunk 1:56-58)
    quote: Currently, context is either too verbose to be reusable, or too abstract
      to support concrete continuation and extension
  description: 'A failure mode where context management falls into two extremes: (1)
    too verbose - context is so detailed it becomes unreusable due to length and complexity,
    or (2) too abstract - high-level summaries remove fine-grained details, weakening
    the agent''s ability to ground its actions in specific prior thoughts. Neither
    extreme supports effective continuation of work.'
- name: Truncation-Based Context Loss
  sources:
  - chunk_ref: 04-GCC-2508.00031 (Chunk 1:50-53)
    quote: The most straightforward is to truncate older context once the token limit
      is reached. While simple, this risks discarding important historical details—especially
      problematic when the agent needs to revisit earlier decisions
  description: Simple context truncation when token limits are reached risks discarding
    important historical details. This is especially problematic when agents need
    to revisit earlier decisions or maintain consistency across multi-step plans.
    The truncation approach fails to preserve the reasoning chain needed for complex,
    long-horizon tasks.
- name: CLI-Produced CLI Performance Degradation Without GCC
  sources:
  - chunk_ref: 04-GCC-2508.00031 (Chunk 2:53-58)
    quote: the same agent is prompted to reproduce this CLI from scratch without GCC,
      the resulting system performs drastically worse, resolving only 11.7% of tasks
  description: Failure mode where an agent attempting to reproduce a CLI system without
    the GCC (Git-Context-Controller) scaffolding achieves dramatically lower performance
    (11.7% vs 72.7% original), demonstrating that capability loss occurs when structured
    memory management is absent.
- name: RAG-based Memory Failure in Branch Exploration
  sources:
  - chunk_ref: 04-GCC-2508.00031 (Chunk 2:164-170)
    quote: the agent itself concluded that the RAG-based approach introduced more
      drawbacks than benefits. It proved fragile—minor bugs in serialization or indexing
      logic led to unpredictable behaviors
  description: Failure mode where RAG-based memory approaches prove fragile and computationally
    expensive, with minor serialization or indexing bugs causing unpredictable system
    behaviors. The system underperformed compared to simpler memory approaches.
- name: One-Shot Ephemeral Context Limitation
  sources:
  - chunk_ref: 04-GCC-2508.00031 (Chunk 2:176-178)
    quote: Such reflective behavior did not emerge in non-GCC environments, where
      agents tended to solve tasks in one shot, operating within narrow, ephemeral
      contexts
  description: Failure mode in non-structured environments where agents cannot develop
    reflective, iterative behaviors and are confined to narrow, ephemeral contexts
    that prevent architectural exploration and learning.
- name: Pre/Post Fault Answer Discovery Degradation
  sources:
  - chunk_ref: 07-ProtocolBench-2510.17149 (Chunk 2:103-106)
    quote: We measure Answer Discovery Rate as the percentage of queries successfully
      resolved in each window, Latency as the median task completion time, and Recovery
      Time as the duration from fault injection to system stabilization
  description: Failure mode measurement framework for multi-agent systems under fault
    conditions. Discovery rate measures query resolution capability, with systems
    showing degraded performance post-fault (e.g., AGORA preserves only 81.29% of
    pre-failure performance).
- name: Protocol Security Capability Gaps
  sources:
  - chunk_ref: 07-ProtocolBench-2510.17149 (Chunk 2:318-320)
    quote: ACP and A2A offer partial security capabilities, lacking TLS transport
      layer protection and tunnel sniffing resistance while maintaining session hijacking
      protection
  description: Failure mode where certain protocols (ACP, A2A) have security gaps
    lacking TLS transport and tunnel sniffing resistance, creating vulnerabilities
    for privacy-sensitive deployments in healthcare or finance domains.
- name: A2A-ACP Protocol Confusion
  sources:
  - chunk_ref: 07-ProtocolBench-2510.17149 (Chunk 2:373-374)
    quote: The spec-only baseline attains 53.5% scenario accuracy and 71.2% module
      accuracy, with errors dominated by A2A to ACP confusions
  description: Failure mode in protocol router selection where A2A and ACP protocols
    are frequently confused due to similar capability profiles, leading to incorrect
    protocol assignments in 46.5% of scenarios.
- name: TLS Downgrade Attack Vulnerability
  sources:
  - chunk_ref: 07-ProtocolBench-2510.17149 (Chunk 4:55-61)
    quote: Conducts 3 rounds of TLS downgrade attacks using weak cipher suites, obsolete
      TLS versions, and HTTP plaintext fallback, recording success and block rates
      for each attempt
  description: Failure mode where protocols may fail security tests involving TLS
    downgrade attacks with weak cipher suites, obsolete TLS versions, expired/self-signed
    certificates, or hostname mismatches leading to security breaches.
- name: Session Hijack and Replay Attack Failure
  sources:
  - chunk_ref: 07-ProtocolBench-2510.17149 (Chunk 4:72-75)
    quote: For session hijack, injects privilege-escalation tokens (e.g., expired_session_*,
      admin_session_*), measuring interception rates via denials or 404s
  description: Failure mode where systems fail to block privilege-escalation tokens
    or replay attacks with old messages, potentially allowing unauthorized access
    or duplicate message processing.
- name: Malformed Module Record Exclusion
  sources:
  - chunk_ref: 07-ProtocolBench-2510.17149 (Chunk 5:103-106)
    quote: If a module record is malformed or absent, the entire scenario is list-wise
      excluded and the exclusion is logged; no zero-filling
  description: Failure mode handling where malformed or missing module records cause
    entire scenario exclusion from evaluation, requiring proper logging and preventing
    silent data corruption through zero-filling.
- name: Unified Error Taxonomy for Adapter Failures
  sources:
  - chunk_ref: 07-ProtocolBench-2510.17149 (Chunk 6:85-87)
    quote: 'Adapter exceptions are normalized by PAL into: E_TIMEOUT, E_HTTP, E_CONN,
      E_PROTOCOL, E_ENCODE/DECODE, E_UNSUPPORTED. PAL increments failure counters
      and re-raises'
  description: 'Standardized failure mode taxonomy normalizing adapter exceptions
    into categories: timeout, HTTP errors, connection failures, protocol errors, encode/decode
    failures, and unsupported operations, enabling consistent error handling across
    protocols.'
- name: A2A Inbox Implementation Absence
  sources:
  - chunk_ref: 07-ProtocolBench-2510.17149 (Chunk 6:310-311)
    quote: 'A2AAdapter: /inbox is not universally implemented (PAL keeps a negative
      cache); receive_message() is a compatibility stub'
  description: Known failure mode where A2A protocol's inbox endpoint is not universally
    implemented, requiring negative caching and compatibility stubs for receive_message
    operations.
- name: ANP DID Verification Bypass Risk
  sources:
  - chunk_ref: 07-ProtocolBench-2510.17149 (Chunk 6:318-320)
    quote: 'ANPAdapter: Test configs may enable DID verification bypass; if no DID
      service is available, use HTTP fallback POST /anp/message'
  description: Failure mode risk where test configurations may bypass DID verification
    for interoperability, requiring strict enforcement in production and fallback
    to HTTP when DID services are unavailable.
- name: Heterogeneous Link Bridge Failure
  sources:
  - chunk_ref: 07-ProtocolBench-2510.17149 (Chunk 7:103-106)
    quote: We enforce 'change transport, not semantics or security.' Homogeneous links
      use the chosen protocol natively. Heterogeneous links install stateless bridges
      around the UTE
  description: Failure prevention pattern requiring that cross-protocol bridges only
    perform transport translation without altering business content or security attributes,
    maintaining semantic integrity during protocol transitions.
- name: Circuit Breaking and Bulkhead Saturation
  sources:
  - chunk_ref: 07-ProtocolBench-2510.17149 (Chunk 7:168-177)
    quote: 'Testing matrix: Policy conformance: selection, sticky sessions, hedging,
      retry categories. Failure drills: open circuit, half-open probes, bulkhead saturation'
  description: Failure mode testing framework including circuit breaker states (open/half-open/close),
    bulkhead saturation scenarios, and retry policy verification to ensure system
    resilience under various failure conditions.
- name: Ad-Hoc Protocol Communication Chasm
  sources:
  - chunk_ref: 08-LACP-2510.13821 (Chunk 1:39-46)
    quote: This fragmentation creates a persistent communication chasm—hindering interoperability,
      compromising security, and impeding reproducible scientific progress
  description: Failure mode in fragmented LLM agent ecosystems where lack of standardization
    leads to ambiguous semantics, security vulnerabilities, unreliable information
    exchange, and task failures.
- name: Security as Afterthought Vulnerability
  sources:
  - chunk_ref: 08-LACP-2510.13821 (Chunk 1:106-109)
    quote: Security is often not a core, mandatory component of existing protocols.
      This design choice exposes systems to significant risks, including data tampering,
      agent spoofing
  description: Critical failure mode where security is treated as optional rather
    than mandatory, exposing multi-agent systems to data tampering, agent spoofing,
    and adversarial attacks in safety-critical applications.
- name: Monolithic Design Transaction Integrity Failure
  sources:
  - chunk_ref: 08-LACP-2510.13821 (Chunk 1:112-116)
    quote: Current approaches often tightly couple communication logic with core agent
      implementation. This leads to monolithic systems... the general absence of built-in
      support for atomic transactions undermines reliability
  description: Failure mode where tightly coupled monolithic architectures lack atomic
    transaction support, making complex multi-step operations unreliable and systems
    difficult to maintain and extend.
- name: Tampering Attack Detection via Signature Mismatch
  sources:
  - chunk_ref: 08-LACP-2510.13821 (Chunk 2:149-151)
    quote: Upon receiving the tampered message, the server's cryptographic verification
      step immediately failed. The server logged a signature mismatch error and returned
      an HTTP 403 Forbidden
  description: Failure mode detection where payload tampering after signing is caught
    through cryptographic signature verification, resulting in HTTP 403 rejection
    and preventing fraudulent transactions.
- name: Replay Attack Prevention via Transaction ID Tracking
  sources:
  - chunk_ref: 08-LACP-2510.13821 (Chunk 2:165-168)
    quote: The server's signature verification passed, but its Transactional Layer
      logic identified the transaction_id as a duplicate. The server rejected the
      request and returned an HTTP 409 Conflict
  description: Failure mode handling where replay attacks are detected through transaction
    ID tracking, with duplicate IDs triggering HTTP 409 Conflict responses to prevent
    double-processing of operations.
- name: Under-Specification Failure Category
  sources:
  - chunk_ref: 09-SEMAP-2510.12120 (Chunk 1:69-71)
    quote: 'Cemri et al. introduced the Multi-Agent System Failure Taxonomy (MAST)
      and identified three recurring issue categories: under-specification, inter-agent
      misalignment, and inappropriate verification'
  description: 'Systematic failure mode taxonomy (MAST) identifying three core failure
    categories in multi-agent LLM systems: under-specification of requirements, misalignment
    between agents, and inappropriate verification mechanisms.'
- name: Lifecycle Stage Transition Failure
  sources:
  - chunk_ref: 09-SEMAP-2510.12120 (Chunk 1:246-266)
    quote: 'S: a set of lifecycle stages (e.g., initialized, implementing, reviewing,
      completed, failed); Sigma: verification outcomes (e.g., pass, fail); delta:
      S x Sigma -> S: a transition function'
  description: Failure mode modeling through finite state machine where lifecycle
    stages include explicit 'failed' state, with verification outcomes (pass/fail)
    governing transitions and enabling recovery or reassignment actions.
- name: SEMAP Failure Reduction Results
  sources:
  - chunk_ref: 09-SEMAP-2510.12120 (Chunk 1:344-351)
    quote: SEMAP reduces the total number of failures by 64.1% with ChatGPT... The
      largest reduction occurs in under-specification, where ChatGPT drops from 137
      to 39 (71.5%)
  description: Quantified failure reduction through SEMAP protocol showing 64-69%
    total failure reduction, with under-specification failures reduced by 71-73% through
    explicit behavioral contract modeling.
- name: Disorganized Text Communication Failure
  sources:
  - chunk_ref: 10-TalkHier-2502.11098 (Chunk 1:103-112)
    quote: communication often becomes disorganized because it requires explicitly
      describing agent tasks, providing background context, and specifying required
      output formats. These factors lead to lengthy and unstructured exchanges
  description: Failure mode in multi-agent systems where text-based communication
    becomes disorganized, making it difficult for agents to manage subgoals, maintain
    output structures, and retrieve independent memories from prior actions.
- name: Feedback Summarization Bias and Order Effects
  sources:
  - chunk_ref: 10-TalkHier-2502.11098 (Chunk 1:118-123)
    quote: As the number of agents increases, LLM-MA systems face challenges in effectively
      summarizing opinions or feedback. They often fail to balance these inputs, frequently
      overlooking some or exhibiting biases based on the order
  description: Failure mode where increasing agent count leads to feedback summarization
    failures, with systems exhibiting positional bias based on feedback order and
    failing to balance diverse inputs appropriately.
- name: Cascading Hallucination in Multi-Agent Interactions
  sources:
  - chunk_ref: 12-CollabSurvey-2501.06322 (Chunk 1:353-354)
    quote: Issues such as cascading hallucinations — where one erroneous output leads
      to compounding mistakes pose challenges in sustained multi-agent interactions
  description: Critical failure mode in multi-agent LLM systems where a single erroneous
    or hallucinated output propagates through the system, causing compounding mistakes
    across sustained agent interactions.
- name: Single Agent Failure Amplification
  sources:
  - chunk_ref: 12-CollabSurvey-2501.06322 (Chunk 2:354-357)
    quote: the failure of one agent or more agents (e.g., infinite conversation loop,
      amplified hallucinations [56]) can negatively impact the entire system
  description: In cooperative MAS, failure of one agent can cascade through the system.
    Specific failure modes include infinite conversation loops and amplified hallucinations
    that spread from one agent to others, negatively impacting the entire collaborative
    system.
- name: Unpredictable Agent Messaging Behavior
  sources:
  - chunk_ref: 12-CollabSurvey-2501.06322 (Chunk 2:347-348)
    quote: agents may act unpredictably by sending messages to themselves, pretending
      to be clients
  description: In book marketplace applications, agents exhibited unpredictable behavior
    by self-messaging and client impersonation. This represents a failure mode where
    agent identity boundaries break down, leading to deceptive internal communication
    patterns.
- name: Competitive Collaboration Channel Suboptimality
  sources:
  - chunk_ref: 12-CollabSurvey-2501.06322 (Chunk 2:394-397)
    quote: MAS approach with suboptimal design for their competitive collaboration
      channels can be overtaken by single-agent counterparts with strong prompts
  description: When competitive collaboration channels are poorly designed, the entire
    MAS can perform worse than a single agent with well-crafted prompts. This failure
    mode indicates that bad multi-agent design can be counterproductive.
- name: Rule-based System Adaptability Failure
  sources:
  - chunk_ref: 12-CollabSurvey-2501.06322 (Chunk 3:97-101)
    quote: rule-based systems suffer from a lack of adaptability. When confronted
      with unexpected situations or dynamic environments that fall outside the scope
      of the predefined rules, these systems may fail to respond appropriately
  description: Rule-based MAS protocols fail when encountering scenarios outside predefined
    rules. The failure mode requires significant manual intervention to adjust rule
    sets, and complexity grows exponentially as tasks increase.
- name: Role-based Rigidity Failure
  sources:
  - chunk_ref: 12-CollabSurvey-2501.06322 (Chunk 3:134-138)
    quote: if roles are not properly specified, role-based systems can show rigidity,
      which might result in disputes or functional deficiencies
  description: Role-based systems fail through rigidity when roles are improperly
    specified. Interdependencies between agent roles mean ineffective communication
    or blocking of interactions can severely impact overall system performance.
- name: Centralized Node Collapse
  sources:
  - chunk_ref: 12-CollabSurvey-2501.06322 (Chunk 3:193-197)
    quote: If the central node fails the entire system might collapse. System is less
      resilient to disruptions
  description: Centralized MAS architectures have a single point of failure. If the
    central controlling agent fails, the entire system collapses because all agents
    depend on it for coordination and resource allocation.
- name: Dynamic Architecture Adjustment Failure
  sources:
  - chunk_ref: 12-CollabSurvey-2501.06322 (Chunk 4:20-26)
    quote: Higher resource usage due to real-time adjustments. Potential failures
      in dynamic adjustments
  description: Dynamic coordination architectures can fail during real-time adjustments.
    The failure mode involves higher resource consumption and potential breakdowns
    when the system attempts to adapt roles and channels dynamically.
- name: Static Architecture Scalability Failure
  sources:
  - chunk_ref: 12-CollabSurvey-2501.06322 (Chunk 4:14-19)
    quote: Relies on accurate initial design and domain knowledge. Fixed channels
      may deal with scalability and flexibility
  description: Static architectures fail when initial design or domain knowledge is
    inaccurate. Fixed collaboration channels struggle with scalability and flexibility
    requirements as systems grow.
- name: AI Safety Performance Failure - Exploitation
  sources:
  - chunk_ref: 12-CollabSurvey-2501.06322 (Chunk 4:122-123)
    quote: AI safety and performance concerns arise, particularly in competitive scenarios
      where failures like exploitation and hallucination can happen
  description: In competitive MAS scenarios, specific failure modes include exploitation
    (agents gaming the system) and hallucination propagation. These represent safety-critical
    failures that undermine system reliability.
- name: LLM Coherence Degradation in Long Debates
  sources:
  - chunk_ref: 12-CollabSurvey-2501.06322 (Chunk 4:406-407)
    quote: LLMs struggle to maintain coherence and relevance in long scenarios
  description: In multi-agent debate systems, LLMs fail to maintain coherence and
    relevance during extended interactions. This failure mode leads to degraded output
    quality as debate length increases.
- name: Missing Requirements Task Comprehension Failure
  sources:
  - chunk_ref: 12-CollabSurvey-2501.06322 (Chunk 4:441-442)
    quote: Without clear, detailed requirements, agents struggle to grasp task ideas
  description: In ChatDev and similar development frameworks, agents fail to comprehend
    tasks when requirements lack clarity or detail. The absence of explicit specifications
    leads to misaligned development outcomes.
- name: Hallucination Propagation and Reinforcement
  sources:
  - chunk_ref: 12-CollabSurvey-2501.06322 (Chunk 6:125-128)
    quote: A single agent's hallucination can be spread and reinforced by other agents,
      leading to minor inaccuracies into critical and cascading effects
  description: In MAS, a single agent's hallucination can propagate and be reinforced
    by other agents. This creates a cascade where minor initial inaccuracies compound
    into critical errors across the entire system.
- name: Scalability Coordination Complexity
  sources:
  - chunk_ref: 12-CollabSurvey-2501.06322 (Chunk 6:131-135)
    quote: Increasing agent population poses a significant challenge in MASs. Managing
      resources (memory, processing time), coordination and collaboration channels
      among a growing number of agents introduces additional complexities
  description: As agent populations grow, coordination becomes increasingly complex.
    Failure modes include inefficiencies in agent interactions, resource management
    bottlenecks, and prevention failures leading to system degradation.
- name: Error Accumulation in Single-Path Planning
  sources:
  - chunk_ref: 15-AgentSurvey-2503.21460 (Chunk 2:53-55)
    quote: it may suffer from a lack of flexibility and error accumulation during
      chaining, as the agent is required to follow the predefined plan without any
      deviation
  description: Single-path chaining in task decomposition fails through error accumulation.
    When agents must follow predefined plans without deviation, errors compound through
    the chain, reducing flexibility and causing cascading failures.
- name: Degeneration-of-Thought Problem
  sources:
  - chunk_ref: 15-AgentSurvey-2503.21460 (Chunk 2:276-278)
    quote: MAD [80] employs structured communication protocols to address the 'degeneration-of-thought'
      problem, where agents overly fixate on initial solutions
  description: Multi-agent debate systems can fail through 'degeneration-of-thought'
    where agents become overly fixated on initial solutions. This failure mode prevents
    exploration of alternative solutions and leads to suboptimal outcomes.
- name: Centralized Bottleneck Failure
  sources:
  - chunk_ref: 15-AgentSurvey-2503.21460 (Chunk 2:244-246)
    quote: a single control node often becomes a bottleneck due to handling all inter-agent
      communication, task scheduling, and contention resolution
  description: Centralized architectures fail when the control node becomes a bottleneck.
    Handling all inter-agent communication, task scheduling, and contention resolution
    through one point creates performance degradation and potential system failures.
- name: Adversarial Attack Performance Degradation
  sources:
  - chunk_ref: 15-AgentSurvey-2503.21460 (Chunk 4:82-84)
    quote: Adversarial attacks aim to compromise the reliability of the agents, rendering
      them ineffective in specific tasks
  description: Adversarial attacks represent a failure mode where agents are rendered
    ineffective for specific tasks. The attack compromises agent reliability, making
    them unable to perform their intended functions.
- name: Multi-Agent Infectious Attack Propagation
  sources:
  - chunk_ref: 15-AgentSurvey-2503.21460 (Chunk 4:93-95)
    quote: GIGA [181] introduces generalizable infectious gradient attacks to propagate
      adversarial inputs across multi-agent, multi-round LLM-powered systems
  description: Infectious attacks can propagate adversarial inputs across multi-agent
    systems over multiple rounds. This failure mode spreads malicious content through
    agent interactions, compromising the entire system gradually.
- name: Backdoor Trigger Activation
  sources:
  - chunk_ref: 15-AgentSurvey-2503.21460 (Chunk 4:139-141)
    quote: Backdoor attacks implant specific triggers to cause the model to produce
      preset errors when encountering these triggers while performing normally under
      normal inputs
  description: Backdoor attacks create hidden failure modes that only activate under
    specific trigger conditions. The agent performs normally otherwise, making detection
    difficult until the trigger causes preset errors.
- name: Multi-Agent Collaboration Attack Disruption
  sources:
  - chunk_ref: 15-AgentSurvey-2503.21460 (Chunk 4:165-167)
    quote: attackers manipulate the interaction or collaboration mechanisms between
      multiple models to disrupt the overall functionality of the system
  description: Collaboration attacks target inter-model interactions rather than individual
    agents. The failure mode involves disruption of collaboration mechanisms, causing
    system-wide functionality breakdown.
- name: Contagion and Recursion Attack
  sources:
  - chunk_ref: 15-AgentSurvey-2503.21460 (Chunk 4:168-171)
    quote: CORBA [196] introduces a novel yet simple attack method for the LLM multi-agent
      system. It exploits contagion and recursion, which are hard to mitigate via
      alignment
  description: CORBA attacks exploit contagion and recursion properties in MAS that
    are difficult to mitigate through alignment. The failure spreads recursively through
    agent interactions, making containment challenging.
- name: Inter-Agent Message Manipulation
  sources:
  - chunk_ref: 15-AgentSurvey-2503.21460 (Chunk 4:172-173)
    quote: AiTM [197] introduces an attack method to the LLM multi-agent system by
      intercepting and manipulating inter-agent messages using an adversarial agent
  description: Adversary-in-the-Middle attacks intercept and manipulate messages between
    agents. This failure mode corrupts communication channels, causing agents to act
    on falsified information.
- name: Hallucination with Cascading Uncertainty
  sources:
  - chunk_ref: 15-AgentSurvey-2503.21460 (Chunk 6:376-378)
    quote: causing hallucinations [315] and compounding uncertainty in multi-agent
      systems, such as agentic frameworks for medical applications and autonomous
      scientific discovery
  description: LLM stochastic nature causes hallucinations that compound uncertainty
    in multi-agent systems. In high-stakes domains like medical and scientific applications,
    this failure mode can mislead critical decision-making.
- name: Compound Module Interaction Hallucination
  sources:
  - chunk_ref: 18-HallucinationSurvey-2509.18970 (Chunk 1:193-196)
    quote: agent hallucinations are compound behaviors arising from interactions among
      multiple modules, resulting in a broader and more varied range of hallucination
      types
  description: Agent hallucinations differ from simple LLM errors by arising from
    complex interactions between perception, reasoning, and action modules. This creates
    diverse failure types that are harder to diagnose and fix.
- name: Multi-Step Hallucination Propagation Chain
  sources:
  - chunk_ref: 18-HallucinationSurvey-2509.18970 (Chunk 1:196-200)
    quote: agent hallucinations often span multiple steps and involve multi-state
      transitions. Such hallucinations are not limited to the final output; they may
      also arise during intermediate processes such as perception and reasoning, where
      they can propagate and accumulate over time
  description: Agent hallucinations span multiple execution steps with multi-state
    transitions. Unlike localized errors, they propagate through perception and reasoning
    stages, accumulating over time to cause severe downstream failures.
- name: Physically Consequential Action Error
  sources:
  - chunk_ref: 18-HallucinationSurvey-2509.18970 (Chunk 1:201-205)
    quote: Agent hallucinations involve 'physically consequential' errors, where incorrect
      embodied actions can directly affect task execution, system devices, and user
      experiences in the real world
  description: Agent hallucinations create physically consequential failures where
    incorrect embodied actions affect real-world systems. The cost and risk are significantly
    higher than text-only errors due to direct impact on devices and users.
- name: Sub-intention Dependency Failure
  sources:
  - chunk_ref: 18-HallucinationSurvey-2509.18970 (Chunk 2:162-175)
    quote: 'Inadequate modeling of dependency relationships among these sub-intentions
      can give rise to three types of errors: Sub-intention Omission, Sub-intention
      Redundancy and Sub-intentions Disorder'
  description: 'Failure in modeling sub-intention dependencies causes three error
    types: omission (missing critical steps), redundancy (irrelevant additions), and
    disorder (wrong sequencing). All compromise reasoning integrity and efficiency.'
- name: Tool Solvability Awareness Failure
  sources:
  - chunk_ref: 18-HallucinationSurvey-2509.18970 (Chunk 2:298-312)
    quote: A lack of solvability awareness in LLM-based agents can also lead to execution
      hallucinations, where the agent mistakenly assumes that pt is solvable and proceeds
      with unjustified confidence
  description: Agents may proceed with unjustified confidence when they lack awareness
    of whether their plan is solvable. This leads to execution hallucinations where
    agents invoke irrelevant, fabricated, or non-executable tools.
- name: Communication Protocol Asynchrony Failure
  sources:
  - chunk_ref: 18-HallucinationSurvey-2509.18970 (Chunk 2:462-467)
    quote: LLM-based MAS usually follows a manner of Asynchronous Scheduling so when
      receiving and processing instructions, agents may encounter issues of information
      loss and information overload
  description: Asynchronous scheduling in MAS leads to information loss and overload
    during instruction processing. Temporal discrepancies result in information errors
    that increase the risk of hallucinatory outputs.
- name: Ineffective Network Topology Update
  sources:
  - chunk_ref: 18-HallucinationSurvey-2509.18970 (Chunk 2:475-493)
    quote: When network updates are ineffective, they can induce communication hallucinations
      due to inconsistent or outdated inter-agent connections
  description: Ineffective network topology updates cause communication hallucinations
    through inconsistent or outdated agent connections. Messages may be routed to
    inappropriate recipients, causing misunderstandings or redundant reasoning.
- name: Information Loss Through Compression
  sources:
  - chunk_ref: 18-HallucinationSurvey-2509.18970 (Chunk 3:22-30)
    quote: this process is susceptible to Information Compression issues, where the
      generated summaries may be overly general, omit crucial details, or introduce
      distortions
  description: Failure mode where memory writing operations compress information excessively,
    resulting in loss of crucial details. When agents summarize historical information
    for storage, the abstraction process can introduce distortions or omit key semantic
    details that are needed for accurate subsequent decisions.
- name: Erroneous Message Propagation
  sources:
  - chunk_ref: 18-HallucinationSurvey-2509.18970 (Chunk 3:45-51)
    quote: LLMs are prone to the well-known Factuality and Faithfulness Hallucinations,
      some agents may produce messages containing inaccurate facts, misinterpretations
      of shared knowledge, or misleading inferences
  description: Failure mode in multi-agent communication where messages generated
    by LLMs contain hallucinated content. This propagates through the system as agents
    exchange incorrect facts, misinterpretations, or misleading inferences, causing
    downstream communication hallucinations.
- name: Content Redundancy Overload
  sources:
  - chunk_ref: 18-HallucinationSurvey-2509.18970 (Chunk 3:52-55)
    quote: Content Redundancy is also an important cause, where agents generate unnecessary
      or repetitive content that obscures critical signals, increases cognitive load
  description: Failure mode where agents generate redundant or repetitive content
    that obscures critical signals. This increases cognitive load and can lead to
    redundant task execution steps that manifest as logical errors in the workflow.
- name: Uncoordinated Protocol Failure
  sources:
  - chunk_ref: 18-HallucinationSurvey-2509.18970 (Chunk 3:61-65)
    quote: Without a unified and effective protocol, agents may 'talk past each other',
      leading to communication hallucinations
  description: Failure mode where lack of unified communication protocols causes agents
    to misinterpret each other's messages. Agents may encounter information loss and
    information overload due to asynchronous scheduling, leading to hallucinatory
    outputs.
- name: Message Format Ambiguity
  sources:
  - chunk_ref: 18-HallucinationSurvey-2509.18970 (Chunk 3:72-78)
    quote: Current LLM-based agents predominantly rely on the format of natural language
      which often introduces instruction ambiguity...LLM-based MAS demands a robust
      Fault-tolerant Design
  description: Failure mode where natural language message formats introduce instruction
    ambiguity. Without structured formats (e.g., JSON) and fault-tolerant design with
    confirmation conditions and synchronization constraints, agents make erroneous
    decisions caused by message loss or delays.
- name: Network Topology Staleness
  sources:
  - chunk_ref: 18-HallucinationSurvey-2509.18970 (Chunk 3:79-97)
    quote: When network updates are ineffective, they can induce communication hallucinations
      due to inconsistent or outdated inter-agent connections
  description: Failure mode where network topology updates fail to accurately reflect
    agents' current relevance or expertise. Messages may be routed to inappropriate
    recipients, leading to misunderstandings or redundant reasoning across the multi-agent
    system.
- name: Hallucinatory Accumulation
  sources:
  - chunk_ref: 18-HallucinationSurvey-2509.18970 (Chunk 4:153-160)
    quote: agent decision-making is inherently a multi-step and sequential process,
      in which hallucinations can accumulate and amplify over time...hallucinations
      may initially appear as minor issues, but their iterative accumulation can ultimately
      lead to severe consequences
  description: Failure mode where small hallucinations compound across multiple agent
    steps. Minor errors in early steps propagate and amplify through the sequential
    decision-making process, ultimately leading to severe consequences that are much
    harder to detect and mitigate than single-step errors.
- name: Full-Chain Error Propagation
  sources:
  - chunk_ref: 18-HallucinationSurvey-2509.18970 (Chunk 4:170-182)
    quote: agent hallucinations are far more complex, involving full-chain error propagation
      across multiple interdependent components...hallucinations may arise at any
      stage of the decision-making pipeline and often exhibit complex characteristics
  description: Failure mode where errors propagate across the entire agent processing
    chain. Unlike simple textual generation errors, agent hallucinations involve interdependent
    components with hallucinatory accumulation and inter-module dependency, making
    source localization extremely difficult.
- name: Echo Chamber Effect
  sources:
  - chunk_ref: 18-HallucinationSurvey-2509.18970 (Chunk 8:192-205)
    quote: Initially, the client states that they require a 'code review'. As the
      message passes along a chain of agents, the first agent mishears it as 'cold
      review', the second relays it as 'gold review'
  description: Failure mode demonstrating telephone game style message distortion
    in multi-agent communication. Minor mis-hearings or misinterpretations are amplified
    through multiple rounds of transmission until the information becomes completely
    detached from the original meaning.
- name: Tool Selection Hallucination
  sources:
  - chunk_ref: 18-HallucinationSurvey-2509.18970 (Chunk 8:174-182)
    quote: The agent incorrectly calls a non-existent tool named 'get kyoto travel
      info', whereas the correct tool in the system should be 'recommend tourist spots'
  description: Failure mode where agents invent plausible-sounding but non-existent
    APIs or function names when selecting external tools. This leads to tool selection
    hallucinations that cause complete task failure when the fabricated tool cannot
    be executed.
- name: Tool Parameter Hallucination
  sources:
  - chunk_ref: 18-HallucinationSurvey-2509.18970 (Chunk 8:184-190)
    quote: the agent selects the correct tool but arbitrarily appends an extra parameter
      language='English', which is not part of the tool's actual specification
  description: Failure mode where even with correct tool selection, the agent hallucinates
    at the parameter level. Agents may engage in hallucinatory argument extension,
    adding parameters that are not part of the tool specification, causing calls to
    fail or produce unintended behavior.
- name: Generalization Bound Violation
  sources:
  - chunk_ref: 19-HalMit-2507.15903 (Chunk 1:55-58)
    quote: Hallucinations typically arise when the generated content significantly
      exceeds the generalization bounds of the agent. If a generated response lies
      outside the bound, it is highly likely that this response is hallucinated
  description: Failure mode where agent responses exceed the learned generalization
    boundary. When queries fall outside the agent's domain-specific competence boundary,
    the agent produces hallucinated responses that are flagged as potential errors
    requiring mitigation.
- name: Domain-Specific Boundary Uncertainty
  sources:
  - chunk_ref: 19-HalMit-2507.15903 (Chunk 1:308-316)
    quote: the semantic entropy values of agent responses vary substantially across
      application domains, with noticeable differences in both medians and variances...no
      universal generalization bound can be established across all domains
  description: Failure mode where single threshold-based detection fails due to domain
    variation. Semantic entropy varies significantly across domains, making fixed
    thresholds insufficient for detecting hallucinations. High-entropy yet non-hallucinatory
    responses (and vice versa) require domain-aware detection.
- name: Boundary Proximity Detection
  sources:
  - chunk_ref: 19-HalMit-2507.15903 (Chunk 2:381-405)
    quote: If the query closely resembles the retrieved records in the vector base,
      it is considered near the boundary, and the response corresponding to the input
      query is flagged as a potential hallucination
  description: Failure mode detection via vector database comparison. When input queries
    are semantically similar to known boundary points stored in the vector database,
    the response is flagged as potentially hallucinated. Centroid calculation of similar
    items enables detection of queries beyond the generalization bound.
- name: Error Propagation Through Agent Chain
  sources:
  - chunk_ref: 22-PROV-AGENT-2508.02866 (Chunk 1:21-25)
    quote: agents can hallucinate or reason incorrectly, propagating errors when one
      agent's output becomes another's input. Thus, assuring that agents' actions
      are transparent, traceable, reproducible, and reliable is critical
  description: Failure mode where hallucinated outputs from one agent become inputs
    for subsequent agents. Without provenance tracking, errors propagate through the
    workflow chain, compounding mistakes and making it difficult to assess the correctness
    of final results.
- name: Iterative Decision Loop Corruption
  sources:
  - chunk_ref: 22-PROV-AGENT-2508.02866 (Chunk 1:439-445)
    quote: because the agent relies on an LLM, there is a risk of hallucinated or
      incorrect outputs. Since each decision influences the next in this iterative
      loop, a single error may propagate across layers, potentially compromising downstream
      outputs
  description: Failure mode in iterative agentic workflows where decisions at iteration
    i influence iteration i+1. A single hallucinated decision propagates across subsequent
    layers, potentially corrupting all downstream outputs and requiring provenance
    tracking to identify the error source.
- name: Surprising Decision Traceability
  sources:
  - chunk_ref: 22-PROV-AGENT-2508.02866 (Chunk 2:111-122)
    quote: Given that a hallucination was identified when the agent was deciding on
      the scores for layer 2, after identifying the unexpected Agent_Decision_2, the
      query traces back to Agent_Tool_2
  description: Failure mode remediation pattern via provenance query. When a surprising/hallucinated
    agent decision is identified, the system traces back through the tool invocation
    and LLM interaction to retrieve the corresponding prompt and response for root
    cause analysis.
- name: Payload Corruption During Transmission
  sources:
  - chunk_ref: 24-EffectiveCollab-2412.05449 (Chunk 3:200-217)
    quote: Reduces the likelihood of payload corruption during agent-to-agent transmission...Maintains
      formatting and structure of the payload across agent communications
  description: Failure mode where payloads (especially code blocks) are corrupted
    during agent-to-agent transmission. Without payload referencing mechanisms, large
    content blocks may lose formatting, structure, or become corrupted, requiring
    expensive token regeneration and degrading goal success rates by up to 23%.
