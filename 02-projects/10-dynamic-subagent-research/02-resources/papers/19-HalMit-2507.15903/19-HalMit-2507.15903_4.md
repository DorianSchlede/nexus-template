<!-- Source: 19-HalMit-2507.15903.pdf | Chunk 4/4 -->



0.6

|Col1|Col2|Col3|Col4|
|---|---|---|---|
|||||
|||||

0.3 0.4 0.5 0.6 0.7



0.3
0.6 0.7 0.8 0.9



(a)



(b) (c)

**Figure 7** : Ablation Study of _γ_ and _ϵ_ .



**5** **Related Work**


Most of the related approaches take LLM as a white box to detect
hallucinations. Ji et al. [14] used a mutual information-based feature
selection method to select sensitive neurons from the last activation



layer and trained a classifier using Llama MLP. Han et al. [10] proposed a method to approximate semantic entropy from the hidden
state of the model, converting the semantic entropy into binary labels,
and trained a logistic regression classifier to predict hallucinations.
Zhu et al. [38] used PCA to reduce the dimensionality of the hidden
layer embedding and adopted interval partitioning or GMM clustering to establish abstract states. In addition, they also used Markov
models and hidden Markov models to capture state transitions, and
used a small amount of annotated reference data to link internal state
transitions with hallucination/factual output behaviors. He et al. [11]
combined the static features within the model with the dynamic features and used Siamese networks to identify situations where the answers of the large language model deviate from the facts. Gaurang et
al. [24] proposes a hallucination detection method that analyze internal model signals. Xiaoling et al. [37] propose HADEMIF for detecting hallucinations in LLMs by leveraging a Deep Dynamic Decision
Tree and an MLP to calibrate model predictions. Requiring the access
to internal states of LLMs to detect hallucinations, these methods not
only suffer from high complexity and computational demand but also
may not be feasible for commercial LLM software. This highlights
the need for research on the detection of hallucinations without accessing the internal states of the LLMs.
The other solutions detect hallucinations whereas considering the
black-box nature of LLMs. Hou et al. [12] proposed using the belief tree, a probabilistic framework, to detect hallucinations using
the logical consistency between model beliefs. Quevedo et al. [21]
extracted features using two LLMs and used these features to train
logistic regression and simple neural networks to detect hallucinations. Although these methods detect hallucination through output
features or associated confidence scores, the limited understanding
of the generalization bound of LLMs leads to poorly calibrated confidence estimates.


**6** **Conclusion**


In this work, we present an in-depth study of the hallucination phenomenon in LLM-empowered agents through the lens of fine-grained
domains. Based on our findings, we propose an effective and efficient
hallucination monitor HalMit, according to a few key observations
from our study. Our research reveals that LLMs exhibit similar generalization bounds within the same domain, providing a foundation for
accurately monitoring hallucinations in specific domains. To take advantage of this key insight, we have designed a reinforced probabilistic fractal exploration method that efficiently identifies the general

ization bound of an LLM-empowered agent within a domain. Despite
the black-box nature of LLM-empowered agents, this approach significantly accelerates the boundary identification process while improving both the accuracy and efficiency of hallucination monitoring
based on the generalization bound. Extensive experimental evaluations demonstrate that our method outperforms existing mainstream
hallucination monitoring techniques across multi-topic datasets and
different foundation LLMs. This work not only offers a novel technical pathway for monitoring hallucinations for agents in a specific
domain, but also provides robust theoretical and practical support to
enhance their security and dependability in critical applications.


**References**


[1] K. Andriopoulos and J. Pouwelse. Augmenting llms with knowledge: A
survey on hallucination prevention. _arXiv preprint arXiv:2309.16459_,
2023.

[2] A. Ben Abacha and D. Demner-Fushman. A question-entailment approach to question answering. _BMC Bioinf_, 20, 2019.

[3] S. Casper, X. Davies, C. Shi, T. K. Gilbert, J. Scheurer, J. Rando,
R. Freedman, T. Korbak, D. Lindner, P. Freire, et al. Open problems and
fundamental limitations of reinforcement learning from human feedback. _arXiv preprint arXiv:2307.15217_, 2023.

[4] Y. Chen, Q. Fu, Y. Yuan, Z. Wen, G. Fan, D. Liu, D. Zhang, Z. Li, and
Y. Xiao. Hallucination detection: Robustly discerning reliable answers
in large language models. In _CIKM_, 2023.

[5] Elastic. Elasticsearch. https://www.elastic.co/guide/en/elasticsearch/
reference/current/elasticsearch-intro-what-is-es.html, 2023. Accessed:
2023-10-10.

[6] S. Farquhar, J. Kossen, L. Kuhn, and Y. Gal. Detecting hallucinations
in large language models using semantic entropy. _Nature_, 630(8017),
2024.

[7] O. Freyer, I. C. Wiest, J. N. Kather, and S. Gilbert. A future role for
health applications of large language models depends on regulators enforcing safety standards. _Lancet Digit Health_, 6(9), 2024.

[8] D. Gao, Z. Li, X. Pan, W. Kuang, Z. Ma, B. Qian, F. Wei, W. Zhang,
Y. Xie, D. Chen, L. Yao, H. Peng, Z. Y. Zhang, L. Zhu, C. Cheng, H. Shi,
Y. Li, B. Ding, and J. Zhou. Agentscope: A flexible yet robust multiagent platform. _CoRR_, abs/2402.14034, 2024.

[9] C. M. Greco and A. Tagarelli. Bringing order into the realm of
transformer-based language models for artificial intelligence and law.
_Artif Intell Law_, 2023.

[10] J. Han, J. Kossen, M. Razzak, L. Schut, S. A. Malik, and Y. Gal. Semantic entropy probes: Robust and cheap hallucination detection in llms. In
_ICML_, 2024.

[11] J. He, Y. Gong, Z. Lin, C. Wei, Y. Zhao, and K. Chen. Llm factoscope:
Uncovering llms’ factual discernment through measuring inner states.
In _ACL Findings_, 2024.

[12] B. Hou, Y. Zhang, J. Andreas, and S. Chang. A probabilistic framework for llm hallucination detection via belief tree propagation. _arXiv_
_preprint arXiv:2406.06950_, 2024.

[13] L. Huang, W. Yu, W. Ma, W. Zhong, Z. Feng, H. Wang, Q. Chen,
W. Peng, X. Feng, B. Qin, et al. A survey on hallucination in large language models: Principles, taxonomy, challenges, and open questions.
_ACM Trans Inf Syst_, 2023.

[14] Z. Ji, D. Chen, E. Ishii, S. Cahyawijaya, Y. Bang, B. Wilie, and P. Fung.
Llm internal states reveal hallucination risk faced with a query. _arXiv_
_preprint arXiv:2407.03282_, 2024.

[15] E. Kasneci, K. Seßler, S. Küchemann, M. Bannert, D. Dementieva,
F. Fischer, U. Gasser, G. Groh, S. Günnemann, E. Hüllermeier, et al.
Chatgpt for good? on opportunities and challenges of large language
models for education. _Learn Individ Differ_, 103, 2023.

[16] L. Kuhn, Y. Gal, and S. Farquhar. Semantic uncertainty: Linguistic
invariances for uncertainty estimation in natural language generation.
_arXiv preprint arXiv:2302.09664_, 2023.

[17] S. Lin, J. Hilton, and O. Evans. Truthfulqa: Measuring how models
mimic human falsehoods. In _ACL_, 2022.

[18] S. Lotfi, M. Finzi, Y. Kuang, T. G. Rudner, M. Goldblum, and A. G.
Wilson. Non-vacuous generalization bounds for large language models.
_arXiv preprint arXiv:2312.17173_, 2023.

[19] S. Lotfi, Y. Kuang, B. Amos, M. Goldblum, M. Finzi, and A. G. Wilson. Unlocking tokens as data points for generalization bounds on larger
language models. _arXiv preprint arXiv:2407.18158_, 2024.

[20] P. Manakul, A. Liusie, and M. Gales. Selfcheckgpt: Zero-resource



black-box hallucination detection for generative large language models.
In _EMNLP_, 2023.

[21] E. Quevedo, J. Yero, R. Koerner, P. Rivas, and T. Cerny. Detecting
hallucinations in large language model generation: A token probability
approach. _arXiv preprint arXiv:2405.19648_, 2024.

[22] P. Rajpurkar, J. Zhang, K. Lopyrev, and P. Liang. SQuAD: 100,000+
questions for machine comprehension of text. In _EMNLP_, 2016.

[23] N. Riemer. Remetonymizing metaphor: Hypercategories in semantic
extension. 2002.

[24] G. Sriramanan, S. Bharti, V. S. Sadasivan, S. Saha, P. Kattakinda, and
S. Feizi. Llm-check: Investigating detection of hallucinations in large
language models. _Adv Neural Inf Process Syst_, 37:34188–34216, 2024.

[25] D. Su, X. Li, J. Zhang, L. Shang, X. Jiang, Q. Liu, and P. Fung. Read
before generate! faithful long form question answering with machine
reading. In _ACL Foundings_, 2022.

[26] D. Udekwe, O. ofe Ajayi, O. Ubadike, K. Ter, and E. Okafor. Comparing actor-critic deep reinforcement learning controllers for enhanced
performance on a ball-and-plate system. _Expert Syst Appl_, 245, 2024.
ISSN 0957-4174.

[27] Y. Wang, Q. Sun, and S. He. M3E: Moka Massive Mixed Embedding
Model, 2023.

[28] J. Wei, Y. Yao, J.-F. Ton, H. Guo, A. Estornell, and Y. Liu. Measuring and reducing llm hallucination without gold-standard answers via
expertise-weighting. _arXiv preprint arXiv:2402.10412_, 2024.

[29] S. Xu and C. Zhang. Misconfidence-based demonstration selection for
llm in-context learning. _arXiv preprint arXiv:2401.06301_, 2024.

[30] H. Yang, H. Lu, W. Lam, and D. Cai. Exploring compositional generalization of large language models. In _NAACL-HLT_, pages 16–24, 2024.

[31] J. Yang, H. Jin, R. Tang, X. Han, Q. Feng, H. Jiang, S. Zhong, B. Yin,
and X. Hu. Harnessing the power of llms in practice: A survey on
chatgpt and beyond. _ACM Trans Knowl Discov Data_, 18(6), 2024.

[32] C. Zhang. User-controlled knowledge fusion in large language
models: Balancing creativity and hallucination. _arXiv preprint_
_arXiv:2307.16139_, 2023.

[33] C. Zhang, S. Bengio, M. Hardt, B. Recht, and O. Vinyals. Understanding deep learning (still) requires rethinking generalization. _Communi-_
_cations of the ACM_, 64(3):107–115, 2021.

[34] Y. Zhang, S. Li, J. Liu, P. Yu, Y. R. Fung, J. Li, M. Li, and H. Ji.
Knowledge overshadowing causes amalgamated hallucination in large
language models. _arXiv preprint arXiv:2407.08039_, 2024.

[35] H. Zhao, Z. Liu, Z. Wu, Y. Li, T. Yang, P. Shu, S. Xu, H. Dai, L. Zhao,
G. Mai, et al. Revolutionizing finance with llms: An overview of applications and insights. _arXiv preprint arXiv:2401.11641_, 2024.

[36] Z. Zhao, B. Wang, L. Ouyang, X. Dong, J. Wang, and C. He. Beyond hallucinations: Enhancing lvlms through hallucination-aware direct preference optimization. _arXiv preprint arXiv:2311.16839_, 2023.

[37] X. Zhou, M. Zhang, Z. Lee, W. Ye, and S. Zhang. Hademif: Hallucination detection and mitigation in large language models. In _ICLR_ .

[38] D. Zhu, D. Chen, Q. Li, Z. Chen, L. Ma, J. Grossklags, and M. Fritz.
Pollmgraph: Unraveling hallucinations in large language models via
state transition dynamics. _arXiv preprint arXiv:2404.04722_, 2024.


