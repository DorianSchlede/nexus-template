<!-- Source: 19-HalMit-2507.15903.pdf | Chunk 3/4 -->

vector database. If _HQ_ _[τ]_ _v_ [is larger, the input query is likely to be]
outside the generalization bound and may cause a hallucination
(corresponding to the red point in Figure 5e).


2.5


2


1.5


1


0.5


0



Domain: Treatment


0 10 20 30
Step



2.5


2


1.5


1


0.5


0



Domain: Inheritance


0 10 20 30
Step



2.5


2


1.5


1


0.5


0



Domain: New York City


0 10 20 30
Step



2.5


2


1.5


1


0.5


0



Domain: Modern History


0 10 20 30
Step



**Figure 6** : Convergence of our exploration process.

All agents in specific domains are implemented using RAG technology. Specifically, this RAG pipeline is constructed with Elasticsearch

[5] served as the vector database, which is used to record generalized
bounds in the vector format to serve as references for identifying conditions where hallucinations occur. In addition, the m3e-base [27] is
used as the embedding model to vectorize the information of a generalized bound, including the query and the corresponding responses
stored in the database that represent the bounds, as well as the input query in the hallucination monitoring process. We also utilize _4.4_
a repository within Agentscope V0.1.0 [8] to enable exploration of
the generalization bound and monitoring of hallucinations of LLMempowered agents. sented in Table 1.
During modeling the generalization bound, Qwen-max is used to
generate queries, while GPT 4 is used to judge whether each response
of the target LLM has hallucinations. In addition, we also incorporate
a supervised method. When using GPT 4 for assistance in judgment,
we evaluate the confidence in the inference results. If confidence is
less than 60%, we perform a manual review to ensure the accuracy of
the judgment before proceeding [3]. During training the policy network of reinforcement determination on fractal probability, the learning rate is set to 10 _[−]_ [4] and the batch size is 64. It converges in 300 better equipped to handle.
epochs. We set the ratio _γ_ to 0.6 to guide the search for bounds, randomly initialize ten queries of the bound searching for each domain, tion detecting, HalMit
and the similarity threshold _ϵ_ to 0.8 for the monitoring of hallucinations. The impacts of setting the two parameters to different values
are studied in Section 4.5.
Three popular hallucination detection methods are included
as baselines: 1) Predictive Probability (PP) [20], 2) In-ContextLearning Prompt (ICL) [29], and 3) SelfCheckGPT (SCG) [20]. In
addition, a comprehensive set of metrics is used in our evaluation,
including: 1) the area under the receiver operator characteristic curve
(AUROC), 2) the area under the precision recall curve (AUC-PR),
3) the F1 score, and 4) the accuracy. In addition, we also recorded
the semantic entropy defined in Formula (2) in the Appendix of the
output of the target agent to illustrate the uncertainty metric.



lection, each exploration step consistently increases or maintains semantic entropy, signaling effective converge on the generalization
bound. In contrast, the random probability strategy yields volatile
entropy values, with no clear trend, indicating an unreliable and less
directed search process. These findings suggest that reinforcementguided fractal exploration offers a more robust and targeted approach
to identifying generalization boundaries.



_4.4_ _Effectiveness of Hallucination Monitoring_



_4.3_ _Convergence Study of The Exploration_


Our probabilistic fractal exploration method is reinforced to explore towards the generalization bound. Through a reward mechanism based on increases in semantic entropy, our fractal exploration
process is directed toward generating statements with higher uncertainty—indicating proximity to the generalization bound. To evaluate
the completeness of fractal exploration, we compare the performance
of using the reinforced determination of fractal probabilities against
a baseline that uses randomly assigned probabilities.
As shown in Figure 6, we present the semantic entropy values over
the final 30 steps of the exploration process. This can also be observed to investigate how semantic entropy evolves with the exploration of the generalization bound through fractal transformations.
The results demonstrate that with reinforced fractal probability se


We evaluated the effectiveness of HalMit, and the results are presented in Table 1. HalMit achieves the best performance in Inheritance and Modern History domains, demonstrating both its superiority and adaptability to various types of agent. Specifically,
our method improves the AUROC and AUC-PR metrics up to 8%
over the best baseline, highlighting its effectiveness in distinguishing qualified output from hallucinations. The only exception is the
New York City topic, where SelfCheckGPT outperforms our method
in monitoring hallucinations. This may be due to the miscellaneous
slangy dialogues on this topic, which SelfCheckGPT appears to be
better equipped to handle.
Compared to baselines that use a fixed threshold for hallucination detecting, HalMit shows greater effectiveness across agent hallucinations in different domains. The Treatment and Inheritance domains primarily involve scientific knowledge, while New York City
and Modern History consist of miscellaneous questions. HalMit performs particularly well in domains that allow for divergent responses,
probably because it better adapts to the semantic diversity and complexity inherent in such queries. Finally, as a basic scheme only relying on LLM to detect hallucinations, ICL performs poorly for hallucination identification, while the used LLMs struggle to reliably
detect their own hallucinations without external guidance.
To evaluate the effectiveness of HalMit in monitoring hallucinations among agents empowered by other LLM models, Table 2 presents the hallucination monitoring performance for agents
with the other four models, Mistral-7b, Qwen2-1.5b, Falcon-7b and
Vicuna-7b. Without loss of generalization, the agents in Treatment
domain are selected to construct experiments. The experimental results demonstrate that HalMit consistently outperforms the baselines, achieving the highest accuracy and F1 scores. In particular,
HalMit shows the most significant improvement for agents empowered by Qwen2-1.5b, reaching the highest accuracy of 0.85. PP and
ICL show lower monitoring accuracy in agents with Vicuna, SelfCheckGPT, and HalMit experiences a marked increase in F1 score.
These results demonstrate the superiority of HalMit as the most robust approach, enhancing its performance across a variety of LLM
architectures.



_4.5_ _Ablation Study_


We investigate the impact of parameter _γ_ and _ϵ_ on the monitoring
accuracy in the Inheritance domain. As shown in Figures 7a and 7b,


**Table 1** : Hallucination monitoring performance on different agents
**Backbone** **AUROC** _↑_ **AUC-PR** _↑_ **F1** _↑_ **Acc** _↑_ **Backbone** **AUROC** _↑_ **AUC-PR** _↑_ **F1** _↑_ **Acc** _↑_
Treatment
PP 0.56 0.54 0.56 0.66 0.56 0.69 0.6 0.65

ICL 0.59 0.56 0.60 0.55 0.48 0.71 0.7 0.61

Llama2 Llama3.1

SCG 0.71 0.72 0.69 0.7 0.74 0.84 0.7 0.75
HalMit **0.76** **0.80** **0.79** **0.73** **0.80** **0.86** **0.82** **0.88**
Inheritance
PP 0.68 0.60 0.77 0.75 0.71 0.79 0.72 0.71

ICL 0.69 0.67 0.59 0.56 0.61 0.73 0.65 0.65

Llama2 Llama3.1

SCG 0.70 0.73 0.68 0.75 0.85 0.84 **0.85** 0.85
HalMit **0.70** **0.79** **0.8** **0.78** **0.90** **0.86** 0.82 **0.88**
New York City
PP 0.54 0.12 0.21 0.74 0.60 0.58 0.52 0.53

ICL 0.59 0.22 0.32 0.75 0.58 0.64 0.63 0.45

Llama2 Llama3.1

SCG 0.82 0.72 **0.85** 0.82 0.86 **0.84** 0.87 **0.86**
HalMit **0.88** **0.77** 0.75 **0.89** **0.89** 0.82 **0.88** 0.84
Modern History
PP 0.74 0.76 0.73 0.79 0.48 0.45 0.46 0.56

ICL 0.69 0.67 0.64 0.67 0.59 0.55 0.54 0.61

Llama2 Llama3.1

SCG 0.78 0.79 **0.81** 0.82 0.78 0.77 0.6 0.76
HalMit **0.84** **0.80** 0.77 **0.89** **0.84** **0.84** **0.67** **0.89**



0.56 0.54 0.56 0.66



Llama2



Llama3.1



0.68 0.60 0.77 0.75



Llama2



Llama3.1



0.54 0.12 0.21 0.74



Llama2



Llama3.1



0.74 0.76 0.73 0.79



Llama2



Llama3.1



**Table 2** : Additional results for evaluating hallucination monitoring
performance

Model Method AUROC _↑_ AUC-PR _↑_ Acc _↑_ F1 _↑_



Mistral-7b


qwen2-1.5b


Falcon-7b


Vicuna-7b



PP 0.43 0.49 0.56 0.37
ICL 0.58 0.52 0.59 0.49
SCG 0.56 0.67 0.69 0.64
HalMit **0.69** **0.70** **0.79** **0.77**

PP 0.49 0.50 0.50 0.42
ICL 0.52 0.39 0.54 0.44
SCG 0.72 0.80 0.78 0.68
HalMit **0.79** **0.80** **0.85** **0.81**

PP 0.64 0.59 0.49 0.44
ICL 0.57 0.60 0.51 0.55
SCG 0.68 0.71 0.75 0.79
HalMit **0.73** **0.75** **0.79** **0.80**

PP 0.39 0.41 0.45 0.67
ICL 0.60 0.58 0.56 0.79
SCG 0.68 **0.72** 0.71 0.81
HalMit **0.75** 0.71 **0.84** **0.89**



when _γ_ varies from 0.35 to 0.65, the monitoring accuracy remains
consistently high, maintaining a level between 0.78 and 0.88. The
curve exhibits remarkable stability across different _γ_ values, with
only a slight fluctuation. The stable performance across a wide range
of _γ_ values demonstrates that our proposed monitoring method is relatively insensitive to the choice of _γ_ . This robustness is particularly
valuable for practical applications, as it suggests that the method can
maintain reliable performance without requiring precise fine-tuning
of _γ_ . Similarly, the impact of the parameter _ϵ_ on the accuracy of
the monitoring is evaluated, and the tested values of _ϵ_ vary between
0.6 and 0.9. As illustrated in Figure 7c, _ϵ_ increases from 0.6 to 0.8,
the accuracy improves, and the highest performance is achieved at _ϵ_
of 0 _._ 8. These observations suggest that _ϵ_ = 0 _._ 8 strikes an optimal
trade-off, yielding the best performance.



0.9


0.8


0.7



0.9

0.8

0.7

0.6

0.5

0.4



0.9

0.85

0.8

0.75

0.7

0.65



Model: Llama2



Model: Llama3.1



Model: Llama3.1



0.6
0.35 0.4 0.45 0.5 0.55 0.6 0.65



0.6

|Col1|Col2|Col3|Col4|
|---|---|---|---|
|||||
|||||

0.3 0.4 0.5 0.6 0.7



0.3
0.6 0.7 0.8 0.9



(a)



(b) (c)

**Figure 7** : Ablation Study of _γ_ and _ϵ_ .



**5** **Related Work**


Most of the related approaches take LLM as a white box to detect
hallucinations. Ji et al. [14] used a mutual information-based feature
selection method to select sensitive neurons from the last activation



layer and trained a classifier using Llama MLP. Han et al. [10] proposed a method to approximate semantic entropy from the hidden
state of the model, converting the semantic entropy into binary labels,
and trained a logistic regression classifier to predict hallucinations.
Zhu et al. [38] used PCA to reduce the dimensionality of the hidden
layer embedding and adopted interval partitioning or GMM clustering to establish abstract states. In addition, they also used Markov
models and hidden Markov models to capture state transitions, and
used a small amount of annotated reference data to link internal state
transitions with hallucination/factual output behaviors. He et al. [11]
combined the static features within the model with the dynamic features and used Siamese networks to identify situations where the answers of the large language model deviate from the facts. Gaurang et
al. [24] proposes a hallucination detection method that analyze internal model signals. Xiaoling et al. [37] propose HADEMIF for detecting hallucinations in LLMs by leveraging a Deep Dynamic Decision
Tree and an MLP to calibrate model predictions. Requiring the access
to internal states of LLMs to detect hallucinations, these methods not
only suffer from high complexity and computational demand but also
may not be feasible for commercial LLM software. This highlights
the need for research on the detection of hallucinations without accessing the internal states of the LLMs.
The other solutions detect hallucinations whereas considering the
black-box nature of LLMs. Hou et al. [12] proposed using the belief tree, a probabilistic framework, to detect hallucinations using
the logical consistency between model beliefs. Quevedo et al. [21]
extracted features using two LLMs and used these features to train
logistic regression and simple neural networks to detect hallucinations. Although these methods detect hallucination through output
features or associated confidence scores, the limited understanding
of the generalization bound of LLMs leads to poorly calibrated confidence estimates.


**6** **Conclusion**


In this work, we present an in-depth study of the hallucination phenomenon in LLM-empowered agents through the lens of fine-grained
domains. Based on our findings, we propose an effective and efficient
hallucination monitor HalMit, according to a few key observations
from our study. Our research reveals that LLMs exhibit similar generalization bounds within the same domain, providing a foundation for
accurately monitoring hallucinations in specific domains. To take advantage of this key insight, we have designed a reinforced probabilistic fractal exploration method that efficiently identifies the general

ization bound of an LLM-empowered agent within a domain. Despite
the black-box nature of LLM-empowered agents, this approach significantly accelerates the boundary identification process while improving both the accuracy and efficiency of hallucination monitoring
based on the generalization bound. Extensive experimental evaluations demonstrate that our method outperforms existing mainstream
hallucination monitoring techniques across multi-topic datasets and
different foundation LLMs. This work not only offers a novel technical pathway for monitoring hallucinations for agents in a specific
domain, but also provides robust theoretical and practical support to
enhance their security and dependability in critical applications.


**References**


[1] K. Andriopoulos and J. Pouwelse. Augmenting llms with knowledge: A
survey on hallucination prevention. _arXiv preprint arXiv:2309.16459_,
2023.

[2] A. Ben Abacha and D. Demner-Fushman. A question-entailment approach to question answering. _BMC Bioinf_, 20, 2019.

[3] S. Casper, X. Davies, C. Shi, T. K. Gilbert, J. Scheurer, J. Rando,
R. Freedman, T. Korbak, D. Lindner, P. Freire, et al. Open problems and
fundamental limitations of reinforcement learning from human feedback. _arXiv preprint arXiv:2307.15217_, 2023.

[4] Y. Chen, Q. Fu, Y. Yuan, Z. Wen, G. Fan, D. Liu, D. Zhang, Z. Li, and
Y. Xiao. Hallucination detection: Robustly discerning reliable answers
in large language models. In _CIKM_, 2023.

[5] Elastic. Elasticsearch. https://www.elastic.co/guide/en/elasticsearch/
reference/current/elasticsearch-intro-what-is-es.html, 2023. Accessed:
2023-10-10.
