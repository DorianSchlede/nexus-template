<!-- Source: 19-HalMit-2507.15903.pdf | Chunk 2/4 -->

the response from the target and provides essential feedback to refine
the query generation process.


**CA** **QGA1** **QGA2** **Target** **EA**









_**Evaluation**_











**Figure 3** : Multi-agent collaboration in HalMit.



_**Evaluation**_



_3.1.1_ _Probabilistic Fractal-based Query Generation_


Leveraging the self-similarity feature of the fractal in natural language, we propose a novel probabilistic fractal-based query generation method for use in QGAs. This method iteratively constructs increasingly complex query structures that progressively approach the
generalization bound of the target agent. Unlike conventional fractal
systems that apply all affine transformations in each iteration step,
our method proposes an Iterated Function System with Probabilities (IFSP) (See appendix A) that enough queries can be generated
quickly to cover the generalization bound of the target agent _τ_ . More
specifically, according to semantic theory [23], three semantic extension patterns, _induction_, _deduction_, and _analogy_, are used as fractal
affine transformations to extend specific queries and navigate the semantic space. The execution probability of each transformation is
dynamically adjusted based on the IFSP system:



_F_ = _{FTi_ : _Pt_ _[τ]_ _−_ 1 _→Ppi_ _t_ _[τ]_ _[,]_



3

- _pi_ = 1 _},_ (1)


_i_ =1


_•_ **FT3:** **Semantic** **Induction.** This transformation generates
broader, more abstract queries by generalizing from specific instances and inferring underlying linguistic or conceptual patterns.
As another example, _“How can we assess the reliability of com-_
_monly accepted events in the history of space exploration?”_, is
obtained by following FT3. This supports exploration of overarching themes and epistemological questions within the domain.


These three affine transformations are applied in each iteration
of the exploration process. In this way, new queries are generated
through the iterative process that starts from basic concepts or principles. The core agent can order multiple query generation agents
to generate queries, so the iteration can be realized through parallel
processes, to significantly increase the speed in identifying the generalization bound. This iteration process includes four steps:
1) To cover a broader semantic space with in the bound, the CA
randomly initializes multiple queries in a domain and sends each
query to the target agent _τ_ as initial questions.
2) The target agent responds to queries with responses that may contain hallucinations. Therefore, each QA pair is sent to an EA. After
receiving the QA pair, the EA assesses whether the response in the
received QA pair contains a hallucination, and sends a report back
to the CA, including the QA pair and the corresponding evaluation
results.
3) Depending on the evaluation result, QGA will perform query generation in two ways. In case a hallucination is reported, the CA
embeds the QA pair and the context information into a vector
database as a point of the generalization bound of agent _τ_ (detailed
in Section 3.2). To expand the exploration range and cover more
of the generalization bound, the CA schedules the QGAs to generate new queries through the fractal affine transformations, FT1
and FT2, where the probability of each is determined through reinforcement learning (details in Section 3.1.2). Otherwise, in case
no hallucination is reported, new queries will be randomly generated, and sent back to the CA.
4) The new queries generated by the QGAs are sent back to the target agent. In this way, a new round of fractal exploration with a
pipeline is scheduled by CA, where multiple new iterations are
generated in parallel in response to each exploration path. More
EAs are scheduled to assess all QA pairs and the evaluation reports will be sent to the CA. The bound search speed can be exponentially increased in this way.
During this iterative process, the ratio of the hallucinations among all
QA pairs, _γ_, is incrementally updated by the core agent. Once _γ_ becomes larger than an empirical threshold _ϵ_, it indicates that the generalization bound of agent _τ_ can be identified by the vector database,
and this iterative process of _F_ ends. These parameters are evaluated
in ablation studies in Section 4.5.


_3.1.2_ _Reinforced Determination of Fractal Probabilities_


To further increase the efficiency in identifying the generalization
bound, we use deep reinforcement learning [26] to determine the
probability of each transformation function in IFSP _F_ to go for in
the next step. This probability is adjusted in each iteration so that the
exploration process can more efficiently converge towards the bound.
Deep reinforcement learning is a framework in which a policy network is trained to maximize long-term objectives. In our design, the
policy network is trained to efficiently select the appropriate probabilities of fractal affine transformations in each iteration, driving the
fast convergence to the bound as its long-term objective, as shown
in Figure 4. This design significantly accelerates the exploration process.





















































**Figure 4** : Process of exploring generalization bound. The numbers in
the circles are the corresponding semantic entropies.
**Training Data Prepared for Policy Network:**
To evaluate the effectiveness of a fractal affine transformation in
state _si_, we repetitively send the corresponding query, _Pi_ _[τ]_ [, to the]
target agent _K_ times, collect _K_ responses, _{a_ _[τ]_ _i_ _[}][K]_ [, and according]
to [6], calculate the semantic entropy for the target agent in state
_si_, _Hi_ _[τ]_ [. More specifically, every item in] _[ {][a]_ _i_ _[τ]_ _[}][K]_ [,] _[ a]_ _i_ _[τ]_ [(] _[k]_ [)][, is included]
in the token sequence group for calculating _Hi_ _[τ]_ [. Define a function]
sig( _a_ _[τ]_ _i_ [(] _[k]_ [))][, which is equal to 0 if the answer of the target agent con-]
tains hallucinations; otherwise, sig( _a_ _[τ]_ _i_ [(] _[k]_ [))][ is equal to 1. Hence, the]
reward for the triple _{Pi_ _[τ]_ _[,][ {][a]_ _i_ _[τ]_ _[}]_ _K_ _[, H]_ _i_ _[τ]_ _−_ 1 _[}]_ [ can be given as:]



_Ri_ _[τ]_ - _Pi_ _[τ]_ _[,][ {][a]_ _i_ _[τ]_ _[}]_ _K_ _[, H]_ _i_ _[τ]_ _−_ 1 _[, H]_ _i_ _[τ]_ - =



�∆ _Hi_ _[τ]_ if [�] _k_ _[K]_ =1 [sig (] _[a]_ _i_ _[τ]_ [(] _[k]_ [))] _[τ][ ̸]_ [= 0]
��� _Ri_ ~~_[τ]_~~ 1 _−_ 1 ��� if [�] _k_ _[K]_ =1 [sig (] _[a]_ _i_ _[τ]_ [(] _[k]_ [))] _[τ]_ [ = 0]



_i_ _i_ _i_ _K_ _i−_ 1 _i_ ��� _Ri_ ~~_[τ]_~~ 1 _−_ 1 ��� if [�] _k_ _[K]_ =1 [sig (] _[a]_ _i_ _[τ]_ [(] _[k]_ [))] _[τ]_ [ = 0]

(2)
We generate queries based on three fractal affine transformations,
collect their queries, answers, and semantic entropy, and calculate
their rewards. Considering that each of these rewards represents the
degree to which the enhancement in the exploration of generalization bounds is achieved through the corresponding affine transformation, we configure the probability for an affine transformation _j_
( _j ∈{_ 1 _,_ 2 _,_ 3 _}_ ) in our IFSP as:



_pj_ = _Rj_ _[τ]_ _[/]_



3

- _Rk_ _[τ]_ _[.]_ (3)


_k_ =1



Each _pj_, along with the corresponding input query, response, and
semantic entropy, is included in the training database of the policy
network, which consists of a quadruple, _{Pj_ _[τ]_ _[,][ {][a]_ _j_ _[τ]_ _[}]_ _K_ _[, H]_ _j_ _[τ]_ _−_ 1 _[, p][j][}]_ [.]
Through the iteration process of fractal sampling on the target agent,
more quadruples can be collected until the scale of this dataset is
sufficient for the policy network training.
**Policy Network Training:** A popular design of the policy network
multilayer perceptron (MLP) [26] is introduced to capture the nonlinear representation of state features and predict the probability distribution of three affine transformations _{pj}_ in IFSP _F_ . States in the
generalization space of the target agent are defined as:



0 _[,][ P]_ _i_ _[τ]_ [)] _[ ×][ e][H]_ _i_ _[τ]_
_s_ _[τ]_ _i_ [=] _[ ⌊]_ [∆][Sim][(] _[P]_ _[τ]_ _⌋,_ (4)

_ω_



where _P_ 0 _[τ]_ [is the initial query used to explore the generalization]
bound. Its semantic similarity to _Pi_ _[τ]_ [is involved as a scaling parame-]
ter and _ω_ is a normalization parameter.
For each state _s_ _[τ]_ _i_ [, the action-value function] _[ Q]_ [()][ for affine transfor-]
mations _fi_ _[τ]_ [at the time step] _[ i]_ [ is] _[ Q]_ [(] _[s]_ _i_ _[τ]_ _[, f]_ _i_ _[ τ]_ [) =][ E][ [] _[R]_ _i_ _[τ]_ _[|][ s]_ _i_ _[τ]_ _[, f][i]_ []][, where]
_Ri_ _[τ]_ [is the reward defined in Formula (2). The objective] _[ Q]_ [ (] _[s]_ _i_ _[τ]_ _[, f][i]_ [;] _[ θ][i]_ [)]


is to train the policy network, parameterized by _θ_ denoted. The network is optimized to align with the optimal action-value _Q_ _[∗]_ ( _s_ _[τ]_ _i_ _[, f][i]_ [)][,]
which corresponds to the highest possible reward. To achieve this, a
loss function is defined using the L2 norm:



_L_ ( _θ_ ) =



_N_

- _∥Q_ _[∗]_ ( _s_ _[τ]_ _i_ _[, f]_ _i_ [)] _[ −]_ _[Q]_ [ (] _[s][τ]_ _i_ _[, f]_ _i_ [;] _[ θ]_ _i_ [)] _[∥]_ _[,]_ (5)


_i_ =1



where _N_ is the mini-batch of training samples. Finally, the optimal
parameters _θ_ _[∗]_ of the policy network can be obtained by:




_•_ If _HQ_ _[τ]_ _v_ _[< H]_ _vi_ _[τ]_ [, the input query is within the generalization bound,]
and will obtain a rational response (corresponding to the white
point in Figure 5).


The details of the monitoring process are given in Algorithm 1.


**Algorithm 1** Hallucination monitoring algorithm

1: **Input:** Input query _Pq_ _[τ]_ [, vector database] _[ V]_ _[τ]_ [, similarity threshold] _[ ϵ]_ [ of]
hallucination monitoring;
2: **Output:** Hallucination status of _Pq_ _[τ]_ [;]
3: **for** each vector _vi ∈V_ _[τ]_ **do**
4: Normalize _vi_ : _vi ←_ _∥vvii∥_ [;]
5: **end for**
6: Represent _Pq_ _[τ]_ [with a vector:] _[ Q]_ _v_ _[τ]_ _[←]_ [Embedding][(] _[P]_ _q_ _[ τ]_ [)][;]

7: Normalize _Q_ _[τ]_ _v_ [:] _[ Q][τ]_ _v_ _[←]_ _∥QQ_ ~~_[τ]_~~ _v_ _[τ]_ _v_ _∥_ [;]
8: Initialize an empty list of _results_ ;
9: **for** each vector _vi ∈V_ _[τ]_ **do**
10: Compute cosine similarity _Si_ ;
11: Append ( _vi, Si_ ) to _results_ ;
12: Sort _results_ by similarity score _Si_ in descending order;
13: **end for**
14: **if** _results_ [3] _> ϵ_ **then**
15: Compute the centroid by Formula (7);
16: Normalize the centroid: _C ←_ _∥CC∥_ [;]
17: **end if**
18: Compute the similarity _SC_ between _Q_ _[τ]_ _v_ [and the centroid] _[ C]_ [;]
19: **if** _SC ≥_ _ϵ_ **then**
20: Report _Pq_ _[τ]_ [may cause a hallucination;]
21: **else**
22: Compute the semantic entropy _H_ ( _Q_ _[τ]_ _v_ [)][ of the query;]
23: **if** _H_ ( _Q_ _[τ]_ _v_ [)] _[ > max]_ [(] _[H]_ [(] _[v]_ _i_ _[τ]_ [))] **[ then]**
24: Report _Pq_ _[τ]_ [may cause a hallucination;]
25: **else**
26: Return the response of the agent for _Pq_ _[τ]_ [;]
27: **end if**
28: **end if**


**4** **Experimental Evaluation**


_4.1_ _Datasets_


To evaluate the performance of HalMit, two popular public QueryAnswer (QA) datasets, MedQuAD [2] and SQuAD [22], are used.

_•_ MedQuAD is a collection of question-answer pairs meticulously
curated from 12 trusted National Institute of Health (NIH) websites and covers various medical topics including diseases, medications, and diagnostic tests.

_•_ SQuAD consists of questions posed by crowd-workers on a set of
Wikipedia articles, where the answer to every question is a segment of text or span, and each QA pair is paired with a title.

Without loss of generality, we randomly select four domains from
each of these two datasets, including "Treatment", "Inheritance",
"New York City", and "Modern History", to construct different types
of agents. Since the response from each target agent may not be completely matched with the correct response in the QA pair from the
database, it is necessary to determine whether the response is a hallucination or not. We follow existing work [25] to use the GQA metric
to identify the responses that include hallucinations. More specifically, a binary label is assigned according to the average of unigram
F1 and ROUGE-L. A hallucination is labeled if this average is less
than 0.5 [4].


_4.2_ _Evaluation Setup_


To evaluate the performance of HalMit, six LLMs, including Llama
(Llama2-7B-Instruct and Llama3.1-8B-Instruct), Mistral-7b, qwen21.5b, Falcon-7b and Vicuna-7b, are used to support agent inference.



_θ_ _[∗]_ = arg min
_θi_



_N_

- _L_ ( _Q_ _[∗]_ ( _s_ _[τ]_ _i_ _[, f]_ _i_ [)] _[, Q]_ [ (] _[s][τ]_ _i_ _[, f]_ _i_ [;] _[ θ]_ _i_ [))] _[ .]_ (6)


_i_ =1



The convergence study of the exploration is provided in the experimental section (Section 4.3).


_3.2_ _Hallucination Monitoring_


In this section, we describe how the generalization bound can be
leveraged to monitor if there exist hallucinations in the response from
a target agent. This is achieved by comparing the response with the
information retrieved from the vector database that represents the
generalization bound of the target agent. As the bound often has an
irregular shape (as illustrated in Figure 1), hallucination monitoring
can be very difficult.















start hallucination monitoring, the input query _Pq_ _[τ]_ [is compared with]
all related items in the vector database built in Section 3. This is
achieved by evaluating the cosine similarity of the query vector _Qv_
and each related vector _vi_ denoted by _Si_ . Specifically, we consider
three cases:


_•_ When there are more than three similar items in the database that
exceed a threshold _ϵ_, we calculate the centroid of three most similar items _Pv_ _[τ]_ 1 _[, P]_ _v_ _[ τ]_ 2 _[, P]_ _v_ _[ τ]_ 3 [:]


�3 _i_ =1 _[S][i]_ _[·][ v][i]_
_C_ = ~~�~~ 3 _,_ (7)
_i_ =1 _[S][i]_


where _Si_ is the similarity score of the _i_ -th vector. Next, we calculate the cosine similarity _SC_ between the query vector _Qv_ and
the normalized centroid _C_ . If _SC > ϵ_, the input query is considered to be beyond the generalization bound of the target agent
(corresponding to the blue point in Figure 5).

_•_ Otherwise, we compare the semantic entropy of the query _HQ_ _[τ]_ _v_
with the semantic entropy of the most similar vector _Hvi_ _[τ]_ [in the]
vector database. If _HQ_ _[τ]_ _v_ [is larger, the input query is likely to be]
outside the generalization bound and may cause a hallucination
(corresponding to the red point in Figure 5e).


2.5


2


1.5


1


0.5


0



Domain: Treatment


0 10 20 30
Step



2.5


2


1.5


1


0.5


0



Domain: Inheritance


0 10 20 30
Step



2.5


2


1.5


1


0.5


0



Domain: New York City


0 10 20 30
Step



2.5


2


1.5


1


0.5
