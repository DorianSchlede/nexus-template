<!-- Source: 19-HalMit-2507.15903.pdf | Chunk 1/4 -->

# **Towards Mitigation of Hallucination for** **LLM-empowered Agents: Progressive Generalization** **Bound Exploration and Watchdog Monitor**

**Siyuan Liu** [a,b] **, Wenjing Liu** [c] **, Zhiwei Xu** [b,d,*] **, Xin Wang** [e] **, Bo Chen** [f] **and Tao Li** [a,**]


aCollege of Computer Science, Nankai University
bHaihe Lab of ITAI
cCollege of intelligent Science and Technology, Inner Mongolia University of Technology
dInstitute of Computing Technology, Chinese Academy of Sciences
eDepartment of Electrical and Computer Engineering, Stony Brook University
fDepartment of Computer Science, Michigan Technological University



**Abstract.** Empowered by large language models (LLMs), intelligent agents have become a popular paradigm for interacting with
open environments to facilitate AI deployment. However, hallucinations generated by LLMs—where outputs are inconsistent with
facts—pose a significant challenge, undermining the credibility of
intelligent agents. Only if hallucinations can be mitigated, the intelligent agents can be used in real-world without any catastrophic
risk. Therefore, effective detection and mitigation of hallucinations
are crucial to ensure the dependability of agents. Unfortunately, the
related approaches either depend on white-box access to LLMs or
fail to accurately identify hallucinations. To address the challenge
posed by hallucinations of intelligent agents, we present HalMit, a
novel black-box watchdog framework that models the generalization bound of LLM-empowered agents and thus detect hallucinations without requiring internal knowledge of the LLM’s architecture. Specifically, a probabilistic fractal sampling technique is proposed to generate a sufficient number of queries to trigger the incredible responses in parallel, efficiently identifying the generalization bound of the target agent. Experimental evaluations demonstrate
that HalMit significantly outperforms existing approaches in hallucination monitoring. Its black-box nature and superior performance
make HalMit a promising solution for enhancing the dependability
of LLM-powered systems.


**1** **Introduction**


With the rapid proliferation of artificial intelligence in contemporary life, agents empowered by large language models (LLMs) have
emerged as pioneers of this technology transformation [31]. However, in conjunction with their widespread deployment, the phenomenon of hallucination has become a major concern in LLM and
their agents [13]. LLM hallucination refers to instances in which
LLM generated content is inconsistent, unfaithful, contradictory, or
unverifiable against established real-world knowledge [13], although
it may be presented in a convincing and confident tone. This issue


_∗_ Corresponding Author. Email: xuzhiwei2001@ict.ac.cn.
_∗∗_ Corresponding Author. Email: litao@nankai.edu.cn.



has been recognized by many academic studies and technical public reports as one of the primary ethical and safety risks associated
with LLM agents, along with issues such as bias and toxic content.
The hallucination phenomenon becomes thorny when considering
the black-box nature of LLMs, and severely undermines the credibility of LLM agents, especially in truth-sensitive fields such as law

[9], medicine [7], finance [35], and education [15], where it can have
catastrophic cognitive consequences.
Mitigating hallucinations is critical to improving the dependability of LLM agents in real-world applications [13]. Hallucinations
typically arise when the generated content significantly exceeds the
generalization bounds of the agent [30]. If a generated response lies
outside the bound, it is highly likely that this response is hallucinated [34]. Therefore, identifying the generalization bounds is of
critical importance in mitigating hallucinations in LLM agents. Although recent efforts have been focused on computing non-vacuous
generalization bounds for deep learning models, these bounds tend
to become vacuous at the scale of billion-parameter models [19, 34].
In addition, such theoretical bounds are often derived from restrictive statistical assumptions that limit their applicability to models
with low generalization capacity. Given the vastness of the semantic space, the tightness of the existing generalization bounds remains
difficult to establish [18].
Existing approaches attempt to identify hallucinated responses by
analyzing the internal state of the model [14, 10, 38, 11]. This requires full transparency and access to the internal state of the model,
known as “white-box access”, and cannot work for large-scale commercial models, which are usually close-sourced. The other approaches [1, 32, 28] are mainly based on cross-checking of the LLM
output against external databases. Recent work for modeling generalization bound computes non-vacuous generalization bounds for deep
learning models, but these bounds are vacuous for large models at
the billion-parameter scale [12, 21] that involve directly asking the
LLM to produce its confidence scores in the truthfulness of the statements (referred to as “model confidence” in this paper). Although
these black-box/gray-box approaches allow hallucination monitoring through output text or associated confidence scores, they are degraded by limited knowledge of LLMs and often poorly calibrated


confidence estimates. Ultimately, these score-based approaches may
be incorrect, reducing the effectiveness and accuracy of the monitoring. Therefore, there is a strong demand for developing new techniques that can mitigate hallucinations while avoiding the aforementioned limitations.







**Figure 1** : The complexity of generalization bound modeling.


Due to the complexity of the semantic space, deriving a universal generalization bound across all domains is extremely challenging [30]. However, we observe that this bound may be identified
much more easily within the domain of each specific agent (Section 2). Building on this insight, we propose HalMit, a fine-grained
approach for modeling per-agent generalization bounds to monitor
hallucinations that fall outside these boundaries. A major challenge
in designing HalMit lies in the need to efficiently identify and model
the complex generalization bound of the target agent in a specific domain. As shown in Figure 1, the generalization bound is difficult to
pinpoint. The bound exploration process may deviate from the true
boundary and become trapped in local loops (red arrows), considering that the bounded space itself is vast.
To accelerate the generalization bound exploration, we propose a
probabilistic fractal sampling method to generate a sufficient number of parallel queries so that they can efficiently cover the generalization bound of the targeted agent. Each identified boundary point
is stored in a vector database, where the point is represented by its
query–response pair along with associated context. During hallucination monitoring, HalMit compares the input query with those retrieved from the vector database. If the query closely resembles the
retrieved records in the vector base, it is considered near the boundary, and the response corresponding to the input query is flagged as
a potential hallucination. Our major contributions are summarized as
follows:


1) Through a preliminary study on agent hallucinations, we confirm
that hallucinated responses correspond to the agent’s generalization bound and that it is possible to model this bound within specific application domains.
2) We propose a novel probabilistic fractal exploration scheme to enable our MAS system to incrementally probe the generalization
boundary. In this process, deep reinforcement learning guides the
multi-agent exploration by adjusting the probabilities of fractal
transformations based on three common semantic patterns. These
probabilities are continuously updated to efficiently and effectively model the generalization bound within a specific domain.
3) A unique hallucination mitigation technology is provided based
on the generalization boundary to enable more dependable monitoring and persistently detecting potential hallucinations.
4) We have conducted extensive experimental evaluations and the results are encouraging, demonstrating a significant improvement in
hallucination monitoring effectiveness over baseline solutions. A
unique hallucination mitigation technology is provided to enable
a more dependable monitoring and monitoring of potential hallucinations.



These contributions address vital challenges in mitigating hallucination of LLM-empowered agents, paving the way for more trustworthy intelligent systems in real-world applications. To the best of
our knowledge, this is the first hallucination monitoring approach
that operates without access to internal model knowledge or reliance
on cross-verification algorithms. This design enables real-time and
persistent hallucination mitigation in deployed intelligent systems.
The source code for HalMit will be available on GitHub after the
paper is accepted.


**2** **Motivation**


In this section, we present preliminary studies that investigate how
hallucinations manifest in LLM-empowered agents across various
domains.


_•_ **PS1** : We investigate whether the statistic characteristics of hallucinations vary across different domains.

_•_ **PS2** : We examine the statistic characteristics within each domain
to identify potential patterns or regularities in LLM hallucinations.

_•_ **PS3** : We assess whether certain existing characteristics can be directly leveraged to monitor hallucinations.



(e) Fiction (f) Paranormal

**Figure 2** : The semantic entropy values for six popular domains.
**Experimental Settings:** Without loss of generality, we study the response quality of agents powered by Llama3.1-8B across six popular domains, including health, nutrition, sociology, law, fiction, and
paranormal. All query-answer pairs and domains used in the study
are randomly sampled from a real-world hallucination dataset, TruthfulQA [17], which includes both truthful and hallucinated responses.
Semantic entropy [6] is one metric that has been extensively used to
assess the uncertainty level of agent responses (see Appendix B),
with higher entropy value typically indicating greater uncertainty
and a higher likelihood of hallucinations. Unlike general confidence
scores of LLM’s response, semantic entropy is specifically designed
to characterize the hallucinations of LLMs. Based on semantic entropy, we evaluate response quality across the six domains by measuring semantic entropy, with the results presented in Figure 2.



6


5


4


3


2


1


6


5


4


3


2


1


6


5


4


3


2


1



1.8 2.0 2.2 2.4
Semantic Entropy


(a) Health


1.8 2.0 2.2 2.4
Semantic Entropy


(c) Sociology


1.8 2.0 2.2 2.4
Semantic Entropy



1.8 2.0 2.2 2.4
Semantic Entropy


(b) Nutrition


1.8 2.0 2.2 2.4
Semantic Entropy


(d) Law


1.8 2.0 2.2 2.4
Semantic Entropy



6


5


4


3


2


1


6


5


4


3


2


1


6


5


4


3


2


1


_•_ **PS1** : **Significant Variations across Domains.** As shown in Figure 2, the semantic entropy values of agent responses vary substantially across application domains, with noticeable differences
in both medians and variances. This indicates that the statistical
characteristics of hallucinations differ significantly by domains,
suggesting that no universal generalization bound can be established across all domains.

_•_ **PS2** : **Statistic Stability within The Same Domain.** Further analysis of the boxplots reveals that within each individual domain,
the semantic entropy values, while subject to some fluctuation
across testing groups, tend to follow a consistent distribution. This
internal stability suggests that hallucination patterns are coherent within each domain, making it feasible to identify a domainspecific generalization bound.

_•_ **PS3** : **Limitations of a Threshold Based on the Existing Met-**
**ric.** As observed in the boxplots for the Health, Nutrition, Law,
Fiction, and Paranormal domains, outliers extending beyond the
whiskers in the boxplots are present. This suggests that while semantic entropy has been shown to support hallucination detection [16, 6, 10], relying solely on a fixed threshold is insufficient.
The presence of high-entropy yet potentially non-hallucinatory responses (and vice versa) highlights the need for more nuanced detection methods.


**Summary of findings:** Based on the above observations, our findings reveal that: (1) hallucination patterns vary across application domains but tend to exhibit consistent statistical behavior within the
same domain; and (2) hallucinations cannot be effectively detected
using a simple threshold on any single existing metric, even one as
significant as semantic entropy. These insights suggest that the generalization bound distinguishing hallucinated from non-hallucinated
responses is more clearly defined when tailored to a specific agent
within a specific domain, rather than derived from a collective set of
agents or domains. Consequently, accurate hallucination monitoring
requires the identification of these generalization bounds in a finegrained, domain-aware manner.
**Motivated by these observations, we propose** _**HalMit**_ **, a hallu-**
**cination mitigation paradigm for LLM-empowered agents.**


**3** **Methodology**


To persistently mitigate hallucinations, HalMit functions as a
“watchdog” framework for each target agent to monitor hallucinations. Before being used to monitor hallucinations, HalMit first models the agent’s generalization bound based on a proposed multi-agent
exploration system. This allows hallucinations to be identified and
mitigated based on their deviation from the learned generalization
boundary.


_3.1_ _Generalization Exploration Bound with a MAS_


Given the black-box nature of LLM-empowered agents, exploring
their generalization bounds and identifying hallucinations are critical and challenging [33]. To address this, we introduce a multi-agent
bound exploration method that integrates probabilistic fractal sampling into a multi-agent system (MAS) for parallel query generation.
To improve the efficiency and relevance of the queries, we propose a
reinforcement learning-based scheme to dynamically adjust the fractal probabilities.
As illustrated in Figure 3, the proposed MAS consists of three specialized agent types: core agent (CA), query generation agent (QGA),


|Col1|(1)Q GA Scheduling<br>Cmd 1|(2) Query Generatio<br>Query|n<br>1<br>result 1|(3) Response Genera<br>Response 1|tion<br>(4) Qu<br>Eval<br>tion<br>(10)Q<br>Eva|
|---|---|---|---|---|---|
|||||||
|||**Evaluation**|**Evaluation**|**Evaluation**|**Evaluation**|
|||**Evaluation**|**Evaluation**|||
||**(6)****_QGA Scheduling_**<br>**(5)****_Extend The Bound_**<br>**Cmd 2**|<br>**Cmd 3**|**(8)****_Query Generation_**<br>**Query 3**|**(9)****_Response Genera_**<br>**Response 2**|**(9)****_Response Genera_**<br>**Response 2**|
|||**Query 2**<br>**(7)****_Query Generation_**|**Query 2**<br>**(7)****_Query Generation_**|**Query 2**<br>**(7)****_Query Generation_**|**Query 2**<br>**(7)****_Query Generation_**|
|||**Evaluation**|** Result 2**|** Result 2**|** Result 2**|
|||**Evaluation**|** Result 2**|**Response 3**|**Response 3**|
||**(11)****_Extend The Boun_**|**Evaluation**<br>**_  d_**|** Result 3**|||



where _Pt_ _[τ]_ [are queries used in round] _[ t]_ [, functions] _[ FT]_ [1] _[, FT]_ [2] _[, FT]_ [3]
correspond to three fractal affine transformations, _pi_ is the execution
probability for _FTi_ . These three types of fractal affine transformations are introduced as follows:

_•_ **FT1: Semantic Deduction.** This transformation generates more
specific queries by deriving them from general rules or concepts
presented in the previous iteration. For example, given a query,
_“Did humans really land on the moon in 1969?”_, a deductive
transformation would produce a more focused follow-up such as
_“What were the technological advancements that enabled humans_
_to land on the moon in 1969?”_ .

_•_ **FT2: Semantic Analog.** This transformation broadens the scope
of the original query by leveraging semantic associations such as
synonyms, antonyms, or functional analogies. In this way, a new
query, _“What historical events in space exploration paralleled the_
_significance of the moon landing in 1969?”_, can be generated.
This helps the system probe parallel narratives and conceptual
similarities in the semantic space.



and evaluation agent (EA). Among them, the CA coordinates interactions between QGAs and the target LLM-powered agent. Meanwhile,
the EA, guided by HalluBench criteria [36], evaluates the quality of
the response from the target and provides essential feedback to refine
the query generation process.


**CA** **QGA1** **QGA2** **Target** **EA**









_**Evaluation**_











**Figure 3** : Multi-agent collaboration in HalMit.



_**Evaluation**_



_3.1.1_ _Probabilistic Fractal-based Query Generation_


Leveraging the self-similarity feature of the fractal in natural language, we propose a novel probabilistic fractal-based query generation method for use in QGAs. This method iteratively constructs increasingly complex query structures that progressively approach the
generalization bound of the target agent. Unlike conventional fractal
systems that apply all affine transformations in each iteration step,
our method proposes an Iterated Function System with Probabilities (IFSP) (See appendix A) that enough queries can be generated
quickly to cover the generalization bound of the target agent _τ_ . More
specifically, according to semantic theory [23], three semantic extension patterns, _induction_, _deduction_, and _analogy_, are used as fractal
affine transformations to extend specific queries and navigate the semantic space. The execution probability of each transformation is
dynamically adjusted based on the IFSP system:



_F_ = _{FTi_ : _Pt_ _[τ]_ _−_ 1 _→Ppi_ _t_ _[τ]_ _[,]_



3

- _pi_ = 1 _},_ (1)


_i_ =1


_•_ **FT3:** **Semantic** **Induction.** This transformation generates
broader, more abstract queries by generalizing from specific instances and inferring underlying linguistic or conceptual patterns.
As another example, _“How can we assess the reliability of com-_
_monly accepted events in the history of space exploration?”_, is
obtained by following FT3. This supports exploration of overarching themes and epistemological questions within the domain.


These three affine transformations are applied in each iteration
of the exploration process. In this way, new queries are generated
through the iterative process that starts from basic concepts or principles. The core agent can order multiple query generation agents
to generate queries, so the iteration can be realized through parallel
processes, to significantly increase the speed in identifying the generalization bound. This iteration process includes four steps:
1) To cover a broader semantic space with in the bound, the CA
randomly initializes multiple queries in a domain and sends each
query to the target agent _τ_ as initial questions.
2) The target agent responds to queries with responses that may contain hallucinations. Therefore, each QA pair is sent to an EA. After
receiving the QA pair, the EA assesses whether the response in the
received QA pair contains a hallucination, and sends a report back
to the CA, including the QA pair and the corresponding evaluation
results.
3) Depending on the evaluation result, QGA will perform query generation in two ways. In case a hallucination is reported, the CA
embeds the QA pair and the context information into a vector
database as a point of the generalization bound of agent _τ_ (detailed
in Section 3.2). To expand the exploration range and cover more
of the generalization bound, the CA schedules the QGAs to generate new queries through the fractal affine transformations, FT1
and FT2, where the probability of each is determined through reinforcement learning (details in Section 3.1.2). Otherwise, in case
no hallucination is reported, new queries will be randomly generated, and sent back to the CA.
4) The new queries generated by the QGAs are sent back to the target agent. In this way, a new round of fractal exploration with a
pipeline is scheduled by CA, where multiple new iterations are
generated in parallel in response to each exploration path. More
EAs are scheduled to assess all QA pairs and the evaluation reports will be sent to the CA. The bound search speed can be exponentially increased in this way.
During this iterative process, the ratio of the hallucinations among all
QA pairs, _γ_, is incrementally updated by the core agent. Once _γ_ becomes larger than an empirical threshold _ϵ_, it indicates that the generalization bound of agent _τ_ can be identified by the vector database,
and this iterative process of _F_ ends. These parameters are evaluated
in ablation studies in Section 4.5.


_3.1.2_ _Reinforced Determination of Fractal Probabilities_

