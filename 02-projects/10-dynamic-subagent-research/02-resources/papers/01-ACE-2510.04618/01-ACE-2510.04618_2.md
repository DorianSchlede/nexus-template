<!-- Source: 01-ACE-2510.04618.pdf | Chunk 2/5 -->

**MIPROv2 [36].** MIPROv2 is a popular prompt optimizer for LLM applications that works by jointly
optimizing system instructions and in-context demonstrations via bayesian optimization. We use the official
DSPy implementation [15], setting auto="heavy" to maximize optimization performance.


**GEPA [4].** GEPA (Genetic-Pareto) is a sample-efficient prompt optimizer based on reflective prompt
evolution. It collects execution traces (reasoning, tool calls, intermediate outputs) and applies naturallanguage reflection to diagnose errors, assign credit, and propose prompt updates. A genetic Pareto search
maintains a frontier of high-performing prompts, mitigating local optima. Empirically, GEPA outperforms
reinforcement learning methods such as GRPO and prompt optimizers like MIPROv2, achieving up to
10–20% higher accuracy with as much as 35× fewer rollouts. We use the official DSPy implementation [14],
setting auto="heavy" to maximize optimization performance.


**Dynamic Cheatsheet (DC) [41].** DC is a test-time learning approach that introduces an adaptive external
memory of reusable strategies and code snippets. By continuously updating this memory with newly
encountered inputs and outputs, DC enables models to accumulate knowledge and reuse it across tasks,
often leading to substantial improvements over static prompting methods. A key advantage of DC is that it
does not require ground-truth labels: the model can curate its own memory from its generations, making
the method highly flexible and broadly applicable. We use the official implementation released by the
authors [42] and set it to use the cumulative mode (DC-CU).


**ACE (ours).** ACE optimizes LLM contexts for both offline and online adaptation through an agentic context
engineering framework. To ensure fairness, we use the same LLM for the Generator, Reflector, and Curator
(non-thinking mode of DeepSeek-V3.1 [13]), preventing knowledge transfer from a stronger Reflector or
Curator to a weaker Generator. This isolates the benefit of context construction itself. We adopt a batch size
of 1 (constructing a delta context from each sample). We set the maximum number of Reflector refinement
rounds and the maximum number of epochs in offline adaptation to 5.


**4.3** **Results on Agent Benchmark**


**Analysis.** As shown in Table 1, ACE consistently improves over strong baselines on the AppWorld
benchmark. In the offline setting, ReAct + ACE outperforms both ReAct + ICL and ReAct + GEPA by
significant margins (12.3% and 11.9%, respectively), demonstrating that structured, evolving, and detailed
contexts enable more effective agent learning than fixed demonstrations or single optimized instruction
prompts. These gains extend to the online setting, where ACE continues to outperform prior adaptive
methods such as Dynamic Cheatsheet by an average of 7.6%.


In the agent use case, ACE remains effective even _without_ access to ground-truth labels during adaptation:
ReAct + ACE achieves an average improvement of 14.8% over the ReAct baseline in this setting. This
robustness arises because ACE leverages signals naturally available during execution ( _e.g.,_ code execution
success or failure) to guide the Reflector and Curator in forming structured lessons of successes and failures.
Together, these results establish ACE as a strong and versatile framework for building self-improving agents
that adapt reliably both with and without labeled supervision.


Notably, on the latest AppWorld leaderboard (as of September 20, 2025; Figure 5), on average, ReAct +
ACE (59.4%) matches the top-ranked IBM CUGA (60.3%), a production-level GPT-4.1–based agent [35],
despite using the smaller open-source model DeepSeek-V3.1. With online adaptation, ReAct + ACE even


7


Table 1: **Results on the AppWorld Agent Benchmark.** "GT labels" indicates whether ground-truth labels are
available to the Reflector during adaptation. We evaluate the ACE framework against multiple baselines
on top of the official ReAct implementation, both for offline and online context adaptation. ReAct + ACE
outperforms selected baselines by an average of 10.6%, and could achieve good performance even without
access to GT labels.


Table 2: **Results on Financial Analysis Benchmark.** "GT labels" indicates whether ground-truth labels
are available to the Reflector during adaptation. With GT labels, ACE outperforms selected baselines by
an average of 8.6%, highlighting the advantage of structured and evolving contexts for domain-specific
reasoning. However, we also observe that in the absence of reliable feedback signals ( _e.g.,_ ground-truth
labels or execution outcomes), both ACE and other adaptive methods such as Dynamic Cheatsheet may
degrade, suggesting that context adaptation depends critically on feedback quality.


surpasses IBM CUGA by 8.4% in TGC and 0.7% in SGC on the harder test-challenge split, underscoring the
effectiveness of ACE in building comprehensive and self-evolving contexts for agents.


**4.4** **Results on Domain-Specific Benchmark**


**Analysis.** As shown in Table 2, ACE delivers strong improvements on financial analysis benchmarks.
In the offline setting, when provided with ground-truth answers from the training split, ACE surpasses
ICL, MIPROv2, and GEPA by clear margins (an average of 10.9%), showing that structured and evolving
contexts are particularly effective when tasks require precise domain knowledge ( _e.g.,_ financial concepts,


8


Table 3: **Ablation Studies on AppWorld.** We study how particular design choices of ACE (iterative
refinement, multi-epoch adaptation, and offline warmup) could help high-quality context adaptation.



**Method** **Latency (s)** _↓_ **# Rollouts** _↓_


ReAct + GEPA 53898 1434
ReAct + ACE 9517(-82.3%) 357(-75.1%)


(a) **Offline** (AppWorld).



**Method** **Latency (s)** _↓_ **Token Cost ($)** _↓_


DC (CU) 65104 17.7
ACE 5503(-91.5%) 2.9(-83.6%)


(b) **Online** (FiNER).



Table 4: **Cost and Speed Analysis.** We measure the context adaptation latency, number of rollouts, and
dollar costs of ACE against GEPA (offline) and DC (online).


XBRL rules) that goes beyond fixed demonstrations or monolithic optimized prompts. In the online setting,
ACE continues to exceed prior adaptive methods such as DC by an average of 6.2%, further confirming the
benefit of agentic context engineering for accumulating reusable insights across specialized domains.


Moreover, we also observe that when ground-truth supervision or reliable execution signals are absent,
both ACE and DC may degrade in performance. In such cases, the constructed context can be polluted by
spurious or misleading signals, highlighting a potential limitation of inference-time adaptation without
reliable feedback. This suggests that while ACE is robust under rich feedback ( _e.g.,_ code execution results or
formula correctness in agent tasks), its effectiveness depends on the availability of signals that allow the
Reflector and Curator to make sound judgments. We return to this limitation in Appendix B.


**4.5** **Ablation Study**


Table 3 reports ablation studies on the AppWorld benchmark, analyzing how individual design choices
of ACE contribute to effective context adaptation. We examine three factors: (1) _the Reflector with iterative_
_refinement_, our addition to the agentic framework beyond Dynamic Cheatsheet, (2) _multi-epoch adaptation_,
which refines contexts over training samples multiple times, and (3) _offline warmup_, which initializes the
context through offline adaptation before online adaptation begins.


**4.6** **Cost and Speed Analysis**


Due to its support for incremental, “delta" context updates and non-LLM-based context merging and deduplication, ACE demonstrates particular advantages in reducing the cost (in terms of the number of rollouts
or the amount of dollar cost for token ingestion/generation) and latency of adaptation.


As examples, on the offline adaptation of AppWorld, ACE achieves 82.3% reduction in adaptation latency
and 75.1% reduction in the number of rollouts as compared to GEPA (Table 4(a)). On the online adaptation


9


of FiNER, ACE achieves 91.5% reduction in adaptation latency and 83.6% reduction in token dollar cost for
token ingestion and generation as compared to DC (Table 4(b)).


**5** **Discussion**


**Longer Context** _̸_ = **Higher Serving Cost.** Although ACE produces longer contexts than methods such
as GEPA, this does not translate to linearly higher inference cost or GPU memory usage. Modern serving
infrastructures are increasingly optimized for long-context workloads through techniques such as the
reuse [17, 51], compression [30, 32], and offload [25] of KV cache. These mechanisms allow frequently reused
context segments to be cached locally or remotely, avoiding repetitive and expensive prefill operations.
Ongoing advances in ML systems suggest that the amortized cost of handling long contexts will continue to
decrease, making context-rich approaches like ACE increasingly practical in deployment.


**Implications for Online and Continuous Learning.** Online and continuous learning are key research
directions in machine learning for addressing issues like distribution shifts [19, 24] and limited training
data [21, 37, 60]. ACE offers a flexible and efficient alternative to conventional model fine-tuning, as
adapting contexts is generally cheaper than updating model weights [9, 20, 26, 28]. Moreover, because
contexts are human-interpretable, ACE enables _selective unlearning_ [8, 10, 29]—whether due to privacy or
legal constraints [1, 2], or when outdated or incorrect information is identified by domain experts. These are
promising directions for future work, where ACE could play a central role in advancing continuous and
responsible learning.


**References**


[1] General Data Protection Regulation article 17: Right to erasure. EU Regulation 2016/679, 2016. Official
consolidated text.


[2] California consumer privacy act, civil code §1798.105: Right to delete. State of California Civil Code,
2018.


[3] Rishabh Agarwal, Avi Singh, Lei Zhang, Bernd Bohnet, Luis Rosias, Stephanie Chan, Biao Zhang,
Ankesh Anand, Zaheer Abbas, Azade Nova, et al. Many-shot in-context learning. _Advances in Neural_
_Information Processing Systems_, 37:76930–76966, 2024.


[4] Lakshya A Agrawal, Shangyin Tan, Dilara Soylu, Noah Ziems, Rishi Khare, Krista Opsahl-Ong, Arnav
Singhvi, Herumb Shandilya, Michael J Ryan, Meng Jiang, et al. Gepa: Reflective prompt evolution can
outperform reinforcement learning. _arXiv preprint arXiv:2507.19457_, 2025.


[[5] AppWorld. Leaderboard. https://appworld.dev/leaderboard, 2025. Accessed: 2025-09-20.](https://appworld.dev/leaderboard)


[6] Akari Asai, Zeqiu Wu, Yizhong Wang, Avirup Sil, and Hannaneh Hajishirzi. Self-rag: Learning to
retrieve, generate, and critique through self-reflection. 2024.


[7] Sebastian Borgeaud, Arthur Mensch, Jordan Hoffmann, Trevor Cai, Eliza Rutherford, Katie Millican,
George Bm Van Den Driessche, Jean-Baptiste Lespiau, Bogdan Damoc, Aidan Clark, et al. Improving
language models by retrieving from trillions of tokens. In _International conference on machine learning_,
pages 2206–2240. PMLR, 2022.


[8] Lucas Bourtoule, Varun Chandrasekaran, Christopher Choquette-Choo, Hengrui Jia, Adelin Travers,
Baiwu Zhang, David Lie, and Nicolas Papernot. Machine unlearning. _IEEE Symposium on Security and_
_Privacy_, pages 141–159, 2021.


[9] Tom Brown et al. Language models are few-shot learners. In _NeurIPS_, 2020.


[10] Yinzhi Cao and Junfeng Yang. Towards making systems forget with machine unlearning. In _IEEE_
_Symposium on Security and Privacy_, 2015.


10


[11] Tianxiang Chen, Zhentao Tan, Xiaofan Bo, Yue Wu, Tao Gong, Qi Chu, Jieping Ye, and Nenghai Yu.
Flora: Effortless context construction to arbitrary length and scale. _arXiv preprint arXiv:2507.19786_, 2025.


[12] Yeounoh Chung, Gaurav T Kakkar, Yu Gan, Brenton Milne, and Fatma Ozcan. Is long context all you
need? leveraging llm’s extended context for nl2sql. _arXiv preprint arXiv:2501.12372_, 2025.


[13] DeepSeek-AI. Deepseek-v3 technical report, 2024.


[[14] DSPy. dspy.gepa: Reflective prompt optimizer. https://dspy.ai/api/optimizers/GEPA/overview/,](https://dspy.ai/api/optimizers/GEPA/overview/)
2025. Accessed: 2025-09-24.


[[15] DSPy. dspy.miprov2. https://dspy.ai/api/optimizers/MIPROv2/, 2025. Accessed: 2025-09-24.](https://dspy.ai/api/optimizers/MIPROv2/)


[16] Shuzheng Gao, Chaozheng Wang, Cuiyun Gao, Xiaoqian Jiao, Chun Yong Chong, Shan Gao, and
Michael Lyu. The prompt alchemist: Automated llm-tailored prompt optimization for test case generation. _arXiv preprint arXiv:2501.01329_, 2025.


[17] In Gim, Guojun Chen, Seung-seob Lee, Nikhil Sarda, Anurag Khandelwal, and Lin Zhong. Prompt
cache: Modular attention reuse for low-latency inference. _Proceedings of Machine Learning and Systems_,
6:325–338, 2024.


[18] Neel Guha, Julian Nyarko, Daniel Ho, Christopher Ré, Adam Chilton, Alex Chohlas-Wood, Austin
Peters, Brandon Waldon, Daniel Rockmore, Diego Zambrano, et al. Legalbench: A collaboratively built
benchmark for measuring legal reasoning in large language models. _Advances in neural information_
_processing systems_, 36:44123–44279, 2023.


[19] Ishaan Gulrajani and David Lopez-Paz. In search of lost domain generalization. In _ICLR_, 2021.


[20] Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and
Weizhu Chen. LoRA: Low-rank adaptation of large language models. _arXiv:2106.09685_, 2021.


[21] Maxwell L Hutchinson, Erin Antono, Brenna M Gibbons, Sean Paradiso, Julia Ling, and Bryce Meredig.
Overcoming data scarcity with transfer learning. _arXiv preprint arXiv:1711.05099_, 2017.


[22] Mingjian Jiang, Yangjun Ruan, Luis Lastras, Pavan Kapanipathi, and Tatsunori Hashimoto. Putting it
all into context: Simplifying agents with lclms. _arXiv preprint arXiv:2505.08120_, 2025.


[23] Tushar Khot, Harsh Trivedi, Matthew Finlayson, Yao Fu, Kyle Richardson, Peter Clark, and Ashish
Sabharwal. Decomposed prompting: A modular approach for solving complex tasks. _arXiv preprint_
_arXiv:2210.02406_, 2022.


[24] Pang Wei Koh, Shiori Sagawa, Henrik Marklund, Sang Michael Xie, Marvin Zhang, Akshay Balsubramani, Weihua Hu, Michihiro Yasunaga, Richard Lanas Phillips, Irena Gao, et al. Wilds: A benchmark of
in-the-wild distribution shifts. In _International conference on machine learning_, pages 5637–5664. PMLR,
2021.


[25] Wonbeom Lee, Jungi Lee, Junghwan Seo, and Jaewoong Sim. _{_ InfiniGen _}_ : Efficient generative inference
of large language models with dynamic _{_ KV _}_ cache management. In _18th USENIX Symposium on_
_Operating Systems Design and Implementation (OSDI 24)_, pages 155–172, 2024.


[26] Brian Lester, Rami Al-Rfou, and Noah Constant. The power of scale for parameter-efficient prompt
tuning. In _EMNLP_, 2021.


[27] Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal,
Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, et al. Retrieval-augmented generation for
knowledge-intensive nlp tasks. _Advances in neural information processing systems_, 33:9459–9474, 2020.


[28] Xiang Lisa Li and Percy Liang. Prefix-tuning: Optimizing continuous prompts for generation. _ACL_,
2021.


11


[29] Shiyang Liu et al. Rethinking machine unlearning for large language models. _arXiv:2402.08787_, 2024.


[30] Yuhan Liu, Hanchen Li, Yihua Cheng, Siddhant Ray, Yuyang Huang, Qizheng Zhang, Kuntai Du, Jiayi
Yao, Shan Lu, Ganesh Ananthanarayanan, et al. Cachegen: Kv cache compression and streaming for
fast large language model serving. In _Proceedings of the ACM SIGCOMM 2024 Conference_, pages 38–56,
2024.


[31] Zhining Liu, Rana Ali Amjad, Ravinarayana Adkathimar, Tianxin Wei, and Hanghang Tong. Selfelicit:
Your language model secretly knows where is the relevant evidence. _arXiv preprint arXiv:2502.08767_,
2025.


[32] Zirui Liu, Jiayi Yuan, Hongye Jin, Shaochen Zhong, Zhaozhuo Xu, Vladimir Braverman, Beidi Chen, and
Xia Hu. Kivi: A tuning-free asymmetric 2bit quantization for kv cache. _arXiv preprint arXiv:2402.02750_,
2024.


[33] Lefteris Loukas, Manos Fergadiotis, Ilias Chalkidis, Eirini Spyropoulou, Prodromos Malakasiotis, Ion
Androutsopoulos, and Georgios Paliouras. Finer: Financial numeric entity recognition for xbrl tagging.
_arXiv preprint arXiv:2203.06482_, 2022.


[34] Yansheng Mao, Jiaqi Li, Fanxu Meng, Jing Xiong, Zilong Zheng, and Muhan Zhang. Lift: Improving
long context understanding through long input fine-tuning. _arXiv preprint arXiv:2412.13626_, 2024.


[35] Sami Marreed, Alon Oved, Avi Yaeli, Segev Shlomov, Ido Levy, Offer Akrabi, Aviad Sela, Asaf Adi, and
Nir Mashkif. Towards enterprise-ready computer using generalist agent. _arXiv preprint arXiv:2503.01861_,
2025.


[36] Krista Opsahl-Ong, Michael J Ryan, Josh Purtell, David Broman, Christopher Potts, Matei Zaharia, and
Omar Khattab. Optimizing instructions and demonstrations for multi-stage language model programs.
_arXiv preprint arXiv:2406.11695_, 2024.


[37] Sinno Jialin Pan and Qiang Yang. A survey on transfer learning. _IEEE Transactions on Knowledge and_
_Data Engineering_, 22(10):1345–1359, 2010.


[38] Shishir G Patil, Tianjun Zhang, Xin Wang, and Joseph E Gonzalez. Gorilla: Large language model
connected with massive apis. _Advances in Neural Information Processing Systems_, 37:126544–126565, 2024.


[39] Bowen Peng, Jeffrey Quesnelle, Honglu Fan, and Enrico Shippole. Yarn: Efficient context window
extension of large language models. _arXiv preprint arXiv:2309.00071_, 2023.


[40] Noah Shinn, Federico Cassano, Ashwin Gopinath, Karthik Narasimhan, and Shunyu Yao. Reflexion:
Language agents with verbal reinforcement learning. _Advances in Neural Information Processing Systems_,
36:8634–8652, 2023.


[41] Mirac Suzgun, Mert Yuksekgonul, Federico Bianchi, Dan Jurafsky, and James Zou. Dynamic cheatsheet:
Test-time learning with adaptive memory. _arXiv preprint arXiv:2504.07952_, 2025.


[42] Mirac Suzgun, Mert Yuksekgonul, Federico Bianchi, Dan Jurafsky, and James Zou. Dynamic cheatsheet:
[Test-time learning with adaptive memory. https://github.com/suzgunmirac/dynamic-cheatsheet,](https://github.com/suzgunmirac/dynamic-cheatsheet)
2025. Accessed: 2025-09-24.


[43] Harsh Trivedi, Tushar Khot, Mareike Hartmann, Ruskin Manku, Vinty Dong, Edward Li, Shashank
Gupta, Ashish Sabharwal, and Niranjan Balasubramanian. Appworld: A controllable world of apps
and people for benchmarking interactive coding agents. _arXiv preprint arXiv:2407.18901_, 2024.


[44] Dannong Wang, Jaisal Patel, Daochen Zha, Steve Y Yang, and Xiao-Yang Liu. Finlora: Benchmarking
lora methods for fine-tuning llms on financial datasets. _arXiv preprint arXiv:2505.19819_, 2025.


[45] Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery,
and Denny Zhou. Self-consistency improves chain of thought reasoning in language models. _arXiv_
_preprint arXiv:2203.11171_, 2022.


12


[46] Zora Zhiruo Wang, Jiayuan Mao, Daniel Fried, and Graham Neubig. Agent workflow memory. _arXiv_
_preprint arXiv:2409.07429_, 2024.


[47] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou,
et al. Chain-of-thought prompting elicits reasoning in large language models. _Advances in neural_
_information processing systems_, 35:24824–24837, 2022.


[48] Wujiang Xu, Kai Mei, Hang Gao, Juntao Tan, Zujie Liang, and Yongfeng Zhang. A-mem: Agentic
memory for llm agents. _arXiv preprint arXiv:2502.12110_, 2025.


[49] John Yang, Carlos E Jimenez, Alexander Wettig, Kilian Lieret, Shunyu Yao, Karthik Narasimhan, and
Ofir Press. Swe-agent: Agent-computer interfaces enable automated software engineering. _Advances in_
_Neural Information Processing Systems_, 37:50528–50652, 2024.


[50] Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William W Cohen, Ruslan Salakhutdinov, and
Christopher D Manning. Hotpotqa: A dataset for diverse, explainable multi-hop question answering.
_arXiv preprint arXiv:1809.09600_, 2018.


[51] Jiayi Yao, Hanchen Li, Yuhan Liu, Siddhant Ray, Yihua Cheng, Qizheng Zhang, Kuntai Du, Shan Lu,
and Junchen Jiang. Cacheblend: Fast large language model serving for rag with cached knowledge
fusion. In _Proceedings of the Twentieth European Conference on Computer Systems_, pages 94–109, 2025.


[52] Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao.
React: Synergizing reasoning and acting in language models. In _International Conference on Learning_
_Representations (ICLR)_, 2023.


[53] Jiacheng Ye, Chengzu Li, Lingpeng Kong, and Tao Yu. Generating data for symbolic language with
large language models. _arXiv preprint arXiv:2305.13917_, 2023.


[54] Mert Yuksekgonul, Federico Bianchi, Joseph Boen, Sheng Liu, Zhi Huang, Carlos Guestrin, and James
Zou. Textgrad: Automatic" differentiation" via text. _arXiv preprint arXiv:2406.07496_, 2024.


[55] Matei Zaharia, Omar Khattab, Lingjiao Chen, Jared Quincy Davis, Heather Miller, Chris Potts, James
Zou, Michael Carbin, Jonathan Frankle, Naveen Rao, and Ali Ghodsi. The shift from models to
[compound ai systems. https://bair.berkeley.edu/blog/2024/02/18/compound-ai-systems/, 2024.](https://bair.berkeley.edu/blog/2024/02/18/compound-ai-systems/)


[56] Genghan Zhang, Weixin Liang, Olivia Hsu, and Kunle Olukotun. Adaptive self-improvement llm
agentic system for ml library development. _arXiv preprint arXiv:2502.02534_, 2025.


[57] Qizheng Zhang, Ali Imran, Enkeleda Bardhi, Tushar Swamy, Nathan Zhang, Muhammad Shahbaz,
and Kunle Olukotun. Caravan: Practical online learning of _{_ In-Network _}{_ ML _}_ models with labeling
agents. In _18th USENIX Symposium on Operating Systems Design and Implementation (OSDI 24)_, pages
325–345, 2024.


[58] Qizheng Zhang, Michael Wornow, and Kunle Olukotun. Cost-efficient serving of llm agents via test-time
plan caching. _arXiv preprint arXiv:2506.14852_, 2025.


[59] Huichi Zhou, Yihang Chen, Siyuan Guo, Xue Yan, Kin Hei Lee, Zihan Wang, Ka Yiu Lee, Guchun
Zhang, Kun Shao, Linyi Yang, et al. Agentfly: Fine-tuning llm agents without fine-tuning llms. _arXiv_
_preprint arXiv:2508.16153_, 2025.


[60] Fuzhen Zhuang, Zhiyuan Qi, Keyu Duan, Dongbo Xi, Yongchun Zhu, Hengshu Zhu, Hui Xiong, and
Qing He. A comprehensive survey on transfer learning. _arXiv:1911.02685_, 2019.


13


**A** **Related Work on Agent Memory**


A growing body of work explores how agents can accumulate experience from past trajectories and leverage
external (often non-parametric) memory to guide future actions. AgentFly [59] presents an extensible framework where memory evolves continuously as agents solve tasks, enabling scalable reinforcement learning
and long-horizon reasoning across diverse environments. AWM (Agent Workflow Memory) [46] induces
reusable _workflows_ —structured routines distilled from past trajectories—and selectively injects them into
memory to improve efficiency and generalization in web navigation benchmarks. A-MEM [48] introduces
a dynamically organized memory system inspired by the Zettelkasten method: each stored memory is
annotated with structured attributes ( _e.g.,_ tags, keywords, contextual descriptions) and automatically linked
to relevant past entries, while existing entries are updated to integrate new knowledge, yielding adaptive
and context-aware retrieval. Agentic Plan Caching [58] instead focuses on cost efficiency by extracting
reusable plan templates from agent trajectories and caching them for fast execution at test time.


Together, these works demonstrate the value of external memory for improving adaptability, efficiency, and
generalization in LLM agents. Our work differs by tackling the broader challenge of _context adaptation_, which
spans not only agent memory but also system prompts, factual evidence, and other inputs underpinning AI
systems. We further highlight two fundamental limitations of existing adaptation methods— _brevity bias_ and
_context collapse_ —and show that addressing them is essential for robustness, reliability, and scalability beyond
raw task performance. Accordingly, our evaluation considers not only accuracy but also cost, latency, and
scalability.


**B** **Limitations and Challenges**

