<!-- Source: 04-GCC-2508.00031.pdf | Chunk 2/2 -->

relevant locations in the reference patch.


Table 1: Results on SWEBench-Lite. The ‘–’ symbol denotes missing / unreleased information
needed to compute the corresponding value. Claude 3.5 S denotes Claude 3.5 Sonnet.

|Tool LLM|Avg. Avg. % Correct Location<br>% Resolved<br>$ Cost # Tokens Line Function File|
|---|---|
|CodeStory Aide cod (2024)<br>GPT-4o+ Claude 3.5 S<br>Bytedance MarsCode Liu et al. (2024)<br>NA<br>Honeycomb hon (2024)<br>NA<br>MentatBot men (2024)<br>GPT-4o<br>Gru gru (2024)<br>NA<br>Isoform iso (2024)<br>NA<br>SuperCoder2.0 sup (2024)<br>NA<br>Alibaba Lingma Agent lin (2024)<br>GPT-4o+ Claude 3.5 S<br>Factory Code Droid fac (2024)<br>NA<br>Amazon Q Developer-v2 ama (2024)<br>NA<br>SpecRover Ruan et al. (2024)<br>GPT-4o+ Claude 3.5 S<br>CodeR Chen et al. (2024)<br>GPT-4<br>MASAI Arora et al. (2024)<br>NA<br>SIMA sim (2024)<br>GPT-4o<br>IBM Research Agent-101 ibm (2024)<br>NA<br>OpenCSG StarShip ope (2024a)<br>GPT-4<br>Amazon Q Developer ama (2024)<br>NA<br>RepoUnderstander Ma et al. (2024)<br>GPT-4<br>AutoCodeRover-v2 aut (2024)<br>GPT-4o<br>RepoGraph rep (2024)<br>GPT-4o<br>Moatless moa (2024)<br>Claude 3.5 S<br>GPT-4o<br>OpenDevin+CodeAct v1.8 ope (2024b)<br>Claude 3.5 S<br>Aider Gauthier (2024)<br>GPT-4o+ Claude 3.5 S<br>SWE-agent Yang et al. (2024)<br>Claude 3.5 S<br>GPT-4o<br>GPT-4<br>AppMap Navie app (2024)<br>GPT-4o<br>AutoCodeRover Zhang et al. (2024)<br>GPT-4<br>RAG Yang et al. (2024)<br>Claude 3 Opus<br>GPT-4<br>Claude-2<br>GPT-3.5<br>AgentLess Xia et al. (2024)<br>GPT-4o|129 (43.00%)<br>-<br>-<br>41.7%<br>58.7%<br>72.0%<br>118 (39.33%)<br>-<br>-<br>42.7%<br>58.0%<br>79.7%<br>115 (38.33%)<br>-<br>-<br>44.3%<br>57.0%<br>69.3%<br>114 (38.00%)<br>-<br>-<br>37.3%<br>53.3%<br>69.3%<br>107 (35.67%)<br>-<br>-<br>38.3%<br>54.3%<br>75.0%<br>105 (35.00%)<br>-<br>41,963<br>38.7%<br>55.3%<br>72.0%<br>102 (34.00%)<br>-<br>-<br>41.7%<br>63.7%<br>65.7%<br>99 (33.00%)<br>-<br>-<br>40.0%<br>58.7%<br>75.0%<br>94 (31.33%)<br>-<br>-<br>36.7%<br>55.7%<br>72.7%<br>89 (29.67%)<br>-<br>-<br>40.3%<br>52.0%<br>74.3%<br>93 (31.00%)<br>$0.65<br>-<br>-<br>-<br>-<br>85 (28.33%)<br>$3.34<br>323,802<br>35.7%<br>52.3%<br>67.0%<br>84 (28.00%)<br>-<br>-<br>38.7%<br>56.3%<br>75.0%<br>83 (27.67%)<br>$0.82<br>-<br>37.0%<br>54.0%<br>79.0%<br>80 (26.67%)<br>-<br>-<br>39.7%<br>56.7%<br>73.3%<br>71 (23.67%)<br>-<br>-<br>39.0%<br>61.7%<br>90.7%<br>61 (20.33%)<br>-<br>-<br>34.0%<br>43.7%<br>71.7%<br>64 (21.33%)<br>-<br>-<br>-<br>-<br>-<br>92 (30.67%)<br>-<br>-<br>35.0%<br>52.3%<br>69.3%<br>89 (29.67%)<br>-<br>-<br>36.7%<br>51.3%<br>71.0%<br>80 (26.67%)<br>$0.17<br>-<br>38.7%<br>54.7%<br>78.7%<br>74 (24.67%)<br>$0.14<br>-<br>36.0%<br>52.0%<br>73.0%<br>80 (26.67%)<br>$1.14<br>-<br>38.0%<br>49.7%<br>67.3%<br>79 (26.33%)<br>-<br>-<br>35.3%<br>50.0%<br>69.7%<br>69 (23.00%)<br>$1.62<br>521,208<br>40.7%<br>54.3%<br>72.0%<br>55 (18.33%)<br>$2.53<br>498,346<br>29.3%<br>42.3%<br>58.3%<br>54 (18.00%)<br>$2.51<br>245,008<br>30.7%<br>45.3%<br>61.0%<br>65 (21.67%)<br>-<br>-<br>29.7%<br>44.7%<br>59.7%<br>57 (19.00%)<br>$0.45<br>38,663<br>29.0%<br>42.3%<br>62.3%<br>13 (4.33%)<br>$0.25<br>-<br>22.0%<br>30.0%<br>57.0%<br>8 (2.67%)<br>$0.13<br>-<br>12.7%<br>23.3%<br>47.3%<br>9 (3.00%)<br>-<br>-<br>16.7%<br>24.3%<br>46.7%<br>1 (0.33%)<br>-<br>-<br>6.3%<br>11.3%<br>27.3%<br>96 (32.00%)<br>$0.70<br>78,166<br>35.3%<br>52.0%<br>69.7%|
|Ours<br>Claude 3.5 S|144 (48.00%)<br>$2.77<br>569,468<br>44.3%<br>61.7%<br>78.7%|



3.1 RESULTS AND ANALYSIS


We report results on the SWE-Benchlite benchmark in Table 1, comparing GCC against a diverse
set of 26 agentic coding systems. GCC achieves the highest resolution rate on SWE-Benchlite
at **48.00%**, outperforming all previously reported systems, including both open and closed-source
baselines. The next-best method, CodeStory Aide (43.00%), relies on a hybrid of GPT-4o and Claude
3.5 Sonnet, but does not release intermediate reasoning traces. Other high-performing closed-source
agents such as ByteDance MarsCode (39.33%) and Honeycomb (38.33%) also fall short.


In terms of localization accuracy, GCC reaches **44.3% line-level**, **61.7% function-level**, and **78.7%**
**file-level** correctness—consistently ranking among the top performers. For example, it outperforms
the function-level accuracy of Lingma (58.7%) and matches or surpasses the line/file-level precision
of Honeycomb, Sima, and ByteDance MarsCode. These results indicate that GCC not only solves
more tasks, but also more accurately targets the correct edit regions. The retrieval-only RAG baselines
remain far behind, resolving fewer than 5% of tasks and achieving poor localization performance.
AgentLess improves upon this (32.00% resolved) but still lags behind agent-based approaches. These
findings further underscore the advantage of GCC for complex software reasoning tasks.


6


3.2 CASE STUDY OF SELF-EVOLVING AGENTS: REPRODUCE CLI BY CLI


In this case study, we examine a self-replicating experiment in which a Claude Code CLI, equipped
with GCC, is tasked with implementing a new CLI system from scratch.


We examine three configurations on the SWEBench benchmark: (1) the original CLI system,
(2) a second-generation CLI reproduced by the original CLI agent without GCC, and (3) a
second-generation CLI reproduced by a GCC-augmented agent. All systems are evaluated on the
same task distribution under consistent execution constraints.


As shown in Table 2, the original CLI resolves 72.7% of SWEBench tasks. However, when the same
agent is prompted to reproduce this CLI from scratch without GCC, the resulting system performs
drastically worse, resolving only 11.7% of tasks. In contrast, when equipped with GCC—and
without access to the original CLI’s design or source code, the agent’s reproduction achieves a 40.7%
resolution rate on SWEBench. This performance gain emerges despite both systems using the same
language model and tool API, which demonstrates the difference lies not in capability, but in how
that capability is scaffolded.


Table 2: SWEBench Task Resolution Rates Across CLI Variants


**Setting** **Resolved (%)**


CLI 72.7


CLI-Produced CLI w/o GCC 11.7
**CLI-Produced CLI w/ GCC** **40.7**


This result underscores the potential of self-evolving agents that capable of reproducing itself from
scratch, and incrementally improving performance through iterative design. Such behavior hints at a
developmental trajectory toward systems that can autonomously bootstrap increasingly sophisticated
capabilities, an early glimpse into the mechanics of emergent superintelligence.


The remainder of this section analyzes two illustrative behaviors of agents in producing CLI: a
commit-based modularization and branch-driven memory exploration, that surfaced spontaneously
through the agent’s interaction with the GCC protocol.


3.2.1 COMMIT BEHAVIOR: IMPLEMENTING W R I T E F I L E


We observe that the GCC-powered CLI system spontaneously committed to implementing a
write ~~f~~ ile function during the construction of a child CLI. Before arriving at this implementation,
the agent began reasoning about the limitations of transient output. In its early, non-GCC setup,
commands returned results directly to the terminal via stdout, a pattern sufficient for simple,
one-shot tasks but fragile for workflows requiring state persistence or multi-step composition. Once
embedded in the structured memory environment of GCC, the agent began reflecting on richer
scenarios that transforming a file and retrieving the result later and chaining outputs as inputs into
downstream commands. Without any explicit prompting, its reasoning shifted from solving isolated
tasks to considering the infrastructure needed for long-horizon reusable workflows. Given access
to an organized context of prior experience, the agent inferred that a persistent file I/O mechanism
would be essential, and proposed introducing a reusable utility function—write ~~f~~ ile(path,
content) to generalize file output operations across the CLI.


In its committing, the agent implemented the utility in a standalone module, io.py. It also generated
a minimal test routine that wrote content to a temporary file, read it back, and confirmed correctness.
Only after the test passed did the agent determine that the implementation was stable enough to
warrant a versioned checkpoint. The commit.md was structurally recorded the contribution as:


## This Commit Contribution
Defines
_�→_ a reusable file output abstraction ``` write_file(path, content) ``` in ``` io.py
_�→_ ``` . Validated with a test to ensure correctness and future extensibility.


7


What is notable about this sequence is that it was not procedurally enforced. No rigid rule
instructed the agent to write a test, validate correctness, and then commit. These behaviors emerged
spontaneously from the framing structure of GCC itself. The system offered affordances: explicit
commit boundaries and modular memory locations, and the agent internalized these affordances
as cognitive norms. It came to understand that a commit contribution as a conceptually complete and
stable architectural addition. In effect, the agent behaved as if it were operating within a disciplined
software engineering workflow without having been explicitly told to do so.


3.2.2 BRANCHING BEHAVIOR: EXPLORING THE DESIGN OF HISTORY PROCESSOR


We observed that the CLI system, once equipped with GCC, began spontaneously reasoning about
how historical context could evolve beyond basic textual logging. In its earlier, non-GCC form, the
agent had been compressing prior interactions into a single memory.md file, a structure sufficient
for tracking high-level progress, but lossy in detail and inadequate for fine-grained access. With
the affordances provided by GCC, particularly the ability to branch architectural experiments, the
agent began to reflect on what kind of memory processor could better support long-horizon reasoning
and detailed recall. This reflection emerged organically, not from failure or instruction, but from
the structural flexibility GCC introduced.


Rather than modifying the compressed memory system in-place, we find the agent invoked the
BRANCH command to explore an alternate hypothesis that memory could be reimagined as a
retriever-augmented knowledge base. In this new branch, it began prototyping a RAG-style memory
layer, logging each OTA step as an atomic, vector-indexed unit to enable semantic querying and
trace-level introspection. The new branch, titled RAG-memory, was initialized with a minimal
commit.md defining its architectural intent:


Branch Purpose


Prototype a retriever-augmented memory system that indexes fine-grained
_�→_ OTA records to support semantic retrieval and long-horizon reasoning.


In this branch, the agent began prototyping a RAG-style memory module. Rather than compressing
context into a single summary file, it logged each OTA block as an individual JSON entry. These
records were embedded using a vector encoder and indexed to support semantic retrieval via a
dedicated retrieve ~~e~~ vents(query) function. To assess the effectiveness of this design, the
agent integrated the RAG-based memory into its full reasoning loop and tested it on the SWE-Bench
benchmark, aiming to determine whether fine-grained historical retrieval could enhance performance
on complex, multi-step software engineering tasks.


After a round of experimentation, the agent itself concluded that the RAG-based approach introduced
more drawbacks than benefits. It proved fragile—minor bugs in serialization or indexing logic led to
unpredictable behaviors. It was computationally expensive, requiring non-trivial overhead to embed
and store every intermediate step. It also underperformed in this implementation, the retriever-based
memory yielded slower resolution times and lower task success rates than the simpler, compressed
summary-based memory in the main branch. The agent documented these trade-offs and ultimately
chose to abandon the RAG-based approach, reverting to the mainline memory system. Although
the alternative was not retained in the final design, the process revealed an agent capable of exploring
divergent ideas, testing them in isolation, and converging on a working solution only when sufficient
empirical evidence supported it.


Such reflective behavior did not emerge in non-GCC environments, where agents tended to solve
tasks in one shot, operating within narrow, ephemeral contexts. In contrast, GCC provided a
structured space where architectural hypotheses could be explored safely—via branches—and
evaluated on their own merits.


What makes this behavior remarkable is its spontaneity. The branching was not prompted. The
comparison was not hard-coded. The decision to abandon the RAG memory system was not
pre-scripted. These steps emerged organically from the agent’s interaction with the affordances of
the GCC. The agent behaved like a modular system architect—reasoning over its own workflows
and encoding that reasoning structurally into the project itself.


8


CONCLUSION


In this work, we present Git-Context-Controller (GCC), a structured context management framework
that equips LLM-based agents with version control-inspired operations to persist, organize, and
retrieve memory across long-horizon workflows. By treating agent reasoning as a modular, evolving
codebase, GCC enables disciplined behaviors such as milestone-based committing, architectural
branching, and structured reflection. Empirical results on SWE-Bench-Lite show that GCC-equipped
agents achieve state-of-the-art performance, and a self-replication case study demonstrates the
emergence of recursive improvement. These results suggest that memory scaffolding not just model
capability but a key to building autonomous, self-evolving agents.


REFERENCES


Amazon q developer the most capable generative ai–powered assistant for software development.
[https://aws.amazon.com/q/developer//, 2024.](https://aws.amazon.com/q/developer//)


[Appmap speedruns to the top of the swe bench leaderboard. https://appmap.io/blog/](https://appmap.io/blog/2024/06/20/appmap-navie-swe-bench-leader/)
[2024/06/20/appmap-navie-swe-bench-leader/, 2024.](https://appmap.io/blog/2024/06/20/appmap-navie-swe-bench-leader/)


[Autocoderover autonomous software engineering. https://autocoderover.dev/, 2024.](https://autocoderover.dev/)


Aide by codestory. [https://github.com/swe-bench/experiments/tree/main/](https://github.com/swe-bench/experiments/tree/main/evaluation/lite/20240702_codestory_aide_mixed)
[evaluation/lite/20240702_codestory_aide_mixed, 2024.](https://github.com/swe-bench/experiments/tree/main/evaluation/lite/20240702_codestory_aide_mixed)


[Factory bringing autonomy to software engineering. https://www.factory.ai/, 2024.](https://www.factory.ai/)


The road to ultimate pull request machine. [https://gru.ai/blog/](https://gru.ai/blog/road-to-ultimate-pull-request-machine/)
[road-to-ultimate-pull-request-machine/, 2024.](https://gru.ai/blog/road-to-ultimate-pull-request-machine/)


[Honeycomb. https://honeycomb.sh, 2024.](https://honeycomb.sh)


Agent-101: A software engineering agent for code assistance developed by ibm research.
[https://github.com/swe-bench/experiments/blob/main/evaluation/](https://github.com/swe-bench/experiments/blob/main/evaluation/lite/20240612_IBM_Research_Agent101/README.md/)
[lite/20240612_IBM_Research_Agent101/README.md/, 2024.](https://github.com/swe-bench/experiments/blob/main/evaluation/lite/20240612_IBM_Research_Agent101/README.md/)


Isoform. [https://github.com/swe-bench/experiments/tree/main/](https://github.com/swe-bench/experiments/tree/main/evaluation/lite/20240829_Isoform)
[evaluation/lite/20240829_Isoform, 2024.](https://github.com/swe-bench/experiments/tree/main/evaluation/lite/20240829_Isoform)


Lingma agent. [https://github.com/swe-bench/experiments/tree/main/](https://github.com/swe-bench/experiments/tree/main/evaluation/lite/20240622_Lingma_Agent)
[evaluation/lite/20240622_Lingma_Agent, 2024.](https://github.com/swe-bench/experiments/tree/main/evaluation/lite/20240622_Lingma_Agent)


Mentatbot: New sota coding agent, available now. [https://mentat.ai/blog/](https://mentat.ai/blog/mentatbot-sota-coding-agent)
[mentatbot-sota-coding-agent, 2024.](https://mentat.ai/blog/mentatbot-sota-coding-agent)


[Moatless tools. https://github.com/aorwall/moatless-tools, 2024.](https://github.com/aorwall/moatless-tools)


[Opencsg starship. https://opencsg.com/product?class=StarShip/, 2024a.](https://opencsg.com/product?class=StarShip/)


Opendevin: Code less, make more. [https://github.com/OpenDevin/OpenDevin/,](https://github.com/OpenDevin/OpenDevin/)
2024b.


Repograph: Enhancing ai software engineering with repository-level code graph.
[https://github.com/ozyyshr/RepoGraph, 2024.](https://github.com/ozyyshr/RepoGraph)


Alex sima. [https://github.com/swe-bench/experiments/tree/main/](https://github.com/swe-bench/experiments/tree/main/evaluation/lite/20240706_sima_gpt4o)
[evaluation/lite/20240706_sima_gpt4o, 2024.](https://github.com/swe-bench/experiments/tree/main/evaluation/lite/20240706_sima_gpt4o)


[Supercoder. https://superagi.com/supercoder/, 2024.](https://superagi.com/supercoder/)


[Swe-bench lite. https://www.swebench.com/lite.html, 2024.](https://www.swebench.com/lite.html)


Daman Arora, Atharv Sonwane, Nalin Wadhwa, Abhav Mehrotra, Saiteja Utpala, Ramakrishna Bairi,
Aditya Kanade, and Nagarajan Natarajan. Masai: Modular architecture for software-engineering
ai agents. _arXiv preprint arXiv:2406.11638_, 2024.


9


Dong Chen, Shaoxin Lin, Muhan Zeng, Daoguang Zan, Jian-Gang Wang, Anton Cheshkov, Jun
Sun, Hao Yu, Guoliang Dong, Artem Aliev, et al. Coder: Issue resolving with multi-agent and
task graphs. _arXiv preprint arXiv:2406.01304_, 2024.


[Paul Gauthier. Aider is ai pair programming in your terminal. https://aider.chat/, 2024.](https://aider.chat/)


Carlos E Jimenez, John Yang, Alexander Wettig, Shunyu Yao, Kexin Pei, Ofir Press, and
Karthik R Narasimhan. SWE-bench: Can language models resolve real-world github issues? In _The Twelfth International Conference on Learning Representations_, 2024. URL
[https://openreview.net/forum?id=VTF8yNQM66.](https://openreview.net/forum?id=VTF8yNQM66)


Yizhou Liu, Pengfei Gao, Xinchen Wang, Chao Peng, and Zhao Zhang. Marscode agent: Ai-native
automated bug fixing. _arXiv preprint arXiv:2409.00899_, 2024.


Yingwei Ma, Qingping Yang, Rongyu Cao, Binhua Li, Fei Huang, and Yongbin Li. How to
understand whole software repository? _arXiv preprint arXiv:2406.01422_, 2024.


Haifeng Ruan, Yuntong Zhang, and Abhik Roychoudhury. Specrover: Code intent extraction via
llms. _arXiv preprint arXiv:2408.02232_, 2024.


Junde Wu, Jiayuan Zhu, Yuyuan Liu, Min Xu, and Yueming Jin. Agentic reasoning: A streamlined
framework for enhancing llm reasoning with agentic tools. In _Proceedings of the 63rd_
_Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)_, pp.
28489–28503, 2025.


Chunqiu Steven Xia, Yinlin Deng, Soren Dunn, and Lingming Zhang. Agentless: Demystifying
llm-based software engineering agents. _arXiv preprint arXiv:2407.01489_, 2024.


John Yang, Carlos E Jimenez, Alexander Wettig, Kilian Lieret, Shunyu Yao, Karthik Narasimhan,
and Ofir Press. Swe-agent: Agent-computer interfaces enable automated software engineering.
_arXiv preprint arXiv:2405.15793_, 2024.


Yuntong Zhang, Haifeng Ruan, Zhiyu Fan, and Abhik Roychoudhury. Autocoderover: Autonomous
program improvement, 2024.


A APPENDIX


10


