<!-- Source: 02-ContextSurvey-2507.13334.pdf | Chunk 18/26 -->

Enabling language representation with knowledge graph. _AAAI Conference on Artificial Intelligence_,
2019.


[697] Weiwen Liu, Xu Huang, Xingshan Zeng, Xinlong Hao, Shuai Yu, Dexun Li, Shuai Wang, Weinan Gan,
Zhengying Liu, Yuanqing Yu, Zezhong Wang, Yuxian Wang, Wu Ning, Yutai Hou, Bin Wang, Chuhan
Wu, Xinzhi Wang, Yong Liu, Yasheng Wang, Duyu Tang, Dandan Tu, Lifeng Shang, Xin Jiang, Ruiming
Tang, Defu Lian, Qun Liu, and Enhong Chen. Toolace: Winning the points of llm function calling,
[arXiv preprint arXiv:2409.00920, 2024. URL https://arxiv.org/abs/2409.00920v1.](https://arxiv.org/abs/2409.00920v1)


[698] Weiwen Liu, Jiarui Qin, Xu Huang, Xingshan Zeng, Yunjia Xi, Jianghao Lin, Chuhan Wu, Yasheng
Wang, Lifeng Shang, Ruiming Tang, Defu Lian, Yong Yu, and Weinan Zhang. The real barrier to llm
[agent usability is agentic roi, arXiv preprint arXiv:2505.17767, 2025. URL https://arxiv.org/](https://arxiv.org/abs/2505.17767v1)
[abs/2505.17767v1.](https://arxiv.org/abs/2505.17767v1)


[699] Wentao Liu, Hanglei Hu, Jie Zhou, Yuyang Ding, Junsong Li, Jiayi Zeng, Mengliang He, Qin Chen,
Bo Jiang, Aimin Zhou, and Liang He. Mathematical language models: A survey, arXiv preprint
[arXiv:2312.07622, 2023. URL https://arxiv.org/abs/2312.07622v4.](https://arxiv.org/abs/2312.07622v4)


[700] Wentao Liu, Ruohua Zhang, Aimin Zhou, Feng Gao, and JiaLi Liu. Echo: A large language model
[with temporal episodic memory, arXiv preprint arXiv:2502.16090, 2025. URL https://arxiv.](https://arxiv.org/abs/2502.16090v1)
[org/abs/2502.16090v1.](https://arxiv.org/abs/2502.16090v1)


[701] Xu Liu, S. Ramirez, Petti T. Pang, C. Puryear, A. Govindarajan, K. Deisseroth, and S. Tonegawa.
Optogenetic stimulation of a hippocampal engram activates fear memory recall. _Nature_, 2012.


[702] Yang Liu, Xiaobin Tian, Zequn Sun, and Wei Hu. Finetuning generative large language models
with discrimination instructions for knowledge graph completion. In _International Semantic Web_
_Conference_, 2024.


[703] Yue Liu, Jiaying Wu, Yufei He, Hongcheng Gao, Hongyu Chen, Baolong Bi, Jiaheng Zhang, Zhiqi
Huang, and Bryan Hooi. Efficient inference for large reasoning models: A survey, arXiv preprint
[arXiv:2503.23077, 2025. URL https://arxiv.org/abs/2503.23077v2.](https://arxiv.org/abs/2503.23077v2)


111


[704] Zijun Liu, Yanzhe Zhang, Peng Li, Yang Liu, and Diyi Yang. A dynamic llm-powered agent network for
[task-oriented agent collaboration, arXiv preprint arXiv:2310.02170, 2023. URL https://arxiv.](https://arxiv.org/abs/2310.02170v2)
[org/abs/2310.02170v2.](https://arxiv.org/abs/2310.02170v2)


[705] Zijun Liu, Zhennan Wan, Peng Li, Ming Yan, Ji Zhang, Fei Huang, and Yang Liu. Scaling external
knowledge input beyond context windows of llms via multi-agent collaboration, arXiv preprint
[arXiv:2505.21471, 2025. URL https://arxiv.org/abs/2505.21471v1.](https://arxiv.org/abs/2505.21471v1)


[706] Zinan Liu, Haoran Li, Jingyi Lu, Gaoyuan Ma, Xu Hong, Giovanni Iacca, Arvind Kumar, Shaojun
Tang, and Lin Wang. Nature’s insight: A novel framework and comprehensive analysis of agentic
reasoning through the lens of neuroscience. arXiv preprint, 2025.


[707] Zuxin Liu, Thai Hoang, Jianguo Zhang, Ming Zhu, Tian Lan, Shirley Kokane, Juntao Tan, Weiran
Yao, Zhiwei Liu, Yihao Feng, Rithesh Murthy, Liangwei Yang, Silvio Savarese, Juan Carlos Niebles,
Huan Wang, Shelby Heinecke, and Caiming Xiong. Apigen: Automated pipeline for generating
verifiable and diverse function-calling datasets. _Neural Information Processing Systems_, 2024.


[708] Leo S. Lo. The art and science of prompt engineering: A new literacy in the information age. _Internet_
_Reference Services Quarterly_, 2023.


[709] Joseph R. Loffredo and Suyeol Yun. Agent-enhanced large language models for researching political
institutions, arXiv preprint arXiv:2503.13524, 2025. [URL https://arxiv.org/abs/2503.](https://arxiv.org/abs/2503.13524v1)
[13524v1.](https://arxiv.org/abs/2503.13524v1)


[710] Jianqiao Lu, Wanjun Zhong, Wenyong Huang, Yufei Wang, Fei Mi, Baojun Wang, Weichao
Wang, Lifeng Shang, and Qun Liu. Self: Self-evolution with language feedback, arXiv preprint
[arXiv:2310.00533, 2023. URL https://arxiv.org/abs/2310.00533v4.](https://arxiv.org/abs/2310.00533v4)


[711] Junru Lu, Siyu An, Mingbao Lin, Gabriele Pergola, Yulan He, Di Yin, Xing Sun, and Yunsheng Wu.
Memochat: Tuning llms to use memos for consistent long-range open-domain conversation, arXiv
[preprint arXiv:2308.08239, 2023. URL https://arxiv.org/abs/2308.08239.](https://arxiv.org/abs/2308.08239)


[712] Junting Lu, Zhiyang Zhang, Fangkai Yang, Jue Zhang, Lu Wang, Chao Du, Qingwei Lin, Saravan
Rajmohan, Dongmei Zhang, and Qi Zhang. Axis: Efficient human-agent-computer interaction with
[api-first llm-based agents, arXiv preprint arXiv:2409.17140, 2025. URL https://arxiv.org/](https://arxiv.org/abs/2409.17140)
[abs/2409.17140.](https://arxiv.org/abs/2409.17140)


[713] Keer Lu, Xiaonan Nie, Zheng Liang, Da Pan, Shusen Zhang, Keshi Zhao, Weipeng Chen, Zenan
Zhou, Guosheng Dong, Bin Cui, and Wentao Zhang. Datasculpt: Crafting data landscapes for
long-context llms through multi-objective partitioning, arXiv preprint arXiv:2409.00997, 2024. URL
[https://arxiv.org/abs/2409.00997v2.](https://arxiv.org/abs/2409.00997v2)


[714] Liqiang Lu, Yicheng Jin, Hangrui Bi, Zizhang Luo, Peng Li, Tao Wang, and Yun Liang. Sanger: A
co-design framework for enabling sparse attention using reconfigurable architecture. _Micro_, 2021.


[715] Pan Lu, Baolin Peng, Hao Cheng, Michel Galley, Kai-Wei Chang, Y. Wu, Song-Chun Zhu, and Jianfeng
Gao. Chameleon: Plug-and-play compositional reasoning with large language models. _Neural_
_Information Processing Systems_, 2023.


[716] Y Lu, H Yu, and D Khashabi. Gear: Augmenting language models with generalizable and efficient
[tool resolution. 2023. URL https://arxiv.org/abs/2307.08775.](https://arxiv.org/abs/2307.08775)


112


[717] Yao Lu, Max Bartolo, Alastair Moore, Sebastian Riedel, and Pontus Stenetorp. Fantastically ordered
prompts and where to find them: Overcoming few-shot prompt order sensitivity. _Annual Meeting of_
_the Association for Computational Linguistics_, 2021.


[718] Yinquan Lu, H. Lu, Guirong Fu, and Qun Liu. Kelm: Knowledge enhanced pre-trained language representations with message passing on hierarchical relational graphs, arXiv preprint arXiv:2109.04223,
[2021. URL https://arxiv.org/abs/2109.04223v2.](https://arxiv.org/abs/2109.04223v2)


[719] Elias Lumer, Anmol Gulati, V. K. Subbiah, Pradeep Honaganahalli Basavaraju, and James A. Burke.
Scalemcp: Dynamic and auto-synchronizing model context protocol tools for llm agents, arXiv
[preprint arXiv:2505.06416, 2025. URL https://arxiv.org/abs/2505.06416v1.](https://arxiv.org/abs/2505.06416v1)


[720] Cheng Luo, Jiawei Zhao, Zhuoming Chen, Beidi Chen, and Anima Anandkumar. Mini-sequence
transformer: Optimizing intermediate memory for long sequences training, arXiv preprint
[arXiv:2407.15892, 2024. URL https://arxiv.org/abs/2407.15892v4.](https://arxiv.org/abs/2407.15892v4)


[721] Feng Luo, Yu-Neng Chuang, Guanchu Wang, Hoang Anh Duy Le, Shaochen Zhong, Hongyi Liu,
Jiayi Yuan, Yang Sui, Vladimir Braverman, Vipin Chaudhary, and Xia Hu. Autol2s: Auto longshort reasoning for efficient large language models, arXiv preprint arXiv:2505.22662, 2025. URL
[https://arxiv.org/abs/2505.22662v1.](https://arxiv.org/abs/2505.22662v1)


[722] Haitong Luo, Xuying Meng, Suhang Wang, Tianxiang Zhao, Fali Wang, Hanyun Cao, and Yujun
Zhang. Enhance graph alignment for large language models, arXiv preprint arXiv:2410.11370v1,
[2024. URL https://arxiv.org/abs/2410.11370v1.](https://arxiv.org/abs/2410.11370v1)


[723] Haoran Luo, E. Haihong, Guanting Chen, Yandan Zheng, Xiaobao Wu, Yikai Guo, Qika Lin, Yu Feng,
Zemin Kuang, Meina Song, Yifan Zhu, and Anh Tuan Luu. Hypergraphrag: Retrieval-augmented
generation with hypergraph-structured knowledge representation. arXiv preprint, 2025.


[724] Haotian Luo, Li Shen, Haiying He, Yibo Wang, Shiwei Liu, Wei Li, Naiqiang Tan, Xiaochun Cao, and
Dacheng Tao. O1-pruner: Length-harmonizing fine-tuning for o1-like reasoning pruning. arXiv
preprint, 2025.


[725] Junyu Luo, Weizhi Zhang, Ye Yuan, Yusheng Zhao, Junwei Yang, Yiyang Gu, Bohan Wu, Binqi
Chen, Ziyue Qiao, Qingqing Long, Rongcheng Tu, Xiaoming Luo, Wei Ju, Zhiping Xiao, Yifan Wang,
Mengxue Xiao, Chenwu Liu, Jingyang Yuan, Shichang Zhang, Yiqiao Jin, Fan Zhang, Xianhong
Wu, Hanqing Zhao, Dacheng Tao, Philip S. Yu, and Ming Zhang. Large language model agent: A
survey on methodology, applications and challenges, arXiv preprint arXiv:2503.21460, 2025. URL
[https://arxiv.org/abs/2503.21460v1.](https://arxiv.org/abs/2503.21460v1)


[726] Linhao Luo, Yuan-Fang Li, Gholamreza Haffari, and Shirui Pan. Reasoning on graphs: Faithful and
interpretable large language model reasoning. _International Conference on Learning Representations_,
2023.


[727] Renjie Luo, Jiaxi Li, Chen Huang, and Wei Lu. Through the valley: Path to effective long cot training
[for small language models, arXiv preprint arXiv:2506.07712, 2025. URL https://arxiv.org/](https://arxiv.org/abs/2506.07712v1)
[abs/2506.07712v1.](https://arxiv.org/abs/2506.07712v1)


[728] Xindi Luo, Zequn Sun, Jing Zhao, Zhe Zhao, and Wei Hu. Knowla: Enhancing parameter-efficient
finetuning with knowledgeable adaptation. In _Proceedings of the 2024 Conference of the North_
_American Chapter of the Association for Computational Linguistics_, 2024.


113


[729] Yifan Luo, Yiming Tang, Chengfeng Shen, Zhennan Zhou, and Bin Dong. Prompt engineering
through the lens of optimal control. _Journal of Machine Learning_, 2023.


[730] Panagiotis Lymperopoulos and Vasanth Sarathy. Tools in the loop: Quantifying uncertainty of llm
[question answering systems that use tools, arXiv preprint arXiv:2505.16113, 2025. URL https:](https://arxiv.org/abs/2505.16113v1)
[//arxiv.org/abs/2505.16113v1.](https://arxiv.org/abs/2505.16113v1)


[731] Yougang Lyu, Xiaoyu Zhang, Lingyong Yan, M. D. Rijke, Zhaochun Ren, and Xiuying Chen. Deepshop:
A benchmark for deep research shopping agents, arXiv preprint arXiv:2506.02839, 2025. URL
[https://arxiv.org/abs/2506.02839v1.](https://arxiv.org/abs/2506.02839v1)


[732] Jianxiang Ma. Research on the role of llm in multi-agent systems: A survey. _Applied and Computational_
_Engineering_, 2024.


[733] Jie Ma, Zhitao Gao, Qianyi Chai, Wangchun Sun, Pinghui Wang, Hongbin Pei, Jing Tao, Lingyun
Song, Jun Liu, Chen Zhang, and Li zhen Cui. Debate on graph: a flexible and reliable reasoning
[framework for large language models, arXiv preprint arXiv:2409.03155, 2024. URL https://](https://arxiv.org/abs/2409.03155v1)
[arxiv.org/abs/2409.03155v1.](https://arxiv.org/abs/2409.03155v1)


[734] Qun Ma, Xiao Xue, Deyu Zhou, Xiangning Yu, Donghua Liu, Xuwen Zhang, Zihan Zhao, Yifan
Shen, Peilin Ji, Juanjuan Li, Gang Wang, and Wanpeng Ma. Computational experiments meet large
language model based agents: A survey and perspective, arXiv preprint arXiv:2402.00262, 2024.
[URL https://arxiv.org/abs/2402.00262v1.](https://arxiv.org/abs/2402.00262v1)


[735] Xin Ma, Yang Liu, Jingjing Liu, and Xiaoxu Ma. Mesa-extrapolation: A weave position encoding
method for enhanced extrapolation in llms. _Neural Information Processing Systems_, 2024.


[736] Xinbei Ma, Yeyun Gong, Pengcheng He, Hai Zhao, and Nan Duan. Query rewriting for retrieval[augmented large language models, arXiv preprint arXiv:2305.14283, 2023. URL https://arxiv.](https://arxiv.org/abs/2305.14283v3)
[org/abs/2305.14283v3.](https://arxiv.org/abs/2305.14283v3)


[737] Xuezhe Ma, Xiaomeng Yang, Wenhan Xiong, Beidi Chen, Lili Yu, Hao Zhang, Jonathan May, Luke S.
Zettlemoyer, Omer Levy, and Chunting Zhou. Megalodon: Efficient llm pretraining and inference
with unlimited context length. _Neural Information Processing Systems_, 2024.


[738] Y Ma, Z Gou, J Hao, R Xu, S Wang, and L Pan.... Sciagent: Tool-augmented language models for
[scientific reasoning. 2024. URL https://arxiv.org/abs/2402.11451.](https://arxiv.org/abs/2402.11451)


[739] Zhiyuan Ma, Zhenya Huang, Jiayu Liu, Minmao Wang, Hongke Zhao, and Xin Li. Automated creation
of reusable and diverse toolsets for enhancing llm reasoning. _AAAI Conference on Artificial Intelligence_,
2025.


[740] Aman Madaan, Niket Tandon, Peter Clark, and Yiming Yang. Memory-assisted prompt editing to
improve gpt-3 after deployment. _Conference on Empirical Methods in Natural Language Processing_,
2022.


[741] Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon,
Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, S. Welleck, Bodhisattwa Prasad Majumder, Shashank
Gupta, A. Yazdanbakhsh, and Peter Clark. Self-refine: Iterative refinement with self-feedback. _Neural_
_Information Processing Systems_, 2023.


114


[742] Xinji Mai, Haotian Xu, W. Xing, Weinong Wang, Yingying Zhang, and Wenqiang Zhang. Agent rl
scaling law: Agent rl with spontaneous code execution for mathematical problem solving, arXiv
[preprint arXiv:2505.07773, 2025. URL https://arxiv.org/abs/2505.07773v2.](https://arxiv.org/abs/2505.07773v2)


[743] Amjad Yousef Majid, Serge Saaybi, Tomas van Rietbergen, Vincent François-Lavet, R. V. Prasad, and
Chris Verhoeven. Deep reinforcement learning versus evolution strategies: A comparative survey.
_IEEE Transactions on Neural Networks and Learning Systems_, 2021.


[744] D. Maldonado, Edison Cruz, Jackeline Abad Torres, P. Cruz, and Silvana del Pilar Gamboa Benitez.
Multi-agent systems: A survey about its components, framework and workflow. _IEEE Access_, 2024.


[745] Zhao Mandi, Shreeya Jain, and Shuran Song. Roco: Dialectic multi-robot collaboration with large
language models. _IEEE International Conference on Robotics and Automation_, 2023.


[746] Jeremy R. Manning, Sean M. Polyn, G. Baltuch, B. Litt, and M. Kahana. Oscillatory patterns in
temporal lobe reveal context reinstatement during memory search. _Proceedings of the National_
_Academy of Sciences of the United States of America_, 2011.


[747] Qiheng Mao, Zemin Liu, Chenghao Liu, Zhuo Li, and Jianling Sun. Advancing graph representation learning with large language models: A comprehensive survey of techniques, arXiv preprint
[arXiv:2402.05952v1, 2024. URL https://arxiv.org/abs/2402.05952v1.](https://arxiv.org/abs/2402.05952v1)


[748] Amin Hosseiny Marani, Ulie Schnaithmann, Youngseo Son, Akil Iyer, Manas Paldhe, and Arushi
Raghuvanshi. Graph integrated language transformers for next action prediction in complex phone
calls. _North American Chapter of the Association for Computational Linguistics_, 2024.


[[749] Sophia Maria. Compass-v2 technical report, arXiv preprint arXiv:2504.15527, 2025. URL https:](https://arxiv.org/abs/2504.15527v1)
[//arxiv.org/abs/2504.15527v1.](https://arxiv.org/abs/2504.15527v1)


[750] Viorica Marian and U. Neisser. Language-dependent recall of autobiographical memories. _Journal of_
_experimental psychology. General_, 2000.


[751] S. Mariani and Andrea Omicini. Special issue “multi-agent systems”: Editorial. _Applied Sciences_,
2019.


[752] Vasilije Markovic, Lazar Obradović, L’aszl’o Hajdu, and Jovan Pavlović. Optimizing the interface
between knowledge graphs and llms for complex reasoning, arXiv preprint arXiv:2505.24478, 2025.
[URL https://arxiv.org/abs/2505.24478v1.](https://arxiv.org/abs/2505.24478v1)


[753] Sami Marreed, Alon Oved, Avi Yaeli, Segev Shlomov, Ido Levy, Offer Akrabi, Aviad Sela, Asaf Adi,
and Nir Mashkif. Towards enterprise-ready computer using generalist agent. 2025.


[754] Tula Masterman, Sandi Besen, Mason Sawtell, and Alex Chao. The landscape of emerging ai agent
architectures for reasoning, planning, and tool calling: A survey, arXiv preprint arXiv:2404.11584,
[2024. URL https://arxiv.org/abs/2404.11584v1.](https://arxiv.org/abs/2404.11584v1)


[755] Nicholas Matsumoto, Jay Moran, Hyunjun Choi, Miguel E. Hernandez, Mythreye Venkatesan, Paul
Wang, and Jason H. Moore. Kragen: a knowledge graph-enhanced rag framework for biomedical
problem solving using large language models. _Bioinformatics_, 2024.


[756] Ryoga Matsuo, Stefan Uhlich, Arun Venkitaraman, Andrea Bonetti, Chia-Yu Hsieh, Ali Momeni,
Lukas Mauch, Augusto Capone, Eisaku Ohbuchi, and Lorenzo Servadei. Schemato - an llm for
netlist-to-schematic conversion. arXiv preprint, 2024.


115


[757] Costas Mavromatis, V. Ioannidis, Shen Wang, Da Zheng, Soji Adeshina, Jun Ma, Han Zhao, C. Faloutsos, and G. Karypis. Train your own gnn teacher: Graph-aware distillation on textual graphs.
_ECML/PKDD_, 2023.


[758] James L. McClelland, B. McNaughton, and R. O’Reilly. Why there are complementary learning
systems in the hippocampus and neocortex: insights from the successes and failures of connectionist
models of learning and memory. _Psychology Review_, 1995.


[759] R. Thomas McCoy, Ellie Pavlick, and Tal Linzen. Right for the wrong reasons: Diagnosing syntactic
heuristics in natural language inference. _Annual Meeting of the Association for Computational_
_Linguistics_, 2019.


[760] Daniel McDuff, M. Schaekermann, Tao Tu, Anil Palepu, Amy Wang, Jake Garrison, Karan Singhal,
Yash Sharma, Shekoofeh Azizi, Kavita Kulkarni, Le Hou, Yong Cheng, Yun Liu, S. Mahdavi, Sushant
Prakash, Anupam Pathak, Christopher Semturs, Shwetak N. Patel, D. Webster, Ewa Dominowska,
Juraj Gottweis, Joelle Barral, Katherine Chou, G. Corrado, Yossi Matias, Jacob Sunshine, A. Karthikesalingam, and Vivek Natarajan. Towards accurate differential diagnosis with large language models.
_Nature_, 2023.


[761] AD McNaughton, G Ramalaxmi, A Kruel, and CR Knutson.... Cactus: Chemistry agent connecting
tool-usage to science, arxiv, 2024.


[762] Sushant Mehta, R. Dandekar, R. Dandekar, and S. Panat. Latent multi-head attention for small
[language models, arXiv preprint arXiv:2506.09342, 2025. URL https://arxiv.org/abs/2506.](https://arxiv.org/abs/2506.09342v2)
[09342v2.](https://arxiv.org/abs/2506.09342v2)


[763] Kai Mei, Zelong Li, Shuyuan Xu, Ruosong Ye, Yingqiang Ge, and Yongfeng Zhang. Aios: Llm agent
[operating system, arXiv preprint arXiv:2403.16971, 2024. URL https://arxiv.org/abs/2403.](https://arxiv.org/abs/2403.16971v4)
[16971v4.](https://arxiv.org/abs/2403.16971v4)


[764] Lingrui Mei, Shenghua Liu, Yiwei Wang, Baolong Bi, and Xueqi Cheng. Slang: New concept
comprehension of large language models. In _Proceedings of the 2024 Conference on Empirical Methods_
_in Natural Language Processing_, page 12558–12575. Association for Computational Linguistics,
[2024. doi: 10.18653/v1/2024.emnlp-main.698. URL http://dx.doi.org/10.18653/v1/](http://dx.doi.org/10.18653/v1/2024.emnlp-main.698)
[2024.emnlp-main.698.](http://dx.doi.org/10.18653/v1/2024.emnlp-main.698)


[765] Lingrui Mei, Shenghua Liu, Yiwei Wang, Baolong Bi, Ruibin Yuan, and Xueqi Cheng. Hiddenguard:
Fine-grained safe generation with specialized representation router, arXiv preprint arXiv:2410.02684,
[2024. URL https://arxiv.org/abs/2410.02684.](https://arxiv.org/abs/2410.02684)


[766] Lingrui Mei, Shenghua Liu, Yiwei Wang, Baolong Bi, Yuyao Ge, Jun Wan, Yurong Wu, and Xueqi
Cheng. a1: Steep test-time scaling law via environment augmented generation, arXiv preprint
[arXiv:2504.14597, 2025. URL https://arxiv.org/abs/2504.14597.](https://arxiv.org/abs/2504.14597)


[767] Lingrui Mei, Shenghua Liu, Yiwei Wang, Baolong Bi, Jiayi Mao, and Xueqi Cheng. "not aligned" is not
"malicious": Being careful about hallucinations of large language models’ jailbreak, arXiv preprint
[arXiv:2406.11668, 2025. URL https://arxiv.org/abs/2406.11668.](https://arxiv.org/abs/2406.11668)


[768] T. Meiser and A. Bröder. Memory for multidimensional source information. _Journal of Experimental_
_Psychology. Learning, Memory and Cognition_, 2002.


116


[769] Gábor Melis, Chris Dyer, and Phil Blunsom. On the state of the art of evaluation in neural language
models. _International Conference on Learning Representations_, 2017.


[770] Kevin Meng, David Bau, A. Andonian, and Yonatan Belinkov. Locating and editing factual associations
in gpt. _Neural Information Processing Systems_, 2022.


[771] Yuxian Meng, Shi Zong, Xiaoya Li, Xiaofei Sun, Tianwei Zhang, Fei Wu, and Jiwei Li. Gnn-lm:
Language modeling based on global contexts via gnn. _International Conference on Learning Represen-_
_tations_, 2021.


[772] Agnieszka Mensfelt, Kostas Stathis, and Vince Trencsenyi. Towards logically sound natural language
reasoning with logic-enhanced language model agents, arXiv preprint arXiv:2408.16081, 2024. URL
[https://arxiv.org/abs/2408.16081v2.](https://arxiv.org/abs/2408.16081v2)


[773] G. M. Mensink and J. Raaijmakers. A model for interference and forgetting. arXiv preprint, 1988.


[774] Thomas Merth, Qichen Fu, Mohammad Rastegari, and Mahyar Najibi. Superposition prompting:
Improving and accelerating retrieval-augmented generation. _International Conference on Machine_
_Learning_, 2024.


[775] B. Meskó. Prompt engineering as an important emerging skill for medical professionals: Tutorial.
_Journal of Medical Internet Research_, 2023.


[776] Yapeng Mi, Zhi Gao, Xiaojian Ma, and Qing Li. Building llm agents by incorporating insights from
[computer systems, arXiv preprint arXiv:2504.04485, 2025. URL https://arxiv.org/abs/](https://arxiv.org/abs/2504.04485v1)
[2504.04485v1.](https://arxiv.org/abs/2504.04485v1)


[777] G. Mialon, Roberto Dessì, M. Lomeli, Christoforos Nalmpantis, Ramakanth Pasunuru, R. Raileanu,
Baptiste Rozière, Timo Schick, Jane Dwivedi-Yu, Asli Celikyilmaz, Edouard Grave, Yann LeCun, and
Thomas Scialom. Augmented language models: a survey. _Trans. Mach. Learn. Res._, 2023.


[778] G. Mialon, Clémentine Fourrier, Craig Swift, Thomas Wolf, Yann LeCun, and Thomas Scialom.
[Gaia: a benchmark for general ai assistants, arXiv preprint arXiv:2311.12983, 2023. URL https:](https://arxiv.org/abs/2311.12983v1)
[//arxiv.org/abs/2311.12983v1.](https://arxiv.org/abs/2311.12983v1)


[779] Xupeng Miao, Gabriele Oliaro, Zhihao Zhang, Xinhao Cheng, Hongyi Jin, Tianqi Chen, and Zhihao
Jia. Towards efficient generative large language model serving: A survey from algorithms to systems,
[arXiv preprint arXiv:2312.15234, 2023. URL https://arxiv.org/abs/2312.15234v1.](https://arxiv.org/abs/2312.15234v1)


[780] Jacob Miller, Guillaume Rabusseau, and John Terilla. Tensor networks for language modeling. arXiv
preprint, 2020.


[781] Xing ming Guo, Darioush Keivan, U. Syed, Lianhui Qin, Huan Zhang, G. Dullerud, Peter J. Seiler,
and Bin Hu. Controlagent: Automating control system design via novel integration of llm agents
[and domain expertise, arXiv preprint arXiv:2410.19811, 2024. URL https://arxiv.org/abs/](https://arxiv.org/abs/2410.19811v1)
[2410.19811v1.](https://arxiv.org/abs/2410.19811v1)


[782] Soroush Mirjalili, Patrick S. Powell, Jonathan Strunk, Taylor A James, and Audrey Duarte. Context
memory encoding and retrieval temporal dynamics are modulated by attention across the adult
lifespan. _eNeuro_, 2021.


117


[783] Ishan Misra and L. Maaten. Self-supervised learning of pretext-invariant representations. _Computer_
_Vision and Pattern Recognition_, 2019.


[784] Ali Modarressi, Ayyoob Imani, Mohsen Fayyaz, and Hinrich Schütze. Ret-llm: Towards a general
[read-write memory for large language models, arXiv preprint arXiv:2305.14322, 2024. URL https:](https://arxiv.org/abs/2305.14322)
[//arxiv.org/abs/2305.14322.](https://arxiv.org/abs/2305.14322)


[785] Ali Modarressi, Abdullatif Köksal, Ayyoob Imani, Mohsen Fayyaz, and Hinrich Schutze. Memllm:
Finetuning llms to use an explicit read-write memory. _Trans. Mach. Learn. Res._, 2024.


[786] Behnam Mohammadi. Pel, a programming language for orchestrating ai agents, arXiv preprint
[arXiv:2505.13453, 2025. URL https://arxiv.org/abs/2505.13453v2.](https://arxiv.org/abs/2505.13453v2)


[787] Amirkeivan Mohtashami and Martin Jaggi. Landmark attention: Random-access infinite context
length for transformers. _Neural Information Processing Systems_, 2023.


[788] Fedor Moiseev, Zhe Dong, Enrique Alfonseca, and Martin Jaggi. Skill: Structured knowledge infusion
for large language models. _North American Chapter of the Association for Computational Linguistics_,
2022.


[789] Dimitri Coelho Mollo and Raphael Milliere. The vector grounding problem, arXiv preprint
[arXiv:2304.01481, 2023. URL https://arxiv.org/abs/2304.01481v2.](https://arxiv.org/abs/2304.01481v2)


[790] Nieves Montes, N. Osman, and C. Sierra. Combining theory of mind and abduction for cooperation
under imperfect information. _European Workshop on Multi-Agent Systems_, 2022.


[791] Suhong Moon, Siddharth Jha, Lutfi Eren Erdogan, Sehoon Kim, Woosang Lim, Kurt Keutzer, and
A. Gholami. Efficient and scalable estimation of tool representations in vector space, arXiv preprint
[arXiv:2409.02141, 2024. URL https://arxiv.org/abs/2409.02141v1.](https://arxiv.org/abs/2409.02141v1)


[792] Shinsuke Mori. A stochastic parser based on an slm with arboreal context trees. _International_
_Conference on Computational Linguistics_, 2002.


[793] Meredith Ringel Morris. Prompting considered harmful. _Communications of the ACM_, 2024.


[794] Marius Mosbach, Tiago Pimentel, Shauli Ravfogel, D. Klakow, and Yanai Elazar. Few-shot fine-tuning
vs. in-context learning: A fair comparison and evaluation. _Annual Meeting of the Association for_
_Computational Linguistics_, 2023.

