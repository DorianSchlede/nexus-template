<!-- Source: 02-ContextSurvey-2507.13334.pdf | Chunk 2/26 -->


_Pθ_ ( _Y|C_ ) =



_T_
∏︁ _Pθ_ ( _yt|y<t_, _C_ ) (1)

_t_ =1



Historically, in the paradigm of prompt engineering, the context _C_ was treated as a monolithic, static string
of text, i.e., _C_ = prompt. This view is insufficient for modern systems.


Context Engineering re-conceptualizes the context _C_ as a dynamically structured set of informational
components, _c_ 1, _c_ 2, . . ., _cn_ . These components are sourced, filtered, and formatted by a set of functions, and
finally orchestrated by a high-level assembly function, _A_ :


_C_ = _A_ ( _c_ 1, _c_ 2, . . ., _cn_ ) (2)


The components _ci_ are not arbitrary; they map directly to the core technical domains of this survey:


8


 - _c_ instr: System instructions and rules ( **Context Retrieval and Generation**, Sec. 4.1).

 - _c_ know: External knowledge, retrieved via functions like RAG or from integrated knowledge graphs
( **RAG**, Sec. 5.1; **Context Processing**, Sec. 4.2).

 - _c_ tools: Definitions and signatures of available external tools ( **Function Calling** & **Tool-Integrated**
**Reasoning**, Sec. 5.3).

 - _c_ mem: Persistent information from prior interactions ( **Memory Systems**, Sec. 5.2; **Context Manage-**
**ment**, Sec. 4.3).

 - _c_ state: The dynamic state of the user, world, or multi-agent system ( **Multi-Agent Systems** & **Orchestra-**
**tion**, Sec. 5.4).

 - _c_ query: The user’s immediate request.


**The Optimization Problem of Context Engineering.** From this perspective, Context Engineering is the
formal optimization problem of finding the ideal set of context-generating functions (which we denote
collectively as _F_ = _{A_, Retrieve, Select, . . . _}_ ) that maximizes the expected quality of the LLM’s output.
Given a distribution of tasks _T_, the objective is:

_F_ _[∗]_ = arg max **E** _τ∼T_ [Reward( _Pθ_ ( _Y|CF_ ( _τ_ )), _Yτ_ _[∗]_ [)]] (3)
_F_


where _τ_ is a specific task instance, _CF_ ( _τ_ ) is the context generated by the functions in _F_ for that task, and
_Yτ_ _[∗]_ [is the ground-truth or ideal output. This optimization is subject to hard constraints, most notably the]
model’s context length limit, _|C| ≤_ _L_ max.


**Mathematical Principles and Theoretical Frameworks.** This formalization reveals deeper mathematical
principles. The assembly function _A_ is a form of **Dynamic Context Orchestration**, a pipeline of formatting
and concatenation operations, _A_ = Concat _◦_ (Format1, . . ., Format _n_ ), where each function must be optimized
for the LLM’s architectural biases (e.g., attention patterns).


The retrieval of knowledge, _c_ know = Retrieve(. . . ), can be framed as an **Information-Theoretic Optimal-**
**ity** problem. The goal is to select knowledge that maximizes the mutual information with the target answer
_Y_ _[∗]_, given the query _c_ query:
Retrieve _[∗]_ = arg max (4)
Retrieve _[I]_ [(] _[Y][∗]_ [;] _[ c]_ [know] _[|][c]_ [query][)]

This ensures that the retrieved context is not just semantically similar, but maximally informative for solving
the task.


Furthermore, the entire process can be viewed through the lens of **Bayesian Context Inference** . Instead of
deterministically constructing the context, we infer the optimal context posterior _P_ ( _C|c_ query, History, World).
Using Bayes’ theorem, this posterior is proportional to the likelihood of the query given the context and the
prior probability of the context’s relevance:


_P_ ( _C|c_ query, . . . ) ∝ _P_ ( _c_ query _|C_ ) _· P_ ( _C|_ History, World) (5)


The decision-theoretic objective is then to find the context _C_ _[∗]_ that maximizes the expected reward over the
distribution of possible answers:



_C_ _[∗]_ = arg max
_C_



∫︁
_P_ ( _Y|C_, _c_ query) _·_ Reward( _Y_, _Y_ _[∗]_ ) _dY · P_ ( _C|c_ query, . . . ) (6)



This Bayesian formulation provides a principled way to handle uncertainty, perform adaptive retrieval by
updating priors, and maintain belief states over context in multi-step reasoning tasks.


9


|Dimension|Prompt Engineering|Context Engineering|
|---|---|---|
|**Model**|_C_ = prompt (static string)|_C_ =_ A_(_c_1,_ c_2, . . .,_ cn_) (dynamic, structured assembly)|
|**Target**|arg maxprompt _Pθ_(_Y|_prompt)|_F ∗_= arg max_F_ **E**_τ∼T_ [Reward(_Pθ_(_Y|CF_(_τ_)),_ Y∗_<br>_τ_ )]|
|**Complexity**|Manual or automated search over a string space.|System-level optimization of_ F_ =_ {A_, Retrieve, Select, . . ._ }_.|
|**Information**|Information content is fxed within the prompt.|Aims to maximize task-relevant information under constraint_ |C| ≤L_max.|
|**State**|Primarily stateless.|Inherently stateful, with explicit components for_ c_mem and_ c_state.|
|**Scalability**|Brittleness increases with length and complexity.|Manages complexity through modular composition.|
|**Error Analysis**|Manual inspection and iterative refnement.|Systematic evaluation and debugging of individual context functions.|


Table 1: Comparison of Prompt Engineering and Context Engineering Paradigms.


**Comparison of Paradigms** The formalization of Context Engineering highlights its fundamental distinctions
from traditional prompt engineering. The following table summarizes the key differences.


In summary, Context Engineering provides the formal, systematic framework required to build, understand, and optimize the sophisticated, context-aware AI systems that are coming to define the future of the
field. It shifts the focus from the “art” of prompt design to the “science” of information logistics and system
optimization.


**Context Scaling** Context scaling encompasses two fundamental dimensions that collectively define the
scope and sophistication of contextual information processing. The first dimension, **length scaling**, addresses
the computational and architectural challenges of processing ultra-long sequences, extending context windows
from thousands to millions of tokens while maintaining coherent understanding across extended narratives,
documents, and interactions. This involves sophisticated attention mechanisms, memory management
techniques, and architectural innovations that enable models to maintain contextual coherence over vastly
extended input sequences.


The second, equally critical dimension is **multi-modal and structural scaling**, which expands context
beyond simple text to encompass multi-dimensional, dynamic, cross-modal information structures. This
includes temporal context (understanding time-dependent relationships and sequences), spatial context
(interpreting location-based and geometric relationships), participant states (tracking multiple entities and
their evolving conditions), intentional context (understanding goals, motivations, and implicit objectives),
and cultural context (interpreting communication within specific social and cultural frameworks).


Modern context engineering must address both dimensions simultaneously, as real-world applications
require models to process not only lengthy textual information but also diverse data types including structured knowledge graphs, multimodal inputs (text, images, audio, video), temporal sequences, and implicit
contextual cues that humans naturally understand. This multi-dimensional approach to context scaling
represents a fundamental shift from parameter scaling toward developing systems capable of understanding
complex, ambiguous contexts that mirror the nuanced nature of human intelligence in facing a complex
world [1044].


10


**3.2. Why Context Engineering**


_**3.2.1. Current Limitations**_


Large Language Models face critical technical barriers necessitating sophisticated context engineering
approaches. The self-attention mechanism imposes quadratic computational and memory overhead as
sequence length increases, creating substantial obstacles to processing extended contexts and significantly
impacting real-world applications such as chatbots and code comprehension models [1025, 985]. Commercial
deployment compounds these challenges through repeated context processing that introduces additional
latency and token-based pricing costs [1025].


Beyond computational constraints, LLMs demonstrate concerning reliability issues including frequent
hallucinations, unfaithfulness to input context, problematic sensitivity to input variations, and responses
that appear syntactically correct while lacking semantic depth or coherence [959, 1288, 529].


The prompt engineering process presents methodological challenges through approximation-driven and
subjective approaches that focus narrowly on task-specific optimization while neglecting individual LLM
behavior [806]. Despite these challenges, prompt engineering remains critical for effective LLM utilization
through precise and contextually rich prompts that reduce ambiguity and enhance response consistency

[972].


_**3.2.2. Performance Enhancement**_


Context engineering delivers substantial performance improvements through techniques like retrievalaugmented generation and superposition prompting, achieving documented improvements including 18-fold
enhancement in text navigation accuracy, 94% success rates, and significant gains from careful prompt
construction and automatic optimization across specialized domains [271, 774, 687].


Structured prompting techniques, particularly chain-of-thought approaches, enable complex reasoning
through intermediate steps while enhancing element-aware summarization capabilities that integrate finegrained details from source documents [1147, 756, 1129]. Few-shot learning implementations through
carefully selected demonstration examples yield substantial performance gains, including 9.90% improvements in BLEU-4 scores for code summarization and 175.96% in exact match metrics for bug fixing [310].


Domain-specific context engineering proves especially valuable in specialized applications, with executionaware debugging frameworks achieving up to 9.8% performance improvements on code generation benchmarks and hardware design applications benefiting from specialized testbench generation and security
property verification [1370, 881, 44]. These targeted approaches bridge the gap between general-purpose
model training and specialized domain requirements.


_**3.2.3. Resource Optimization**_


Context engineering provides efficient alternatives to resource-intensive traditional approaches by enabling
intelligent content filtering and direct knowledge transmission through carefully crafted prompts [636, 676].
LLMs can generate expected responses even when relevant information is deleted from input context,
leveraging contextual clues and prior knowledge to optimize context length usage while maintaining
response quality, particularly valuable in domains with significant data acquisition challenges [636, 676].


Specialized optimization techniques further enhance efficiency gains through context awareness and
responsibility tuning that significantly reduce token consumption, dynamic context optimization employing


11


precise token-level content selection, and attention steering mechanisms for long-context inference [430, 952,
354]. These approaches maximize information density while reducing processing overhead and maintaining
performance quality [952, 354].


_**3.2.4. Future Potential**_


Context engineering enables flexible adaptation mechanisms through in-context learning that allows models
to adapt to new tasks without explicit retraining, with context window size directly influencing available
examples for task adaptation [623]. Advanced techniques integrate compression and selection mechanisms
for efficient model editing while maintaining contextual coherence [625]. This adaptability proves especially
valuable in low-resource scenarios, enabling effective utilization across various prompt engineering techniques
including zero-shot approaches, few-shot examples, and role context without requiring domain-specific
fine-tuning [932, 129, 1083].


Sophisticated context engineering techniques including in-context learning, chain-of-thought, tree-ofthought, and planning approaches establish foundations for nuanced language understanding and generation
capabilities while optimizing retrieval and generation processes for robust, context-aware AI applications

[803, 982].


Future research directions indicate substantial potential for advancing context-sensitive applications
through chain-of-thought augmentation with logit contrast mechanisms [961], better leveraging different
context types across domains, particularly in code intelligence tasks combining syntax, semantics, execution
flow, and documentation [1102], and understanding optimal context utilization strategies as advanced
language models continue demonstrating prompt engineering’s persistent value [1087]. Evolution toward
sophisticated filtering and selection mechanisms represents a critical pathway for addressing transformer
architectures’ scaling limitations while maintaining performance quality.

##### **4. Foundational Components**


Context Engineering is built upon three fundamental components that collectively address the core challenges of information management in large language models: **Context Retrieval and Generation** sources
appropriate contextual information through prompt engineering, external knowledge retrieval, and dynamic
context assembly; **Context Processing** transforms and optimizes acquired information through long sequence
processing, self-refinement mechanisms, and structured data integration; and **Context Management** tackles
efficient organization and utilization of contextual information through addressing fundamental constraints,
implementing sophisticated memory hierarchies, and developing compression techniques. These foundational
components establish the theoretical and practical basis for all context engineering implementations, forming
a comprehensive framework where each component addresses distinct aspects of the context engineering
pipeline while maintaining synergistic relationships that enable comprehensive contextual optimization and
effective context engineering strategies.


**4.1. Context Retrieval and Generation**


Context Retrieval and Generation forms the foundational layer of context engineering, encompassing the
systematic retrieval and construction of relevant information for LLMs. This component addresses the critical
challenge of sourcing appropriate contextual information through three primary mechanisms: prompt-based
generation that crafts effective instructions and reasoning frameworks, external knowledge retrieval that


12


**Figure** 3: Context Engineering Framework: A comprehensive taxonomy of Context Engineering components
including Context Retrieval and Generation, Context Processing, and Context Management, integrated
into System Implementations such as RAG systems, memory architectures, tool-integrated reasoning, and
multi-agent coordination mechanisms.


accesses dynamic information sources, and dynamic context assembly that orchestrates acquired components
into coherent, task-optimized contexts.


_**4.1.1. Prompt Engineering and Context Generation**_


Prompt engineering and context generation forms the foundational layer of context retrieval, encompassing
strategic input design that combines art and science to craft effective instructions for LLMs. The CLEAR
Framework—conciseness, logic, explicitness, adaptability, and reflectiveness—governs effective prompt
construction, while core architecture integrates task instructions, contextual information, input data, and
output indicators [708, 1142, 575, 213, 25].


**Zero-Shot and Few-Shot Learning Paradigms** Zero-shot prompting enables task performance without prior
examples, relying exclusively on instruction clarity and pre-trained knowledge [1371, 340, 559, 67, 1054].
Few-shot prompting extends this capability by incorporating limited exemplars to guide model responses,
demonstrating task execution through strategic example selection [1371, 405, 103, 552, 794, 1381]. Incontext learning facilitates adaptation to novel tasks without parameter updates by leveraging demonstration
examples within prompts, with performance significantly influenced by example selection and ordering
strategies [369, 103, 1296, 1024, 928, 852, 1148, 352, 582].


**Chain-of-Thought Foundations** Chain-of-Thought (CoT) prompting decomposes complex problems into
intermediate reasoning steps, mirroring human cognition [1147, 405, 340, 947, 609]. Zero-shot CoT
uses trigger phrases like “Let’s think step by step,” improving MultiArith accuracy from 17.7% to 78.7%

[559, 1107, 478, 668], with Automatic Prompt Engineer refinements yielding additional gains [1224, 532].


Tree-of-Thoughts (ToT) organizes reasoning as hierarchical structures with exploration, lookahead, and
backtracking capabilities, increasing Game of 24 success rates from 4% to 74% [1255, 221, 563, 604].
Graph-of-Thoughts (GoT) models reasoning as arbitrary graphs with thoughts as vertices and dependencies
as edges, improving quality by 62% and reducing costs by 31% compared to ToT [69, 832, 1376].


13


**Cognitive Architecture Integration** Cognitive prompting implements structured human-like operations
including goal clarification, decomposition, filtering, abstraction, and pattern recognition, enabling systematic
multi-step task resolution through deterministic, self-adaptive, and hybrid variants [564, 563, 1214, 1173].
Guilford’s Structure of Intellect model provides psychological foundations for categorizing cognitive operations
such as pattern recognition, memory retrieval, and evaluation, enhancing reasoning clarity, coherence, and
adaptability [562, 195]. Advanced implementations incorporate cognitive tools as modular reasoning
operations, with GPT-4.1 performance on AIME2024 increasing from 26.7% to 43.3% through structured
cognitive operation sequences [247, 1038].

|Method|Description|
|---|---|
|**Self-Refne** [741, 924]|Enables LLMs to improve outputs through iterative feedback and refnement cycles using the same model as the generator,<br>feedback provider, and refner, without supervised training.|
|**Multi-Aspect Feedback** [805]|Integrates multiple feedback modules (frozen LMs and external tools), each focusing on specifc error categories to enable<br>more comprehensive, independent evaluation.|
|**N-CRITICS** [795]|Implements an ensemble of critics that evaluate an initial output. Compiled feedback from the generating LLM and other<br>models guides refnement until a stopping criterion is met.|
|**ISR-LLM** [1383]|Improves LLM-based planning by translating natural language to formal specifcations, creating an initial plan, and then<br>systematically refning it with a validator.|
|**SELF** [710]|Teaches LLMs meta-skills (self-feedback, self-refnement) with limited examples, then has the model continuously self-<br>evolve by generating and fltering its own training data.|
|**ProMiSe** [892]|Addresses self-refnement in smaller LMs using principle-guided iterative refnement, combining proxy metric thresholds<br>with few-shot refnement and rejection sampling.|
|**A2R** [583]|Augments LLMs through Metric-based Iterative Feedback Learning, using explicit evaluation across multiple dimensions<br>(e.g., correctness) to generate feedback and refne outputs.|
|**Experience Refnement** [863]|Enables LLM agents to refne experiences during task execution by learning from recent (successive) or all previous<br>(cumulative) experiences, prioritizing high-quality ones.|
|**I-SHEEP** [660]|Allows LLMs to continuously self-align from scratch by generating, assessing, fltering, and training on high-quality<br>synthetic datasets without external guidance.|
|**CaP** [1280]|Uses external tools to refne chain-of-thought (CoT) responses, addressing the limitation of models that get stuck in<br>non-correcting reasoning loops.|
|**Agent-R** [1286]|Enables language agents to refect “on the fy” through iterative self-training, using Monte Carlo Tree Search (MCTS) to<br>construct training data that corrects erroneous paths.|
|**GenDiE** [616]|Enhances context faithfulness with sentence-level optimization, combining generative and discriminative training to give<br>LLMs self-generation and self-scoring capabilities.|
|**Self-Developing** [472]|Enables LLMs to autonomously discover, implement, and refne their own improvement algorithms by generating them as<br>code, evaluating them, and using DPO to recursively improve.|
|**SR-NLE** [1130]|Improves the faithfulness of post-hoc natural language explanations via an iterative critique and refnement process using<br>self-feedback and feature attribution.|



Table 2: Self-refinement methods in large language models and their key characteristics.


_**4.1.2. External Knowledge Retrieval**_


External knowledge retrieval represents a critical component of context retrieval, addressing fundamental
limitations of parametric knowledge through dynamic access to external information sources including
databases, knowledge graphs, and document collections.


**Retrieval-Augmented Generation Fundamentals** RAG combines parametric knowledge stored in model
parameters with non-parametric information retrieved from external sources, enabling access to current,
domain-specific knowledge while maintaining parameter efficiency [597, 315, 257]. FlashRAG provides
comprehensive evaluation and modular implementation of RAG systems, while frameworks like KRAGEN


14


and ComposeRAG demonstrate advanced retrieval strategies with substantial performance improvements
across diverse benchmarks [506, 755, 1168].


Self-RAG introduces adaptive retrieval mechanisms where models dynamically decide when to retrieve
information and generate special tokens to control retrieval timing and quality assessment [41]. Advanced
implementations include RAPTOR for hierarchical document processing, HippoRAG for memory-inspired
retrieval architectures, and Graph-Enhanced RAG systems that leverage structured knowledge representations
for improved information access [936, 370, 364].


**Knowledge Graph Integration and Structured Retrieval** Knowledge graph integration addresses structured information retrieval through frameworks like KAPING, which retrieves relevant facts based on semantic
similarities and prepends them to prompts without requiring model training [48, 679]. KARPA provides
training-free knowledge graph adaptation through pre-planning, semantic matching, and relation path
reasoning, achieving state-of-the-art performance on knowledge graph question answering tasks [262].


Think-on-Graph enables sequential reasoning over knowledge graphs to locate relevant triples, conducting
exploration to retrieve related information from external databases while generating multiple reasoning
pathways [1008, 726]. StructGPT implements iterative reading-then-reasoning approaches that construct
specialized functions to collect relevant evidence from structured data sources [495].


**Agentic and Modular Retrieval Systems** Agentic RAG systems treat retrieval as dynamic operations
where agents function as intelligent investigators analyzing content and cross-referencing information

[654, 166, 973]. These systems incorporate sophisticated planning and reflection mechanisms requiring
integration of task decomposition, multi-plan selection, and iterative refinement capabilities [444, 1192].


Modular RAG architectures enable flexible composition of retrieval components through standardized
interfaces and plug-and-play designs. Graph-Enhanced RAG systems leverage structured knowledge representations for improved information access, while Real-time RAG implementations address dynamic information
requirements in streaming applications [316, 1401].


_**4.1.3. Dynamic Context Assembly**_


Dynamic context assembly represents the sophisticated orchestration of acquired information components
into coherent, task-optimized contexts that maximize language model performance while respecting computational constraints.


**Assembly Functions and Orchestration Mechanisms** The assembly function _A_ encompasses templatebased formatting, priority-based selection, and adaptive composition strategies that must adapt to varying task
requirements, model capabilities, and resource constraints [708, 1142, 575]. Contemporary orchestration
mechanisms manage agent selection, context distribution, and interaction flow control in multi-agent systems,
enabling effective cooperation through user input processing, contextual distribution, and optimal agent
selection based on capability assessment [902, 53, 175].


Advanced orchestration frameworks incorporate intent recognition, contextual memory maintenance,
and task dispatching components for intelligent coordination across domain-specific agents. The Swarm
Agent framework utilizes real-time outputs to direct tool invocations while addressing limitations in static
tool registries and bespoke communication frameworks [814, 267, 250].


15


**Multi-Component Integration Strategies** Context assembly must address cross-modal integration challenges, incorporating diverse data types including text, structured knowledge, temporal sequences, and
external tool interfaces while maintaining coherent semantic relationships [535, 1230, 502]. Verbalization
techniques convert structured data including knowledge graph triples, table rows, and database records
into natural language sentences, enabling seamless integration with existing language systems without
architectural modifications [12, 788, 1072, 13].


Programming language representations of structured data, particularly Python implementations for
knowledge graphs and SQL for databases, outperform traditional natural language representations in
complex reasoning tasks by leveraging inherent structural properties [1175]. Multi-level structurization
approaches reorganize input text into layered structures based on linguistic relationships, while structured
data representations leverage existing LLMs to extract structured information and represent key elements as
graphs, tables, or relational schemas [687, 1134, 1334].


**Automated Assembly Optimization** Automated prompt engineering addresses manual optimization limitations through systematic prompt generation and refinement algorithms. Automatic Prompt Engineer (APE)
employs search algorithms for optimal prompt discovery, while LM-BFF introduces automated pipelines combining prompt-based fine-tuning with dynamic demonstration incorporation, achieving up to 30% absolute
improvement across NLP tasks [311, 421, 596]. Promptbreeder implements self-referential evolutionary
systems where LLMs improve both task-prompts and mutation-prompts governing these improvements
through natural selection analogies [279, 514].


Self-refine enables iterative output improvement through self-critique and revision across multiple
iterations, with GPT-4 achieving approximately 20% absolute performance improvement through this
methodology [741, 676]. Multi-agent collaborative frameworks simulate specialized team dynamics with
agents assuming distinct roles (analysts, coders, testers), resulting in 29.9-47.1% relative improvement in
Pass@1 metrics compared to single-agent approaches [440, 1266].


Tool integration frameworks combine Chain-of-Thought reasoning with external tool execution, automating intermediate reasoning step generation as executable programs strategically incorporating external data.
LangChain provides comprehensive framework support for sequential processing chains, agent development,
and web browsing capabilities, while specialized frameworks like Auto-GPT and Microsoft’s AutoGen facilitate
complex AI agent development through user-friendly interfaces [971, 1095, 25, 875].


**4.2. Context Processing**


Context Processing focuses on transforming and optimizing acquired contextual information to maximize its
utility for LLMs. This component addresses challenges in handling ultra-long sequence contexts, enables
iterative self-refinement and adaptation mechanisms, and facilitates integration of multimodal, relational
and structured information into coherent contextual representations.


_**4.2.1. Long Context Processing**_


Ultra-long sequence context processing addresses fundamental computational challenges arising from
transformer self-attention’s O(n [2] ) complexity, which creates significant bottlenecks as sequence lengths
increase and substantially impacts real-world applications [1067, 737, 299, 272, 420]. Increasing Mistral-7B
input from 4K to 128K tokens requires 122-fold computational increase, while memory constraints during


16


prefilling and decoding stages create substantial resource demands, with Llama 3.1 8B requiring up to 16GB
per 128K-token request [1040, 1236, 429].


**Architectural Innovations for Long Context** State Space Models (SSMs) maintain linear computational
complexity and constant memory requirements through fixed-size hidden states, with models like Mamba
offering efficient recurrent computation mechanisms that scale more effectively than traditional transformers

[1267, 351, 350]. Dilated attention approaches like LongNet employ exponentially expanding attentive
fields as token distance grows, achieving linear computational complexity while maintaining logarithmic
dependency between tokens, enabling processing of sequences exceeding one billion tokens [220].

