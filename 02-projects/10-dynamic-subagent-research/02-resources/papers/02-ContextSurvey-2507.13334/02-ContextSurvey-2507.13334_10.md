<!-- Source: 02-ContextSurvey-2507.13334.pdf | Chunk 10/26 -->


[57] Saikat Barua. Exploring autonomous agents through the lens of large language models: A review,
[arXiv preprint arXiv:2404.04442, 2024. URL https://arxiv.org/abs/2404.04442v1.](https://arxiv.org/abs/2404.04442v1)


[58] Kinjal Basu, Ibrahim Abdelaziz, Kelsey Bradford, M. Crouse, Kiran Kate, Sadhana Kumaravel, Saurabh
Goyal, Asim Munawar, Yara Rizk, Xin Wang, Luis A. Lastras, and P. Kapanipathi. Nestful: A benchmark
for evaluating llms on nested sequences of api calls, arXiv preprint arXiv:2409.03797, 2024. URL
[https://arxiv.org/abs/2409.03797v3.](https://arxiv.org/abs/2409.03797v3)


63


[59] Amin Beheshti. Natural language-oriented programming (nlop): Towards democratizing software
creation. _2024 IEEE International Conference on Software Services Engineering (SSE)_, 2024.


[60] Azadeh Beiranvand and S. M. Vahidipour. Integrating structural and semantic signals in text[attributed graphs with bigtex, arXiv preprint arXiv:2504.12474, 2025. URL https://arxiv.org/](https://arxiv.org/abs/2504.12474v2)
[abs/2504.12474v2.](https://arxiv.org/abs/2504.12474v2)


[61] Assaf Ben-Kish, Itamar Zimerman, Shady Abu-Hussein, Nadav Cohen, Amir Globerson, Lior Wolf,
and Raja Giryes. Decimamba: Exploring the length extrapolation potential of mamba. _International_
_Conference on Learning Representations_, 2024.


[62] Assaf Ben-Kish, Itamar Zimerman, M. J. Mirza, James R. Glass, Leonid Karlinsky, and Raja Giryes.
Overflow prevention enhances long-context recurrent llms. arXiv preprint, 2025.


[63] M. Benna and Stefano Fusi. Complex synapses as efficient memory systems. _BMC Neuroscience_,
2015.


[64] M. Benna and Stefano Fusi. Computational principles of biological memory, arXiv preprint
[arXiv:1507.07580, 2015. URL https://arxiv.org/abs/1507.07580v1.](https://arxiv.org/abs/1507.07580v1)


[65] Shelly Bensal, Umar Jamil, Christopher Bryant, M. Russak, Kiran Kamble, Dmytro Mozolevskyi,
Muayad Ali, and Waseem Alshikh. Reflect, retry, reward: Self-improving llms via reinforcement learn[ing, arXiv preprint arXiv:2505.24726, 2025. URL https://arxiv.org/abs/2505.24726v1.](https://arxiv.org/abs/2505.24726v1)


[66] Idoia Berges, J. Bermúdez, A. Goñi, and A. Illarramendi. Semantic web technology for agent
communication protocols. _Extended Semantic Web Conference_, 2008.


[67] Gaurav Beri and Vaishnavi Srivastava. Advanced techniques in prompt engineering for large language
models: A comprehensive study. _2024 IEEE 4th International Conference on ICT in Business Industry_
_& Government (ICTBIG)_, 2024.


[68] Amanda Bertsch, Uri Alon, Graham Neubig, and Matthew R. Gormley. Unlimiformer: Long-range
transformers with unlimited length input. _Neural Information Processing Systems_, 2023.


[69] Maciej Besta, Nils Blach, Aleš Kubíček, Robert Gerstenberger, Lukas Gianinazzi, Joanna Gajda,
Tomasz Lehmann, Michal Podstawski, H. Niewiadomski, P. Nyczyk, and Torsten Hoefler. Graph of
thoughts: Solving elaborate problems with large language models. _AAAI Conference on Artificial_
_Intelligence_, 2023.


[70] Gregor Betz and Kyle Richardson. Judgment aggregation, discursive dilemma and reflective equilibrium: Neural language models as self-improving doxastic agents. _Frontiers in Artificial Intelligence_,
2022.


[71] L. Bezalel, Eyal Orgad, and Amir Globerson. Teaching models to improve on tape. _AAAI Conference_
_on Artificial Intelligence_, 2024.


[72] Umang Bhatt, Sanyam Kapoor, Mihir Upadhyay, Ilia Sucholutsky, Francesco Quinzan, Katherine M.
Collins, Adrian Weller, Andrew Gordon Wilson, and Muhammad Bilal Zafar. When should we
[orchestrate multiple agents?, arXiv preprint arXiv:2503.13577, 2025. URL https://arxiv.org/](https://arxiv.org/abs/2503.13577v1)
[abs/2503.13577v1.](https://arxiv.org/abs/2503.13577v1)


64


[73] Baolong Bi, Shaohan Huang, Yiwei Wang, Tianchi Yang, Zihan Zhang, Haizhen Huang, Lingrui
Mei, Junfeng Fang, Zehao Li, Furu Wei, et al. Context-dpo: Aligning language models for contextfaithfulness. _ACL 2025_, 2024.


[74] Baolong Bi, Shenghua Liu, Lingrui Mei, Yiwei Wang, Pengliang Ji, and Xueqi Cheng. Decoding by
contrasting knowledge: Enhancing llms’ confidence on edited facts. _ACL 2025_, 2024.


[75] Baolong Bi, Shenghua Liu, Yiwei Wang, Lingrui Mei, and Xueqi Cheng. Lpnl: Scalable link prediction
with large language models. _ACL 2024_, 2024.


[76] Baolong Bi, Shenghua Liu, Yiwei Wang, Lingrui Mei, Hongcheng Gao, Junfeng Fang, and Xueqi
Cheng. Struedit: Structured outputs enable the fast and accurate knowledge editing for large
language models. 2024.


[77] Baolong Bi, Shenghua Liu, Yiwei Wang, Lingrui Mei, Hongcheng Gao, Yilong Xu, and Xueqi Cheng.
Adaptive token biaser: Knowledge editing via biasing key entities. _EMNLP 2024_, 2024.


[78] Baolong Bi, Shenghua Liu, Xingzhang Ren, Dayiheng Liu, Junyang Lin, Yiwei Wang, Lingrui Mei,
Junfeng Fang, Jiafeng Guo, and Xueqi Cheng. Refinex: Learning to refine pre-training data at scale
from expert-guided programs. 2025.


[79] Baolong Bi, Shenghua Liu, Yiwei Wang, Lingrui Mei, Junfeng Fang, Hongcheng Gao, Shiyu Ni, and
Xueqi Cheng. Is factuality enhancement a free lunch for llms? better factuality can lead to worse
context-faithfulness. _ICLR 2025_, 2025.


[80] Baolong Bi, Shenghua Liu, Yiwei Wang, Yilong Xu, Junfeng Fang, Lingrui Mei, and Xueqi Cheng.
Parameters vs. context: Fine-grained control of knowledge reliance in language models. 2025.


[81] Bin Bi, Chenliang Li, Chen Wu, Ming Yan, and Wei Wang. Palm: Pre-training an autoencoding&autoregressive language model for context-conditioned generation. _Conference on Empirical_
_Methods in Natural Language Processing_, 2020.


[82] Dinh Doan Van Bien, David Lillis, and Rem W. Collier. Call graph profiling for multi agent systems.
_Multi-Agent Logics, Languages, and Organisations Federated Workshops_, 2009.


[83] Jonas Bode, Bastian Pätzold, Raphael Memmesheimer, and Sven Behnke. A comparison of prompt
engineering techniques for task planning and execution in service robotics. _IEEE-RAS International_
_Conference on Humanoid Robots_, 2024.


[84] P. Bonzon. Grounding mental representations in a virtual multi-level functional framework. _Journal_
_of Cognition_, 2023.


[85] Sebastian Borgeaud, A. Mensch, Jordan Hoffmann, Trevor Cai, Eliza Rutherford, Katie Millican,
George van den Driessche, Jean-Baptiste Lespiau, Bogdan Damoc, Aidan Clark, Diego de Las Casas,
Aurelia Guy, Jacob Menick, Roman Ring, T. Hennigan, Saffron Huang, Lorenzo Maggiore, Chris Jones,
Albin Cassirer, Andy Brock, Michela Paganini, G. Irving, O. Vinyals, Simon Osindero, K. Simonyan,
Jack W. Rae, Erich Elsen, and L. Sifre. Improving language models by retrieving from trillions of
tokens. _International Conference on Machine Learning_, 2021.


[86] Zalán Borsos, Raphaël Marinier, Damien Vincent, E. Kharitonov, O. Pietquin, Matthew Sharifi,
Dominik Roblek, O. Teboul, David Grangier, M. Tagliasacchi, and Neil Zeghidour. Audiolm: A
language modeling approach to audio generation. _IEEE/ACM Transactions on Audio Speech and_
_Language Processing_, 2022.


65


[87] FutureSearch Nikos I. Bosse, Jon Evans, Robert G. Gambee, Daniel Hnyk, Peter Muhlbacher, Lawrence
Phillips, Dan Schwarz, and Jack Wildman. Deep research bench: Evaluating ai web research agents,
[arXiv preprint arXiv:2506.06287, 2025. URL https://arxiv.org/abs/2506.06287v1.](https://arxiv.org/abs/2506.06287v1)


[88] Vicent Botti. Agentic ai and multiagentic: Are we reinventing the wheel?, arXiv preprint
[arXiv:2506.01463, 2025. URL https://arxiv.org/abs/2506.01463v1.](https://arxiv.org/abs/2506.01463v1)


[89] William Brach, Kristián Kostál, and Michal Ries. The effectiveness of large language models in
transforming unstructured text to standardized formats. _IEEE Access_, 2025.


[90] C. Brainerd, C. F. Gomes, and K. Nakamura. Dual recollection in episodic memory. _Journal of_
_experimental psychology. General_, 2015.


[91] Inês Bramão, Jiefeng Jiang, A. Wagner, and M. Johansson. Encoding contexts are incidentally
reinstated during competitive retrieval and track the temporal dynamics of memory interference.
_Cerebral Cortex_, 2022.


[92] Andrés M Bran, Sam Cox, Oliver Schilter, Carlo Baldassari, Andrew D. White, and P. Schwaller.
Augmenting large language models with chemistry tools. _Nat. Mac. Intell._, 2023.


[93] Maricela Claudia Bravo and Martha Coronel. Aligning agent communication protocols - a pragmatic
approach. _International Conference on Software and Data Technologies_, 2008.


[94] F. Brazier, B. Dunin-Keplicz, N. Jennings, and Jan Treur. Desire: Modelling multi-agent systems in a
compositional formal framework. _International Journal of Cooperative Information Systems_, 1997.


[95] Lorenz Brehme, Thomas Ströhle, and Ruth Breu. Can llms be trusted for evaluating rag systems? a
[survey of methods and datasets, arXiv preprint arXiv:2504.20119, 2025. URL https://arxiv.](https://arxiv.org/abs/2504.20119v2)
[org/abs/2504.20119v2.](https://arxiv.org/abs/2504.20119v2)


[96] R. Breil, D. Delahaye, Laurent Lapasset, and E. Feron. Multi-agent systems to help managing air
traffic structure. arXiv preprint, 2017.


[97] Alexander Brinkmann and Christian Bizer. Self-refinement strategies for llm-based product attribute
value extraction. _Datenbanksysteme für Business, Technologie und Web_, 2025.


[98] D. Britz, M. Guan, and Minh-Thang Luong. Efficient attention using a fixed-size memory representation. _Conference on Empirical Methods in Natural Language Processing_, 2017.


[99] Adam W. Broitman and M. J. Kahana. Neural context reinstatement of recurring events. _bioRxiv_,
2024.


[100] Ethan A. Brooks, Logan Walls, Richard L. Lewis, and Satinder Singh. Large language models can
implement policy iteration. _Neural Information Processing Systems_, 2022.


[101] Rodney A. Brooks. A robust layered control system for a mobile robot. _IEEE J. Robotics Autom._, 1986.


[102] Tim Brooks, Aleksander Holynski, and Alexei A. Efros. Instructpix2pix: Learning to follow image
editing instructions. _Computer Vision and Pattern Recognition_, 2022.


66


[103] Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, J. Kaplan, Prafulla Dhariwal, Arvind
Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss,
Gretchen Krueger, T. Henighan, R. Child, A. Ramesh, Daniel M. Ziegler, Jeff Wu, Clemens Winter,
Christopher Hesse, Mark Chen, Eric Sigler, Ma teusz Litwin, Scott Gray, Benjamin Chess, Jack Clark,
Christopher Berner, Sam McCandlish, Alec Radford, I. Sutskever, and Dario Amodei. Language
models are few-shot learners. _Neural Information Processing Systems_, 2020.


[104] Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal,
Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel HerbertVoss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,
Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin
Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario
Amodei. Language models are few-shot learners, arXiv preprint arXiv:2005.14165, 2020. URL
[https://arxiv.org/abs/2005.14165.](https://arxiv.org/abs/2005.14165)


[105] Davide Caffagni, Federico Cocchi, Nicholas Moratelli, Sara Sarto, Marcella Cornia, L. Baraldi, and
R. Cucchiara. Wiki-llava: Hierarchical retrieval-augmented generation for multimodal llms. _2024_
_IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)_, 2024.


[106] Joyce Cahoon, Prerna Singh, Nick Litombe, Jonathan Larson, Ha Trinh, Yiwen Zhu, Andreas Mueller,
Fotis Psallidas, and Carlo Curino. Optimizing open-domain question answering with graph-based
[retrieval augmented generation, arXiv preprint arXiv:2503.02922, 2025. URL https://arxiv.](https://arxiv.org/abs/2503.02922v1)
[org/abs/2503.02922v1.](https://arxiv.org/abs/2503.02922v1)


[107] Hongru Cai, Yongqi Li, Wenjie Wang, Fengbin Zhu, Xiaoyu Shen, Wenjie Li, and Tat-Seng Chua.
Large language models empowered personalized web agents. _The Web Conference_, 2024.


[108] Yujun Cai, Liuhao Ge, Jianfei Cai, and Junsong Yuan. Weakly-supervised 3d hand pose estimation
from monocular rgb images. In _Proceedings of the European conference on computer vision (ECCV)_,
pages 666–682, 2018.


[109] Yujun Cai, Liuhao Ge, Jun Liu, Jianfei Cai, Tat-Jen Cham, Junsong Yuan, and Nadia Magnenat
Thalmann. Exploiting spatial-temporal relationships for 3d pose estimation via graph convolutional
networks. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pages 2272–
2281, 2019.


[110] Yujun Cai, Liuhao Ge, Jianfei Cai, Nadia Magnenat Thalmann, and Junsong Yuan. 3d hand pose
estimation using synthetic data and weakly labeled rgb images. _IEEE transactions on pattern analysis_
_and machine intelligence_, 43(11):3739–3753, 2020.


[111] Yujun Cai, Lin Huang, Yiwei Wang, Tat-Jen Cham, Jianfei Cai, Junsong Yuan, Jun Liu, Xu Yang, Yiheng
Zhu, Xiaohui Shen, et al. Learning progressive joint propagation for human motion prediction.
In _Computer Vision–ECCV 2020: 16th European Conference, Glasgow, UK, August 23–28, 2020,_
_Proceedings, Part VII 16_, pages 226–242. Springer International Publishing, 2020.


[112] Yujun Cai, Yiwei Wang, Yiheng Zhu, Tat-Jen Cham, Jianfei Cai, Junsong Yuan, Jun Liu, Chuanxia
Zheng, Sijie Yan, Henghui Ding, et al. A unified 3d human motion synthesis model via conditional
variational auto-encoder. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_,
pages 11645–11655, 2021.


67


[113] V. Camos and P. Barrouillet. Attentional and non-attentional systems in the maintenance of verbal information in working memory: the executive and phonological loops. _Frontiers in Human_
_Neuroscience_, 2014.


[114] Boxi Cao, Qiaoyu Tang, Hongyu Lin, Xianpei Han, Jiawei Chen, Tianshu Wang, and Le Sun. Retentive
or forgetful? diving into the knowledge memorizing mechanism of language models. _International_
_Conference on Language Resources and Evaluation_, 2023.


[115] He Cao, Zhenwei An, Jiazhan Feng, Kun Xu, Liwei Chen, and Dongyan Zhao. A step closer to comprehensive answers: Constrained multi-stage question decomposition with large language models,
[arXiv preprint arXiv:2311.07491, 2023. URL https://arxiv.org/abs/2311.07491v1.](https://arxiv.org/abs/2311.07491v1)


[116] Nicola De Cao, Wilker Aziz, and Ivan Titov. Question answering by reasoning across documents
with graph convolutional networks. _North American Chapter of the Association for Computational_
_Linguistics_, 2018.


[117] Pengfei Cao, Tianyi Men, Wencan Liu, Jingwen Zhang, Xuzhao Li, Xixun Lin, Dianbo Sui, Yanan Cao,
Kang Liu, and Jun Zhao. Large language models for planning: A comprehensive and systematic survey,
[arXiv preprint arXiv:2505.19683, 2025. URL https://arxiv.org/abs/2505.19683v1.](https://arxiv.org/abs/2505.19683v1)


[118] Yongcan Cao, Wenwu Yu, W. Ren, and Guanrong Chen. An overview of recent progress in the study
of distributed multi-agent coordination. _IEEE Transactions on Industrial Informatics_, 2012.


[119] Yuji Cao, Huan Zhao, Yuheng Cheng, Ting Shu, Yue Chen, Guolong Liu, Gaoqi Liang, Junhua
Zhao, Jinyue Yan, and Yunjie Li. Survey on large language model-enhanced reinforcement learning:
Concept, taxonomy, and methods. _IEEE Transactions on Neural Networks and Learning Systems_, 2024.


[120] Yukun Cao, Zengyi Gao, Zhiyang Li, Xike Xie, and S. K. Zhou. Lego-graphrag: Modularizing
graph-based retrieval-augmented generation for design space exploration. arXiv preprint, 2024.


[121] R. C. Cardoso and Angelo Ferrando. A review of agent-based programming for multi-agent systems.
_De Computis_, 2021.


[122] Nicholas Carlini, Chang Liu, Ú. Erlingsson, Jernej Kos, and D. Song. The secret sharer: Evaluating
and testing unintended memorization in neural networks. _USENIX Security Symposium_, 2018.


[123] Nicholas Carlini, Florian Tramèr, Eric Wallace, Matthew Jagielski, Ariel Herbert-Voss, Katherine Lee,
Adam Roberts, Tom B. Brown, D. Song, Ú. Erlingsson, Alina Oprea, and Colin Raffel. Extracting
training data from large language models. _USENIX Security Symposium_, 2020.


[124] Daniel Casanueva-Morato, A. Ayuso-Martinez, J. P. Dominguez-Morales, A. Jiménez-Fernandez, and
G. Jiménez-Moreno. A bio-inspired implementation of a sparse-learning spike-based hippocampus
memory model. _IEEE Transactions on Emerging Topics in Computing_, 2022.


[125] Daniel Casanueva-Morato, A. Ayuso-Martinez, J. P. Dominguez-Morales, A. Jiménez-Fernandez, and
G. Jiménez-Moreno. Bio-inspired computational memory model of the hippocampus: an approach
to a neuromorphic spike-based content-addressable memory. _Neural Networks_, 2023.


[126] Amartya Chakraborty, Paresh Dashore, Nadia Bathaee, Anmol Jain, Anirban Das, Shi-Xiong Zhang,
Sambit Sahu, M. Naphade, and Genta Indra Winata. T1: A tool-oriented conversational dataset for
[multi-turn agentic planning, arXiv preprint arXiv:2505.16986, 2025. URL https://arxiv.org/](https://arxiv.org/abs/2505.16986v1)
[abs/2505.16986v1.](https://arxiv.org/abs/2505.16986v1)


68


[127] Kranti Chalamalasetti, Jana Gotze, Sherzod Hakimov, Brielen Madureira, Philipp Sadler, and David
Schlangen. clembench: Using game play to evaluate chat-optimized language models as conversational agents. _Conference on Empirical Methods in Natural Language Processing_, 2023.


[128] Edward Y. Chang and Longling Geng. Sagallm: Context management, validation, and transaction
[guarantees for multi-agent llm planning, arXiv preprint arXiv:2503.11951, 2025. URL https:](https://arxiv.org/abs/2503.11951v2)
[//arxiv.org/abs/2503.11951v2.](https://arxiv.org/abs/2503.11951v2)


[129] Yu-Chu Chang, Xu Wang, Jindong Wang, Yuan Wu, Kaijie Zhu, Hao Chen, Linyi Yang, Xiaoyuan
Yi, Cunxiang Wang, Yidong Wang, Weirong Ye, Yue Zhang, Yi Chang, Philip S. Yu, Qian Yang, and
Xingxu Xie. A survey on evaluation of large language models. _ACM Transactions on Intelligent Systems_
_and Technology_, 2023.


[130] Subhajit Chaudhury, Payel Das, Sarathkrishna Swaminathan, Georgios Kollias, Elliot Nelson, Khushbu
Pahwa, Tejaswini Pedapati, Igor Melnyk, and Matthew Riemer. Epman: Episodic memory attention
[for generalizing to longer contexts, arXiv preprint arXiv:2502.14280, 2025. URL https://arxiv.](https://arxiv.org/abs/2502.14280v1)
[org/abs/2502.14280v1.](https://arxiv.org/abs/2502.14280v1)


[131] Xueqi CHEGN, Shenghua Liu, and Ruqing ZHANG. Thinking on new system for big data technology.
_Bulletin of Chinese Academy of Sciences (Chinese Version)_, 37(1):60–67, 2022.


[132] Viktoriia Chekalina, Anton Razzigaev, Elizaveta Goncharova, and Andrey Kuznetsov. Addressing
hallucinations in language models with knowledge graph embeddings as an additional modality,
[arXiv preprint arXiv:2411.11531, 2024. URL https://arxiv.org/abs/2411.11531v2.](https://arxiv.org/abs/2411.11531v2)


[133] Bo Chen, Yingyu Liang, Zhizhou Sha, Zhenmei Shi, and Zhao Song. Hsr-enhanced sparse attention
[acceleration, arXiv preprint arXiv:2410.10165, 2024. URL https://arxiv.org/abs/2410.](https://arxiv.org/abs/2410.10165v2)
[10165v2.](https://arxiv.org/abs/2410.10165v2)


[134] Boyu Chen, Zirui Guo, Zidan Yang, Yuluo Chen, Junze Chen, Zhenghao Liu, Chuan Shi, and Cheng
Yang. Pathrag: Pruning graph-based retrieval augmented generation with relational paths, arXiv
[preprint arXiv:2502.14902, 2025. URL https://arxiv.org/abs/2502.14902v1.](https://arxiv.org/abs/2502.14902v1)


[135] Fei-Long Chen, Du-Zhen Zhang, Ming-Lun Han, Xiu-Yi Chen, Jing Shi, Shuang Xu, and Bo Xu. Vlp:
A survey on vision-language pre-training. _Machine Intelligence Research_, 20(1):38–56, 2023.


[136] Feiyang Chen, Yu Cheng, Lei Wang, Yuqing Xia, Ziming Miao, Lingxiao Ma, Fan Yang, Jilong
Xue, Zhi Yang, Mao Yang, and Haibo Chen. Attentionengine: A versatile framework for efficient
attention mechanisms on diverse hardware platforms, arXiv preprint arXiv:2502.15349, 2025. URL
[https://arxiv.org/abs/2502.15349v1.](https://arxiv.org/abs/2502.15349v1)


[137] Huajun Chen. Large knowledge model: Perspectives and challenges. _Data Intelligence_, 2023.


[138] Jianing Chen, Zehao Li, Yujun Cai, Hao Jiang, Chengxuan Qian, Juyuan Kang, Shuqin Gao, Honglong
Zhao, Tianlu Mao, and Yucheng Zhang. Haif-gs: Hierarchical and induced flow-guided gaussian
splatting for dynamic scene. 2025.


[139] Jiaqi Chen, Xiaoye Zhu, Yue Wang, Tianyang Liu, Xinhui Chen, Ying Chen, Chak Tou Leong, Yifei
Ke, Joseph Liu, Yiwen Yuan, Julian McAuley, and Li jia Li. Symbolic representation for any-to-any
[generative tasks, arXiv preprint arXiv:2504.17261v1, 2025. URL https://arxiv.org/abs/](https://arxiv.org/abs/2504.17261v1)
[2504.17261v1.](https://arxiv.org/abs/2504.17261v1)


69


[140] Jiayi Chen, J. Ye, and Guiling Wang. From standalone llms to integrated intelligence: A survey of
[compound al systems, arXiv preprint arXiv:2506.04565, 2025. URL https://arxiv.org/abs/](https://arxiv.org/abs/2506.04565v1)
[2506.04565v1.](https://arxiv.org/abs/2506.04565v1)


[141] Jin Chen, Zheng Liu, Xu Huang, Chenwang Wu, Qi Liu, Gangwei Jiang, Yuanhao Pu, Yuxuan Lei,
Xiaolong Chen, Xingmei Wang, Defu Lian, and Enhong Chen. When large language models meet
personalization: Perspectives of challenges and opportunities. _World wide web (Bussum)_, 2023.


[142] Jiyu Chen, Shuang Peng, Daxiong Luo, Fan Yang, Renshou Wu, Fangyuan Li, and Xiaoxin Chen.
Edgeinfinite: A memory-efficient infinite-context transformer for edge devices, arXiv preprint
[arXiv:2503.22196, 2025. URL https://arxiv.org/abs/2503.22196v1.](https://arxiv.org/abs/2503.22196v1)


[143] Justin Chih-Yao Chen, Archiki Prasad, Swarnadeep Saha, Elias Stengel-Eskin, and Mohit Bansal. Magicore: Multi-agent, iterative, coarse-to-fine refinement for reasoning, arXiv preprint arXiv:2409.12147,
[2024. URL https://arxiv.org/abs/2409.12147v1.](https://arxiv.org/abs/2409.12147v1)


[144] Mingyang Chen, Haoze Sun, Tianpeng Li, Fan Yang, Hao Liang, Keer Lu, Bin Cui, Wentao Zhang,
Zenan Zhou, and Weipeng Chen. Facilitating multi-turn function calling for llms via compositional
instruction tuning. _International Conference on Learning Representations_, 2024.


[145] Nuo Chen, Yuhan Li, Jianheng Tang, and Jia Li. Graphwiz: An instruction-following language model
for graph computational problems. In _Proceedings of the 30th ACM SIGKDD Conference on Knowledge_
_Discovery and Data Mining_, pages 353–364, 2024.


[146] Nuo Chen, Zhiyuan Hu, Qingyun Zou, Jiaying Wu, Qian Wang, Bryan Hooi, and Bingsheng He.
[Judgelrm: Large reasoning models as a judge, arXiv preprint arXiv:2504.00050, 2025. URL https:](https://arxiv.org/abs/2504.00050v1)
[//arxiv.org/abs/2504.00050v1.](https://arxiv.org/abs/2504.00050v1)


[147] Qiguang Chen, Libo Qin, Jiaqi Wang, Jinxuan Zhou, and Wanxiang Che. Unlocking the capabilities
of thought: A reasoning boundary framework to quantify and optimize chain-of-thought, arXiv
[preprint arXiv:2410.05695, 2024. URL https://arxiv.org/abs/2410.05695.](https://arxiv.org/abs/2410.05695)


[148] Qiguang Chen, Libo Qin, Jinhao Liu, Dengyun Peng, Jiannan Guan, Peng Wang, Mengkang Hu,
Yuhang Zhou, Te Gao, and Wangxiang Che. Towards reasoning era: A survey of long chainof-thought for reasoning large language models, arXiv preprint arXiv:2503.09567, 2025. URL
[https://arxiv.org/abs/2503.09567v3.](https://arxiv.org/abs/2503.09567v3)


[149] Qiguang Chen, Libo Qin, Jinhao Liu, Dengyun Peng, Jiannan Guan, Peng Wang, Mengkang Hu,
Yuhang Zhou, Te Gao, and Wanxiang Che. Towards reasoning era: A survey of long chain-of[thought for reasoning large language models, arXiv preprint arXiv:2503.09567, 2025. URL https:](https://arxiv.org/abs/2503.09567)
[//arxiv.org/abs/2503.09567.](https://arxiv.org/abs/2503.09567)


[150] Qiguang Chen, Libo Qin, Jinhao Liu, Dengyun Peng, Jiaqi Wang, Mengkang Hu, Zhi Chen, Wanxiang
Che, and Ting Liu. Ecm: A unified electronic circuit model for explaining the emergence of in-context
learning and chain-of-thought in large language model, arXiv preprint arXiv:2502.03325, 2025.
[URL https://arxiv.org/abs/2502.03325.](https://arxiv.org/abs/2502.03325)


[151] Qiguang Chen, Mingda Yang, Libo Qin, Jinhao Liu, Zheng Yan, Jiannan Guan, Dengyun Peng, Yiyan
Ji, Hanjing Li, Mengkang Hu, Yimeng Zhang, Yihao Liang, Yuhang Zhou, Jiaqi Wang, Zhi Chen, and
Wanxiang Che. Ai4research: A survey of artificial intelligence for scientific research, arXiv preprint
[arXiv:2507.01903, 2025. URL https://arxiv.org/abs/2507.01903.](https://arxiv.org/abs/2507.01903)


70


[152] S Chen, Y Wang, YF Wu, and Q Chen.... Advancing tool-augmented large
language models: Integrating insights from errors in inference trees. 2024.
URL [https://proceedings.neurips.cc/paper_files/paper/2024/hash/](https://proceedings.neurips.cc/paper_files/paper/2024/hash/c0f7ee1901fef1da4dae2b88dfd43195-Abstract-Conference.html)
[c0f7ee1901fef1da4dae2b88dfd43195-Abstract-Conference.html.](https://proceedings.neurips.cc/paper_files/paper/2024/hash/c0f7ee1901fef1da4dae2b88dfd43195-Abstract-Conference.html)


[153] Shouyuan Chen, Sherman Wong, Liangjian Chen, and Yuandong Tian. Extending context window
of large language models via positional interpolation, arXiv preprint arXiv:2306.15595, 2023. URL
[https://arxiv.org/abs/2306.15595v2.](https://arxiv.org/abs/2306.15595v2)


[154] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey E. Hinton. A simple framework for
contrastive learning of visual representations. _International Conference on Machine Learning_, 2020.


[155] Wenhu Chen, Xueguang Ma, Xinyi Wang, and William W. Cohen. Program of thoughts prompting:
Disentangling computation from reasoning for numerical reasoning tasks. _Trans. Mach. Learn. Res._,
2022.


[156] Yanda Chen, Ruiqi Zhong, Sheng Zha, G. Karypis, and He He. Meta-learning via language model
in-context tuning. _Annual Meeting of the Association for Computational Linguistics_, 2021.
