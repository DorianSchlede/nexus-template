<!-- Source: 02-ContextSurvey-2507.13334.pdf | Chunk 21/26 -->

[2020. URL https://arxiv.org/abs/2005.00048v1.](https://arxiv.org/abs/2005.00048v1)


[932] G. Santos, Rita Maria Silva Julia, and Marcelo Zanchetta do Nascimento. Diverse prompts: Illuminating the prompt space of large language models with map-elites. _IEEE Congress on Evolutionary_
_Computation_, 2025.


[933] Ranjan Sapkota, Konstantinos I. Roumeliotis, and Manoj Karkee. Ai agents vs. agentic ai: A
conceptual taxonomy, applications and challenges, arXiv preprint arXiv:2505.10468, 2025. URL
[https://arxiv.org/abs/2505.10468v4.](https://arxiv.org/abs/2505.10468v4)


[934] Anjana Sarkar and Soumyendu Sarkar. Survey of llm agent communication with mcp: A software
[design pattern centric review, arXiv preprint arXiv:2506.05364, 2025. URL https://arxiv.org/](https://arxiv.org/abs/2506.05364v1)
[abs/2506.05364v1.](https://arxiv.org/abs/2506.05364v1)


[935] Soumajyoti Sarkar and Leonard Lausen. Testing the limits of unified sequence to sequence llm
[pretraining on diverse table data tasks, arXiv preprint arXiv:2310.00789, 2023. URL https:](https://arxiv.org/abs/2310.00789v1)
[//arxiv.org/abs/2310.00789v1.](https://arxiv.org/abs/2310.00789v1)


[936] Parth Sarthi, Salman Abdullah, Aditi Tuli, Shubh Khanna, Anna Goldie, and Christopher D. Manning.
Raptor: Recursive abstractive processing for tree-organized retrieval. _International Conference on_
_Learning Representations_, 2024.


[937] Gabriele Sarti. Umberto-mtsa @ accompl-it: Improving complexity and acceptability prediction
with multi-task learning on self-supervised annotations (short paper). _International Workshop on_
_Evaluation of Natural Language and Speech Tools for Italian_, 2020.


[938] Apoorv Saxena, Adrian Kochsiek, and Rainer Gemulla. Sequence-to-sequence knowledge graph
completion and question answering. _Annual Meeting of the Association for Computational Linguistics_,
2022.


[939] Timo Schick, Jane Dwivedi-Yu, Roberto Dessì, R. Raileanu, M. Lomeli, Luke Zettlemoyer, Nicola
Cancedda, and Thomas Scialom. Toolformer: Language models can teach themselves to use tools.
_Neural Information Processing Systems_, 2023.


[940] Guido Schillaci, Uwe Schmidt, and Luis Miranda. Prediction error-driven memory consolidation for
continual learning: On the case of adaptive greenhouse models. _KI - Künstliche Intelligenz_, 35(1):
[71–80, 2021. ISSN 1610-1987. doi: 10.1007/s13218-020-00700-8. URL http://dx.doi.org/](http://dx.doi.org/10.1007/s13218-020-00700-8)
[10.1007/s13218-020-00700-8.](http://dx.doi.org/10.1007/s13218-020-00700-8)


129


[941] Florian Schneider, Narges Baba Ahmadi, Niloufar Baba Ahmadi, Iris Vogel, Martin Semmann, and
Christian Biemann. Collex - a multimodal agentic rag system enabling interactive exploration of
scientific collections. arXiv preprint, 2025.


[942] Sheila Schoepp, Masoud Jafaripour, Yingyue Cao, Tianpei Yang, Fatemeh Abdollahi, Shadan Golestan,
Zahin Sufiyan, Osmar R. Zaiane, and Matthew E. Taylor. The evolving landscape of llm- and vlmintegrated reinforcement learning. arXiv preprint, 2025.


[943] Lianlei Shan, Shixian Luo, Zezhou Zhu, Yu Yuan, and Yong Wu. Cognitive memory in large
[language models, arXiv preprint arXiv:2504.02441, 2025. URL https://arxiv.org/abs/2504.](https://arxiv.org/abs/2504.02441v2)
[02441v2.](https://arxiv.org/abs/2504.02441v2)


[944] Wenbo Shang and Xin Huang. A survey of large language models on generative graph analytics:
[Query, learning, and applications, arXiv preprint arXiv:2404.14809v2, 2024. URL https://arxiv.](https://arxiv.org/abs/2404.14809v2)
[org/abs/2404.14809v2.](https://arxiv.org/abs/2404.14809v2)


[945] Yunfan Shao, Linyang Li, Junqi Dai, and Xipeng Qiu. Character-llm: A trainable agent for role-playing,
[arXiv preprint arXiv:2310.10158, 2023. URL https://arxiv.org/abs/2310.10158.](https://arxiv.org/abs/2310.10158)


[946] Yutong Shao and N. Nakashole. On linearizing structured data in encoder-decoder language models:
Insights from text-to-sql. _North American Chapter of the Association for Computational Linguistics_,
2024.


[947] Zhihong Shao, Yeyun Gong, Yelong Shen, Minlie Huang, Nan Duan, and Weizhu Chen. Synthetic
prompting: Generating chain-of-thought demonstrations for large language models. _International_
_Conference on Machine Learning_, 2023.


[948] Peter Shaw, Mandar Joshi, James Cohan, Jonathan Berant, Panupong Pasupat, Hexiang Hu, Urvashi
Khandelwal, Kenton Lee, and Kristina Toutanova. From pixels to ui actions: Learning to follow
instructions via graphical user interfaces. _Neural Information Processing Systems_, 2023.


[949] Jonathan Shen, Ruoming Pang, Ron J. Weiss, M. Schuster, N. Jaitly, Zongheng Yang, Z. Chen,
Yu Zhang, Yuxuan Wang, R. Skerry-Ryan, R. Saurous, Yannis Agiomyrgiannakis, and Yonghui Wu.
Natural tts synthesis by conditioning wavenet on mel spectrogram predictions. _IEEE International_
_Conference on Acoustics, Speech, and Signal Processing_, 2017.


[950] Junhong Shen, Atishay Jain, Zedian Xiao, Ishan Amlekar, Mouad Hadji, Aaron Podolny, and Ameet
Talwalkar. Scribeagent: Towards specialized web agents using production-scale workflow data.
2024.


[951] Junhong Shen, Hao Bai, Lunjun Zhang, Yifei Zhou, Amrith Setlur, Shengbang Tong, Diego Caples,
Nan Jiang, Tong Zhang, Ameet Talwalkar, and Aviral Kumar. Thinking vs. doing: Agents that reason
[by scaling test-time interaction, arXiv preprint arXiv:2506.07976, 2025. URL https://arxiv.](https://arxiv.org/abs/2506.07976)
[org/abs/2506.07976.](https://arxiv.org/abs/2506.07976)


[952] Weizhou Shen, Chenliang Li, Fanqi Wan, Shengyi Liao, Shaopeng Lai, Bo Zhang, Yingcheng Shi,
Yuning Wu, Gang Fu, Zhansheng Li, Bin Yang, Ji Zhang, Fei Huang, Jingren Zhou, and Ming Yan.
Qwenlong-cprs: Towards ∞-llms with dynamic context optimization. arXiv preprint, 2025.


[953] Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li, Weiming Lu, and Y. Zhuang. Hugginggpt:
Solving ai tasks with chatgpt and its friends in hugging face. _Neural Information Processing Systems_,
2023.


130


[[954] Zhuocheng Shen. Llm with tools: A survey, arXiv preprint arXiv:2409.18807, 2024. URL https:](https://arxiv.org/abs/2409.18807v1)
[//arxiv.org/abs/2409.18807v1.](https://arxiv.org/abs/2409.18807v1)


[955] Ying Sheng, Lianmin Zheng, Binhang Yuan, Zhuohan Li, Max Ryabinin, Daniel Y. Fu, Zhiqiang Xie,
Beidi Chen, Clark W. Barrett, Joseph Gonzalez, Percy Liang, Christopher Ré, Ion Stoica, and Ce Zhang.
High-throughput generative inference of large language models with a single gpu. _International_
_Conference on Machine Learning_, 2023.


[956] Dingfeng Shi, Jingyi Cao, Qianben Chen, Weichen Sun, Weizhen Li, Hongxuan Lu, Fangchen Dong,
Tianrui Qin, King Zhu, Minghao Liu, Jian Yang, Ge Zhang, Jiaheng Liu, Changwang Zhang, Jun
Wang, Y. Jiang, and Wangchunshu Zhou. Taskcraft: Automated generation of agentic tasks, arXiv
[preprint arXiv:2506.10055, 2025. URL https://arxiv.org/abs/2506.10055v2.](https://arxiv.org/abs/2506.10055v2)


[957] Han Shi, Jiahui Gao, Xiaozhe Ren, Hang Xu, Xiaodan Liang, Zhenguo Li, and J. Kwok. Sparsebert:
Rethinking the importance analysis in self-attention. _International Conference on Machine Learning_,
2021.


[958] Peng Shi, Patrick Ng, Zhiguo Wang, Henghui Zhu, Alexander Hanbo Li, Jun Wang, C. D. Santos, and
Bing Xiang. Learning contextual representations for semantic parsing with generation-augmented
pre-training. _AAAI Conference on Artificial Intelligence_, 2020.


[959] Weijia Shi, Xiaochuang Han, M. Lewis, Yulia Tsvetkov, Luke Zettlemoyer, and S. Yih. Trusting your
evidence: Hallucinate less with context-aware decoding. _North American Chapter of the Association_
_for Computational Linguistics_, 2023.


[960] Zhengliang Shi, Shen Gao, Xiuyi Chen, Yue Feng, Lingyong Yan, Haibo Shi, Dawei Yin, Zhumin
Chen, Suzan Verberne, and Zhaochun Ren. Tool learning in the wild: Empowering language models
as automatic tool agents. _The Web Conference_, 2024.


[961] Jay Shim, Grant Kruttschnitt, Alyssa Ma, Daniel Kim, Benjamin Chek, Athul Anand, Kevin Zhu,
and Sean O’Brien. Chain-of-thought augmentation with logit contrast for enhanced reasoning in
[language models, arXiv preprint arXiv:2407.03600, 2024. URL https://arxiv.org/abs/2407.](https://arxiv.org/abs/2407.03600v2)
[03600v2.](https://arxiv.org/abs/2407.03600v2)


[962] Jiho Shin, Reem Aleithan, Hadi Hemmati, and Song Wang. Retrieval-augmented test generation:
[How far are we?, arXiv preprint arXiv:2409.12682, 2024. URL https://arxiv.org/abs/2409.](https://arxiv.org/abs/2409.12682v1)
[12682v1.](https://arxiv.org/abs/2409.12682v1)


[963] Seongjin Shin, Sang-Woo Lee, Hwijeen Ahn, Sungdong Kim, Hyoungseok Kim, Boseop Kim,
Kyunghyun Cho, Gichang Lee, W. Park, Jung-Woo Ha, and Nako Sung. On the effect of pretraining corpora on in-context learning by a large-scale language model. _North American Chapter of_
_the Association for Computational Linguistics_, 2022.


[964] Noah Shinn, Federico Cassano, Beck Labash, A. Gopinath, Karthik Narasimhan, and Shunyu Yao.
Reflexion: language agents with verbal reinforcement learning. _Neural Information Processing_
_Systems_, 2023.


[965] Fatemeh Shiri, Xiao-Yu Guo, Mona Far, Xin Yu, Reza Haf, and Yuan-Fang Li. An empirical analysis
on spatial reasoning capabilities of large multimodal models. _Conference on Empirical Methods in_
_Natural Language Processing_, 2024.


131


[966] Masoud Shokrnezhad, Hao Yu, T. Taleb, Renwei Li, Kyunghan Lee, Jaeseung Song, and Cedric
Westphal. Toward a dynamic future with adaptable computing and network convergence (acnc).
_IEEE Network_, 2024.


[967] Connor Shorten, T. Khoshgoftaar, and B. Furht. Text data augmentation for deep learning. _Journal_
_of Big Data_, 2021.


[968] Mohit Shridhar, Jesse Thomason, Daniel Gordon, Yonatan Bisk, Winson Han, Roozbeh Mottaghi,
Luke Zettlemoyer, and D. Fox. Alfred: A benchmark for interpreting grounded instructions for
everyday tasks. _Computer Vision and Pattern Recognition_, 2019.


[969] Altun Shukurlu. Improving deep knowledge tracing via gated architectures and adaptive optimization,
[arXiv preprint arXiv:2504.20070, 2025. URL https://arxiv.org/abs/2504.20070v1.](https://arxiv.org/abs/2504.20070v1)


[970] Lynn L Siegel and M. Kahana. A retrieved context account of spacing and repetition effects in free
recall. _Journal of Experimental Psychology. Learning, Memory and Cognition_, 2014.


[971] Aditi Singh, Abul Ehtesham, Gaurav Kumar Gupta, Nikhil Kumar Chatta, Saket Kumar, and T. T.
Khoei. Exploring prompt engineering: A systematic review with swot analysis, arXiv preprint
[arXiv:2410.12843, 2024. URL https://arxiv.org/abs/2410.12843v1.](https://arxiv.org/abs/2410.12843v1)


[972] Anmolika Singh and Yuhang Diao. Leveraging large language models for optimized item categorization using unspsc taxonomy. _International Journal on Cybernetics & Informatics_, 2024.


[973] Joykirat Singh, Raghav Magazine, Yash Pandya, and A. Nambi. Agentic reasoning and tool integration
[for llms via reinforcement learning, arXiv preprint arXiv:2505.01441, 2025. URL https://arxiv.](https://arxiv.org/abs/2505.01441v1)
[org/abs/2505.01441v1.](https://arxiv.org/abs/2505.01441v1)


[974] Krishnakant Singh, Thanush Navaratnam, Jannik Holmer, Simone Schaub-Meyer, and Stefan Roth.
Is synthetic data all we need? benchmarking the robustness of models trained with synthetic images.
_2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)_, 2024.


[975] Ramneet Singh, Sathvik Joel, Abhav Mehrotra, Nalin Wadhwa, Ramakrishna Bairi, Aditya Kanade,
and Nagarajan Natarajan. Code researcher: Deep research agent for large systems code and
[commit history, arXiv preprint arXiv:2506.11060, 2025. URL https://arxiv.org/abs/2506.](https://arxiv.org/abs/2506.11060v1)
[11060v1.](https://arxiv.org/abs/2506.11060v1)


[976] Aarush Sinha and CU Omkumar. Gmlm: Bridging graph neural networks and language models for
[heterophilic node classification, arXiv preprint arXiv:2503.05763, 2025. URL https://arxiv.](https://arxiv.org/abs/2503.05763v3)
[org/abs/2503.05763v3.](https://arxiv.org/abs/2503.05763v3)


[977] Sanchit Sinha, Yuguang Yue, Victor Soto, Mayank Kulkarni, Jianhua Lu, and Aidong Zhang. Mamlen-llm: Model agnostic meta-training of llms for improved in-context learning. _Knowledge Discovery_
_and Data Mining_, 2024.


[978] Colin Sisate, Alistair Goldfinch, Vincent Waterstone, Sebastian Kingsley, and Mariana Blackthorn. Contextually entangled gradient mapping for optimized llm comprehension, arXiv preprint
[arXiv:2502.00048, 2025. URL https://arxiv.org/abs/2502.00048v1.](https://arxiv.org/abs/2502.00048v1)


[979] Paloma Sodhi, S. R. K. Branavan, Yoav Artzi, and Ryan McDonald. Step: Stacked llm policies for web
[actions, arXiv preprint arXiv:2310.03720, 2024. URL https://arxiv.org/abs/2310.03720.](https://arxiv.org/abs/2310.03720)


132


[980] Manthankumar Solanki. Efficient document retrieval with g-retriever. arXiv preprint, 2025.


[981] Karthik Soman, Peter W Rose, John H Morris, Rabia E Akbas, Brett Smith, Braian Peetoom, Catalina
Villouta-Reyes, G. Cerono, Yongmei Shi, Angela Rizk-Jackson, Sharat Israni, Charlotte A. Nelson,
Sui Huang, and Sergio Baranzini. Biomedical knowledge graph-optimized prompt generation for
large language models. _Bioinformatics_, 2023.


[982] Lilian Some, Wenli Yang, Michael Bain, and Byeong Kang. A comprehensive survey on integrating
large language models with knowledge-based methods. _Knowledge-Based Systems_, 2025.


[983] Chan Hee Song, Jiaman Wu, Clay Washington, Brian M. Sadler, Wei-Lun Chao, and Yu Su. Llmplanner: Few-shot grounded planning for embodied agents with large language models. _IEEE_
_International Conference on Computer Vision_, 2022.


[984] Huatong Song, Jinhao Jiang, Yingqian Min, Jie Chen, Zhipeng Chen, Wayne Xin Zhao, Lei Fang, and
Ji-Rong Wen. R1-searcher: Incentivizing the search capability in llms via reinforcement learning.
arXiv preprint, 2025.


[985] Woomin Song, Seunghyuk Oh, Sangwoo Mo, Jaehyung Kim, Sukmin Yun, Jung-Woo Ha, and
Jinwoo Shin. Hierarchical context merging: Better long context understanding for pre-trained llms.
_International Conference on Learning Representations_, 2024.


[986] Woomin Song, Sai Muralidhar Jayanthi, S. Ronanki, Kanthashree Mysore Sathyendra, Jinwoo Shin,
A. Galstyan, Shubham Katiyar, and S. Bodapati. Compress, gather, and recompute: Reforming
[long-context processing in transformers, arXiv preprint arXiv:2506.01215, 2025. URL https:](https://arxiv.org/abs/2506.01215v1)
[//arxiv.org/abs/2506.01215v1.](https://arxiv.org/abs/2506.01215v1)


[987] Yewei Song, Xunzhu Tang, Cedric Lothritz, Saad Ezzini, Jacques Klein, Tegawend’e F. Bissyand’e,
A. Boytsov, Ulrick Ble, and Anne Goujon. Callnavi, a challenge and empirical study on llm function
[calling and routing, arXiv preprint arXiv:2501.05255, 2025. URL https://arxiv.org/abs/](https://arxiv.org/abs/2501.05255v2)
[2501.05255v2.](https://arxiv.org/abs/2501.05255v2)


[988] Yueqi Song, Frank Xu, Shuyan Zhou, and Graham Neubig. Beyond browsing: Api-based web agents,
[arXiv preprint arXiv:2410.16464, 2025. URL https://arxiv.org/abs/2410.16464.](https://arxiv.org/abs/2410.16464)


[989] S. Srinivasa and Jayati Deshmukh. Paradigms of computational agency. _Novel Approaches to Informa-_
_tion Systems Design_, 2021.


[990] B. Staresina, R. Henson, N. Kriegeskorte, and Arjen Alink. Episodic reinstatement in the medial
temporal lobe. _Journal of Neuroscience_, 2012.


[991] T. Staudigl, C. Vollmar, S. Noachtar, and S. Hanslmayr. Temporal-pattern similarity analysis reveals
the beneficial and detrimental effects of context reinstatement on human memory. _Journal of_
_Neuroscience_, 2015.


[992] Kaya Stechly, Karthik Valmeekam, and Subbarao Kambhampati. Chain of thoughtlessness? an
analysis of cot in planning. _Neural Information Processing Systems_, 2024.


[993] R. Sterken and James Ravi Kirkpatrick. Conversational alignment with artificial intelligence in
context. _Philosophical Perspectives_, 2025.


133


[994] Paul Stoewer, Achim Schilling, Andreas K. Maier, and Patrick Krauss. Multi-modal cognitive maps
based on neural networks trained on successor representations, arXiv preprint arXiv:2401.01364,
[2023. URL https://arxiv.org/abs/2401.01364v1.](https://arxiv.org/abs/2401.01364v1)


[995] Olly Styles, Sam Miller, Patricio Cerda-Mardini, T. Guha, Victor Sanchez, and Bertie Vidgen.
Workbench: a benchmark dataset for agents in a realistic workplace setting, arXiv preprint
[arXiv:2405.00823, 2024. URL https://arxiv.org/abs/2405.00823v2.](https://arxiv.org/abs/2405.00823v2)


[996] Guangxin Su, Yifan Zhu, Wenjie Zhang, Hanchen Wang, and Ying Zhang. Bridging large language
models and graph structure learning models for robust representation learning, arXiv preprint
[arXiv:2410.12096, 2024. URL https://arxiv.org/abs/2410.12096v1.](https://arxiv.org/abs/2410.12096v1)


[997] Hong Su, Elke A. Rundensteiner, and Murali Mani. Automaton in or out: run-time plan optimization
for xml stream processing. _International Symposium on Signal Processing Systems_, 2008.


[998] Hongjin Su, Ruoxi Sun, Jinsung Yoon, Pengcheng Yin, Tao Yu, and Sercan Ö. Arık. Learn-by-interact:
A data-centric framework for self-adaptive agents in realistic environments. 2025.


[999] Jinyan Su, Jennifer Healey, Preslav Nakov, and Claire Cardie. Between underthinking and overthinking: An empirical study of reasoning length and correctness in llms, arXiv preprint arXiv:2505.00127,
[2025. URL https://arxiv.org/abs/2505.00127v1.](https://arxiv.org/abs/2505.00127v1)


[1000] Weihang Su, Yichen Tang, Qingyao Ai, Zhijing Wu, and Yiqun Liu. Dragin: Dynamic retrieval
augmented generation based on the real-time information needs of large language models. _Annual_
_Meeting of the Association for Computational Linguistics_, 2024.


[1001] Xin Su, Man Luo, Kris W Pan, Tien Pei Chou, Vasudev Lal, and Phillip Howard. Sk-vqa: Synthetic
knowledge generation at scale for training context-augmented multimodal llms. arXiv preprint,
2024.


[1002] Budhitama Subagdja and A. Tan. Neural modeling of sequential inferences and learning over episodic
memory. _Neurocomputing_, 2015.


[1003] Yang Sui, Yu-Neng Chuang, Guanchu Wang, Jiamu Zhang, Tianyi Zhang, Jiayi Yuan, Hongyi
Liu, Andrew Wen, Shaochen Zhong, Hanjie Chen, and Xia Hu. Stop overthinking: A survey on
[efficient reasoning for large language models, arXiv preprint arXiv:2503.16419, 2025. URL https:](https://arxiv.org/abs/2503.16419v3)
[//arxiv.org/abs/2503.16419v3.](https://arxiv.org/abs/2503.16419v3)


[1004] Chuanneng Sun, Songjun Huang, and D. Pompili. Llm-based multi-agent reinforcement learning:
[Current and future directions, arXiv preprint arXiv:2405.11106, 2024. URL https://arxiv.org/](https://arxiv.org/abs/2405.11106v1)
[abs/2405.11106v1.](https://arxiv.org/abs/2405.11106v1)


[1005] Hanshi Sun, Zhuoming Chen, Xinyu Yang, Yuandong Tian, and Beidi Chen. Triforce: Lossless
acceleration of long sequence generation with hierarchical speculative decoding, arXiv preprint
[arXiv:2404.11912, 2024. URL https://arxiv.org/abs/2404.11912v3.](https://arxiv.org/abs/2404.11912v3)


[1006] Haotian Sun, Yuchen Zhuang, Lingkai Kong, Bo Dai, and Chao Zhang. Adaplanner: Adaptive
planning from feedback with language models. _Neural Information Processing Systems_, 2023.


[1007] Jiankai Sun, Chuanyang Zheng, E. Xie, Zhengying Liu, Ruihang Chu, Jianing Qiu, Jiaqi Xu, Mingyu
Ding, Hongyang Li, Mengzhe Geng, Yue Wu, Wenhai Wang, Junsong Chen, Zhangyue Yin, Xiaozhe


134


Ren, Jie Fu, Junxian He, Wu Yuan, Qi Liu, Xihui Liu, Yu Li, Hao Dong, Yu Cheng, Ming Zhang,
P. Heng, Jifeng Dai, Ping Luo, Jingdong Wang, Jingwei Wen, Xipeng Qiu, Yi-Chen Guo, Hui Xiong,
Qun Liu, and Zhenguo Li. A survey of reasoning with foundation models: Concepts, methodologies,
and outlook. _ACM Computing Surveys_, 2023.


[1008] Jiashuo Sun, Chengjin Xu, Lumingyuan Tang, Sai Wang, Chen Lin, Yeyun Gong, H. Shum, and Jian
Guo. Think-on-graph: Deep and responsible reasoning of large language model with knowledge
graph. arXiv preprint, 2023.


[1009] Lei Sun, Zhengwei Tao, Youdi Li, and Hiroshi Arakawa. Oda: Observation-driven agent for integrating
[llms and knowledge graphs, arXiv preprint arXiv:2404.07677, 2024. URL https://arxiv.org/](https://arxiv.org/abs/2404.07677)
[abs/2404.07677.](https://arxiv.org/abs/2404.07677)


[1010] Lei Sun, Xinchen Wang, and Youdi Li. Pyramid-driven alignment: Pyramid principle guided integration of large language models and knowledge graphs, arXiv preprint arXiv:2410.12298, 2024. URL
[https://arxiv.org/abs/2410.12298v2.](https://arxiv.org/abs/2410.12298v2)


[1011] Liangtai Sun, Xingyu Chen, Lu Chen, Tianle Dai, Zichen Zhu, and Kai Yu. Meta-gui: Towards multimodal conversational agents on mobile gui. _Conference on Empirical Methods in Natural Language_
_Processing_, 2022.


[1012] Lijun Sun, Yijun Yang, Qiqi Duan, Yuhui Shi, Chao Lyu, Yu-Cheng Chang, Chin-Teng Lin, and
Yang Shen. Multi-agent coordination across diverse applications: A survey, arXiv preprint
[arXiv:2502.14743, 2025. URL https://arxiv.org/abs/2502.14743v2.](https://arxiv.org/abs/2502.14743v2)


[1013] Qianru Sun, Yaoyao Liu, Tat-Seng Chua, and B. Schiele. Meta-transfer learning for few-shot learning.
_Computer Vision and Pattern Recognition_, 2018.


[1014] Yu Sun, Shuohuan Wang, Yukun Li, Shikun Feng, Hao Tian, Hua Wu, and Haifeng Wang. Ernie
2.0: A continual pre-training framework for language understanding. _AAAI Conference on Artificial_
_Intelligence_, 2019.


[1015] Rao Surapaneni, Miku Jha, Michael Vakoc, and Todd Segal. Announcing the
agent2agent protocol (a2a). [https://developers.googleblog.com/en/](https://developers.googleblog.com/en/a2a-a-new-era-of-agent-interoperability/)
[a2a-a-new-era-of-agent-interoperability/, April 2025.](https://developers.googleblog.com/en/a2a-a-new-era-of-agent-interoperability/) [Online; accessed 17July-2025].


[1016] Stefan Szeider. Mcp-solver: Integrating language models with constraint programming systems.
arXiv preprint, 2024.


[1017] Daniel Szelogowski. Engram memory encoding and retrieval: A neurocomputational perspective,
[arXiv preprint arXiv:2506.01659, 2025. URL https://arxiv.org/abs/2506.01659v1.](https://arxiv.org/abs/2506.01659v1)


[1018] N. Taatgen, David Huss, D. Dickison, and John R. Anderson. The acquisition of robust and flexible
cognitive skills. _Journal of experimental psychology. General_, 2008.


[1019] Jihoon Tack, Jaehyung Kim, Eric Mitchell, Jinwoo Shin, Yee Whye Teh, and Jonathan Richard
Schwarz. Online adaptation of language models with a memory of amortized contexts. _Neural_
_Information Processing Systems_, 2024.


[1020] Yan Tai, Weichen Fan, Zhao Zhang, Feng Zhu, Rui Zhao, and Ziwei Liu. Link-context learning for
multimodal llms. _Computer Vision and Pattern Recognition_, 2023.


135


[1021] Fahim Tajwar, Yiding Jiang, Abitha Thankaraj, Sumaita Sadia Rahman, J. Z. Kolter, Jeff Schneider,
and Ruslan Salakhutdinov. Training a generally curious agent, arXiv preprint arXiv:2502.17543,
[2025. URL https://arxiv.org/abs/2502.17543v3.](https://arxiv.org/abs/2502.17543v3)


[1022] K. Tallam. From autonomous agents to integrated systems, a new paradigm: Orchestrated distributed
[intelligence, arXiv preprint arXiv:2503.13754, 2025. URL https://arxiv.org/abs/2503.](https://arxiv.org/abs/2503.13754v2)
[13754v2.](https://arxiv.org/abs/2503.13754v2)


[1023] A. Tan, Budhitama Subagdja, Di Wang, and Lei Meng. Self-organizing neural networks for universal
learning and multimodal memory encoding. _Neural Networks_, 2019.


[1024] Chuanyuan Tan, Yuehe Chen, Wenbiao Shao, and Wenliang Chen. Make a choice! knowledge
base question answering with in-context learning, arXiv preprint arXiv:2305.13972, 2023. URL
[https://arxiv.org/abs/2305.13972v1.](https://arxiv.org/abs/2305.13972v1)


[1025] Sijun Tan, Xiuyu Li, Shishir G. Patil, Ziyang Wu, Tianjun Zhang, Kurt Keutzer, Joseph E. Gonzalez,
and Raluca A. Popa. Lloco: Learning long contexts offline. _Conference on Empirical Methods in_
_Natural Language Processing_, 2024.


[1026] Xiaoyu Tan, Haoyu Wang, Xihe Qiu, Yuan Cheng, Yinghui Xu, Wei Chu, and Yuan Qi. Struct-x:
Enhancing large language models reasoning with structured data. arXiv preprint, 2024.


[1027] Zhaoxuan Tan and Meng Jiang. User modeling in the era of large language models: Current research
and future directions. _IEEE Data Engineering Bulletin_, 2023.


[1028] Zhijie Tan, Xu Chu, Weiping Li, and Tong Mo. Order matters: Exploring order sensitivity in
[multimodal large language models, arXiv preprint arXiv:2410.16983v1, 2024. URL https://](https://arxiv.org/abs/2410.16983v1)
[arxiv.org/abs/2410.16983v1.](https://arxiv.org/abs/2410.16983v1)


[1029] Matthew Tancik, Pratul P. Srinivasan, B. Mildenhall, Sara Fridovich-Keil, N. Raghavan, Utkarsh
Singhal, R. Ramamoorthi, J. Barron, and Ren Ng. Fourier features let networks learn high frequency
functions in low dimensional domains. _Neural Information Processing Systems_, 2020.


[1030] Fei Tang, Haolei Xu, Hang Zhang, Siqi Chen, Xingyu Wu, Yongliang Shen, Wenqi Zhang, Guiyang
Hou, Zeqi Tan, Yuchen Yan, Kaitao Song, Jian Shao, Weiming Lu, Jun Xiao, and Yueting Zhuang. A
[survey on (m)llm-based gui agents, arXiv preprint arXiv:2504.13865, 2025. URL https://arxiv.](https://arxiv.org/abs/2504.13865v2)
[org/abs/2504.13865v2.](https://arxiv.org/abs/2504.13865v2)

