<!-- Source: 02-ContextSurvey-2507.13334.pdf | Chunk 26/26 -->

[where, and how well?, arXiv preprint arXiv:2503.24235, 2025. URL https://arxiv.org/abs/](https://arxiv.org/abs/2503.24235v3)
[2503.24235v3.](https://arxiv.org/abs/2503.24235v3)


[1323] Ruichen Zhang, Mufan Qiu, Zhen Tan, Mohan Zhang, Vincent Lu, Jie Peng, Kaidi Xu, Leandro Z.
Agudelo, Peter Qian, and Tianlong Chen. Symbiotic cooperation for web agents: Harnessing
complementary strengths of large and small llms, arXiv preprint arXiv:2502.07942, 2025. URL
[https://arxiv.org/abs/2502.07942v2.](https://arxiv.org/abs/2502.07942v2)


[1324] Tengchao Zhang, Yonglin Tian, Fei Lin, Jun Huang, Patrik P. Süli, Rui Qin, and Fei-Yue Wang.
Coordfield: Coordination field for agentic uav task allocation in low-altitude urban scenarios, arXiv
[preprint arXiv:2505.00091, 2025. URL https://arxiv.org/abs/2505.00091v3.](https://arxiv.org/abs/2505.00091v3)


[1325] Weizhi Zhang, Xinyang Zhang, Chenwei Zhang, Liangwei Yang, Jingbo Shang, Zhepei Wei,
Henry Peng Zou, Zijie Huang, Zhengyang Wang, Yifan Gao, Xiaoman Pan, Lian Xiong, Jingguo
Liu, Philip S. Yu, and Xian Li. Personaagent: When large language model agents meet personaliza[tion at test time, arXiv preprint arXiv:2506.06254, 2025. URL https://arxiv.org/abs/2506.](https://arxiv.org/abs/2506.06254v1)
[06254v1.](https://arxiv.org/abs/2506.06254v1)


[1326] Wen Zhang, Long Jin, Yushan Zhu, Jiaoyan Chen, Zhiwei Huang, Junjie Wang, Yin Hua, Lei Liang,
and Hua zeng Chen. Trustuqa: A trustful framework for unified structured data question answering.
_AAAI Conference on Artificial Intelligence_, 2024.


[1327] Wenlin Zhang, Xiangyang Li, Kuicai Dong, Yichao Wang, Pengyue Jia, Xiaopeng Li, Yingyi Zhang,
Derong Xu, Zhaochen Du, Huifeng Guo, Ruiming Tang, and Xiangyu Zhao. Process vs. outcome
reward: Which is better for agentic rag reinforcement learning, arXiv preprint arXiv:2505.14069,
[2025. URL https://arxiv.org/abs/2505.14069v2.](https://arxiv.org/abs/2505.14069v2)


[1328] Wentao Zhang, Ce Cui, Yilei Zhao, Rui Hu, Yang Liu, Yahui Zhou, and Bo An. Agentorchestra: A hierarchical multi-agent framework for general-purpose task solving, arXiv preprint arXiv:2506.12508,
[2025. URL https://arxiv.org/abs/2506.12508v2.](https://arxiv.org/abs/2506.12508v2)


[1329] Xiaoyi Zhang, Zhaoyang Jia, Zongyu Guo, Jiahao Li, Bin Li, Houqiang Li, and Yan Lu. Deep
video discovery: Agentic search with tool use for long-form video understanding, arXiv preprint
[arXiv:2505.18079, 2025. URL https://arxiv.org/abs/2505.18079v2.](https://arxiv.org/abs/2505.18079v2)


159


[1330] Xikun Zhang, Antoine Bosselut, Michihiro Yasunaga, Hongyu Ren, Percy Liang, Christopher D.
Manning, and J. Leskovec. Greaselm: Graph reasoning enhanced language models for question
answering. _International Conference on Learning Representations_, 2022.


[1331] Yao Zhang, Zijian Ma, Yunpu Ma, Zhen Han, Yu Wu, and Volker Tresp. Webpilot: A versatile and
autonomous multi-agent system for web task execution with strategic exploration, arXiv preprint
[arXiv:2408.15978, 2024. URL https://arxiv.org/abs/2408.15978.](https://arxiv.org/abs/2408.15978)


[1332] Yinger Zhang, Hui Cai, Yicheng Chen, Rui Sun, and Jing Zheng. Reverse chain: A generic-rule for
[llms to master multi-api planning, arXiv preprint arXiv:2310.04474, 2023. URL https://arxiv.](https://arxiv.org/abs/2310.04474v3)
[org/abs/2310.04474v3.](https://arxiv.org/abs/2310.04474v3)


[1333] Yongheng Zhang, Qiguang Chen, Jingxuan Zhou, Peng Wang, Jiasheng Si, Jin Wang, Wenpeng
Lu, and Libo Qin. Wrong-of-thought: An integrated reasoning framework with multi-perspective
[verification and wrong information, arXiv preprint arXiv:2410.04463, 2024. URL https://arxiv.](https://arxiv.org/abs/2410.04463)
[org/abs/2410.04463.](https://arxiv.org/abs/2410.04463)


[1334] Youjia Zhang, Jin Wang, Liang-Chih Yu, and Xuejie Zhang. Ma-bert: Learning representation by
incorporating multi-attribute knowledge in transformers. _Findings_, 2021.


[1335] Yu Zhang, Jinlong Ma, Yongshuai Hou, Xuefeng Bai, Kehai Chen, Yang Xiang, Jun Yu, and Min
Zhang. Evaluating and steering modality preferences in multimodal large language model, arXiv
[preprint arXiv:2505.20977v1, 2025. URL https://arxiv.org/abs/2505.20977v1.](https://arxiv.org/abs/2505.20977v1)


[1336] Yunyi Zhang, Ming Zhong, Siru Ouyang, Yizhu Jiao, Sizhe Zhou, Linyi Ding, and Jiawei Han.
Automated mining of structured knowledge from text in the era of large language models. _Knowledge_
_Discovery and Data Mining_, 2024.


[1337] Yusen Zhang, Ruoxi Sun, Yanfei Chen, Tomas Pfister, Rui Zhang, and Sercan Ö. Arik. Chain of agents:
Large language models collaborating on long-context tasks. _Neural Information Processing Systems_,
2024.


[1338] Yuxiang Zhang, Yuqi Yang, Jiangming Shu, Xinyan Wen, and Jitao Sang. Agent models: Internalizing
chain-of-action generation into reasoning models, arXiv preprint arXiv:2503.06580, 2025. URL
[https://arxiv.org/abs/2503.06580v1.](https://arxiv.org/abs/2503.06580v1)


[1339] Zeyu Zhang, Xiaohe Bo, Chen Ma, Rui Li, Xu Chen, Quanyu Dai, Jieming Zhu, Zhenhua Dong, and
Ji-Rong Wen. A survey on the memory mechanism of large language model based agents, arXiv
[preprint arXiv:2404.13501, 2024. URL https://arxiv.org/abs/2404.13501v1.](https://arxiv.org/abs/2404.13501v1)


[1340] Zeyu Zhang, Quanyu Dai, Luyu Chen, Zeren Jiang, Rui Li, Jieming Zhu, Xu Chen, Yi Xie, Zhenhua
Dong, and Ji-Rong Wen. Memsim: A bayesian simulator for evaluating memory of llm-based
[personal assistants, arXiv preprint arXiv:2409.20163, 2024. URL https://arxiv.org/abs/](https://arxiv.org/abs/2409.20163v1)
[2409.20163v1.](https://arxiv.org/abs/2409.20163v1)


[1341] Zeyu Zhang, Quanyu Dai, Xu Chen, Rui Li, Zhongyang Li, and Zhenhua Dong. Memengine: A unified
and modular library for developing advanced memory of llm-based agents. _The Web Conference_,
2025.


[1342] Zheng Zhang, Liang Ding, Dazhao Cheng, Xuebo Liu, Min Zhang, and Dacheng Tao. Bliss:
Robust sequence-to-sequence learning via self-supervised input representation, arXiv preprint
[arXiv:2204.07837, 2022. URL https://arxiv.org/abs/2204.07837v2.](https://arxiv.org/abs/2204.07837v2)


160


[1343] Zhenyu (Allen) Zhang, Ying Sheng, Tianyi Zhou, Tianlong Chen, Lianmin Zheng, Ruisi Cai, Zhao
Song, Yuandong Tian, Christopher Ré, Clark W. Barrett, Zhangyang Wang, and Beidi Chen. H2o:
Heavy-hitter oracle for efficient generative inference of large language models. _Neural Information_
_Processing Systems_, 2023.


[1344] Zhihan Zhang, Zhenwen Liang, Wenhao Yu, Dian Yu, Mengzhao Jia, Dong Yu, and Meng Jiang.
Learn beyond the answer: Training language models with reflection for mathematical reasoning.
_Conference on Empirical Methods in Natural Language Processing_, 2024.


[1345] Zhuosheng Zhang and Aston Zhang. You only look at screens: Multimodal chain-of-action agents.
_Annual Meeting of the Association for Computational Linguistics_, 2023.


[1346] Andrew Zhao, Daniel Huang, Quentin Xu, Matthieu Lin, Y. Liu, and Gao Huang. Expel: Llm agents
are experiential learners. _AAAI Conference on Artificial Intelligence_, 2023.


[1347] Andrew Zhao, Daniel Huang, Quentin Xu, Matthieu Lin, Yong-Jin Liu, and Gao Huang. Expel: Llm
[agents are experiential learners, arXiv preprint arXiv:2308.10144, 2024. URL https://arxiv.](https://arxiv.org/abs/2308.10144)
[org/abs/2308.10144.](https://arxiv.org/abs/2308.10144)


[1348] Jianan Zhao, Meng Qu, Chaozhuo Li, Hao Yan, Qian Liu, Rui Li, Xing Xie, and Jian Tang. Learning
on large-scale text-attributed graphs via variational inference. _International Conference on Learning_
_Representations_, 2022.


[1349] Penghao Zhao, Hailin Zhang, Qinhan Yu, Zhengren Wang, Yunteng Geng, Fangcheng Fu, Ling Yang,
Wentao Zhang, and Bin Cui. Retrieval-augmented generation for ai-generated content: A survey,
[arXiv preprint arXiv:2402.19473, 2024. URL https://arxiv.org/abs/2402.19473v6.](https://arxiv.org/abs/2402.19473v6)


[1350] Pengyu Zhao, Zijian Jin, and Ning Cheng. An in-depth survey of large language model-based artificial
[intelligence agents, arXiv preprint arXiv:2309.14365, 2023. URL https://arxiv.org/abs/](https://arxiv.org/abs/2309.14365v1)
[2309.14365v1.](https://arxiv.org/abs/2309.14365v1)


[1351] Pu Zhao, Xuan Shen, Zhenglun Kong, Yixin Shen, Sung-En Chang, Timothy Rupprecht, Lei Lu, Enfu
Nan, Changdi Yang, Yumei He, Xingchen Xu, Yu Huang, Wei Wang, Yue Chen, Yongchun He, and
Yanzhi Wang. 7b fully open source moxin-llm/vlm – from pretraining to grpo-based reinforcement
learning enhancement. arXiv preprint, 2024.


[1352] Qi Zhao, Hongyu Yang, Qi Song, Xinwei Yao, and Xiangyang Li. Knowpath: Knowledge-enhanced
reasoning via llm-generated inference paths over knowledge graphs, arXiv preprint arXiv:2502.12029,
[2025. URL https://arxiv.org/abs/2502.12029v3.](https://arxiv.org/abs/2502.12029v3)


[1353] Qifang Zhao, Weidong Ren, Tianyu Li, Xiaoxiao Xu, and Hong Liu. Graphgpt: Generative pre-trained
[graph eulerian transformer, arXiv preprint arXiv:2401.00529v3, 2023. URL https://arxiv.org/](https://arxiv.org/abs/2401.00529v3)
[abs/2401.00529v3.](https://arxiv.org/abs/2401.00529v3)


[1354] Ruilin Zhao, Feng Zhao, Long Wang, Xianzhi Wang, and Guandong Xu. Kg-cot: Chain-of-thought
prompting of large language models over knowledge graphs for knowledge-aware question answering.
_International Joint Conference on Artificial Intelligence_, 2024.


[1355] Shangziqi Zhao, Jiahao Yuan, Guisong Yang, and Usman Naseem. Can pruning improve reasoning? revisiting long-cot compression with capability in mind for better reasoning, arXiv preprint
[arXiv:2505.14582, 2025. URL https://arxiv.org/abs/2505.14582v1.](https://arxiv.org/abs/2505.14582v1)


161


[1356] Shitian Zhao, Zhuowan Li, Yadong Lu, Alan L. Yuille, and Yan Wang. Causal-cog: A causal-effect
look at context generation for boosting multi-modal language models. _Computer Vision and Pattern_
_Recognition_, 2023.


[1357] Tony Zhao, Eric Wallace, Shi Feng, D. Klein, and Sameer Singh. Calibrate before use: Improving
few-shot performance of language models. _International Conference on Machine Learning_, 2021.


[1358] Weixiang Zhao, Xingyu Sui, Jiahe Guo, Yulin Hu, Yang Deng, Yanyan Zhao, Bing Qin, Wanxiang
Che, Tat-Seng Chua, and Ting Liu. Trade-offs in large reasoning models: An empirical analysis of
deliberative and adaptive reasoning over foundational capabilities, arXiv preprint arXiv:2503.17979,
[2025. URL https://arxiv.org/abs/2503.17979v1.](https://arxiv.org/abs/2503.17979v1)


[1359] Yibo Zhao, Jiapeng Zhu, Ye Guo, Kangkang He, and Xiang Li. E [2] graphrag: Streamlining graph-based
rag for high efficiency and effectiveness. arXiv preprint, 2025.


[1360] Boyuan Zheng, Boyu Gou, Jihyung Kil, Huan Sun, and Yu Su. Gpt-4v(ision) is a generalist web
agent, if grounded. _International Conference on Machine Learning_, 2024.


[1361] Changmeng Zheng, Dayong Liang, Wengyu Zhang, Xiao Wei, Tat seng Chua, and Qing Li. A picture
is worth a graph: A blueprint debate paradigm for multimodal reasoning. _ACM Multimedia_, 2024.


[1362] Chuanyang Zheng, Yihang Gao, Han Shi, Minbin Huang, Jingyao Li, Jing Xiong, Xiaozhe Ren,
Michael Ng, Xin Jiang, Zhenguo Li, and Yu Li. Dape: Data-adaptive positional encoding for length
extrapolation. _Neural Information Processing Systems_, 2024.


[1363] Chunmo Zheng, Saika Wong, Xing Su, Yinqiu Tang, Ahsan Nawaz, and Mohamad Kassem. Automating
construction contract review using knowledge graph-enhanced large language models. _Automation_
_in Construction_, 2023.


[1364] Junhao Zheng, Shengjie Qiu, Chengming Shi, and Qianli Ma. Towards lifelong learning of large
language models: A survey. _ACM Computing Surveys_, 2024.


[1365] Junhao Zheng, Xidi Cai, Qiuke Li, Duzhen Zhang, Zhongzhi Li, Yingying Zhang, Le Song, and Qianli
Ma. Lifelongagentbench: Evaluating llm agents as lifelong learners, arXiv preprint arXiv:2505.11942,
[2025. URL https://arxiv.org/abs/2505.11942v3.](https://arxiv.org/abs/2505.11942v3)


[1366] Longtao Zheng, R. Wang, and Bo An. Synapse: Trajectory-as-exemplar prompting with memory for
computer control. _International Conference on Learning Representations_, 2023.


[1367] Longtao Zheng, Rundong Wang, Xinrun Wang, and Bo An. Synapse: Trajectory-as-exemplar
prompting with memory for computer control, arXiv preprint arXiv:2306.07863, 2024. URL
[https://arxiv.org/abs/2306.07863.](https://arxiv.org/abs/2306.07863)


[1368] Xu Zheng, Chenfei Liao, Yuqian Fu, Kaiyu Lei, Yuanhuiyi Lyu, Lutao Jiang, Bin Ren, Jialei Chen,
Jiawen Wang, Chengxin Li, Linfeng Zhang, D. Paudel, Xuanjing Huang, Yu-Gang Jiang, N. Sebe,
Dacheng Tao, L. V. Gool, and Xuming Hu. Mllms are deeply affected by modality bias, arXiv preprint
[arXiv:2505.18657v1, 2025. URL https://arxiv.org/abs/2505.18657v1.](https://arxiv.org/abs/2505.18657v1)


[1369] Yuxiang Zheng, Dayuan Fu, Xiangkun Hu, Xiaojie Cai, Lyumanshan Ye, Pengrui Lu, and Pengfei Liu.
Deepresearcher: Scaling deep research via reinforcement learning in real-world environments, arXiv
[preprint arXiv:2504.03160, 2025. URL https://arxiv.org/abs/2504.03160v4.](https://arxiv.org/abs/2504.03160v4)


162


[1370] Li Zhong, Zilong Wang, and Jingbo Shang. Debug like a human: A large language model debugger
via verifying runtime execution step by step. _Annual Meeting of the Association for Computational_
_Linguistics_, 2024.


[1371] Rui Zhong, Yang Cao, Jun Yu, and M. Munetomo. Large language model assisted adversarial
robustness neural architecture search. _2024 6th International Conference on Data-driven Optimization_
_of Complex Systems (DOCS)_, 2024.


[1372] Wanjun Zhong, Lianghong Guo, Qi-Fei Gao, He Ye, and Yanlin Wang. Memorybank: Enhancing
large language models with long-term memory. _AAAI Conference on Artificial Intelligence_, 2023.


[1373] Wanjun Zhong, Lianghong Guo, Qiqi Gao, He Ye, and Yanlin Wang. Memorybank: Enhancing
large language models with long-term memory, arXiv preprint arXiv:2305.10250, 2023. URL
[https://arxiv.org/abs/2305.10250.](https://arxiv.org/abs/2305.10250)


[1374] Andy Zhou, Kai Yan, Michal Shlapentokh-Rothman, Haohan Wang, and Yu-Xiong Wang. Language
agent tree search unifies reasoning acting and planning in language models. _International Conference_
_on Machine Learning_, 2023.


[1375] Bin Zhou, Xingwang Shen, Yuqian Lu, Xinyu Li, B. Hua, Tianyuan Liu, and Jinsong Bao. Semanticaware event link reasoning over industrial knowledge graph embedding time series data. _International_
_Journal of Production Research_, 2022.


[1376] Denny Zhou, Nathanael Scharli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, D. Schuurmans,
O. Bousquet, Quoc Le, and Ed H. Chi. Least-to-most prompting enables complex reasoning in large
language models. _International Conference on Learning Representations_, 2022.


[1377] Huichi Zhou, Kin-Hei Lee, Zhonghao Zhan, Yue Chen, Zhenhao Li, Zhaoyang Wang, Hamed Haddadi,
and Emine Yilmaz. Trustrag: Enhancing robustness and trustworthiness in rag, arXiv preprint
[arXiv:2501.00879, 2025. URL https://arxiv.org/abs/2501.00879.](https://arxiv.org/abs/2501.00879)


[1378] Shuyan Zhou, Frank F. Xu, Hao Zhu, Xuhui Zhou, Robert Lo, Abishek Sridhar, Xianyi Cheng, Yonatan
Bisk, Daniel Fried, Uri Alon, and Graham Neubig. Webarena: A realistic web environment for
building autonomous agents. _International Conference on Learning Representations_, 2023.


[1379] Wangchunshu Zhou, Y. Jiang, Long Li, Jialong Wu, Tiannan Wang, Shi Qiu, Jintian Zhang, Jing
Chen, Ruipu Wu, Shuai Wang, Shiding Zhu, Jiyu Chen, Wentao Zhang, Xiangru Tang, Ningyu Zhang,
Huajun Chen, Peng Cui, and Mrinmaya Sachan. Agents: An open-source framework for autonomous
[language agents, arXiv preprint arXiv:2309.07870, 2023. URL https://arxiv.org/abs/2309.](https://arxiv.org/abs/2309.07870v3)
[07870v3.](https://arxiv.org/abs/2309.07870v3)


[1380] Yingli Zhou, Yaodong Su, Youran Sun, Shu Wang, Taotao Wang, Runyuan He, Yongwei Zhang,
Sicong Liang, Xilin Liu, Yuchi Ma, and Yixiang Fang. In-depth analysis of graph-based rag in a
[unified framework, arXiv preprint arXiv:2503.04338, 2025. URL https://arxiv.org/abs/](https://arxiv.org/abs/2503.04338v1)
[2503.04338v1.](https://arxiv.org/abs/2503.04338v1)


[1381] Yuhang Zhou and Wei Ai. Teaching-assistant-in-the-loop: Improving knowledge distillation from
imperfect teacher models in low-budget scenarios. _Annual Meeting of the Association for Computational_
_Linguistics_, 2024.


163


[1382] Yujia Zhou, Yan Liu, Xiaoxi Li, Jiajie Jin, Hongjin Qian, Zheng Liu, Chaozhuo Li, Zhicheng Dou,
Tsung-Yi Ho, and Philip S. Yu. Trustworthiness in retrieval-augmented generation systems: A survey,
[arXiv preprint arXiv:2409.10102, 2024. URL https://arxiv.org/abs/2409.10102v1.](https://arxiv.org/abs/2409.10102v1)


[1383] Zhehua Zhou, Jiayang Song, Kunpeng Yao, Zhan Shu, and Lei Ma. Isr-llm: Iterative self-refined
large language model for long-horizon sequential task planning. _IEEE International Conference on_
_Robotics and Automation_, 2023.


[1384] Zihan Zhou, Chong Li, Xinyi Chen, Shuo Wang, Yu Chao, Zhili Li, Haoyu Wang, Rongqiao An, Qi Shi,
Zhixing Tan, Xu Han, Xiaodong Shi, Zhiyuan Liu, and Maosong Sun. Llm _×_ mapreduce: Simplified
long-sequence processing using large language models, arXiv preprint arXiv:2410.09342, 2024. URL
[https://arxiv.org/abs/2410.09342v1.](https://arxiv.org/abs/2410.09342v1)


[1385] Zijian Zhou, Ao Qu, Zhaoxuan Wu, Sunghwan Kim, Alok Prakash, Daniela Rus, Jinhua Zhao, B. Low,
and P. Liang. Mem1: Learning to synergize memory and reasoning for efficient long-horizon agents,
[arXiv preprint arXiv:2506.15841, 2025. URL https://arxiv.org/abs/2506.15841v1.](https://arxiv.org/abs/2506.15841v1)


[1386] Andrew Zhu, Liam Dugan, and Christopher Callison-Burch. Redel: A toolkit for llm-powered recursive
multi-agent systems. _Conference on Empirical Methods in Natural Language Processing_, 2024.


[1387] Dawei Zhu, Nan Yang, Liang Wang, Yifan Song, Wenhao Wu, Furu Wei, and Sujian Li. Pose: Efficient
context window extension of llms via positional skip-wise training. _International Conference on_
_Learning Representations_, 2023.


[1388] Hongyin Zhu. Metaaid 2.5: A secure framework for developing metaverse applications via large
language models. arXiv preprint, 2023.


[1389] Jason Zhu, Yanling Cui, Yuming Liu, Hao Sun, Xue Li, Markus Pelger, Liangjie Zhang, Tianqi Yan,
Ruofei Zhang, and Huasha Zhao. Textgnn: Improving text encoder via graph neural network in
sponsored search. _The Web Conference_, 2021.


[1390] Jiachen Zhu, Menghui Zhu, Renting Rui, Rong Shan, Congmin Zheng, Bo Chen, Yunjia Xi, Jianghao
Lin, Weiwen Liu, Ruiming Tang, Yong Yu, and Weinan Zhang. Evolutionary perspectives on the
evaluation of llm-based ai agents: A comprehensive survey, arXiv preprint arXiv:2506.11102, 2025.
[URL https://arxiv.org/abs/2506.11102v1.](https://arxiv.org/abs/2506.11102v1)


[1391] Jinguo Zhu, Weiyun Wang, Zhe Chen, Zhaoyang Liu, Shenglong Ye, Lixin Gu, Yuchen Duan, Hao
Tian, Weijie Su, Jie Shao, Zhangwei Gao, Erfei Cui, Yue Cao, Yangzhou Liu, Haomin Wang, Weiye
Xu, Hao Li, Jiahao Wang, Han Lv, Dengnian Chen, Songze Li, Yinan He, Tan Jiang, Jiapeng Luo,
Yi Wang, Cong He, Botian Shi, Xingcheng Zhang, Wenqi Shao, Junjun He, Ying Xiong, Wenwen Qu,
Peng Sun, Penglong Jiao, Lijun Wu, Kai Zhang, Hui Deng, Jiaye Ge, Kaiming Chen, Limin Wang, Min
Dou, Lewei Lu, Xizhou Zhu, Tong Lu, Dahua Lin, Yu Qiao, Jifeng Dai, and Wenhai Wang. Internvl3:
Exploring advanced training and test-time recipes for open-source multimodal models, arXiv preprint
[arXiv:2504.10479v3, 2025. URL https://arxiv.org/abs/2504.10479v3.](https://arxiv.org/abs/2504.10479v3)


[1392] Mingwei Zhu, Leigang Sha, Yu Shu, Kangjia Zhao, Tiancheng Zhao, and Jianwei Yin. Benchmarking
sequential visual input reasoning and prediction in multimodal large language models, arXiv preprint
[arXiv:2310.13473v1, 2023. URL https://arxiv.org/abs/2310.13473v1.](https://arxiv.org/abs/2310.13473v1)


[1393] Rongzhi Zhu, Xiangyu Liu, Zequn Sun, Yiwei Wang, and Wei Hu. Mitigating lost-in-retrieval problems
in retrieval augmented multi-hop question answering. 2025.


164


[1394] Rongzhi Zhu, Yi Liu, Zequn Sun, Yiwei Wang, and Wei Hu. When can large reasoning models save
thinking? mechanistic analysis of behavioral divergence in reasoning. 2025.


[1395] Runchuan Zhu, Zinco Jiang, Jiang Wu, Zhipeng Ma, Jiahe Song, Fengshuo Bai, Dahua Lin, Lijun Wu,
and Conghui He. Grait: Gradient-driven refusal-aware instruction tuning for effective hallucination
mitigation. 2025.


[1396] Runchuan Zhu, Zhipeng Ma, Jiang Wu, Junyuan Gao, Jiaqi Wang, Dahua Lin, and Conghui He.
Utilize the flow before stepping into the same river twice: Certainty represented knowledge flow
for refusal-aware instruction tuning. In _Proceedings of the AAAI Conference on Artificial Intelligence_,
volume 39, pages 26157–26165, 2025.


[1397] Tongyao Zhu, Qian Liu, L. Pang, Zhengbao Jiang, Min-Yen Kan, and Min Lin. Beyond memorization:
The challenge of random memory access in language models. _Annual Meeting of the Association for_
_Computational Linguistics_, 2024.


[1398] Xizhou Zhu, Yuntao Chen, Hao Tian, Chenxin Tao, Weijie Su, Chenyu Yang, Gao Huang, Bin Li,
Lewei Lu, Xiaogang Wang, Yu Qiao, Zhaoxiang Zhang, and Jifeng Dai. Ghost in the minecraft:
Generally capable agents for open-world environments via large language models with text-based
[knowledge and memory, arXiv preprint arXiv:2305.17144, 2023. URL https://arxiv.org/](https://arxiv.org/abs/2305.17144)
[abs/2305.17144.](https://arxiv.org/abs/2305.17144)


[1399] Yue Zhu, Hao Yu, Chen Wang, Zhuoran Liu, and Eun Kyung Lee. Towards efficient key-value cache
management for prefix prefilling in llm inference, arXiv preprint arXiv:2505.21919, 2025. URL
[https://arxiv.org/abs/2505.21919v1.](https://arxiv.org/abs/2505.21919v1)


[1400] Zhengqiu Zhu, Yong Zhao, Bin Chen, S. Qiu, Kai Xu, Quanjun Yin, Jin-Yu Huang, Zhong Liu, and Fei
Wang. Conversational crowdsensing: A parallel intelligence powered novel sensing approach, arXiv
[preprint arXiv:2402.06654, 2024. URL https://arxiv.org/abs/2402.06654v1.](https://arxiv.org/abs/2402.06654v1)


[1401] Zulun Zhu, Tiancheng Huang, Kai Wang, Junda Ye, Xinghe Chen, and Siqiang Luo. Graph-based
approaches and functionalities in retrieval-augmented generation: A comprehensive survey, arXiv
[preprint arXiv:2504.10499, 2025. URL https://arxiv.org/abs/2504.10499v1.](https://arxiv.org/abs/2504.10499v1)


[1402] Alex Zhuang, Ge Zhang, Tianyu Zheng, Xinrun Du, Junjie Wang, Weiming Ren, Stephen W. Huang,
Jie Fu, Xiang Yue, and Wenhu Chen. Structlm: Towards building generalist models for structured
[knowledge grounding, arXiv preprint arXiv:2402.16671, 2024. URL https://arxiv.org/abs/](https://arxiv.org/abs/2402.16671v7)
[2402.16671v7.](https://arxiv.org/abs/2402.16671v7)


[1403] Yuchen Zhuang, Yue Yu, Kuan Wang, Haotian Sun, and Chao Zhang. Toolqa: A dataset for llm
question answering with external tools. _Neural Information Processing Systems_, 2023.


[1404] Zhixiong Zhuang, Maria-Irina Nicolae, Hui-Po Wang, and Mario Fritz. Proxyprompt: Securing
system prompts against prompt extraction attacks, arXiv preprint arXiv:2505.11459, 2025. URL
[https://arxiv.org/abs/2505.11459v1.](https://arxiv.org/abs/2505.11459v1)


[1405] Ziyuan Zhuang, Zhiyang Zhang, Sitao Cheng, Fangkai Yang, Jia Liu, Shujian Huang, Qingwei Lin,
Saravan Rajmohan, Dongmei Zhang, and Qi Zhang. Efficientrag: Efficient retriever for multi-hop
question answering. In _In Proceedings of the 2024 Conference on Empirical Methods in Natural_
_Language Processing_, pages 3392–3411, 2024.


165


[1406] Chang Zong, Yuchen Yan, Weiming Lu, Eliot Huang, Jian Shao, and Y. Zhuang. Triad: A framework
leveraging a multi-role llm-based agent to solve knowledge base question answering. _Conference on_
_Empirical Methods in Natural Language Processing_, 2024.


[1407] Yongshuo Zong, Ondrej Bohdal, and Timothy M. Hospedales. Vl-icl bench: The devil in the details
of benchmarking multimodal in-context learning. arXiv preprint, 2024.


[1408] Yongshuo Zong, Ondrej Bohdal, and Timothy M. Hospedales. Vl-icl bench: The devil in the details
of multimodal in-context learning. _International Conference on Learning Representations_, 2024.


[1409] Henry Peng Zou, Wei-Chieh Huang, Yaozu Wu, Yankai Chen, Chunyu Miao, Hoang Nguyen, Yue
Zhou, Weizhi Zhang, Liancheng Fang, Langzhou He, Yangning Li, Yuwei Cao, Dongyuan Li, Renhe
Jiang, and Philip S. Yu. A survey on large language model based human-agent systems. arXiv
preprint, 2025.


[1410] Tao Zou, Le Yu, Yifei Huang, Leilei Sun, and Bo Du. Pretraining language models with text-attributed
heterogeneous graphs. _Conference on Empirical Methods in Natural Language Processing_, 2023.


[1411] Adam Zweiger, Jyothish Pari, Han Guo, Ekin Akyürek, Yoon Kim, and Pulkit Agrawal. Self-adapting
[language models, arXiv preprint arXiv:2506.10943, 2025. URL https://arxiv.org/abs/2506.](https://arxiv.org/abs/2506.10943v1)
[10943v1.](https://arxiv.org/abs/2506.10943v1)


166


