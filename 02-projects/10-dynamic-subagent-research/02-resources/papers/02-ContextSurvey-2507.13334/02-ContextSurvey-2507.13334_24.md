<!-- Source: 02-ContextSurvey-2507.13334.pdf | Chunk 24/26 -->


[1167] Qingyun Wu, Gagan Bansal, Jieyu Zhang, Yiran Wu, Beibin Li, Erkang Zhu, Li Jiang, Xiaoyun Zhang,
Shaokun Zhang, Jiale Liu, A. Awadallah, Ryen W. White, Doug Burger, and Chi Wang. Autogen:
Enabling next-gen llm applications via multi-agent conversation, arXiv preprint arXiv:2308.08155,
[2023. URL https://arxiv.org/abs/2308.08155v2.](https://arxiv.org/abs/2308.08155v2)


[1168] Ruofan Wu, Youngwon Lee, Fan Shu, Danmei Xu, Seung won Hwang, Zhewei Yao, Yuxiong He,
and Feng Yan. Composerag: A modular and composable rag for corpus-grounded multi-hop ques[tion answering, arXiv preprint arXiv:2506.00232, 2025. URL https://arxiv.org/abs/2506.](https://arxiv.org/abs/2506.00232v1)
[00232v1.](https://arxiv.org/abs/2506.00232v1)


[1169] Shangyu Wu, Ying Xiong, Yufei Cui, Haolun Wu, Can Chen, Ye Yuan, Lianming Huang, Xue Liu,
Tei-Wei Kuo, Nan Guan, and C. Xue. Retrieval-augmented generation for natural language process[ing: A survey, arXiv preprint arXiv:2407.13193, 2024. URL https://arxiv.org/abs/2407.](https://arxiv.org/abs/2407.13193v3)
[13193v3.](https://arxiv.org/abs/2407.13193v3)


[1170] Shirley Wu, Shiyu Zhao, Qian Huang, Kexin Huang, Michihiro Yasunaga, V. Ioannidis, Karthik
Subbian, J. Leskovec, and James Zou. Avatar: Optimizing llm agents for tool usage via contrastive
reasoning. _Neural Information Processing Systems_, 2024.


[1171] Suhang Wu, Minlong Peng, Yue Chen, Jinsong Su, and Mingming Sun. Eva-kellm: A new benchmark
[for evaluating knowledge editing of llms, arXiv preprint arXiv:2308.09954, 2023. URL https:](https://arxiv.org/abs/2308.09954)
[//arxiv.org/abs/2308.09954.](https://arxiv.org/abs/2308.09954)


[1172] Tianhao Wu, Weizhe Yuan, Olga Golovneva, Jing Xu, Yuandong Tian, Jiantao Jiao, Jason E Weston,
and Sainbayar Sukhbaatar. Meta-rewarding language models: Self-improving alignment with
llm-as-a-meta-judge. arXiv preprint, 2024.


[1173] Tong Wu, Chong Xiang, Jiachen T. Wang, and Prateek Mittal. Effectively controlling reasoning
[models through thinking intervention, arXiv preprint arXiv:2503.24370, 2025. URL https://](https://arxiv.org/abs/2503.24370v3)
[arxiv.org/abs/2503.24370v3.](https://arxiv.org/abs/2503.24370v3)


[1174] Xinbo Wu and L. Varshney. A meta-learning perspective on transformers for causal language modeling.
_Annual Meeting of the Association for Computational Linguistics_, 2023.


[1175] Xue Wu and Kostas Tsioutsiouliklis. Thinking with knowledge graphs: Enhancing llm reasoning
[through structured data, arXiv preprint arXiv:2412.10654, 2024. URL https://arxiv.org/](https://arxiv.org/abs/2412.10654v1)
[abs/2412.10654v1.](https://arxiv.org/abs/2412.10654v1)


147


[1176] Yaxiong Wu, Sheng Liang, Chen Zhang, Yichao Wang, Yongyue Zhang, Huifeng Guo, Ruiming Tang,
and Yong Liu. From human memory to ai memory: A survey on memory mechanisms in the era of
[llms, arXiv preprint arXiv:2504.15965, 2025. URL https://arxiv.org/abs/2504.15965v2.](https://arxiv.org/abs/2504.15965v2)


[1177] Zengqing Wu and Takayuki Ito. The hidden strength of disagreement: Unraveling the consensusdiversity tradeoff in adaptive multi-agent systems, arXiv preprint arXiv:2502.16565, 2025. URL
[https://arxiv.org/abs/2502.16565v2.](https://arxiv.org/abs/2502.16565v2)


[1178] Zihao Wu, Lu Zhang, Chao-Yang Cao, Xiao-Xing Yu, Haixing Dai, Chong-Yi Ma, Zheng Liu, Lin Zhao,
Gang Li, Wei Liu, Quanzheng Li, Dinggang Shen, Xiang Li, Dajiang Zhu, and Tianming Liu. Exploring
the trade-offs: Unified large language models vs local fine-tuned models for highly-specific radiology
nli task. _IEEE Transactions on Big Data_, 2023.


[1179] Zhiheng Xi, Wenxiang Chen, Xin Guo, Wei He, Yiwen Ding, Boyang Hong, Ming Zhang, Junzhe
Wang, Senjie Jin, Enyu Zhou, Rui Zheng, Xiaoran Fan, Xiao Wang, Limao Xiong, Qin Liu, Yuhao
Zhou, Weiran Wang, Changhao Jiang, Yicheng Zou, Xiangyang Liu, Zhangyue Yin, Shihan Dou,
Rongxiang Weng, Wensen Cheng, Qi Zhang, Wenjuan Qin, Yongyan Zheng, Xipeng Qiu, Xuanjing
Huan, and Tao Gui. The rise and potential of large language model based agents: A survey, arXiv
[preprint arXiv:2309.07864, 2023. URL https://arxiv.org/abs/2309.07864v3.](https://arxiv.org/abs/2309.07864v3)


[1180] Menglin Xia, Victor Ruehle, Saravan Rajmohan, and Reza Shokri. Minerva: A programmable
[memory test benchmark for language models, arXiv preprint arXiv:2502.03358, 2025. URL https:](https://arxiv.org/abs/2502.03358v2)
[//arxiv.org/abs/2502.03358v2.](https://arxiv.org/abs/2502.03358v2)


[1181] Yuchen Xia, Manthan Shenoy, N. Jazdi, and M. Weyrich. Towards autonomous system: flexible modular production system enhanced with large language model agents. _IEEE International Conference_
_on Emerging Technologies and Factory Automation_, 2023.


[1182] Yutong Xia, Ao Qu, Yunhan Zheng, Yihong Tang, Dingyi Zhuang, Yuxuan Liang, Cathy Wu, Roger
Zimmermann, and Jinhua Zhao. Reimagining urban science: Scaling causal inference with large
[language models, arXiv preprint arXiv:2504.12345v3, 2025. URL https://arxiv.org/abs/](https://arxiv.org/abs/2504.12345v3)
[2504.12345v3.](https://arxiv.org/abs/2504.12345v3)


[1183] Zhishang Xiang, Chuanjie Wu, Qinggang Zhang, Shengyuan Chen, Zijin Hong, Xiao Huang, and
Jinsong Su. When to use graphs in rag: A comprehensive analysis for graph retrieval-augmented gener[ation, arXiv preprint arXiv:2506.05690, 2025. URL https://arxiv.org/abs/2506.05690v1.](https://arxiv.org/abs/2506.05690v1)


[1184] Chaojun Xiao, Pengle Zhang, Xu Han, Guangxuan Xiao, Yankai Lin, Zhengyan Zhang, Zhiyuan Liu,
Song Han, and Maosong Sun. Inf **l** m: Training-free long-context extrapolation for llms with an
efficient context memory. _Neural Information Processing Systems_, 2024.


[1185] Guangxuan Xiao, Yuandong Tian, Beidi Chen, Song Han, and Mike Lewis. Efficient streaming
language models with attention sinks. _International Conference on Learning Representations_, 2023.


[1186] MiniCPM Team Chaojun Xiao, Yuxuan Li, Xu Han, Yuzhuo Bai, Jie Cai, Haotian Chen, Wentong Chen,
Xin Cong, Ganqu Cui, Ning Ding, Shengda Fan, Yewei Fang, Zixuan Fu, Wenyu Guan, Yitong Guan,
Junshao Guo, Yu-Xuan Han, Bingxiang He, Yuxian Huang, Cunliang Kong, Qiu-Tong Li, Siyuan Li,
Wenhao Li, Yanghao Li, Yishan Li, Zhen Li, Dan Liu, Biyuan Lin, Yankai Lin, Xiang Long, Quanyu Lu,
Ya-Ting Lu, Pei Luo, Hongya Lyu, Litu Ou, Yinxu Pan, Zekai Qu, Qundong Shi, Zijun Song, Jiayu Su,
Zhou Su, Ao Sun, Xiang ping Sun, Peijun Tang, Fang-Ming Wang, Feng Wang, Shuo Wang, Yudong


148


Wang, Yesai Wu, Zhenyu Xiao, Jie Xie, Zi-Kang Xie, Yukun Yan, Jia-Li Yuan, Kai Zhang, Lei Zhang,
Linyu Zhang, Xueren Zhang, Yudi Zhang, Hengyu Zhao, Weilin Zhao, Weilun Zhao, Yuanqian Zhao,
Zhijun Zheng, Ge Zhou, Jie Zhou, Wei Zhou, Zihan Zhou, Zi-An Zhou, Zhiyuan Liu, Guoyang Zeng,
Chaochao Jia, Dahai Li, and Maosong Sun. Minicpm4: Ultra-efficient llms on end devices, arXiv
[preprint arXiv:2506.07900, 2025. URL https://arxiv.org/abs/2506.07900v1.](https://arxiv.org/abs/2506.07900v1)


[1187] Yang Xiao, Jiashuo Wang, Ruifeng Yuan, Chunpu Xu, Kaishuai Xu, Wenjie Li, and Pengfei Liu. Limopro:
Reasoning refinement for efficient and effective test-time scaling, arXiv preprint arXiv:2505.19187,
[2025. URL https://arxiv.org/abs/2505.19187v1.](https://arxiv.org/abs/2505.19187v1)


[1188] Yilin Xiao, Chuang Zhou, Qinggang Zhang, Bo Li, Qing Li, and Xiao Huang. Reliable reasoning path: Distilling effective guidance for llm reasoning with knowledge graphs, arXiv preprint
[arXiv:2506.10508, 2025. URL https://arxiv.org/abs/2506.10508v1.](https://arxiv.org/abs/2506.10508v1)


[1189] Jian Xie, Kai Zhang, Jiangjie Chen, Tinghui Zhu, Renze Lou, Yuandong Tian, Yanghua Xiao, and
Yu Su. Travelplanner: A benchmark for real-world planning with language agents. _International_
_Conference on Machine Learning_, 2024.


[1190] Yuxi Xie, Anirudh Goyal, Xiaobao Wu, Xunjian Yin, Xiao Xu, Min-Yen Kan, Liangming Pan, and
William Yang Wang. Coral: Order-agnostic language modeling for efficient iterative refinement,
[arXiv preprint arXiv:2410.09675, 2024. URL https://arxiv.org/abs/2410.09675v1.](https://arxiv.org/abs/2410.09675v1)


[1191] Yue Xing, Tao Yang, Yijiashun Qi, Minggu Wei, Yu Cheng, and Honghui Xin. Structured memory mechanisms for stable context representation in large language models, arXiv preprint arXiv:2505.22921,
[2025. URL https://arxiv.org/abs/2505.22921v1.](https://arxiv.org/abs/2505.22921v1)


[1192] Guangzhi Xiong, Qiao Jin, Xiao Wang, Yin Fang, Haolin Liu, Yifan Yang, Fangyuan Chen, Zhixing Song,
Dengyu Wang, Minjia Zhang, Zhiyong Lu, and Aidong Zhang. Rag-gym: Systematic optimization of
language agents for retrieval-augmented generation. arXiv preprint, 2025.


[1193] Haoyi Xiong, Zhiyuan Wang, Xuhong Li, Jiang Bian, Zeke Xie, Shahid Mumtaz, and Laura E.
Barnes. Converging paradigms: The synergy of symbolic and connectionist ai in llm-empowered
[autonomous agents, arXiv preprint arXiv:2407.08516, 2024. URL https://arxiv.org/abs/](https://arxiv.org/abs/2407.08516v5)
[2407.08516v5.](https://arxiv.org/abs/2407.08516v5)


[1194] Junjie Xiong, Changjia Zhu, Shuhang Lin, Chong Zhang, Yongfeng Zhang, Yao Liu, and Lingyao Li. Invisible prompts, visible threats: Malicious font injection in external resources for large language mod[els, arXiv preprint arXiv:2505.16957, 2025. URL https://arxiv.org/abs/2505.16957v1.](https://arxiv.org/abs/2505.16957v1)


[1195] Zhen Xiong, Yujun Cai, Bryan Hooi, Nanyun Peng, Zhecheng Li, and Yiwei Wang. Enhancing llm
character-level manipulation via divide and conquer. 2025.


[1196] Zhen Xiong, Yujun Cai, Zhecheng Li, and Yiwei Wang. Mapping the minds of llms: A graph-based
analysis of reasoning llm. 2025.


[1197] Zhen Xiong, Yujun Cai, Zhecheng Li, and Yiwei Wang. Unveiling the potential of diffusion large
language model in controllable generation. 2025.


[1198] Zidi Xiong, Yuping Lin, Wenya Xie, Pengfei He, Jiliang Tang, Himabindu Lakkaraju, and Zhen Xiang.
How memory management impacts llm agents: An empirical study of experience-following behavior,
[arXiv preprint arXiv:2505.16067, 2025. URL https://arxiv.org/abs/2505.16067v1.](https://arxiv.org/abs/2505.16067v1)


149


[1199] Chunmei Xu, Shengheng Liu, Cheng Zhang, Yongming Huang, Zhaohua Lu, and Luxi Yang. Multiagent reinforcement learning based distributed transmission in collaborative cloud-edge systems.
_IEEE Transactions on Vehicular Technology_, 2021.


[1200] Haotian Xu, Xing Wu, Weinong Wang, Zhongzhi Li, Da Zheng, Boyuan Chen, Yi Hu, Shijia Kang,
Jiaming Ji, Yingying Zhang, et al. Redstar: Does scaling long-cot data unlock better slow-reasoning
systems? 2025.


[1201] Hongshen Xu, Su Zhu, Zihan Wang, Hang Zheng, Da Ma, Ruisheng Cao, Shuai Fan, Lu Chen, and
Kai Yu. Reducing tool hallucination via reliability alignment, arXiv preprint arXiv:2412.04141, 2024.
[URL https://arxiv.org/abs/2412.04141v3.](https://arxiv.org/abs/2412.04141v3)


[1202] Hu Xu, Gargi Ghosh, Po-Yao (Bernie) Huang, Dmytro Okhonko, Armen Aghajanyan, and Florian
Metze Luke Zettlemoyer Christoph Feichtenhofer. Videoclip: Contrastive pre-training for zero-shot
video-text understanding. _Conference on Empirical Methods in Natural Language Processing_, 2021.


[1203] Mengjia Xu. Understanding graph embedding methods and their applications. _SIAM Review_, 2020.


[1204] Minrui Xu, Hongyang Du, Dusist Niyato, Jiawen Kang, Zehui Xiong, Shiwen Mao, Zhu Han, A. Jamalipour, Dong In Kim, X. Shen, Victor C. M. Leung, and H. Poor. Unleashing the power of edge-cloud
generative ai in mobile networks: A survey of aigc services. _IEEE Communications Surveys and Tuto-_
_rials_, 2023.


[1205] Minrui Xu, D. Niyato, and Christopher G. Brinton. Serving long-context llms at the mobile edge:
Test-time reinforcement learning-based model caching and inference offloading, arXiv preprint
[arXiv:2501.14205, 2025. URL https://arxiv.org/abs/2501.14205v1.](https://arxiv.org/abs/2501.14205v1)


[1206] Nan Xu, Fei Wang, Sheng Zhang, Hoifung Poon, and Muhao Chen. From introspection to best
practices: Principled analysis of demonstrations in multimodal in-context learning. _North American_
_Chapter of the Association for Computational Linguistics_, 2024.


[1207] Shuhang Xu and Fangwei Zhong. Comet: Metaphor-driven covert communication for multi-agent
[language games, arXiv preprint arXiv:2505.18218, 2025. URL https://arxiv.org/abs/2505.](https://arxiv.org/abs/2505.18218v1)
[18218v1.](https://arxiv.org/abs/2505.18218v1)


[1208] Tianyang Xu, Haojie Zheng, Chengze Li, Haoxiang Chen, Yixin Liu, Ruoxi Chen, and Lichao Sun.
Noderag: Structuring graph-based rag with heterogeneous nodes, arXiv preprint arXiv:2504.11544,
[2025. URL https://arxiv.org/abs/2504.11544v1.](https://arxiv.org/abs/2504.11544v1)


[1209] Wenda Xu, Guanglei Zhu, Xuandong Zhao, Liangming Pan, Lei Li, and W. Wang. Pride and prejudice:
Llm amplifies self-bias in self-refinement. _Annual Meeting of the Association for Computational_
_Linguistics_, 2024.


[1210] Wenrui Xu and Keshab K. Parhi. A survey of attacks on large language models, arXiv preprint
[arXiv:2505.12567, 2025. URL https://arxiv.org/abs/2505.12567v1.](https://arxiv.org/abs/2505.12567v1)


[1211] Wujiang Xu, Zujie Liang, Kai Mei, Hang Gao, Juntao Tan, and Yongfeng Zhang. A-mem: Agentic
memory for llm agents. arXiv preprint, 2025.


[1212] Wujiang Xu, Kai Mei, Hang Gao, Juntao Tan, Zujie Liang, and Yongfeng Zhang. A-mem: Agentic
[memory for llm agents, arXiv preprint arXiv:2502.12110, 2025. URL https://arxiv.org/abs/](https://arxiv.org/abs/2502.12110)
[2502.12110.](https://arxiv.org/abs/2502.12110)


150


[1213] Yifei Xu, Jingqiao Zhang, Ru He, Liangzhu Ge, Chao Yang, Cheng Yang, and Ying Wu. Sas: Selfaugmentation strategy for language model pre-training. _AAAI Conference on Artificial Intelligence_,
2021.


[1214] Zhe Xu, Daoyuan Chen, Zhenqing Ling, Yaliang Li, and Ying Shen. Mindgym: What matters in
question synthesis for thinking-centric fine-tuning?, arXiv preprint arXiv:2503.09499, 2025. URL
[https://arxiv.org/abs/2503.09499v2.](https://arxiv.org/abs/2503.09499v2)


[1215] Zhentao Xu, Mark Jerome Cruz, Matthew Guevara, Tie Wang, Manasi Deshpande, Xiaofeng Wang,
and Zheng Li. Retrieval-augmented generation with knowledge graphs for customer service question
answering. _Annual International ACM SIGIR Conference on Research and Development in Information_
_Retrieval_, 2024.


[1216] Eric Xue, Ke Chen, Zeyi Huang, Yuyang Ji, Yong Jae Lee, and Haohan Wang. Improve: Iterative
model pipeline refinement and optimization leveraging llm experts, arXiv preprint arXiv:2502.18530,
[2025. URL https://arxiv.org/abs/2502.18530v2.](https://arxiv.org/abs/2502.18530v2)


[1217] Huiyin Xue and Nikolaos Aletras. Pit one against many: Leveraging attention-head embeddings
for parameter-efficient multi-head attention. _Conference on Empirical Methods in Natural Language_
_Processing_, 2023.


[1218] Xiangyuan Xue, Zeyu Lu, Di Huang, Zidong Wang, Wanli Ouyang, and Lei Bai. Comfybench:
Benchmarking llm-based agents in comfyui for autonomously designing collaborative ai systems,
[arXiv preprint arXiv:2409.01392, 2024. URL https://arxiv.org/abs/2409.01392v2.](https://arxiv.org/abs/2409.01392v2)


[1219] Bingyu Yan, Xiaoming Zhang, Litian Zhang, Lian Zhang, Ziyi Zhou, Dezhuang Miao, and Chaozhuo
Li. Beyond self-talk: A communication-centric survey of llm-based multi-agent systems. arXiv
preprint, 2025.


[1220] Tianqiang Yan and Tiansheng Xu. Refining the responses of llms by themselves, arXiv preprint
[arXiv:2305.04039, 2023. URL https://arxiv.org/abs/2305.04039v1.](https://arxiv.org/abs/2305.04039v1)


[1221] Xu Yan, Junliang Du, Lun Wang, Yingbin Liang, Jiacheng Hu, and Bingxing Wang. The synergistic
role of deep learning and neural architecture search in advancing artificial intelligence. _2024_
_International Conference on Electronics and Devices, Computational Science (ICEDCS)_, 2024.


[1222] Yibo Yan, Haomin Wen, Siru Zhong, Wei Chen, Haodong Chen, Qingsong Wen, Roger Zimmermann,
and Yuxuan Liang. Urbanclip: Learning text-enhanced urban region profiling with contrastive
language-image pretraining from the web. _The Web Conference_, 2023.


[1223] Yuchen Yan, Yongliang Shen, Yang Liu, Jin Jiang, Mengdi Zhang, Jian Shao, and Yueting Zhuang.
Inftythink: Breaking the length limits of long-context reasoning in large language models, arXiv
[preprint arXiv:2503.06692, 2025. URL https://arxiv.org/abs/2503.06692v3.](https://arxiv.org/abs/2503.06692v3)


[1224] Chengrun Yang, Xuezhi Wang, Yifeng Lu, Hanxiao Liu, Quoc V. Le, Denny Zhou, and Xinyun Chen.
Large language models as optimizers. _International Conference on Learning Representations_, 2023.


[1225] Hongkang Yang, Zehao Lin, Wenjin Wang, Hao Wu, Zhiyu Li, Bo Tang, Wenqiang Wei, Jinbo Wang,
Zeyun Tang, Shichao Song, Chenyang Xi, Yu Yu, Kai Chen, Feiyu Xiong, Linpeng Tang, and E. Weinan.
Memory3: Language modeling with explicit memory. _Journal of Machine Learning_, 2024.


151


[1226] Jianxin Yang. Longqlora: Efficient and effective method to extend context length of large lan[guage models, arXiv preprint arXiv:2311.04879, 2023. URL https://arxiv.org/abs/2311.](https://arxiv.org/abs/2311.04879v2)
[04879v2.](https://arxiv.org/abs/2311.04879v2)


[1227] Jinghan Yang, Shuming Ma, and Furu Wei. Auto-icl: In-context learning without human supervision.
arXiv preprint, 2023.


[1228] Jingkang Yang, Yuhao Dong, Shuai Liu, Bo Li, Ziyue Wang, Chencheng Jiang, Haoran Tan, Jiamu Kang, Yuanhan Zhang, Kaiyang Zhou, and Ziwei Liu. Octopus: Embodied vision-language
programmer from environmental feedback. _European Conference on Computer Vision_, 2023.


[1229] John Yang, Carlos E. Jimenez, Alexander Wettig, Kilian Adriano Lieret, Shunyu Yao, Karthik
Narasimhan, and Ofir Press. Swe-agent: Agent-computer interfaces enable automated software
engineering. _Neural Information Processing Systems_, 2024.


[1230] Junhan Yang, Zheng Liu, Shitao Xiao, Chaozhuo Li, Defu Lian, Sanjay Agrawal, Amit Singh,
Guangzhong Sun, and Xing Xie. Graphformers: Gnn-nested transformers for representation learning
on textual graph. _Neural Information Processing Systems_, 2021.


[1231] Ke Yang, Yao Liu, Sapana Chaudhary, Rasool Fakoor, Pratik Chaudhari, George Karypis, and Huzefa
Rangwala. Agentoccam: A simple yet strong baseline for llm-based web agents. 2024.


[1232] Lin F. Yang, Hongyang Chen, Zhao Li, Xiao Ding, and Xindong Wu. Give us the facts: Enhancing
large language models with knowledge graphs for fact-aware language modeling. _IEEE Transactions_
_on Knowledge and Data Engineering_, 2023.


[1233] Ling Yang, Ye Tian, Bowen Li, Xinchen Zhang, Ke Shen, Yunhai Tong, and Mengdi Wang. Mmada:
Multimodal large diffusion language models, arXiv preprint arXiv:2505.15809v1, 2025. URL
[https://arxiv.org/abs/2505.15809v1.](https://arxiv.org/abs/2505.15809v1)


[1234] R Yang, L Song, Y Li, S Zhao, and Y Ge.... Gpt4tools: Teaching large language model to use
[tools via self-instruction. 2023. URL https://proceedings.neurips.cc/paper_files/](https://proceedings.neurips.cc/paper_files/paper/2023/hash/e393677793767624f2821cec8bdd02f1-Abstract-Conference.html?utm_campaign=Artificial%2BIntelligence%2BWeekly&utm_medium=email&utm_source=Artificial_Intelligence_Weekly_411)

[paper/2023/hash/e393677793767624f2821cec8bdd02f1-Abstract-Conference.](https://proceedings.neurips.cc/paper_files/paper/2023/hash/e393677793767624f2821cec8bdd02f1-Abstract-Conference.html?utm_campaign=Artificial%2BIntelligence%2BWeekly&utm_medium=email&utm_source=Artificial_Intelligence_Weekly_411)

[html?utm_campaign=Artificial%2BIntelligence%2BWeekly&utm_medium=email&](https://proceedings.neurips.cc/paper_files/paper/2023/hash/e393677793767624f2821cec8bdd02f1-Abstract-Conference.html?utm_campaign=Artificial%2BIntelligence%2BWeekly&utm_medium=email&utm_source=Artificial_Intelligence_Weekly_411)
[utm_source=Artificial_Intelligence_Weekly_411.](https://proceedings.neurips.cc/paper_files/paper/2023/hash/e393677793767624f2821cec8bdd02f1-Abstract-Conference.html?utm_campaign=Artificial%2BIntelligence%2BWeekly&utm_medium=email&utm_source=Artificial_Intelligence_Weekly_411)


[1235] Rui Yang, Lin Song, Yanwei Li, Sijie Zhao, Yixiao Ge, Xiu Li, and Ying Shan. Gpt4tools: Teaching
large language model to use tools via self-instruction. _Neural Information Processing Systems_, 2023.


[1236] Shang Yang, Junxian Guo, Haotian Tang, Qinghao Hu, Guangxuan Xiao, Jiaming Tang, Yujun
Lin, Zhijian Liu, Yao Lu, and Song Han. Lserve: Efficient long-sequence llm serving with unified
[sparse attention, arXiv preprint arXiv:2502.14866, 2025. URL https://arxiv.org/abs/2502.](https://arxiv.org/abs/2502.14866v2)
[14866v2.](https://arxiv.org/abs/2502.14866v2)


[1237] Shanglong Yang, Zhipeng Yuan, Shunbao Li, Ruoling Peng, Kang Liu, and Po Yang. Gpt-4 as evaluator:
Evaluating large language models on pest management in agriculture. arXiv preprint, 2024.


[1238] Wang Yang, Zirui Liu, Hongye Jin, Qingyu Yin, Vipin Chaudhary, and Xiaotian Han. Longer context, deeper thinking: Uncovering the role of long-context ability in reasoning, arXiv preprint
[arXiv:2505.17315, 2025. URL https://arxiv.org/abs/2505.17315v1.](https://arxiv.org/abs/2505.17315v1)


152


[1239] Wen Yang, Kai Fan, and Minpeng Liao. Markov chain of thought for efficient mathematical reasoning.
_North American Chapter of the Association for Computational Linguistics_, 2024.


[1240] Yaodong Yang, Chengdong Ma, Zihan Ding, S. McAleer, Chi Jin, and Jun Wang. Game-theoretic
[multiagent reinforcement learning, arXiv preprint arXiv:2011.00583, 2020. URL https://arxiv.](https://arxiv.org/abs/2011.00583v4)
[org/abs/2011.00583v4.](https://arxiv.org/abs/2011.00583v4)


[1241] Yazheng Yang, Yuqi Wang, Sankalok Sen, Lei Li, and Qi Liu. Unleashing the potential of large
language models for predictive tabular tasks in data science, arXiv preprint arXiv:2403.20208, 2024.
[URL https://arxiv.org/abs/2403.20208v7.](https://arxiv.org/abs/2403.20208v7)


[1242] Yi Yang, Yixuan Tang, and Kar Yan Tam. Investlm: A large language model for investment using
[financial domain instruction tuning, arXiv preprint arXiv:2309.13064, 2023. URL https://arxiv.](https://arxiv.org/abs/2309.13064)
[org/abs/2309.13064.](https://arxiv.org/abs/2309.13064)


[1243] Yiben Yang, Chaitanya Malaviya, Jared Fernandez, Swabha Swayamdipta, Ronan Le Bras, Ji ping
Wang, Chandra Bhagavatula, Yejin Choi, and Doug Downey. G-daug: Generative data augmentation
for commonsense reasoning. _Findings_, 2020.


[1244] Yingxuan Yang, Huacan Chai, Yuanyi Song, Siyuan Qi, Muning Wen, Ning Li, Junwei Liao, Haoyi
Hu, Jianghao Lin, Gaowei Chang, Weiwen Liu, Ying Wen, Yong Yu, and Weinan Zhang. A survey
[of ai agent protocols, arXiv preprint arXiv:2504.16736, 2025. URL https://arxiv.org/abs/](https://arxiv.org/abs/2504.16736v3)
[2504.16736v3.](https://arxiv.org/abs/2504.16736v3)


[1245] Yuan Yang, Siheng Xiong, Ehsan Shareghi, and F. Fekri. The compressor-retriever architecture for
[language model os, arXiv preprint arXiv:2409.01495, 2024. URL https://arxiv.org/abs/](https://arxiv.org/abs/2409.01495v1)
[2409.01495v1.](https://arxiv.org/abs/2409.01495v1)


[1246] Yuxin Yang, Haoyang Wu, Tao Wang, Jia Yang, Hao Ma, and Guojie Luo. Pseudo-knowledge graph:
Meta-path guided retrieval and in-graph text for rag-equipped llm, arXiv preprint arXiv:2503.00309,
[2025. URL https://arxiv.org/abs/2503.00309v1.](https://arxiv.org/abs/2503.00309v1)


[1247] Zhen Yang, Fang Liu, Zhongxing Yu, J. Keung, Jia Li, Shuo Liu, Yifan Hong, Xiaoxue Ma, Zhi Jin, and
Ge Li. Exploring and unleashing the power of large language models in automated code translation.
_Proc. ACM Softw. Eng._, 2024.


[1248] Chengyuan Yao and Satoshi Fujita. Adaptive control of retrieval-augmented generation for large
language models through reflective tags. _Electronics_, 2024.


[1249] Huaiyuan Yao, Longchao Da, Vishnu Nandam, J. Turnau, Zhiwei Liu, Linsey Pang, and Hua Wei.
Comal: Collaborative multi-agent large language models for mixed-autonomy traffic, arXiv preprint
[arXiv:2410.14368, 2024. URL https://arxiv.org/abs/2410.14368v2.](https://arxiv.org/abs/2410.14368v2)


[1250] Jiayu Yao, Shenghua Liu, Yiwei Wang, Lingrui Mei, Baolong Bi, Yuyao Ge, Zhecheng Li, and Xueqi
Cheng. Who is in the spotlight: The hidden bias undermining multimodal retrieval-augmented
generation. 2025.


[1251] Jinghan Yao, Sam Ade Jacobs, Masahiro Tanaka, Olatunji Ruwase, A. Shafi, H. Subramoni, and
Dhabaleswar K. Panda. Training ultra long context language model with fully pipelined distributed
[transformer, arXiv preprint arXiv:2408.16978, 2024. URL https://arxiv.org/abs/2408.](https://arxiv.org/abs/2408.16978v2)
[16978v2.](https://arxiv.org/abs/2408.16978v2)


153


[1252] Liang Yao, Chengsheng Mao, and Yuan Luo. Graph convolutional networks for text classification.
_AAAI Conference on Artificial Intelligence_, 2018.


[1253] Shunyu Yao, Howard Chen, John Yang, and Karthik Narasimhan. Webshop: Towards scalable
real-world web interaction with grounded language agents. _Neural Information Processing Systems_,
2022.


[1254] Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao.
React: Synergizing reasoning and acting in language models. _International Conference on Learning_
_Representations_, 2022.


[1255] Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, T. Griffiths, Yuan Cao, and Karthik Narasimhan.
Tree of thoughts: Deliberate problem solving with large language models. _Neural Information_
_Processing Systems_, 2023.


[1256] Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao.
React: Synergizing reasoning and acting in language models, arXiv preprint arXiv:2210.03629, 2023.
[URL https://arxiv.org/abs/2210.03629.](https://arxiv.org/abs/2210.03629)


[1257] Shunyu Yao, Noah Shinn, P. Razavi, and Karthik Narasimhan. _Ï„_ -bench: A benchmark for tool-agentuser interaction in real-world domains. arXiv preprint, 2024.


[1258] Weiran Yao, Shelby Heinecke, Juan Carlos Niebles, Zhiwei Liu, Yihao Feng, Le Xue, Rithesh Murthy,
Zeyuan Chen, Jianguo Zhang, Devansh Arpit, Ran Xu, Phil Mui, Huan Wang, Caiming Xiong, and
Silvio Savarese. Retroformer: Retrospective large language agents with policy gradient optimization,
[arXiv preprint arXiv:2308.02151, 2024. URL https://arxiv.org/abs/2308.02151.](https://arxiv.org/abs/2308.02151)


[1259] Michihiro Yasunaga, Hongyu Ren, Antoine Bosselut, Percy Liang, and J. Leskovec. Qa-gnn: Reasoning
with language models and knowledge graphs for question answering. _North American Chapter of the_
_Association for Computational Linguistics_, 2021.


[1260] Michihiro Yasunaga, Antoine Bosselut, Hongyu Ren, Xikun Zhang, Christopher D. Manning, Percy
Liang, and J. Leskovec. Deep bidirectional language-knowledge graph pretraining. _Neural Information_
_Processing Systems_, 2022.


[1261] Fahd Yazin, Moumita Das, A. Banerjee, and Dipanjan Roy. Contextual prediction errors reorganize
naturalistic episodic memories in time. _Scientific Reports_, 2021.


[1262] J Ye, G Li, S Gao, C Huang, Y Wu, S Li, and X Fan.... Tooleyes: Fine-grained evaluation for
[tool learning capabilities of large language models in real-world scenarios. 2024. URL https:](https://arxiv.org/abs/2401.00741)
[//arxiv.org/abs/2401.00741.](https://arxiv.org/abs/2401.00741)


[1263] J Ye, S Li, G Li, C Huang, S Gao, and Y Wu.... Toolsword: Unveiling safety issues of large language
[models in tool learning across three stages. 2024. URL https://arxiv.org/abs/2402.10753.](https://arxiv.org/abs/2402.10753)

