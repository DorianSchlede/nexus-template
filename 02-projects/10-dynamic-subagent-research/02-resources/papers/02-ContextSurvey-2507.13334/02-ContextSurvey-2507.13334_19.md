<!-- Source: 02-ContextSurvey-2507.13334.pdf | Chunk 19/26 -->



[774] Thomas Merth, Qichen Fu, Mohammad Rastegari, and Mahyar Najibi. Superposition prompting:
Improving and accelerating retrieval-augmented generation. _International Conference on Machine_
_Learning_, 2024.


[775] B. Meskó. Prompt engineering as an important emerging skill for medical professionals: Tutorial.
_Journal of Medical Internet Research_, 2023.


[776] Yapeng Mi, Zhi Gao, Xiaojian Ma, and Qing Li. Building llm agents by incorporating insights from
[computer systems, arXiv preprint arXiv:2504.04485, 2025. URL https://arxiv.org/abs/](https://arxiv.org/abs/2504.04485v1)
[2504.04485v1.](https://arxiv.org/abs/2504.04485v1)


[777] G. Mialon, Roberto Dessì, M. Lomeli, Christoforos Nalmpantis, Ramakanth Pasunuru, R. Raileanu,
Baptiste Rozière, Timo Schick, Jane Dwivedi-Yu, Asli Celikyilmaz, Edouard Grave, Yann LeCun, and
Thomas Scialom. Augmented language models: a survey. _Trans. Mach. Learn. Res._, 2023.


[778] G. Mialon, Clémentine Fourrier, Craig Swift, Thomas Wolf, Yann LeCun, and Thomas Scialom.
[Gaia: a benchmark for general ai assistants, arXiv preprint arXiv:2311.12983, 2023. URL https:](https://arxiv.org/abs/2311.12983v1)
[//arxiv.org/abs/2311.12983v1.](https://arxiv.org/abs/2311.12983v1)


[779] Xupeng Miao, Gabriele Oliaro, Zhihao Zhang, Xinhao Cheng, Hongyi Jin, Tianqi Chen, and Zhihao
Jia. Towards efficient generative large language model serving: A survey from algorithms to systems,
[arXiv preprint arXiv:2312.15234, 2023. URL https://arxiv.org/abs/2312.15234v1.](https://arxiv.org/abs/2312.15234v1)


[780] Jacob Miller, Guillaume Rabusseau, and John Terilla. Tensor networks for language modeling. arXiv
preprint, 2020.


[781] Xing ming Guo, Darioush Keivan, U. Syed, Lianhui Qin, Huan Zhang, G. Dullerud, Peter J. Seiler,
and Bin Hu. Controlagent: Automating control system design via novel integration of llm agents
[and domain expertise, arXiv preprint arXiv:2410.19811, 2024. URL https://arxiv.org/abs/](https://arxiv.org/abs/2410.19811v1)
[2410.19811v1.](https://arxiv.org/abs/2410.19811v1)


[782] Soroush Mirjalili, Patrick S. Powell, Jonathan Strunk, Taylor A James, and Audrey Duarte. Context
memory encoding and retrieval temporal dynamics are modulated by attention across the adult
lifespan. _eNeuro_, 2021.


117


[783] Ishan Misra and L. Maaten. Self-supervised learning of pretext-invariant representations. _Computer_
_Vision and Pattern Recognition_, 2019.


[784] Ali Modarressi, Ayyoob Imani, Mohsen Fayyaz, and Hinrich Schütze. Ret-llm: Towards a general
[read-write memory for large language models, arXiv preprint arXiv:2305.14322, 2024. URL https:](https://arxiv.org/abs/2305.14322)
[//arxiv.org/abs/2305.14322.](https://arxiv.org/abs/2305.14322)


[785] Ali Modarressi, Abdullatif Köksal, Ayyoob Imani, Mohsen Fayyaz, and Hinrich Schutze. Memllm:
Finetuning llms to use an explicit read-write memory. _Trans. Mach. Learn. Res._, 2024.


[786] Behnam Mohammadi. Pel, a programming language for orchestrating ai agents, arXiv preprint
[arXiv:2505.13453, 2025. URL https://arxiv.org/abs/2505.13453v2.](https://arxiv.org/abs/2505.13453v2)


[787] Amirkeivan Mohtashami and Martin Jaggi. Landmark attention: Random-access infinite context
length for transformers. _Neural Information Processing Systems_, 2023.


[788] Fedor Moiseev, Zhe Dong, Enrique Alfonseca, and Martin Jaggi. Skill: Structured knowledge infusion
for large language models. _North American Chapter of the Association for Computational Linguistics_,
2022.


[789] Dimitri Coelho Mollo and Raphael Milliere. The vector grounding problem, arXiv preprint
[arXiv:2304.01481, 2023. URL https://arxiv.org/abs/2304.01481v2.](https://arxiv.org/abs/2304.01481v2)


[790] Nieves Montes, N. Osman, and C. Sierra. Combining theory of mind and abduction for cooperation
under imperfect information. _European Workshop on Multi-Agent Systems_, 2022.


[791] Suhong Moon, Siddharth Jha, Lutfi Eren Erdogan, Sehoon Kim, Woosang Lim, Kurt Keutzer, and
A. Gholami. Efficient and scalable estimation of tool representations in vector space, arXiv preprint
[arXiv:2409.02141, 2024. URL https://arxiv.org/abs/2409.02141v1.](https://arxiv.org/abs/2409.02141v1)


[792] Shinsuke Mori. A stochastic parser based on an slm with arboreal context trees. _International_
_Conference on Computational Linguistics_, 2002.


[793] Meredith Ringel Morris. Prompting considered harmful. _Communications of the ACM_, 2024.


[794] Marius Mosbach, Tiago Pimentel, Shauli Ravfogel, D. Klakow, and Yanai Elazar. Few-shot fine-tuning
vs. in-context learning: A fair comparison and evaluation. _Annual Meeting of the Association for_
_Computational Linguistics_, 2023.


[795] Sajad Mousavi, Ricardo Luna Guti’errez, Desik Rengarajan, Vineet Gundecha, Ashwin Ramesh Babu,
Avisek Naug, Antonio Guillen-Perez, and S. Sarkar. N-critics: Self-refinement of large language
models with ensemble of critics. arXiv preprint, 2023.


[796] Manisha Mukherjee, Sungchul Kim, Xiang Chen, Dan Luo, Tong Yu, and Tung Mai. From documents
to dialogue: Building kg-rag enhanced ai assistants, arXiv preprint arXiv:2502.15237, 2025. URL
[https://arxiv.org/abs/2502.15237v1.](https://arxiv.org/abs/2502.15237v1)


[797] Tergel Munkhbat, Namgyu Ho, Seohyun Kim, Yongjin Yang, Yujin Kim, and Se young Yun. Selftraining elicits concise reasoning in large language models, arXiv preprint arXiv:2502.20122, 2025.
[URL https://arxiv.org/abs/2502.20122v3.](https://arxiv.org/abs/2502.20122v3)


118


[798] Tsendsuren Munkhdalai, Manaal Faruqui, and Siddharth Gopal. Leave no context behind: Efficient
infinite context transformers with infini-attention, arXiv preprint arXiv:2404.07143, 2024. URL
[https://arxiv.org/abs/2404.07143v2.](https://arxiv.org/abs/2404.07143v2)


[799] Eliya Nachmani, Alon Levkovitch, Julián Salazar, Chulayutsh Asawaroengchai, Soroosh Mariooryad,
R. Skerry-Ryan, and Michelle Tadmor Ramanovich. Spoken question answering and speech continuation using spectrogram-powered llm. _International Conference on Learning Representations_,
2023.


[800] L. Nadel, Jessica D. Payne, and W. J. Jacobs. The relationship between episodic memory and context:
clues from memory errors made while under stress. _Physiological Research_, 2002.


[801] Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher
Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, Xu Jiang, Karl Cobbe, Tyna Eloundou,
Gretchen Krueger, Kevin Button, Matthew Knight, Benjamin Chess, and John Schulman. Webgpt:
Browser-assisted question-answering with human feedback, arXiv preprint arXiv:2112.09332, 2022.
[URL https://arxiv.org/abs/2112.09332.](https://arxiv.org/abs/2112.09332)


[802] Koichi Namekata, Amirmojtaba Sabour, Sanja Fidler, and Seung Wook Kim. Emerdiff: Emerging
pixel-level semantic knowledge in diffusion models, arXiv preprint arXiv:2401.11739, 2024. URL
[https://arxiv.org/abs/2401.11739.](https://arxiv.org/abs/2401.11739)


[803] Sundaraparipurnan Narayanan and Sandeep Vishwakarma. Guard-d-llm: An llm-based risk assessment engine for the downstream uses of llms. arXiv preprint, 2024.


[804] Usman Naseem, Surendrabikram Thapa, Qi Zhang, Liang Hu, Anum Masood, and Mehwish Nasim.
Reducing knowledge noise for improved semantic analysis in biomedical natural language processing
applications. _Clinical Natural Language Processing Workshop_, 2023.


[805] Deepak Nathani, David Wang, Liangming Pan, and W. Wang. Maf: Multi-aspect feedback for
improving reasoning in large language models. _Conference on Empirical Methods in Natural Language_
_Processing_, 2023.


[806] Aashutosh Nema, Samaksh Gulati, Evangelos Giakoumakis, and Bipana Thapaliya. Modp: Multi
[objective directional prompting, arXiv preprint arXiv:2504.18722, 2025. URL https://arxiv.](https://arxiv.org/abs/2504.18722v1)
[org/abs/2504.18722v1.](https://arxiv.org/abs/2504.18722v1)


[807] Christian D. Newman, Anthony S Peruma, and Reem S. Alsuhaibani. Modeling the relationship
between identifier name and behavior. _IEEE International Conference on Software Maintenance and_
_Evolution_, 2019.


[808] M. Nieznański, Michał Obidziński, Emilia Zyskowska, and Daria Niedziałkowska. Executive resources
and item-context binding: Exploring the influence of concurrent inhibition, updating, and shifting
tasks on context memory. _Advances in Cognitive Psychology_, 2015.


[809] M. Nieznański, Michał Obidziński, and Daria Ford. Does context recollection depend on the base-rate
of contextual features? _Cognitive Processing_, 2023.


[810] C. Nourani and P. Eklund. Concepts ontology algebras and role descriptions. _Conference on Computer_
_Science and Information Systems_, 2017.


119


[811] Felix Ocker, Daniel Tanneberg, Julian Eggert, and Michael Gienger. Tulip agent - enabling llm-based
agents to solve tasks using large tool libraries. arXiv preprint, 2024.


[812] Felix Ocker, J. Deigmöller, Pavel Smirnov, and Julian Eggert. A grounded memory system for smart
[personal assistants, arXiv preprint arXiv:2505.06328, 2025. URL https://arxiv.org/abs/](https://arxiv.org/abs/2505.06328v1)
[2505.06328v1.](https://arxiv.org/abs/2505.06328v1)


[813] OpenAI. Computer-using agent, 2025. URL [https://openai.com/index/](https://openai.com/index/computer-using-agent/)
[computer-using-agent/. OpenAI Technical Report.](https://openai.com/index/computer-using-agent/)


[814] OpenAI. Swarm: Educational framework exploring ergonomic, lightweight multi-agent orchestration.

[https://github.com/openai/swarm, 2025. [Online; accessed 17-July-2025].](https://github.com/openai/swarm)


[815] Jonas Oppenlaender. Dangermaps: Personalized safety advice for travel in urban environments
[using a retrieval-augmented language model, arXiv preprint arXiv:2503.14103, 2025. URL https:](https://arxiv.org/abs/2503.14103v3)
[//arxiv.org/abs/2503.14103v3.](https://arxiv.org/abs/2503.14103v3)


[816] A. Orhan. Recognition, recall, and retention of few-shot memories in large language models, arXiv
[preprint arXiv:2303.17557, 2023. URL https://arxiv.org/abs/2303.17557v1.](https://arxiv.org/abs/2303.17557v1)


[817] Gustavo Ortiz-Hernández, Alejandro Guerra-Hernández, J. Hübner, and W. A. Luna-Ramírez. Modularization in belief-desire-intention agent programming and artifact-based environments. _PeerJ_
_Computer Science_, 2022.


[818] Wendkûuni C. Ouédraogo, A. Kaboré, Haoye Tian, Yewei Song, Anil Koyuncu, Jacques Klein, David
Lo, and Tegawend’e F. Bissyand’e. Large-scale, independent and comprehensive study of the power
of llms for test case generation. arXiv preprint, 2024.


[819] Charles Packer, Vivian Fang, Shishir G. Patil, Kevin Lin, Sarah Wooders, and Joseph Gonzalez.
[Memgpt: Towards llms as operating systems, arXiv preprint arXiv:2310.08560, 2023. URL https:](https://arxiv.org/abs/2310.08560v2)
[//arxiv.org/abs/2310.08560v2.](https://arxiv.org/abs/2310.08560v2)


[820] Charles Packer, Sarah Wooders, Kevin Lin, Vivian Fang, Shishir G. Patil, Ion Stoica, and Joseph E.
Gonzalez. Memgpt: Towards llms as operating systems, arXiv preprint arXiv:2310.08560, 2024.
[URL https://arxiv.org/abs/2310.08560.](https://arxiv.org/abs/2310.08560)


[821] Constantin-Valentin Pal, F. Leon, M. Paprzycki, and M. Ganzha. A review of platforms for the
development of agent systems. _Inf._, 2020.


[822] Qianjun Pan, Wenkai Ji, Yuyang Ding, Junsong Li, Shilian Chen, Junyi Wang, Jie Zhou, Qin Chen, Min
Zhang, Yulan Wu, and Liang He. A survey of slow thinking-based reasoning llms using reinforced
[learning and inference-time scaling law, arXiv preprint arXiv:2505.02665, 2025. URL https:](https://arxiv.org/abs/2505.02665v2)
[//arxiv.org/abs/2505.02665v2.](https://arxiv.org/abs/2505.02665v2)


[823] Shirui Pan, Linhao Luo, Yufei Wang, Chen Chen, Jiapu Wang, and Xindong Wu. Unifying large
language models and knowledge graphs: A roadmap. _IEEE Transactions on Knowledge and Data_
_Engineering_, 2023.


[824] Xu Pan, Ely Hahami, Zechen Zhang, and H. Sompolinsky. Memorization and knowledge injection
[in gated llms, arXiv preprint arXiv:2504.21239, 2025. URL https://arxiv.org/abs/2504.](https://arxiv.org/abs/2504.21239v1)
[21239v1.](https://arxiv.org/abs/2504.21239v1)


120


[825] Bo Pang, Hanze Dong, Jiacheng Xu, Silvio Savarese, Yingbo Zhou, and Caiming Xiong. Bolt: Bootstrap
long chain-of-thought in language models without distillation, arXiv preprint arXiv:2502.03860,
[2025. URL https://arxiv.org/abs/2502.03860v1.](https://arxiv.org/abs/2502.03860v1)


[826] Jianhui Pang, Fanghua Ye, Derek F. Wong, and Longyue Wang. Anchor-based large language models.
_Annual Meeting of the Association for Computational Linguistics_, 2024.


[827] Bhargavi Paranjape, Scott M. Lundberg, Sameer Singh, Hannaneh Hajishirzi, Luke Zettlemoyer, and
Marco Tulio Ribeiro. Art: Automatic multi-step reasoning and tool-use for large language models,
[arXiv preprint arXiv:2303.09014, 2023. URL https://arxiv.org/abs/2303.09014v1.](https://arxiv.org/abs/2303.09014v1)


[[828] A Parisi, Y Zhao, and N Fiedel. Talm: Tool augmented language models. 2022. URL https:](https://arxiv.org/abs/2205.12255)
[//arxiv.org/abs/2205.12255.](https://arxiv.org/abs/2205.12255)


[829] Dongju Park and Chang Wook Ahn. Self-supervised contextual data augmentation for natural
language processing. _Symmetry_, 2019.


[830] J. Park, Lindsay Popowski, Carrie J. Cai, M. Morris, Percy Liang, and Michael S. Bernstein. Social
simulacra: Creating populated prototypes for social computing systems. _ACM Symposium on User_
_Interface Software and Technology_, 2022.


[831] J. Park, Joseph C. O’Brien, Carrie J. Cai, M. Morris, Percy Liang, and Michael S. Bernstein. Generative
agents: Interactive simulacra of human behavior. _ACM Symposium on User Interface Software and_
_Technology_, 2023.


[832] Soya Park, J. Zamfirescu-Pereira, and Chinmay Kulkarni. Model behavior specification by leveraging
[llm self-playing and self-improving, arXiv preprint arXiv:2503.03967, 2025. URL https://arxiv.](https://arxiv.org/abs/2503.03967v1)
[org/abs/2503.03967v1.](https://arxiv.org/abs/2503.03967v1)


[833] Rajvardhan Patil and Venkat Gudivada. A review of current trends, techniques, and challenges in
large language models (llms). _Applied Sciences_, 2024.


[834] Shishir G. Patil, Tianjun Zhang, Xin Wang, and Joseph E. Gonzalez. Gorilla: Large language model
[connected with massive apis, arXiv preprint arXiv:2305.15334, 2023. URL https://arxiv.org/](https://arxiv.org/abs/2305.15334)
[abs/2305.15334.](https://arxiv.org/abs/2305.15334)


[835] Shishir G. Patil, Huanzhi Mao, Charlie Cheng-Jie Ji, Fanjia Yan, Vishnu Suresh, Ion Stoica, and
Joseph E. Gonzalez. The berkeley function calling leaderboard (bfcl): From tool use to agentic
evaluation of large language models. In _Forty-second International Conference on Machine Learning_,
2025.


[836] Shuva Paul, Farhad Alemi, and Richard Macwan. Llm-assisted proactive threat intelligence for
[automated reasoning, arXiv preprint arXiv:2504.00428, 2025. URL https://arxiv.org/abs/](https://arxiv.org/abs/2504.00428v1)
[2504.00428v1.](https://arxiv.org/abs/2504.00428v1)


[837] Saurav Pawar, S. Tonmoy, S. M. M. Zaman, Vinija Jain, Aman Chadha, and Amitava Das. The what,
why, and how of context length extension techniques in large language models - a detailed survey.
arXiv preprint, 2024.


[838] Boci Peng, Yun Zhu, Yongchao Liu, Xiaohe Bo, Haizhou Shi, Chuntao Hong, Yan Zhang, and
Siliang Tang. Graph retrieval-augmented generation: A survey. _ArXiv_, abs/2408.08921, 2024. URL
[https://api.semanticscholar.org/CorpusID:271903170.](https://api.semanticscholar.org/CorpusID:271903170)


121


[839] Bowen Peng, Jeffrey Quesnelle, Honglu Fan, and Enrico Shippole. Yarn: Efficient context window
extension of large language models. _International Conference on Learning Representations_, 2023.


[840] Hao Peng, Tianyu Gao, Xu Han, Yankai Lin, Peng Li, Zhiyuan Liu, Maosong Sun, and Jie Zhou.
Learning from context or names? an empirical study on neural relation extraction. _Conference on_
_Empirical Methods in Natural Language Processing_, 2020.


[841] Ji-Lun Peng, Sijia Cheng, Egil Diau, Yung-Yu Shih, Po-Heng Chen, Yen-Ting Lin, and Yun-Nung
[Chen. A survey of useful llm evaluation, arXiv preprint arXiv:2406.00936, 2024. URL https:](https://arxiv.org/abs/2406.00936v1)
[//arxiv.org/abs/2406.00936v1.](https://arxiv.org/abs/2406.00936v1)


[842] Bryan Perozzi, Bahare Fatemi, Dustin Zelle, Anton Tsitsulin, Mehran Kazemi, Rami Al-Rfou, and
Jonathan J. Halcrow. Let your graph do the talking: Encoding structured data for llms, arXiv preprint
[arXiv:2402.05862, 2024. URL https://arxiv.org/abs/2402.05862v1.](https://arxiv.org/abs/2402.05862v1)


[843] E. Pesce and G. Montana. Improving coordination in small-scale multi-agent deep reinforcement
learning through memory-driven communication. _Machine-mediated learning_, 2019.


[844] F. Petroni, Patrick Lewis, Aleksandra Piktus, Tim Rocktäschel, Yuxiang Wu, Alexander H. Miller, and
Sebastian Riedel. How context affects language models’ factual predictions. _Conference on Automated_
_Knowledge Base Construction_, 2020.


[845] Yue Pi, Wang Zhang, Yong Zhang, Hairong Huang, Baoquan Rao, Yulong Ding, and Shuanghua
Yang. Applications of multi-agent deep reinforcement learning communication in network manage[ment: A survey, arXiv preprint arXiv:2407.17030, 2024. URL https://arxiv.org/abs/2407.](https://arxiv.org/abs/2407.17030v1)
[17030v1.](https://arxiv.org/abs/2407.17030v1)


[846] Nancirose Piazza and Vahid Behzadan. A theory of mind approach as test-time mitigation against
emergent adversarial communication. _Adaptive Agents and Multi-Agent Systems_, 2023.


[847] Mathis Pink, Vy A. Vo, Qinyuan Wu, Jianing Mu, Javier S. Turek, Uri Hasson, Kenneth A. Norman,
Sebastian Michelmann, Alexander Huth, and Mariya Toneva. Assessing episodic memory in llms
[with sequence order recall tasks, arXiv preprint arXiv:2410.08133, 2024. URL https://arxiv.](https://arxiv.org/abs/2410.08133v1)
[org/abs/2410.08133v1.](https://arxiv.org/abs/2410.08133v1)


[848] Fahmida Liza Piya and Rahmatollah Beheshti. Advancing feature extraction in healthcare through
the integration of knowledge graphs and large language models. _AAAI Conference on Artificial_
_Intelligence_, 2025.


[849] A. Plaat, M. V. Duijn, N. V. Stein, Mike Preuss, P. V. D. Putten, and K. Batenburg. Agentic large
[language models, a survey, arXiv preprint arXiv:2503.23037, 2025. URL https://arxiv.org/](https://arxiv.org/abs/2503.23037v2)
[abs/2503.23037v2.](https://arxiv.org/abs/2503.23037v2)


[850] Moritz Plenz and Anette Frank. Graph language models. _Annual Meeting of the Association for_
_Computational Linguistics_, 2024.


[851] Sean M. Polyn, K. Norman, and M. Kahana. A context maintenance and retrieval model of organizational processes in free recall. _Psychology Review_, 2009.


[852] Liam Pond and Ichiro Fujinaga. Teaching llms music theory with in-context learning and chainof-thought prompting: Pedagogical strategies for machines. _International Conference on Computer_
_Supported Education_, 2025.


122


[853] V Porcu. The role of memory in llms: Persistent context for smarter conversations. _Int. J. Sci. Res._
_Manag.(IJSRM)_, 12:1673–1691, 2024.


[854] Ofir Press, Noah A. Smith, and M. Lewis. Train short, test long: Attention with linear biases enables
input length extrapolation. _International Conference on Learning Representations_, 2021.


[855] Xavier Puig, K. Ra, Marko Boben, Jiaman Li, Tingwu Wang, S. Fidler, and A. Torralba. Virtualhome:
Simulating household activities via programs. _2018 IEEE/CVF Conference on Computer Vision and_
_Pattern Recognition_, 2018.


[856] Pranav Putta, Edmund Mills, Naman Garg, S. Motwani, Chelsea Finn, Divyansh Garg, and Rafael
Rafailov. Agent q: Advanced reasoning and learning for autonomous ai agents, arXiv preprint
[arXiv:2408.07199, 2024. URL https://arxiv.org/abs/2408.07199v1.](https://arxiv.org/abs/2408.07199v1)


[857] S. Qasim, Hassan Mahmood, and F. Shafait. Rethinking table recognition using graph neural networks.
_IEEE International Conference on Document Analysis and Recognition_, 2019.


[858] Peng Qi, Haejun Lee, OghenetegiriTGSido, and Christopher D. Manning. Answering open-domain
questions of varying reasoning steps from text. _Conference on Empirical Methods in Natural Language_
_Processing_, 2020.


[859] Yong Qi, Gabriel Kyebambo, Siyuan Xie, Wei Shen, Shenghui Wang, Bitao Xie, Bin He, Zhipeng
Wang, and Shuo Jiang. Safety control of service robots with llms and embodied knowledge graphs,
[arXiv preprint arXiv:2405.17846, 2024. URL https://arxiv.org/abs/2405.17846v1.](https://arxiv.org/abs/2405.17846v1)


[860] Zehan Qi, Xiao Liu, Iat Long Iong, Hanyu Lai, Xueqiao Sun, Xinyue Yang, Jiadai Sun, Yu Yang,
Shuntian Yao, Tianjie Zhang, Wei Xu, Jie Tang, and Yuxiao Dong. Webrl: Training llm web agents
via self-evolving online curriculum reinforcement learning, arXiv preprint arXiv:2411.02337, 2024.
[URL https://arxiv.org/abs/2411.02337v3.](https://arxiv.org/abs/2411.02337v3)


[861] Chen Qian, Wei Liu, Hongzhang Liu, Nuo Chen, Yufan Dang, Jiahao Li, Cheng Yang, Weize Chen,
Yusheng Su, Xin Cong, Juyuan Xu, Dahai Li, Zhiyuan Liu, and Maosong Sun. Chatdev: Communicative
[agents for software development, arXiv preprint arXiv:2307.07924, 2024. URL https://arxiv.](https://arxiv.org/abs/2307.07924)
[org/abs/2307.07924.](https://arxiv.org/abs/2307.07924)


[862] Cheng Qian, Chi Han, Y. Fung, Yujia Qin, Zhiyuan Liu, and Heng Ji. Creator: Tool creation for
disentangling abstract and concrete reasoning of large language models. _Conference on Empirical_
_Methods in Natural Language Processing_, 2023.


[863] Cheng Qian, Jiahao Li, Yufan Dang, Wei Liu, Yifei Wang, Zihao Xie, Weize Chen, Cheng Yang,
Yingli Zhang, Zhiyuan Liu, and Maosong Sun. Iterative experience refinement of software[developing agents, arXiv preprint arXiv:2405.04219, 2024. URL https://arxiv.org/abs/](https://arxiv.org/abs/2405.04219v1)
[2405.04219v1.](https://arxiv.org/abs/2405.04219v1)


[864] Cheng Qian, Emre Can Acikgoz, Qi He, Hongru Wang, Xiusi Chen, Dilek Hakkani-Tur, Gokhan Tur,
and Heng Ji. Toolrl: Reward is all tool learning needs, arXiv preprint arXiv:2504.13958, 2025. URL
[https://arxiv.org/abs/2504.13958v1.](https://arxiv.org/abs/2504.13958v1)


[865] Hongjin Qian, Zheng Liu, Peitian Zhang, Zhicheng Dou, and Defu Lian. Boosting long-context
management via query-guided activation refilling, arXiv preprint arXiv:2412.12486, 2024. URL
[https://arxiv.org/abs/2412.12486v3.](https://arxiv.org/abs/2412.12486v3)


123


[866] Hongjin Qian, Zheng Liu, Peitian Zhang, Kelong Mao, Defu Lian, Zhicheng Dou, and Tiejun Huang.
Memorag: Boosting long context processing with global memory-enhanced retrieval augmentation,
[arXiv preprint arXiv:2409.05591, 2025. URL https://arxiv.org/abs/2409.05591.](https://arxiv.org/abs/2409.05591)


[867] Kangan Qian, Sicong Jiang, Yang Zhong, Ziang Luo, Zilin Huang, Tianze Zhu, Kun Jiang, Mengmeng
Yang, Zheng Fu, Jinyu Miao, Yining Shi, He Zhe Lim, Li Liu, Tianbao Zhou, Hongyi Wang, Huang
Yu, Yifei Hu, Guang Li, Guangyao Chen, Hao Ye, Lijun Sun, and Diange Yang. Agentthink: A
unified framework for tool-augmented chain-of-thought reasoning in vision-language models for
[autonomous driving, arXiv preprint arXiv:2505.15298, 2025. URL https://arxiv.org/abs/](https://arxiv.org/abs/2505.15298v3)
[2505.15298v3.](https://arxiv.org/abs/2505.15298v3)


[868] Changze Qiao and Mingming Lu. Efficiently enhancing general agents with hierarchical-categorical
memory, arXiv preprint arXiv:2505.22006, 2025. [URL https://arxiv.org/abs/2505.](https://arxiv.org/abs/2505.22006v1)
[22006v1.](https://arxiv.org/abs/2505.22006v1)


[869] S Qiao, H Gui, C Lv, Q Jia, H Chen, and N Zhang. Making language models better tool learners with
[execution feedback. 2023. URL https://arxiv.org/abs/2305.13068.](https://arxiv.org/abs/2305.13068)


[870] Binjie Qin, Haohao Mao, Ruipeng Zhang, Y. Zhu, Song Ding, and Xu Chen. Working memory
inspired hierarchical video decomposition with transformative representations, arXiv preprint
[arXiv:2204.10105, 2022. URL https://arxiv.org/abs/2204.10105v3.](https://arxiv.org/abs/2204.10105v3)


[871] Bowen Qin, Binyuan Hui, Lihan Wang, Min Yang, Jinyang Li, Binhua Li, Ruiying Geng, Rongyu Cao,
Jian Sun, Luo Si, Fei Huang, and Yongbin Li. A survey on text-to-sql parsing: Concepts, methods,
[and future directions, arXiv preprint arXiv:2208.13629, 2022. URL https://arxiv.org/abs/](https://arxiv.org/abs/2208.13629v1)
[2208.13629v1.](https://arxiv.org/abs/2208.13629v1)


[872] Libo Qin, Qiguang Chen, Fuxuan Wei, Shijue Huang, and Wanxiang Che. Cross-lingual prompting:
Improving zero-shot chain-of-thought reasoning across languages, arXiv preprint arXiv:2310.14799,
[2023. URL https://arxiv.org/abs/2310.14799.](https://arxiv.org/abs/2310.14799)


[873] Libo Qin, Fuxuan Wei, Qiguang Chen, Jingxuan Zhou, Shijue Huang, Jiasheng Si, Wenpeng Lu,
and Wanxiang Che. Croprompt: Cross-task interactive prompting for zero-shot spoken language
[understanding, arXiv preprint arXiv:2406.10505, 2024. URL https://arxiv.org/abs/2406.](https://arxiv.org/abs/2406.10505)
[10505.](https://arxiv.org/abs/2406.10505)

