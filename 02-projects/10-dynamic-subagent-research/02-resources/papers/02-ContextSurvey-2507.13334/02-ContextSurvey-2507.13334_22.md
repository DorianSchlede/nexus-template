<!-- Source: 02-ContextSurvey-2507.13334.pdf | Chunk 22/26 -->


[1010] Lei Sun, Xinchen Wang, and Youdi Li. Pyramid-driven alignment: Pyramid principle guided integration of large language models and knowledge graphs, arXiv preprint arXiv:2410.12298, 2024. URL
[https://arxiv.org/abs/2410.12298v2.](https://arxiv.org/abs/2410.12298v2)


[1011] Liangtai Sun, Xingyu Chen, Lu Chen, Tianle Dai, Zichen Zhu, and Kai Yu. Meta-gui: Towards multimodal conversational agents on mobile gui. _Conference on Empirical Methods in Natural Language_
_Processing_, 2022.


[1012] Lijun Sun, Yijun Yang, Qiqi Duan, Yuhui Shi, Chao Lyu, Yu-Cheng Chang, Chin-Teng Lin, and
Yang Shen. Multi-agent coordination across diverse applications: A survey, arXiv preprint
[arXiv:2502.14743, 2025. URL https://arxiv.org/abs/2502.14743v2.](https://arxiv.org/abs/2502.14743v2)


[1013] Qianru Sun, Yaoyao Liu, Tat-Seng Chua, and B. Schiele. Meta-transfer learning for few-shot learning.
_Computer Vision and Pattern Recognition_, 2018.


[1014] Yu Sun, Shuohuan Wang, Yukun Li, Shikun Feng, Hao Tian, Hua Wu, and Haifeng Wang. Ernie
2.0: A continual pre-training framework for language understanding. _AAAI Conference on Artificial_
_Intelligence_, 2019.


[1015] Rao Surapaneni, Miku Jha, Michael Vakoc, and Todd Segal. Announcing the
agent2agent protocol (a2a). [https://developers.googleblog.com/en/](https://developers.googleblog.com/en/a2a-a-new-era-of-agent-interoperability/)
[a2a-a-new-era-of-agent-interoperability/, April 2025.](https://developers.googleblog.com/en/a2a-a-new-era-of-agent-interoperability/) [Online; accessed 17July-2025].


[1016] Stefan Szeider. Mcp-solver: Integrating language models with constraint programming systems.
arXiv preprint, 2024.


[1017] Daniel Szelogowski. Engram memory encoding and retrieval: A neurocomputational perspective,
[arXiv preprint arXiv:2506.01659, 2025. URL https://arxiv.org/abs/2506.01659v1.](https://arxiv.org/abs/2506.01659v1)


[1018] N. Taatgen, David Huss, D. Dickison, and John R. Anderson. The acquisition of robust and flexible
cognitive skills. _Journal of experimental psychology. General_, 2008.


[1019] Jihoon Tack, Jaehyung Kim, Eric Mitchell, Jinwoo Shin, Yee Whye Teh, and Jonathan Richard
Schwarz. Online adaptation of language models with a memory of amortized contexts. _Neural_
_Information Processing Systems_, 2024.


[1020] Yan Tai, Weichen Fan, Zhao Zhang, Feng Zhu, Rui Zhao, and Ziwei Liu. Link-context learning for
multimodal llms. _Computer Vision and Pattern Recognition_, 2023.


135


[1021] Fahim Tajwar, Yiding Jiang, Abitha Thankaraj, Sumaita Sadia Rahman, J. Z. Kolter, Jeff Schneider,
and Ruslan Salakhutdinov. Training a generally curious agent, arXiv preprint arXiv:2502.17543,
[2025. URL https://arxiv.org/abs/2502.17543v3.](https://arxiv.org/abs/2502.17543v3)


[1022] K. Tallam. From autonomous agents to integrated systems, a new paradigm: Orchestrated distributed
[intelligence, arXiv preprint arXiv:2503.13754, 2025. URL https://arxiv.org/abs/2503.](https://arxiv.org/abs/2503.13754v2)
[13754v2.](https://arxiv.org/abs/2503.13754v2)


[1023] A. Tan, Budhitama Subagdja, Di Wang, and Lei Meng. Self-organizing neural networks for universal
learning and multimodal memory encoding. _Neural Networks_, 2019.


[1024] Chuanyuan Tan, Yuehe Chen, Wenbiao Shao, and Wenliang Chen. Make a choice! knowledge
base question answering with in-context learning, arXiv preprint arXiv:2305.13972, 2023. URL
[https://arxiv.org/abs/2305.13972v1.](https://arxiv.org/abs/2305.13972v1)


[1025] Sijun Tan, Xiuyu Li, Shishir G. Patil, Ziyang Wu, Tianjun Zhang, Kurt Keutzer, Joseph E. Gonzalez,
and Raluca A. Popa. Lloco: Learning long contexts offline. _Conference on Empirical Methods in_
_Natural Language Processing_, 2024.


[1026] Xiaoyu Tan, Haoyu Wang, Xihe Qiu, Yuan Cheng, Yinghui Xu, Wei Chu, and Yuan Qi. Struct-x:
Enhancing large language models reasoning with structured data. arXiv preprint, 2024.


[1027] Zhaoxuan Tan and Meng Jiang. User modeling in the era of large language models: Current research
and future directions. _IEEE Data Engineering Bulletin_, 2023.


[1028] Zhijie Tan, Xu Chu, Weiping Li, and Tong Mo. Order matters: Exploring order sensitivity in
[multimodal large language models, arXiv preprint arXiv:2410.16983v1, 2024. URL https://](https://arxiv.org/abs/2410.16983v1)
[arxiv.org/abs/2410.16983v1.](https://arxiv.org/abs/2410.16983v1)


[1029] Matthew Tancik, Pratul P. Srinivasan, B. Mildenhall, Sara Fridovich-Keil, N. Raghavan, Utkarsh
Singhal, R. Ramamoorthi, J. Barron, and Ren Ng. Fourier features let networks learn high frequency
functions in low dimensional domains. _Neural Information Processing Systems_, 2020.


[1030] Fei Tang, Haolei Xu, Hang Zhang, Siqi Chen, Xingyu Wu, Yongliang Shen, Wenqi Zhang, Guiyang
Hou, Zeqi Tan, Yuchen Yan, Kaitao Song, Jian Shao, Weiming Lu, Jun Xiao, and Yueting Zhuang. A
[survey on (m)llm-based gui agents, arXiv preprint arXiv:2504.13865, 2025. URL https://arxiv.](https://arxiv.org/abs/2504.13865v2)
[org/abs/2504.13865v2.](https://arxiv.org/abs/2504.13865v2)


[1031] Jiabin Tang, Yuhao Yang, Wei Wei, Lei Shi, Lixin Su, Suqi Cheng, Dawei Yin, and Chao Huang.
Graphgpt: Graph instruction tuning for large language models. _Annual International ACM SIGIR_
_Conference on Research and Development in Information Retrieval_, 2023.


[1032] Jiabin Tang, Yuhao Yang, Wei Wei, Lei Shi, Lixin Su, Suqi Cheng, Dawei Yin, and Chao Huang.
Graphgpt: Graph instruction tuning for large language models. In _Proceedings of the 47th International_
_ACM SIGIR Conference on Research and Development in Information Retrieval_, pages 491–500, 2024.


[1033] Jianheng Tang, Qifan Zhang, Yuhan Li, Nuo Chen, and Jia Li. Grapharena: Evaluating and exploring
large language models on graph computation. _arXiv preprint arXiv:2407.00379_, 2024.


[1034] Xiangru Tang, Yiming Zong, Jason Phang, Yilun Zhao, Wangchunshu Zhou, Arman Cohan, and Mark
Gerstein. Struc-bench: Are large language models good at generating complex structured tabular
data? _North American Chapter of the Association for Computational Linguistics_, 2024.


136


[1035] Xiangru Tang, Tianyu Hu, Muyang Ye, Yanjun Shao, Xunjian Yin, Siru Ouyang, Wangchunshu Zhou,
Pan Lu, Zhuosheng Zhang, Yilun Zhao, Arman Cohan, and Mark Gerstein. Chemagent: Self-updating
library in large language models improves chemical reasoning, arXiv preprint arXiv:2501.06590,
[2025. URL https://arxiv.org/abs/2501.06590v1.](https://arxiv.org/abs/2501.06590v1)


[1036] Xuemei Tang, Jun Wang, and Q. Su. Chinese word segmentation with heterogeneous graph neural net[work, arXiv preprint arXiv:2201.08975, 2022. URL https://arxiv.org/abs/2201.08975v1.](https://arxiv.org/abs/2201.08975v1)


[1037] Yiqing Tang, Xingyuan Dai, Chengchong Zhao, Qi Cheng, and Yisheng Lv. Large language modeldriven urban traffic signal control. _Australian and New Zealand Control Conference_, 2024.


[1038] Yongjian Tang, Rakebul Hasan, and Thomas Runkler. Fsponer: Few-shot prompt optimization for
named entity recognition in domain-specific scenarios. _European Conference on Artificial Intelligence_,
2024.


[1039] Yunlong Tang, Daiki Shimada, Jing Bi, Hang Hua, and Chenliang Xu. Empowering llms with pseudountrimmed videos for audio-visual temporal understanding. _AAAI Conference on Artificial Intelligence_,
2024.


[1040] Yao Tao, Yehui Tang, Yun Wang, Mingjian Zhu, Hailin Hu, and Yunhe Wang. Saliency-driven
dynamic token pruning for large language models, arXiv preprint arXiv:2504.04514, 2025. URL
[https://arxiv.org/abs/2504.04514v2.](https://arxiv.org/abs/2504.04514v2)


[1041] Denis Tarasov and Kumar Shridhar. Distilling llms’ decomposition abilities into compact language mod[els, arXiv preprint arXiv:2402.01812, 2024. URL https://arxiv.org/abs/2402.01812v1.](https://arxiv.org/abs/2402.01812v1)


[1042] Pittawat Taveekitworachai, Potsawee Manakul, Kasima Tharnpipitchai, and Kunat Pipatanakul.
[Typhoon t1: An open thai reasoning model, arXiv preprint arXiv:2502.09042, 2025. URL https:](https://arxiv.org/abs/2502.09042v2)
[//arxiv.org/abs/2502.09042v2.](https://arxiv.org/abs/2502.09042v2)


[1043] Yi Tay, Anh Tuan Luu, Minh C. Phan, and S. Hui. Multi-task neural network for non-discrete attribute
prediction in knowledge graphs. _International Conference on Information and Knowledge Management_,
2017.


[1044] 36Kr Editorial Team. The future of ai: From parameter scaling to context scaling. Online, 2025.
[URL https://36kr.com/p/3337269379328264. Chinese business and technology media pub-](https://36kr.com/p/3337269379328264)
lication discussing context scaling in large language models.


[1045] Junfeng Tian, Da Zheng, Yang Cheng, Rui Wang, Colin Zhang, and Debing Zhang. Untie the knots:
An efficient data augmentation strategy for long-context pre-training in language models, arXiv
[preprint arXiv:2409.04774, 2024. URL https://arxiv.org/abs/2409.04774v1.](https://arxiv.org/abs/2409.04774v1)


[1046] S Tian, R Wang, H Guo, P Wu, Y Dong, and X Wang.... Ego-r1: Chain-of-tool-thought for ultra-long
[egocentric video reasoning. 2025. URL https://arxiv.org/abs/2506.13654.](https://arxiv.org/abs/2506.13654)


[1047] Shulin Tian, Ruiqi Wang, Hongming Guo, Penghao Wu, Yuhao Dong, Xiuying Wang, Jingkang Yang,
Hao Zhang, Hongyuan Zhu, and Ziwei Liu. Ego-r1: Chain-of-tool-thought for ultra-long egocentric
video reasoning. arXiv preprint, 2025.


[1048] Ramine Tinati, Xin Wang, Ian C. Brown, T. Tiropanis, and W. Hall. A streaming real-time web
observatory architecture for monitoring the health of social machines. _The Web Conference_, 2015.


137


[1049] Kushal Tirumala, Aram H. Markosyan, Luke Zettlemoyer, and Armen Aghajanyan. Memorization
without overfitting: Analyzing the training dynamics of large language models. _Neural Information_
_Processing Systems_, 2022.


[1050] Despina Tomkou, George Fatouros, Andreas Andreou, Georgios Makridis, F. Liarokapis, Dimitrios
Dardanis, Athanasios Kiourtis, John Soldatos, and D. Kyriazis. Bridging industrial expertise and
[xr with llm-powered conversational agents, arXiv preprint arXiv:2504.05527, 2025. URL https:](https://arxiv.org/abs/2504.05527v1)
[//arxiv.org/abs/2504.05527v1.](https://arxiv.org/abs/2504.05527v1)


[1051] Sabrina Toro, A. V. Anagnostopoulos, Sue Bello, Kai Blumberg, Rhiannon Cameron, Leigh Carmody,
A. Diehl, Damion M. Dooley, William Duncan, P. Fey, Pascale Gaudet, Nomi L. Harris, marcin p.
joachimiak, Leila Kiani, Tiago Lubiana, M. Munoz-Torres, Shawn T. O’Neil, David Osumi-Sutherland,
Aleix Puig, Justin Reese, L. Reiser, Sofia M C Robb, Troy Ruemping, James Seager, Eric Sid, Ray
Stefancsik, Magalie Weber, Valerie Wood, M. Haendel, and Christopher J. Mungall. Dynamic retrieval
augmented generation of ontologies using artificial intelligence (dragon-ai). _Journal of Biomedical_
_Semantics_, 2023.


[1052] Fernanda M De La Torre, Cathy Mengying Fang, Han Huang, Andrzej Banburski-Fahey, Judith Amores
Fernandez, and Jaron Lanier. Llmr: Real-time prompting of interactive worlds using large language
models. _International Conference on Human Factors in Computing Systems_, 2023.


[1053] Martina Toshevska and Sonja Gievska. Llm-based text style transfer: Have we taken a step forward?
_IEEE Access_, 2025.


[1054] Fouad Trad and Ali Chehab. Evaluating the efficacy of prompt-engineered large multimodal models
versus fine-tuned vision transformers in image-based security applications. _ACM Transactions on_
_Intelligent Systems and Technology_, 2024.


[1055] Khanh-Tung Tran, Dung Dao, Minh-Duong Nguyen, Quoc-Viet Pham, Barry O’Sullivan, and Hoang D.
Nguyen. Multi-agent collaboration mechanisms: A survey of llms, arXiv preprint arXiv:2501.06322,
[2025. URL https://arxiv.org/abs/2501.06322v1.](https://arxiv.org/abs/2501.06322v1)


[1056] Harold Triedman, Rishi Jha, and Vitaly Shmatikov. Multi-agent systems execute arbitrary malicious
[code, arXiv preprint arXiv:2503.12188, 2025. URL https://arxiv.org/abs/2503.12188v1.](https://arxiv.org/abs/2503.12188v1)


[1057] H. Trivedi, Tushar Khot, Mareike Hartmann, R. Manku, Vinty Dong, Edward Li, Shashank Gupta,
Ashish Sabharwal, and Niranjan Balasubramanian. Appworld: A controllable world of apps and people
for benchmarking interactive coding agents. _Annual Meeting of the Association for Computational_
_Linguistics_, 2024.


[1058] Yun-Da Tsai, Ting-Yu Yen, Pei-Fu Guo, Zhe-Yan Li, and Shou-De Lin. Text-centric alignment for
[multi-modality learning, arXiv preprint arXiv:2402.08086v2, 2024. URL https://arxiv.org/](https://arxiv.org/abs/2402.08086v2)
[abs/2402.08086v2.](https://arxiv.org/abs/2402.08086v2)


[1059] Tao Tu, M. Schaekermann, Anil Palepu, Khaled Saab, Jan Freyberg, Ryutaro Tanno, Amy Wang,
Brenna Li, Mohamed Amin, Yong Cheng, Elahe Vedadi, Nenad Tomašev, Shekoofeh Azizi, Karan
Singhal, Le Hou, Albert Webson, Kavita Kulkarni, S. Mahdavi, Christopher Semturs, Juraj Gottweis,
Joelle Barral, Katherine Chou, Greg S. Corrado, Yossi Matias, A. Karthikesalingam, and Vivek
Natarajan. Towards conversational diagnostic artificial intelligence. _Nature_, 2025.


138


[1060] Eduard Tulchinskii, Laida Kushnareva, Kristian Kuznetsov, Anastasia Voznyuk, Andrei Andriiainen,
Irina Piontkovskaya, Evgeny Burnaev, and Serguei Barannikov. Listening to the wise few: Selectand-copy attention heads for multiple-choice qa, arXiv preprint arXiv:2410.02343, 2024. URL
[https://arxiv.org/abs/2410.02343v1.](https://arxiv.org/abs/2410.02343v1)


[1061] Meet Udeshi, Minghao Shao, Haoran Xi, Nanda Rani, Kimberly Milner, Venkata Sai Charan Putrevu,
Brendan Dolan-Gavitt, S. K. Shukla, P. Krishnamurthy, F. Khorrami, Ramesh Karri, and Muhammad Shafique. D-cipher: Dynamic collaborative intelligent multi-agent system with planner and
heterogeneous executors for offensive security. arXiv preprint, 2025.


[1062] M. Ursino, Nicole Cesaretti, and G. Pirazzini. A model of working memory for encoding multiple
items and ordered sequences exploiting the theta-gamma code. _Cognitive Neurodynamics_, 2022.


[1063] D. H. V. Uytsel, Filip Van Aelten, and Dirk Van Compernolle. A structured language model based
on context-sensitive probabilistic left-corner parsing. _North American Chapter of the Association for_
_Computational Linguistics_, 2001.


[1064] Saeid Ario Vaghefi, Aymane Hachcham, Veronica Grasso, Jiska Manicus, Nakiete Msemo, C. Senni,
and Markus Leippold. Ai for climate finance: Agentic retrieval and multi-step reasoning for early
[warning system investments, arXiv preprint arXiv:2504.05104, 2025. URL https://arxiv.org/](https://arxiv.org/abs/2504.05104v2)
[abs/2504.05104v2.](https://arxiv.org/abs/2504.05104v2)


[1065] Priyan Vaithilingam, Tianyi Zhang, and Elena L. Glassman. Expectation vs. experience: Evaluating
the usability of code generation tools powered by large language models. _CHI Extended Abstracts_,
2022.


[1066] Phuc Phan Van, Dat Nguyen Minh, An Dinh Ngoc, and Huy-Phan Thanh. Rx strategist: Prescription
[verification using llm agents system, arXiv preprint arXiv:2409.03440, 2024. URL https://arxiv.](https://arxiv.org/abs/2409.03440v1)
[org/abs/2409.03440v1.](https://arxiv.org/abs/2409.03440v1)


[1067] Ashish Vaswani, Noam M. Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez,
Lukasz Kaiser, and I. Polosukhin. Attention is all you need. _Neural Information Processing Systems_,
2017.


[1068] J. D. Velásquez-Henao, Carlos Jaime Franco-Cardona, and Lorena Cadavid-Higuita. Prompt engineering: a methodology for optimizing interactions with ai-language models in the field of engineering.
_DYNA_, 2023.


[1069] Gaurav Verma, Rachneet Kaur, Nishan Srishankar, Zhen Zeng, T. Balch, and Manuela Veloso. Adaptagent: Adapting multimodal web agents with few-shot learning from human demonstrations, arXiv
[preprint arXiv:2411.13451, 2024. URL https://arxiv.org/abs/2411.13451v1.](https://arxiv.org/abs/2411.13451v1)


[1070] Aliaksei Vertsel and Mikhail Rumiantsau. Hybrid llm/rule-based approaches to business insights
[generation from structured data, arXiv preprint arXiv:2404.15604, 2024. URL https://arxiv.](https://arxiv.org/abs/2404.15604v1)
[org/abs/2404.15604v1.](https://arxiv.org/abs/2404.15604v1)


[1071] Aishwarya Vijayan. A prompt engineering approach for structured data extraction from unstructured
text using conversational llms. _International Conference on Advances in Computing and Artificial_
_Intelligence_, 2023.


139


[1072] Juraj Vladika, Alexander Fichtl, and Florian Matthes. Diversifying knowledge enhancement of
biomedical language models using adapter modules and knowledge graphs. _International Conference_
_on Agents and Artificial Intelligence_, 2023.


[1073] James Vo. Sparseaccelerate: Efficient long-context inference for mid-range gpus, arXiv preprint
[arXiv:2412.06198, 2024. URL https://arxiv.org/abs/2412.06198v1.](https://arxiv.org/abs/2412.06198v1)


[1074] Blavz vSkrlj, Boshko Koloski, S. Pollak, and Nada Lavravc. From symbolic to neural and back:
Exploring knowledge graph-large language model synergies. arXiv preprint, 2025.


[1075] Tom Völker, Jan Pfister, Tobias Koopmann, and Andreas Hotho. From chat to publication management:
Organizing your related work using bibsonomy & llms. _Conference on Human Information Interaction_
_and Retrieval_, 2024.


[1076] D. Walton. Using argumentation schemes to find motives and intentions of a rational agent. _Argument_
_Comput._, 2020.


[1077] Hanlong Wan, Jian Zhang, Yan Chen, Weili Xu, and Fan Feng. Generative ai application for building
industry. _Building Simulation_, 2024.


[1078] Jun Wan and Lingrui Mei. Large language models as computable approximations to solomonoff in[duction, arXiv preprint arXiv:2505.15784, 2025. URL https://arxiv.org/abs/2505.15784.](https://arxiv.org/abs/2505.15784)


[1079] Luanbo Wan and Weizhi Ma. Storybench: A dynamic benchmark for evaluating long-term memory
[with multi turns, arXiv preprint arXiv:2506.13356, 2025. URL https://arxiv.org/abs/2506.](https://arxiv.org/abs/2506.13356v1)
[13356v1.](https://arxiv.org/abs/2506.13356v1)


[1080] Bernie Wang, Si ting Xu, K. Keutzer, Yang Gao, and Bichen Wu. Improving context-based
meta-reinforcement learning with self-supervised trajectory contrastive learning, arXiv preprint
[arXiv:2103.06386, 2021. URL https://arxiv.org/abs/2103.06386v1.](https://arxiv.org/abs/2103.06386v1)


[1081] Bing Wang, Xinnian Liang, Jian Yang, Hui Huang, Shuangzhi Wu, Peihao Wu, Lu Lu, Zejun Ma, and
Zhoujun Li. Scm: Enhancing large language model with self-controlled memory framework, arXiv
[preprint arXiv:2304.13343, 2025. URL https://arxiv.org/abs/2304.13343.](https://arxiv.org/abs/2304.13343)


[1082] Cangqing Wang, Yutian Yang, Ruisi Li, Dan Sun, Ruicong Cai, Yuzhu Zhang, and Chengqian Fu.
Adapting llms for efficient context processing through soft prompt compression. _Proceedings of the_
_International Conference on Modeling, Natural Language Processing and Machine Learning_, 2024.


[1083] Chaozheng Wang, Yuanhang Yang, Cuiyun Gao, Yun Peng, Hongyu Zhang, and Michael R. Lyu.
Prompt tuning in code intelligence: An experimental evaluation. _IEEE Transactions on Software_
_Engineering_, 2023.


[1084] Dongsheng Wang, Zhiqiang Ma, Armineh Nourbakhsh, Kang Gu, and Sameena Shah. Docgraphlm:
Documental graph language model for information extraction. _Annual International ACM SIGIR_
_Conference on Research and Development in Information Retrieval_, 2023.


[1085] Fan Wang, Chuan Lin, Yang Cao, and Yu Kang. Benchmarking general purpose in-context learning,
[arXiv preprint arXiv:2405.17234, 2024. URL https://arxiv.org/abs/2405.17234v6.](https://arxiv.org/abs/2405.17234v6)


[1086] Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei Xiao, Yuke Zhu, Linxi Fan, and
Anima Anandkumar. Voyager: An open-ended embodied agent with large language models, arXiv
[preprint arXiv:2305.16291, 2023. URL https://arxiv.org/abs/2305.16291.](https://arxiv.org/abs/2305.16291)


140


[1087] Guoqing Wang, Zeyu Sun, Zhihao Gong, Sixiang Ye, Yizhou Chen, Yifan Zhao, Qing-Lin Liang, and
Dan Hao. Do advanced language models eliminate the need for prompt engineering in software
[engineering?, arXiv preprint arXiv:2411.02093, 2024. URL https://arxiv.org/abs/2411.](https://arxiv.org/abs/2411.02093v1)
[02093v1.](https://arxiv.org/abs/2411.02093v1)


[1088] Hanlin Wang, Zhan Tong, Kecheng Zheng, Yujun Shen, and Limin Wang. Contextual ad narration
with interleaved multimodal sequence. _Computer Vision and Pattern Recognition_, 2024.


[1089] Hanrui Wang, Zhekai Zhang, and Song Han. Spatten: Efficient sparse attention architecture
with cascade token and head pruning. _International Symposium on High-Performance Computer_
_Architecture_, 2020.


[1090] Haochen Wang, Xiangtai Li, Zilong Huang, Anran Wang, Jiacong Wang, Tao Zhang, Jiani Zheng,
Sule Bai, Zijian Kang, Jiashi Feng, et al. Traceable evidence enhanced visual grounded reasoning:
Evaluation and methodology. _arXiv preprint arXiv:2507.07999_, 2025.


[1091] Haochun Wang, Chi Liu, Nuwa Xi, Zewen Qiang, Sendong Zhao, Bing Qin, and Ting Liu. Huatuo:
Tuning llama model with chinese medical knowledge, arXiv preprint arXiv:2304.06975, 2023. URL
[https://arxiv.org/abs/2304.06975.](https://arxiv.org/abs/2304.06975)


[1092] Haoyu Wang, Tong Teng, Tianyu Guo, An Xiao, Duyu Tang, Hanting Chen, and Yunhe Wang.
Unshackling context length: An efficient selective attention approach through query-key compression,
[arXiv preprint arXiv:2502.14477, 2025. URL https://arxiv.org/abs/2502.14477v1.](https://arxiv.org/abs/2502.14477v1)


[1093] Heng Wang, Shangbin Feng, Tianxing He, Zhaoxuan Tan, Xiaochuang Han, and Yulia Tsvetkov. Can
language models solve graph problems in natural language? _Neural Information Processing Systems_,
2023.


[1094] Hengyi Wang, Haizhou Shi, Shiwei Tan, Weiyi Qin, Wenyuan Wang, Tunyu Zhang, A. Nambi,
T. Ganu, and Hao Wang. Multimodal needle in a haystack: Benchmarking long-context capability of
multimodal large language models. _North American Chapter of the Association for Computational_
_Linguistics_, 2024.


[1095] Hongru Wang, Cheng Qian, Manling Li, Jiahao Qiu, Boyang Xue, Mengdi Wang, Heng Ji, and Kam-Fai
Wong. Toward a theory of agents as tool-use decision-makers, arXiv preprint arXiv:2506.00886,
[2025. URL https://arxiv.org/abs/2506.00886v1.](https://arxiv.org/abs/2506.00886v1)


[1096] Jingjin Wang. Proprag: Guiding retrieval with beam search over proposition paths, arXiv preprint
[arXiv:2504.18070, 2025. URL https://arxiv.org/abs/2504.18070v1.](https://arxiv.org/abs/2504.18070v1)


[1097] Jingyu Wang, Lu Zhang, Xueqing Li, Huazhong Yang, and Yongpan Liu. Ulseq-ta: Ultra-long sequence
attention fusion transformer accelerator supporting grouped sparse softmax and dual-path sparse
layernorm. _IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems_, 2024.


[1098] Jize Wang, Zerun Ma, Yining Li, Songyang Zhang, Cailian Chen, Kai Chen, and Xinyi Le. Gta: A
benchmark for general tool agents. _Neural Information Processing Systems_, 2024.


[1099] Lei Wang, Chengbang Ma, Xueyang Feng, Zeyu Zhang, Hao ran Yang, Jingsen Zhang, Zhi-Yang
Chen, Jiakai Tang, Xu Chen, Yankai Lin, Wayne Xin Zhao, Zhewei Wei, and Ji rong Wen. A survey
on large language model based autonomous agents. _Frontiers Comput. Sci._, 2023.


141


[1100] Lei Wang, Wanyu Xu, Yihuai Lan, Zhiqiang Hu, Yunshi Lan, Roy Ka-Wei Lee, and Ee-Peng Lim. Planand-solve prompting: Improving zero-shot chain-of-thought reasoning by large language models.
_Annual Meeting of the Association for Computational Linguistics_, 2023.


[1101] Lei Wang, Jingsen Zhang, Hao Yang, Zhiyuan Chen, Jiakai Tang, Zeyu Zhang, Xu Chen, Yankai Lin,
Ruihua Song, Wayne Xin Zhao, et al. When large language model based agent meets user behavior
analysis: A novel user simulation paradigm. 2023.


[1102] Libo Wang. Towards humanoid robot autonomy: A dynamic architecture integrating continuous
thought machines (ctm) and model context protocol (mcp), arXiv preprint arXiv:2505.19339, 2025.
[URL https://arxiv.org/abs/2505.19339v1.](https://arxiv.org/abs/2505.19339v1)


[1103] Liya Wang, Jason Chou, Xin Zhou, A. Tien, and Diane M. Baumgartner. Aviationgpt: A large
[language model for the aviation domain, arXiv preprint arXiv:2311.17686, 2023. URL https:](https://arxiv.org/abs/2311.17686v1)
[//arxiv.org/abs/2311.17686v1.](https://arxiv.org/abs/2311.17686v1)


[1104] Liyuan Wang, Bo Lei, Qian Li, Hang Su, Jun Zhu, and Yi Zhong. Triple-memory networks: A
brain-inspired method for continual learning. _IEEE Transactions on Neural Networks and Learning_
_Systems_, 2020.


[1105] Lu Wang, Fangkai Yang, Chaoyun Zhang, Junting Lu, Jiaxu Qian, Shilin He, Pu Zhao, Bo Qiao, Ray
Huang, Si Qin, Qisheng Su, Jiayi Ye, Yudi Zhang, Jian-Guang Lou, Qingwei Lin, Saravan Rajmohan,
Dongmei Zhang, and Qi Zhang. Large action models: From inception to implementation, arXiv
[preprint arXiv:2412.10047, 2025. URL https://arxiv.org/abs/2412.10047.](https://arxiv.org/abs/2412.10047)


[1106] Peijie Wang, Zhong-Zhi Li, Fei Yin, Xin Yang, Dekang Ran, and Cheng-Lin Liu. Mv-math: Evaluating
multimodal math reasoning in multi-visual contexts. 2025.


[1107] Qineng Wang, Zihao Wang, Ying Su, and Yangqiu Song. On the discussion of large language models:
Symmetry of agents and interplay with prompts, arXiv preprint arXiv:2311.07076, 2023. URL
[https://arxiv.org/abs/2311.07076v1.](https://arxiv.org/abs/2311.07076v1)


[1108] Qingyue Wang, Liang Ding, Yanan Cao, Yibing Zhan, Zheng Lin, Shi Wang, Dacheng Tao, and Li Guo.
Divide, conquer, and combine: Mixture of semantic-independent experts for zero-shot dialogue state
[tracking, arXiv preprint arXiv:2306.00434, 2023. URL https://arxiv.org/abs/2306.00434.](https://arxiv.org/abs/2306.00434)


[1109] Rongzheng Wang, Shuang Liang, Qizhi Chen, Jiasheng Zhang, and Ke Qin. Graphtool-instruction:
Revolutionizing graph reasoning in llms through decomposed subtask instruction. _Knowledge Discovery_
_and Data Mining_, 2024.

