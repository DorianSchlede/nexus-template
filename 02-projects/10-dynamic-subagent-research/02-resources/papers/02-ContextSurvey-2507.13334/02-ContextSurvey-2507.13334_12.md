<!-- Source: 02-ContextSurvey-2507.13334.pdf | Chunk 12/26 -->

[217] Frederick Dillon, Gregor Halvorsen, Simon Tattershall, Magnus Rowntree, and Gareth Vanderpool.
Contextual memory reweaving in large language models using layered latent state reconstruction,
[arXiv preprint arXiv:2502.02046, 2025. URL https://arxiv.org/abs/2502.02046v2.](https://arxiv.org/abs/2502.02046v2)


[218] Hanxing Ding, Shuchang Tao, Liang Pang, Zihao Wei, Jinyang Gao, Bolin Ding, Huawei Shen,
and Xueqi Chen. Toolcoder: A systematic code-empowered tool learning framework for large
[language models, arXiv preprint arXiv:2502.11404, 2025. URL https://arxiv.org/abs/2502.](https://arxiv.org/abs/2502.11404v2)
[11404v2.](https://arxiv.org/abs/2502.11404v2)


75


[219] Hongxin Ding, Yue Fang, Runchuan Zhu, Xinke Jiang, Jinyang Zhang, Yongxin Xu, Xu Chu, Junfeng
Zhao, and Yasha Wang. 3ds: Decomposed difficulty data selection’s case study on llm medical
domain adaptation. 2024.


[220] Jiayu Ding, Shuming Ma, Li Dong, Xingxing Zhang, Shaohan Huang, Wenhui Wang, and Furu Wei.
Longnet: Scaling transformers to 1, 000, 000, 000 tokens. arXiv preprint, 2023.


[221] Tianyu Ding, Tianyi Chen, Haidong Zhu, Jiachen Jiang, Yiqi Zhong, Jinxin Zhou, Guangzhi Wang,
Zhihui Zhu, Ilya Zharkov, and Luming Liang. The efficiency spectrum of large language models:
[An algorithmic survey, arXiv preprint arXiv:2312.00678, 2023. URL https://arxiv.org/abs/](https://arxiv.org/abs/2312.00678v2)
[2312.00678v2.](https://arxiv.org/abs/2312.00678v2)


[222] Yiran Ding, L. Zhang, Chengruidong Zhang, Yuanyuan Xu, Ning Shang, Jiahang Xu, Fan Yang,
and Mao Yang. Longrope: Extending llm context window beyond 2 million tokens. _International_
_Conference on Machine Learning_, 2024.


[223] Yiwen Ding, Zhiheng Xi, Wei He, Zhuoyuan Li, Yitao Zhai, Xiaowei Shi, Xunliang Cai, Tao Gui,
Qi Zhang, and Xuanjing Huang. Mitigating tail narrowing in llm self-improvement via socratic-guided
sampling. _North American Chapter of the Association for Computational Linguistics_, 2024.


[224] Christian Djeffal. Reflexive prompt engineering: A framework for responsible prompt engineering
and ai interaction design. _Conference on Fairness, Accountability and Transparency_, 2025.


[225] G Dong, Y Chen, X Li, J Jin, H Qian, and Y Zhu.... Tool-star: Empowering llm-brained multi-tool
[reasoner via reinforcement learning. 2025. URL https://arxiv.org/abs/2505.16410.](https://arxiv.org/abs/2505.16410)


[226] Guanting Dong, Jinxu Zhao, Tingfeng Hui, Daichi Guo, Wenlong Wan, Boqi Feng, Yueyan Qiu,
Zhuoma Gongque, Keqing He, Zechen Wang, and Weiran Xu. Revisit input perturbation problems
for llms: A unified robustness evaluation framework for noisy slot filling task. _Natural Language_
_Processing and Chinese Computing_, 2023.


[227] Guanting Dong, Yifei Chen, Xiaoxi Li, Jiajie Jin, Hongjin Qian, Yutao Zhu, Hangyu Mao, Guorui
Zhou, Zhicheng Dou, and Ji-Rong Wen. Tool-star: Empowering llm-brained multi-tool reasoner via
reinforcement learning. arXiv preprint, 2025.


[228] Kaiwen Dong. Large language model applied in multi-agent systema survey. _Applied and Computa-_
_tional Engineering_, 2024.


[229] Peijie Dong, Zhenheng Tang, Xiang-Hong Liu, Lujun Li, Xiaowen Chu, and Bo Li. Can compressed
llms truly act? an empirical evaluation of agentic capabilities in llm compression, arXiv preprint
[arXiv:2505.19433, 2025. URL https://arxiv.org/abs/2505.19433v2.](https://arxiv.org/abs/2505.19433v2)


[230] Vicky Dong, Hao Yu, and Yao Chen. Graph-augmented relation extraction model with llms-generated
[support document, arXiv preprint arXiv:2410.23452, 2024. URL https://arxiv.org/abs/](https://arxiv.org/abs/2410.23452v1)
[2410.23452v1.](https://arxiv.org/abs/2410.23452v1)


[231] Xiangjue Dong, Maria Teleki, and James Caverlee. A survey on llm inference-time self-improvement,
[arXiv preprint arXiv:2412.14352, 2024. URL https://arxiv.org/abs/2412.14352v1.](https://arxiv.org/abs/2412.14352v1)


[232] Yuxin Dong, Shuo Wang, Hongye Zheng, Jiajing Chen, Zhenhong Zhang, and Chihang Wang.
Advanced rag models with graph structures: Optimizing complex knowledge reasoning and text generation. _2024 5th International Symposium on Computer Engineering and Intelligent Communications_
_(ISCEIC)_, 2024.


76


[233] Zican Dong, Junyi Li, Xin Men, Wayne Xin Zhao, Bingbing Wang, Zhen Tian, Weipeng Chen, and
Ji-Rong Wen. Exploring context window of large language models via decomposed positional vectors.
_Neural Information Processing Systems_, 2024.


[234] Ehsan Doostmohammadi and Marco Kuhlmann. Studying the role of input-neighbor overlap in
retrieval-augmented language models training efficiency, arXiv preprint arXiv:2505.14309, 2025.
[URL https://arxiv.org/abs/2505.14309v1.](https://arxiv.org/abs/2505.14309v1)


[235] Mohammadreza Doostmohammadian, Alireza Aghasi, Mohammad Pirani, Ehsan Nekouei, H. Zarrabi,
Reza Keypour, Apostolos I. Rikos, and K. H. Johansson. Survey of distributed algorithms for resource
[allocation over multi-agent systems, arXiv preprint arXiv:2401.15607, 2024. URL https://arxiv.](https://arxiv.org/abs/2401.15607v1)
[org/abs/2401.15607v1.](https://arxiv.org/abs/2401.15607v1)


[236] A. Dorri, S. Kanhere, and R. Jurdak. Multi-agent systems: A survey. _IEEE Access_, 2018.


[237] Mauro Dragone. Component & service-based agent systems: Self-osgi. _International Conference on_
_Agents and Artificial Intelligence_, 2012.


[238] Alexandre Drouin, Maxime Gasse, Massimo Caccia, Issam H. Laradji, Manuel Del Verme, Tom
Marty, David Vazquez, Nicolas Chapados, and Alexandre Lacoste. WorkArena: How capable are web
agents at solving common knowledge work tasks? In Ruslan Salakhutdinov, Zico Kolter, Katherine
Heller, Adrian Weller, Nuria Oliver, Jonathan Scarlett, and Felix Berkenkamp, editors, _Proceedings_
_of the 41st International Conference on Machine Learning_, volume 235 of _Proceedings of Machine_
_Learning Research_ [, pages 11642–11662. PMLR, 21–27 Jul 2024. URL https://proceedings.](https://proceedings.mlr.press/v235/drouin24a.html)
[mlr.press/v235/drouin24a.html.](https://proceedings.mlr.press/v235/drouin24a.html)


[239] Hung Du, Srikanth Thudumu, Rajesh Vasa, and K. Mouzakis. A survey on context-aware multi-agent
systems: Techniques, challenges and future directions, arXiv preprint arXiv:2402.01968, 2024. URL
[https://arxiv.org/abs/2402.01968v2.](https://arxiv.org/abs/2402.01968v2)


[240] Jingfei Du, Edouard Grave, Beliz Gunel, Vishrav Chaudhary, Onur Çelebi, Michael Auli, Ves Stoyanov,
and Alexis Conneau. Self-training improves pre-training for natural language understanding. _North_
_American Chapter of the Association for Computational Linguistics_, 2020.


[241] Jusen Du, Weigao Sun, Disen Lan, Jiaxi Hu, and Yu Cheng. Mom: Linear sequence modeling with
[mixture-of-memories, arXiv preprint arXiv:2502.13685, 2025. URL https://arxiv.org/abs/](https://arxiv.org/abs/2502.13685v2)
[2502.13685v2.](https://arxiv.org/abs/2502.13685v2)


[242] Mingxuan Du, Benfeng Xu, Chiwei Zhu, Xiaorui Wang, and Zhendong Mao. Deepresearch bench: A
comprehensive benchmark for deep research agents, arXiv preprint arXiv:2506.11763, 2025. URL
[https://arxiv.org/abs/2506.11763v1.](https://arxiv.org/abs/2506.11763v1)


[243] Shangheng Du, Jiabao Zhao, Jinxin Shi, Zhentao Xie, Xin Jiang, Yanhong Bai, and Liang He. A
survey on the optimization of large language model-based agents, arXiv preprint arXiv:2503.12434,
[2025. URL https://arxiv.org/abs/2503.12434v1.](https://arxiv.org/abs/2503.12434v1)


[244] Yiming Du, Hongru Wang, Zhengyi Zhao, Bin Liang, Baojun Wang, Wanjun Zhong, Zezhong Wang,
and Kam-Fai Wong. Perltqa: A personal long-term memory dataset for memory classification,
[retrieval, and synthesis in question answering, arXiv preprint arXiv:2402.16288, 2024. URL https:](https://arxiv.org/abs/2402.16288v1)
[//arxiv.org/abs/2402.16288v1.](https://arxiv.org/abs/2402.16288v1)


77


[245] Hanqi Duan, Yao Cheng, Jianxiang Yu, and Xiang Li. Can large language models act as ensembler
[for multi-gnns?, arXiv preprint arXiv:2410.16822, 2024. URL https://arxiv.org/abs/2410.](https://arxiv.org/abs/2410.16822v2)
[16822v2.](https://arxiv.org/abs/2410.16822v2)


[246] Peitong Duan, Chin yi Chen, Bjoern Hartmann, and Yang Li. Visual prompting with iterative
[refinement for design critique generation, arXiv preprint arXiv:2412.16829, 2024. URL https:](https://arxiv.org/abs/2412.16829v2)
[//arxiv.org/abs/2412.16829v2.](https://arxiv.org/abs/2412.16829v2)


[247] Brown Ebouky, A. Bartezzaghi, and Mattia Rigotti. Eliciting reasoning in language models with
[cognitive tools, arXiv preprint arXiv:2506.12115, 2025. URL https://arxiv.org/abs/2506.](https://arxiv.org/abs/2506.12115v1)
[12115v1.](https://arxiv.org/abs/2506.12115v1)


[248] Darren Edge, Ha Trinh, Newman Cheng, Joshua Bradley, Alex Chao, Apurva Mody, Steven Truitt,
Dasha Metropolitansky, Robert Osazuwa Ness, and Jonathan Larson. From local to global: A graph
rag approach to query-focused summarization. _arXiv preprint arXiv:2404.16130_, 2024.


[249] Candace Edwards. Hybrid context retrieval augmented generation pipeline: Llm-augmented
knowledge graphs and vector database for accreditation reporting assistance, arXiv preprint
[arXiv:2405.15436, 2024. URL https://arxiv.org/abs/2405.15436v1.](https://arxiv.org/abs/2405.15436v1)


[250] Abul Ehtesham, Aditi Singh, Gaurav Kumar Gupta, and Saket Kumar. A survey of agent interoperability protocols: Model context protocol (mcp), agent communication protocol (acp), agent-to-agent
protocol (a2a), and agent network protocol (anp), arXiv preprint arXiv:2505.02279, 2025. URL
[https://arxiv.org/abs/2505.02279v2.](https://arxiv.org/abs/2505.02279v2)


[251] Will Epperson, Gagan Bansal, Victor Dibia, Adam Fourney, Jack Gerrits, Erkang Zhu, and Saleema
Amershi. Interactive debugging and steering of multi-agent ai systems. _International Conference on_
_Human Factors in Computing Systems_, 2025.


[252] Lutfi Eren Erdogan, Nicholas Lee, Siddharth Jha, Sehoon Kim, Ryan Tabrizi, Suhong Moon, Coleman
Hooper, G. Anumanchipalli, Kurt Keutzer, and A. Gholami. Tinyagent: Function calling at the edge.
_Conference on Empirical Methods in Natural Language Processing_, 2024.


[253] Oluwole Fagbohun, Rachel M. Harrison, and Anton Dereventsov. An empirical categorization of
prompting techniques for large language models: A practitioner’s guide. arXiv preprint, 2024.


[254] Kazem Faghih, Wenxiao Wang, Yize Cheng, Siddhant Bharti, Gaurang Sriramanan, S. Balasubramanian, Parsa Hosseini, and S. Feizi. Gaming tool preferences in agentic llms, arXiv preprint
[arXiv:2505.18135, 2025. URL https://arxiv.org/abs/2505.18135v1.](https://arxiv.org/abs/2505.18135v1)


[255] Linxi (Jim) Fan, Guanzhi Wang, Yunfan Jiang, Ajay Mandlekar, Yuncong Yang, Haoyi Zhu, Andrew
Tang, De-An Huang, Yuke Zhu, and Anima Anandkumar. Minedojo: Building open-ended embodied
agents with internet-scale knowledge. _Neural Information Processing Systems_, 2022.


[256] Siqi Fan, Xiusheng Huang, Yiqun Yao, Xuezhi Fang, Kang Liu, Peng Han, Shuo Shang, Aixin Sun, and
Yequan Wang. If an llm were a character, would it know its own story? evaluating lifelong learning in
[llms, arXiv preprint arXiv:2503.23514, 2025. URL https://arxiv.org/abs/2503.23514v1.](https://arxiv.org/abs/2503.23514v1)


[257] Wenqi Fan, Yujuan Ding, Liang bo Ning, Shijie Wang, Hengyun Li, Dawei Yin, Tat-Seng Chua,
and Qing Li. A survey on rag meeting llms: Towards retrieval-augmented large language models.
_Knowledge Discovery and Data Mining_, 2024.


78


[258] Yue Fan, Xiaojian Ma, Rujie Wu, Yuntao Du, Jiaqi Li, Zhi Gao, and Qing Li. Videoagent: A memoryaugmented multimodal agent for video understanding, arXiv preprint arXiv:2403.11481, 2024. URL
[https://arxiv.org/abs/2403.11481v2.](https://arxiv.org/abs/2403.11481v2)


[259] Hongchao Fang and Pengtao Xie. An end-to-end contrastive self-supervised learning framework for
language understanding. _Transactions of the Association for Computational Linguistics_, 2022.


[260] Hongchao Fang, Sicheng Wang, Meng Zhou, Jiayuan Ding, and Pengtao Xie. Cert: Contrastive
self-supervised learning for language understanding, arXiv preprint arXiv:2005.12766, 2020. URL
[https://arxiv.org/abs/2005.12766v2.](https://arxiv.org/abs/2005.12766v2)


[261] Junfeng Fang, Zijun Yao, Ruipeng Wang, Haokai Ma, Xiang Wang, and Tat-Seng Chua. We
should identify and mitigate third-party safety risks in mcp-powered agent systems, arXiv preprint
[arXiv:2506.13666, 2025. URL https://arxiv.org/abs/2506.13666v1.](https://arxiv.org/abs/2506.13666v1)


[262] Siyuan Fang, Kaijing Ma, Tianyu Zheng, Xinrun Du, Ningxuan Lu, Ge Zhang, and Qingkun Tang.
Karpa: A training-free method of adapting knowledge graph as references for large language model’s
reasoning path aggregation. arXiv preprint, 2024.


[263] Wei-Wen Fang, Yang Zhang, Kaizhi Qian, James Glass, and Yada Zhu. Play2prompt: Zero-shot tool
instruction optimization for llm agents via tool play, arXiv preprint arXiv:2503.14432, 2025. URL
[https://arxiv.org/abs/2503.14432v2.](https://arxiv.org/abs/2503.14432v2)


[264] Yi Fang, Dongzhe Fan, D. Zha, and Qiaoyu Tan. Gaugllm: Improving graph contrastive learning for
text-attributed graphs with large language models. _Knowledge Discovery and Data Mining_, 2024.


[265] Yi Fang, Bowen Jin, Jiacheng Shen, Sirui Ding, Qiaoyu Tan, and Jiawei Han. Graphgpt-o: Synergistic
multimodal comprehension and generation on graphs. arXiv preprint, 2025.


[266] Bahare Fatemi, Jonathan J. Halcrow, and Bryan Perozzi. Talk like a graph: Encoding graphs for large
language models. _International Conference on Learning Representations_, 2023.


[267] George Fatouros, Georgios Makridis, George Kousiouris, John Soldatos, A. Tsadimas, and D. Kyriazis.
Towards conversational ai for human-machine collaborative mlops, arXiv preprint arXiv:2504.12477,
[2025. URL https://arxiv.org/abs/2504.12477v1.](https://arxiv.org/abs/2504.12477v1)


[268] M. Fauth, F. Wörgötter, and Christian Tetzlaff. Formation and maintenance of robust long-term
information storage in the presence of synaptic turnover. _bioRxiv_, 2015.


[269] Zahra Fayyaz, Aya Altamimi, Sen Cheng, and Laurenz Wiskott. A model of semantic completion in
generative episodic memory. _Neural Computation_, 2021.


[270] Xiang Fei, Xiawu Zheng, and Hao Feng. Mcp-zero: Proactive toolchain construction for llm agents
from scratch. arXiv preprint, 2025.


[271] Philip Feldman, James R. Foulds, and Shimei Pan. Ragged edges: The double-edged sword of
[retrieval-augmented chatbots, arXiv preprint arXiv:2403.01193, 2024. URL https://arxiv.](https://arxiv.org/abs/2403.01193v3)
[org/abs/2403.01193v3.](https://arxiv.org/abs/2403.01193v3)


[272] Aosong Feng, Rex Ying, and L. Tassiulas. Long sequence modeling with attention tensorization:
From sequence to tensor learning. _Conference on Empirical Methods in Natural Language Processing_,
2024.


79


[273] Erhu Feng, Wenbo Zhou, Zibin Liu, Le Chen, Yunpeng Dong, Cheng Zhang, Yisheng Zhao, Dong Du,
Zhi-Hua Zhou, Yubin Xia, and Haibo Chen. Get experience from practice: Llm agents with record &
[replay, arXiv preprint arXiv:2505.17716, 2025. URL https://arxiv.org/abs/2505.17716v1.](https://arxiv.org/abs/2505.17716v1)


[274] Jiazhan Feng, Shijue Huang, Xingwei Qu, Ge Zhang, Yujia Qin, Baoquan Zhong, Chengquan Jiang,
Jinxin Chi, and Wanjun Zhong. Retool: Reinforcement learning for strategic tool use in llms, arXiv
[preprint arXiv:2504.11536, 2025. URL https://arxiv.org/abs/2504.11536v2.](https://arxiv.org/abs/2504.11536v2)


[275] Kaituo Feng, Changsheng Li, Xiaolu Zhang, Jun Zhou, Ye Yuan, and Guoren Wang. Keypoint-based
progressive chain-of-thought distillation for llms. _International Conference on Machine Learning_,
2024.


[276] Leo Feng, Frederick Tung, Hossein Hajimirsadeghi, Y. Bengio, and M. O. Ahmed. Constant memory
[attention block, arXiv preprint arXiv:2306.12599, 2023. URL https://arxiv.org/abs/2306.](https://arxiv.org/abs/2306.12599v1)
[12599v1.](https://arxiv.org/abs/2306.12599v1)


[277] Yanlin Feng, Xinyue Chen, Bill Yuchen Lin, Peifeng Wang, Jun Yan, and Xiang Ren. Scalable multi-hop
relational reasoning for knowledge-aware question answering. _Conference on Empirical Methods in_
_Natural Language Processing_, 2020.


[278] Yifan Feng, Shiquan Liu, Xiangmin Han, Shaoyi Du, Zongze Wu, Han Hu, and Yue Gao. Hypergraph
[foundation model, arXiv preprint arXiv:2503.01203v1, 2025. URL https://arxiv.org/abs/](https://arxiv.org/abs/2503.01203v1)
[2503.01203v1.](https://arxiv.org/abs/2503.01203v1)


[279] Chrisantha Fernando, Dylan Banarse, H. Michalewski, Simon Osindero, and Tim Rocktäschel. Promptbreeder: Self-referential self-improvement via prompt evolution. _International Conference on Machine_
_Learning_, 2023.


[280] Tharindu Fernando, Simon Denman, A. Mcfadyen, S. Sridharan, and C. Fookes. Tree memory
networks for modelling long-term temporal dependencies. _Neurocomputing_, 2017.


[281] M. Ferrag, Norbert Tihanyi, and M. Debbah. From llm reasoning to autonomous ai agents: A
[comprehensive review, arXiv preprint arXiv:2504.19678, 2025. URL https://arxiv.org/abs/](https://arxiv.org/abs/2504.19678v1)
[2504.19678v1.](https://arxiv.org/abs/2504.19678v1)


[282] M. Ferrag, Norbert Tihanyi, and M. Debbah. Reasoning beyond limits: Advances and open prob[lems for llms, arXiv preprint arXiv:2503.22732, 2025. URL https://arxiv.org/abs/2503.](https://arxiv.org/abs/2503.22732v1)
[22732v1.](https://arxiv.org/abs/2503.22732v1)


[283] Christopher Fifty, Dennis Duan, Ronald G. Junkins, Ehsan Amid, Jurij Leskovec, Christopher R’e, and
Sebastian Thrun. Context-aware meta-learning. _International Conference on Learning Representations_,
2023.


[284] Tim Finin, Richard Fritzson, Donald P McKay, Robin McEntire, et al. Kqml-a language and protocol
for knowledge and information exchange. In _13th Int. Distributed Artificial Intelligence Workshop_,
pages 93–103, 1994.


[285] Chelsea Finn, P. Abbeel, and S. Levine. Model-agnostic meta-learning for fast adaptation of deep
networks. _International Conference on Machine Learning_, 2017.


[286] Paolo Finotelli and Francis Eustache. Mathematical modeling of human memory. _Frontiers in_
_Psychology_, 2023.


80


[287] Ferdinando Fioretto, Enrico Pontelli, and W. Yeoh. Distributed constraint optimization problems and
applications: A survey. _Journal of Artificial Intelligence Research_, 2016.


[288] Meire Fortunato, Melissa Tan, Ryan Faulkner, S. Hansen, Adrià Puigdomènech Badia, Gavin Buttimore,
Charlie Deck, Joel Z. Leibo, and C. Blundell. Generalization of reinforcement learners with working
and episodic memory. _Neural Information Processing Systems_, 2019.


[289] Samy Foudil, Claire Pleche, and E. Macaluso. Memory for spatio-temporal contextual details during
the retrieval of naturalistic episodes. _Scientific Reports_, 2021.


[290] Zafeirios Fountas, Martin A Benfeghoul, Adnan Oomerjee, Fenia Christopoulou, Gerasimos Lampouras, Haitham Bou-Ammar, and Jun Wang. Human-like episodic memory for infinite context llms,
[arXiv preprint arXiv:2407.09450, 2024. URL https://arxiv.org/abs/2407.09450.](https://arxiv.org/abs/2407.09450)


[291] Quentin Fournier, G. Caron, and D. Aloise. A practical survey on faster and lighter transformers.
_ACM Computing Surveys_, 2021.


[292] Luca Franceschi, P. Frasconi, Saverio Salzo, Riccardo Grazzi, and M. Pontil. Bilevel programming
for hyperparameter optimization and meta-learning. _International Conference on Machine Learning_,
2018.


[293] Eduard Frankford, Daniel Crazzolara, Clemens Sauerwein, Michael Vierhauser, and Ruth Breu.
Requirements for an online integrated development environment for automated programming
assessment systems. _International Conference on Computer Supported Education_, 2024.


[294] Chaoyou Fu, Yuhan Dai, Yondong Luo, Lei Li, Shuhuai Ren, Renrui Zhang, Zihan Wang, Chenyu
Zhou, Yunhang Shen, Mengdan Zhang, Peixian Chen, Yanwei Li, Shaohui Lin, Sirui Zhao, Ke Li,
Tong Xu, Xiawu Zheng, Enhong Chen, Rongrong Ji, and Xing Sun. Video-mme: The first-ever
comprehensive evaluation benchmark of multi-modal llms in video analysis. arXiv preprint, 2024.


[295] Honghao Fu, Yilang Shen, Yuxuan Liu, Jingzhong Li, and Xiang Zhang. Sgcn: a multi-order neighborhood feature fusion landform classification method based on superpixel and graph convolutional
network. _International Journal of Applied Earth Observation and Geoinformation_, 122:103441, 2023.


[296] Honghao Fu, Yufei Wang, Wenhan Yang, Alex C Kot, and Bihan Wen. Dp-iqa: Utilizing diffusion
prior for blind image quality assessment in the wild. 2024.


[297] Honghao Fu, Hao Wang, Jing Jih Chin, and Zhiqi Shen. Brainvis: Exploring the bridge between brain
and visual signals via image reconstruction. In _ICASSP 2025-2025 IEEE International Conference on_
_Acoustics, Speech and Signal Processing (ICASSP)_, pages 1–5. IEEE, 2025.


[298] Yuchuan Fu, Xiaohan Yuan, and Dongxia Wang. Ras-eval: A comprehensive benchmark for security
evaluation of llm agents in real-world environments. arXiv preprint, 2025.


[299] Zichuan Fu, Wentao Song, Yejing Wang, Xian Wu, Yefeng Zheng, Yingying Zhang, Derong Xu,
Xuetao Wei, Tong Xu, and Xiangyu Zhao. Sliding window attention training for efficient large
[language models, arXiv preprint arXiv:2502.18845, 2025. URL https://arxiv.org/abs/2502.](https://arxiv.org/abs/2502.18845v2)
[18845v2.](https://arxiv.org/abs/2502.18845v2)


[300] Stefano Fusi. Memory capacity of neural network models, arXiv preprint arXiv:2108.07839, 2021.
[URL https://arxiv.org/abs/2108.07839v2.](https://arxiv.org/abs/2108.07839v2)


81


[301] Tiantian Gan and Qiyao Sun. Rag-mcp: Mitigating prompt bloat in llm tool selection via retrievalaugmented generation. arXiv preprint, 2025.


[302] Kanishk Gandhi, Gala Stojnic, B. Lake, and M. Dillon. Baby intuitions benchmark (bib): Discerning
the goals, preferences, and actions of others. _Neural Information Processing Systems_, 2021.


[303] Anish Ganguli, Prabal Deb, and Debleena Banerjee. Mark: Memory augmented refinement of knowl[edge, arXiv preprint arXiv:2505.05177, 2025. URL https://arxiv.org/abs/2505.05177v1.](https://arxiv.org/abs/2505.05177v1)


[304] Chen Gao, Xiaochong Lan, Nian Li, Yuan Yuan, Jingtao Ding, Zhilun Zhou, Fengli Xu, and Yong Li.
Large language models empowered agent-based modeling and simulation: A survey and perspectives.
_Humanities and Social Sciences Communications_, 2023.


[305] Chen Gao, Xiaochong Lan, Zhihong Lu, Jinzhu Mao, Jinghua Piao, Huandong Wang, Depeng Jin,
and Yong Li. S [3] : Social-network simulation system with large language model-empowered agents,
[arXiv preprint arXiv:2307.14984, 2025. URL https://arxiv.org/abs/2307.14984.](https://arxiv.org/abs/2307.14984)


[306] Hang Gao and Yongfeng Zhang. Memory sharing for large language model based agents, arXiv
[preprint arXiv:2404.09982, 2024. URL https://arxiv.org/abs/2404.09982v2.](https://arxiv.org/abs/2404.09982v2)


[307] L Gao, A Madaan, S Zhou, and U Alon.... Pal: Program-aided language models. 2023. URL

[https://proceedings.mlr.press/v202/gao23f.](https://proceedings.mlr.press/v202/gao23f)


[308] Luyu Gao, Xueguang Ma, Jimmy J. Lin, and Jamie Callan. Precise zero-shot dense retrieval without
relevance labels. _Annual Meeting of the Association for Computational Linguistics_, 2022.


[309] Luyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon, Pengfei Liu, Yiming Yang, Jamie Callan, and
Graham Neubig. Pal: Program-aided language models. _International Conference on Machine Learning_,
2022.


[310] Shuzheng Gao, Xinjie Wen, Cuiyun Gao, Wenxuan Wang, and Michael R. Lyu. What makes good incontext demonstrations for code intelligence tasks with llms? _International Conference on Automated_
_Software Engineering_, 2023.


[311] Tianyu Gao, Adam Fisch, and Danqi Chen. Making pre-trained language models better few-shot
learners. _Annual Meeting of the Association for Computational Linguistics_, 2021.


[312] Weiguo Gao. Mep: Multiple kernel learning enhancing relative positional encoding length extrapola[tion, arXiv preprint arXiv:2403.17698, 2024. URL https://arxiv.org/abs/2403.17698v1.](https://arxiv.org/abs/2403.17698v1)


[313] Xian Gao, Zongyun Zhang, Mingye Xie, Ting Liu, and Yuzhuo Fu. Graph of ai ideas: Leveraging
knowledge graphs and llms for ai research idea generation, arXiv preprint arXiv:2503.08549, 2025.
[URL https://arxiv.org/abs/2503.08549v1.](https://arxiv.org/abs/2503.08549v1)


[314] Xuanqi Gao, Siyi Xie, Juan Zhai, Shqing Ma, and Chao Shen. Mcp-radar: A multi-dimensional
benchmark for evaluating tool use capabilities in large language models. arXiv preprint, 2025.


[315] Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yi Dai, Jiawei Sun, Qianyu Guo,
Meng Wang, and Haofen Wang. Retrieval-augmented generation for large language models: A survey,
[arXiv preprint arXiv:2312.10997, 2023. URL https://arxiv.org/abs/2312.10997v5.](https://arxiv.org/abs/2312.10997v5)


82


[316] Yunfan Gao, Yun Xiong, Meng Wang, and Haofen Wang. Modular rag: Transforming rag systems
[into lego-like reconfigurable frameworks, arXiv preprint arXiv:2407.21059, 2024. URL https:](https://arxiv.org/abs/2407.21059v1)
[//arxiv.org/abs/2407.21059v1.](https://arxiv.org/abs/2407.21059v1)


[317] Yunfan Gao, Yun Xiong, Yijie Zhong, Yuxi Bi, Ming Xue, and Haofen Wang. Synergizing rag and
[reasoning: A systematic review, arXiv preprint arXiv:2504.15909, 2025. URL https://arxiv.](https://arxiv.org/abs/2504.15909v2)
[org/abs/2504.15909v2.](https://arxiv.org/abs/2504.15909v2)

