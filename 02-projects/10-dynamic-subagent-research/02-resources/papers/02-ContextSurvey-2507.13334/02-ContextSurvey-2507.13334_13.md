<!-- Source: 02-ContextSurvey-2507.13334.pdf | Chunk 13/26 -->



[297] Honghao Fu, Hao Wang, Jing Jih Chin, and Zhiqi Shen. Brainvis: Exploring the bridge between brain
and visual signals via image reconstruction. In _ICASSP 2025-2025 IEEE International Conference on_
_Acoustics, Speech and Signal Processing (ICASSP)_, pages 1–5. IEEE, 2025.


[298] Yuchuan Fu, Xiaohan Yuan, and Dongxia Wang. Ras-eval: A comprehensive benchmark for security
evaluation of llm agents in real-world environments. arXiv preprint, 2025.


[299] Zichuan Fu, Wentao Song, Yejing Wang, Xian Wu, Yefeng Zheng, Yingying Zhang, Derong Xu,
Xuetao Wei, Tong Xu, and Xiangyu Zhao. Sliding window attention training for efficient large
[language models, arXiv preprint arXiv:2502.18845, 2025. URL https://arxiv.org/abs/2502.](https://arxiv.org/abs/2502.18845v2)
[18845v2.](https://arxiv.org/abs/2502.18845v2)


[300] Stefano Fusi. Memory capacity of neural network models, arXiv preprint arXiv:2108.07839, 2021.
[URL https://arxiv.org/abs/2108.07839v2.](https://arxiv.org/abs/2108.07839v2)


81


[301] Tiantian Gan and Qiyao Sun. Rag-mcp: Mitigating prompt bloat in llm tool selection via retrievalaugmented generation. arXiv preprint, 2025.


[302] Kanishk Gandhi, Gala Stojnic, B. Lake, and M. Dillon. Baby intuitions benchmark (bib): Discerning
the goals, preferences, and actions of others. _Neural Information Processing Systems_, 2021.


[303] Anish Ganguli, Prabal Deb, and Debleena Banerjee. Mark: Memory augmented refinement of knowl[edge, arXiv preprint arXiv:2505.05177, 2025. URL https://arxiv.org/abs/2505.05177v1.](https://arxiv.org/abs/2505.05177v1)


[304] Chen Gao, Xiaochong Lan, Nian Li, Yuan Yuan, Jingtao Ding, Zhilun Zhou, Fengli Xu, and Yong Li.
Large language models empowered agent-based modeling and simulation: A survey and perspectives.
_Humanities and Social Sciences Communications_, 2023.


[305] Chen Gao, Xiaochong Lan, Zhihong Lu, Jinzhu Mao, Jinghua Piao, Huandong Wang, Depeng Jin,
and Yong Li. S [3] : Social-network simulation system with large language model-empowered agents,
[arXiv preprint arXiv:2307.14984, 2025. URL https://arxiv.org/abs/2307.14984.](https://arxiv.org/abs/2307.14984)


[306] Hang Gao and Yongfeng Zhang. Memory sharing for large language model based agents, arXiv
[preprint arXiv:2404.09982, 2024. URL https://arxiv.org/abs/2404.09982v2.](https://arxiv.org/abs/2404.09982v2)


[307] L Gao, A Madaan, S Zhou, and U Alon.... Pal: Program-aided language models. 2023. URL

[https://proceedings.mlr.press/v202/gao23f.](https://proceedings.mlr.press/v202/gao23f)


[308] Luyu Gao, Xueguang Ma, Jimmy J. Lin, and Jamie Callan. Precise zero-shot dense retrieval without
relevance labels. _Annual Meeting of the Association for Computational Linguistics_, 2022.


[309] Luyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon, Pengfei Liu, Yiming Yang, Jamie Callan, and
Graham Neubig. Pal: Program-aided language models. _International Conference on Machine Learning_,
2022.


[310] Shuzheng Gao, Xinjie Wen, Cuiyun Gao, Wenxuan Wang, and Michael R. Lyu. What makes good incontext demonstrations for code intelligence tasks with llms? _International Conference on Automated_
_Software Engineering_, 2023.


[311] Tianyu Gao, Adam Fisch, and Danqi Chen. Making pre-trained language models better few-shot
learners. _Annual Meeting of the Association for Computational Linguistics_, 2021.


[312] Weiguo Gao. Mep: Multiple kernel learning enhancing relative positional encoding length extrapola[tion, arXiv preprint arXiv:2403.17698, 2024. URL https://arxiv.org/abs/2403.17698v1.](https://arxiv.org/abs/2403.17698v1)


[313] Xian Gao, Zongyun Zhang, Mingye Xie, Ting Liu, and Yuzhuo Fu. Graph of ai ideas: Leveraging
knowledge graphs and llms for ai research idea generation, arXiv preprint arXiv:2503.08549, 2025.
[URL https://arxiv.org/abs/2503.08549v1.](https://arxiv.org/abs/2503.08549v1)


[314] Xuanqi Gao, Siyi Xie, Juan Zhai, Shqing Ma, and Chao Shen. Mcp-radar: A multi-dimensional
benchmark for evaluating tool use capabilities in large language models. arXiv preprint, 2025.


[315] Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yi Dai, Jiawei Sun, Qianyu Guo,
Meng Wang, and Haofen Wang. Retrieval-augmented generation for large language models: A survey,
[arXiv preprint arXiv:2312.10997, 2023. URL https://arxiv.org/abs/2312.10997v5.](https://arxiv.org/abs/2312.10997v5)


82


[316] Yunfan Gao, Yun Xiong, Meng Wang, and Haofen Wang. Modular rag: Transforming rag systems
[into lego-like reconfigurable frameworks, arXiv preprint arXiv:2407.21059, 2024. URL https:](https://arxiv.org/abs/2407.21059v1)
[//arxiv.org/abs/2407.21059v1.](https://arxiv.org/abs/2407.21059v1)


[317] Yunfan Gao, Yun Xiong, Yijie Zhong, Yuxi Bi, Ming Xue, and Haofen Wang. Synergizing rag and
[reasoning: A systematic review, arXiv preprint arXiv:2504.15909, 2025. URL https://arxiv.](https://arxiv.org/abs/2504.15909v2)
[org/abs/2504.15909v2.](https://arxiv.org/abs/2504.15909v2)


[318] Zhangyang Gao, Daize Dong, Cheng Tan, Jun Xia, Bozhen Hu, and Stan Z. Li. A graph is worth k
words: Euclideanizing graph using pure transformer. _International Conference on Machine Learning_,
2024.


[319] Itai Gat, Idan Schwartz, and A. Schwing. Perceptual score: What data modalities does your model
perceive? _Neural Information Processing Systems_, 2021.


[320] Itai Gat, Felix Kreuk, Tu Nguyen, Ann Lee, Jade Copet, Gabriel Synnaeve, Emmanuel Dupoux, and
Yossi Adi. Augmentation invariant discrete representation for generative spoken language modeling.
_International Workshop on Spoken Language Translation_, 2022.


[321] Tao Ge, Jing Hu, Xun Wang, Si-Qing Chen, and Furu Wei. In-context autoencoder for context
compression in a large language model. _International Conference on Learning Representations_, 2023.


[322] Yuyao Ge, Zhongguo Yang, Lizhe Chen, Yiming Wang, and Chengyang Li. Attack based on data: a
novel perspective to attack sensitive points directly. _Cybersecurity_, 6(1):43, 2023.


[323] Yuyao Ge, Shenghua Liu, Baolong Bi, Yiwei Wang, Lingrui Mei, Wenjie Feng, Lizhe Chen, and Xueqi
Cheng. Can graph descriptive order affect solving graph problems with llms? _ACL 2025_, 2024.


[324] Yuyao Ge, Shenghua Liu, Yiwei Wang, Lingrui Mei, Lizhe Chen, Baolong Bi, and Xueqi Cheng. Innate
reasoning is not enough: In-context learning enhances reasoning large language models with less
overthinking. 2025.


[325] Binzong Geng, Zhaoxin Huan, Xiaolu Zhang, Yong He, Liang Zhang, Fajie Yuan, Jun Zhou, and
Linjian Mo. Breaking the length barrier: Llm-enhanced ctr prediction in long textual user behaviors.
_Annual International ACM SIGIR Conference on Research and Development in Information Retrieval_,
2024.


[326] Hejia Geng, Boxun Xu, and Peng Li. Upar: A kantian-inspired prompting framework for enhancing
[large language model capabilities, arXiv preprint arXiv:2310.01441, 2023. URL https://arxiv.](https://arxiv.org/abs/2310.01441v2)
[org/abs/2310.01441v2.](https://arxiv.org/abs/2310.01441v2)


[327] Antonios Georgiou, M. Katkov, and M. Tsodyks. Retroactive interference model of forgetting. _Journal_
_of Mathematical Neuroscience_, 2021.


[328] S. Gershman, A. Schapiro, A. Hupbach, and K. Norman. Neural context reinstatement predicts
memory misattribution. _Journal of Neuroscience_, 2013.


[329] Mor Geva, R. Schuster, Jonathan Berant, and Omer Levy. Transformer feed-forward layers are
key-value memories. _Conference on Empirical Methods in Natural Language Processing_, 2020.


[330] Mor Geva, Avi Caciularu, Ke Wang, and Yoav Goldberg. Transformer feed-forward layers build
predictions by promoting concepts in the vocabulary space. _Conference on Empirical Methods in_
_Natural Language Processing_, 2022.


83


[331] Arda Gezdur and J. Bhattacharjya. Innovators and transformers: enhancing supply chain employee
training with an innovative application of a large language model. _International Journal of Physical_
_Distribution & Logistics Management_, 2025.


[332] Alireza Ghafarollahi and Markus J. Buehler. Protagents: protein discovery via large language model
multi-agent collaborations combining physics and machine learning. _Digital Discovery_, 2024.


[333] Abdellah Ghassel, Ian Robinson, Gabriel Tanase, Hal Cooper, Bryan Thompson, Zhen Han, V. Ioannidis, Soji Adeshina, and H. Rangwala. Hierarchical lexical graph for enhanced multi-hop retrieval,
[arXiv preprint arXiv:2506.08074, 2025. URL https://arxiv.org/abs/2506.08074v1.](https://arxiv.org/abs/2506.08074v1)


[334] S. Ghetti and S. Bunge. Neural changes underlying the development of episodic memory during
middle childhood. _Developmental Cognitive Neuroscience_, 2012.


[335] D. Ghica. Function interface models for hardware compilation: Types, signatures, protocols, arXiv
[preprint arXiv:0907.0749, 2009. URL https://arxiv.org/abs/0907.0749v1.](https://arxiv.org/abs/0907.0749v1)


[336] Tyler Giallanza, Declan Campbell, and Jonathan D. Cohen. Toward the emergence of intelligent
control: Episodic generalization and optimization. _Open Mind_, 2024.


[337] In Gim, Seung seob Lee, and Lin Zhong. Asynchronous llm function calling, arXiv preprint
[arXiv:2412.07017, 2024. URL https://arxiv.org/abs/2412.07017v1.](https://arxiv.org/abs/2412.07017v1)


[338] Amelia Glaese, Nat McAleese, Maja Trębacz, John Aslanides, Vlad Firoiu, Timo Ewalds, Maribeth
Rauh, Laura Weidinger, Martin Chadwick, Phoebe Thacker, Lucy Campbell-Gillingham, Jonathan
Uesato, Po-Sen Huang, Ramona Comanescu, Fan Yang, Abigail See, Sumanth Dathathri, Rory
Greig, Charlie Chen, Doug Fritz, Jaume Sanchez Elias, Richard Green, Soňa Mokrá, Nicholas
Fernando, Boxi Wu, Rachel Foley, Susannah Young, Iason Gabriel, William Isaac, John Mellor, Demis
Hassabis, Koray Kavukcuoglu, Lisa Anne Hendricks, and Geoffrey Irving. Improving alignment of
dialogue agents via targeted human judgements, arXiv preprint arXiv:2209.14375, 2022. URL
[https://arxiv.org/abs/2209.14375.](https://arxiv.org/abs/2209.14375)


[339] D. Godden and A. Baddeley. Context-dependent memory in two natural environments: on land and
underwater. arXiv preprint, 1975.


[340] Arda Goknil, Femke B. Gelderblom, Simeon Tverdal, Shukun Tokas, and Hui Song. Privacy policy
[analysis through prompt engineering for llms, arXiv preprint arXiv:2409.14879, 2024. URL https:](https://arxiv.org/abs/2409.14879v1)
[//arxiv.org/abs/2409.14879v1.](https://arxiv.org/abs/2409.14879v1)


[341] Yaroslav Golubev, Zarina Kurbatova, E. Alomar, T. Bryksin, and Mohamed Wiem Mkaouer. One
thousand and one stories: a large-scale survey of software refactoring. _ESEC/SIGSOFT FSE_, 2021.


[342] Alan M Gordon, Jesse Rissman, Roozbeh Kiani, and Anthony D Wagner. Cortical reinstatement
mediates the relationship between content-specific encoding activity and subsequent recollection
decisions. _Cerebral Cortex_, 2014.


[343] E. Gordon and B. Logan. Managing goals and resources in dynamic environments. arXiv preprint,
2005.


[344] Z Gou, Z Shao, Y Gong, Y Shen, and Y Yang.... Critic: Large language models can self-correct with
[tool-interactive critiquing. 2023. URL https://arxiv.org/abs/2305.11738.](https://arxiv.org/abs/2305.11738)


84


[345] Zhibin Gou, Zhihong Shao, Yeyun Gong, Yelong Shen, Yujiu Yang, Minlie Huang, Nan Duan,
and Weizhu Chen. Tora: A tool-integrated reasoning agent for mathematical problem solving.
_International Conference on Learning Representations_, 2023.


[346] Alex Graves, Abdel rahman Mohamed, and Geoffrey E. Hinton. Speech recognition with deep
recurrent neural networks. _IEEE International Conference on Acoustics, Speech, and Signal Processing_,
2013.


[347] Ekaterina Grishina, Mikhail Gorbunov, and Maxim Rakhuba. Procrustesgpt: Compressing llms with
structured matrices and orthogonal transformations, arXiv preprint arXiv:2506.02818, 2025. URL
[https://arxiv.org/abs/2506.02818v1.](https://arxiv.org/abs/2506.02818v1)


[348] Sven Gronauer and K. Diepold. Multi-agent deep reinforcement learning: a survey. _Artificial_
_Intelligence Review_, 2021.


[349] C. Gros. Complex and adaptive dynamical systems, arXiv preprint arXiv:0807.4838, 2008. URL

[https://arxiv.org/abs/0807.4838v3.](https://arxiv.org/abs/0807.4838v3)


[350] Albert Gu, Karan Goel, and Christopher R’e. Efficiently modeling long sequences with structured
state spaces. _International Conference on Learning Representations_, 2021.


[351] Albert Gu, Ankit Gupta, Karan Goel, and Christopher Ré. On the parameterization and initialization
of diagonal state space models. _Neural Information Processing Systems_, 2022.


[352] Jian Gu, Chunyang Chen, and A. Aleti. Vocabulary-defined semantics: Latent space clustering for
[improving in-context learning, arXiv preprint arXiv:2401.16184, 2024. URL https://arxiv.](https://arxiv.org/abs/2401.16184v6)
[org/abs/2401.16184v6.](https://arxiv.org/abs/2401.16184v6)


[353] Yongli Gu, Xiang Yan, Hanlin Qin, Naveed Akhtar, Shuai Yuan, Honghao Fu, Shuowen Yang, and
Ajmal Mian. Hdtcnet: A hybrid-dimensional convolutional network for multivariate time series
classification. _Pattern Recognition_, page 111837, 2025.


[354] Zhuohan Gu, Jiayi Yao, Kuntai Du, and Junchen Jiang. Llmsteer: Improving long-context llm
inference by steering attention on reused contexts, arXiv preprint arXiv:2411.13009, 2024. URL
[https://arxiv.org/abs/2411.13009v2.](https://arxiv.org/abs/2411.13009v2)


[355] Shengyue Guan, Haoyi Xiong, Jindong Wang, Jiang Bian, Bin Zhu, and Jian guang Lou. Evaluating
llm-based agents for multi-turn conversations: A survey, arXiv preprint arXiv:2503.22458, 2025.
[URL https://arxiv.org/abs/2503.22458v1.](https://arxiv.org/abs/2503.22458v1)


[356] Zhong Guan, Hongke Zhao, Likang Wu, Ming He, and Jianpin Fan. Langtopo: Aligning language
descriptions of graphs with tokenized topological modeling, arXiv preprint arXiv:2406.13250, 2024.
[URL https://arxiv.org/abs/2406.13250v1.](https://arxiv.org/abs/2406.13250v1)


[357] Tom Gunter, Zirui Wang, Chong Wang, Ruoming Pang, Andy Narayanan, Aonan Zhang, Bowen
Zhang, Chen Chen, Chung-Cheng Chiu, David Qiu, Deepak Gopinath, Dian Ang Yap, Dong Yin,
Feng Nan, Floris Weers, Guoli Yin, Haoshuo Huang, Jianyu Wang, Jiarui Lu, John Peebles, Kewei
Ye, Mark Lee, Nan Du, Qibin Chen, Quentin Keunebroek, Sam Wiseman, Syd Evans, Tao Lei, Vivek
Rathod, Xiang Kong, Xianzhi Du, Yanghao Li, Yongqiang Wang, Yuan Gao, Zaid Ahmed, Zhaoyang
Xu, Zhiyun Lu, Al Rashid, Albin Madappally Jose, Alec Doane, Alfredo Bencomo, Allison Vanderby,
Andrew Hansen, Ankur Jain, A. Anupama, Areeba Kamal, Bugu Wu, Carolina Brum, Charlie Maalouf,


85


Chinguun Erdenebileg, Chris Dulhanty, Dominik Moritz, Doug Kang, Eduardo Jimenez, Evan Ladd,
Fang Shi, Felix Bai, Frank Chu, Fred Hohman, Hadas Kotek, Hannah Gillis Coleman, Jane Li, Jeffrey P.
Bigham, Jeffery Cao, Jeff Lai, Jessica Cheung, Jiulong Shan, Joe Zhou, John Li, Jun Qin, Karanjeet
Singh, Karla Vega, Kelvin Zou, Laura Heckman, Lauren Gardiner, Margit Bowler, Maria Cordell,
Meng Cao, Nicole Hay, Nilesh Shahdadpuri, Otto Godwin, Pranay Dighe, Pushyami Rachapudi,
Ramsey Tantawi, Roman Frigg, Sam Davarnia, Sanskruti Shah, Saptarshi Guha, Sasha Sirovica, Shen
Ma, Shuang Ma, Simon Wang, Sulgi Kim, Suma Jayaram, Vaishaal Shankar, Varsha Paidi, Vivek
Kumar, Xin Wang, Xin Zheng, Walker Cheng, Y. Shrager, Yang Ye, Yasu Tanaka, Yihao Guo, Yun
Meng, Zhaoping Luo, Ouyang Zhi, Alp Aygar, Alvin Wan, Andrew D. Walkingshaw, Tzu-Hsiang Lin,
Arsalan Farooq, Brent Ramerth, Colorado Reed, Chris Bartels, Chris Chaney, David Riazati, Eric Liang
Yang, Erin Feldman, Gabriel Hochstrasser, Guillaume Seguin, Irina Belousova, J. Pelemans, Karen
Yang, Keivan A. Vahid, Liangliang Cao, Mahyar Najibi, Marco Zuliani, Max Horton, Minsik Cho,
Nikhil Bhendawade, Patrick Dong, Piotr Maj, Pulkit Agrawal, Qi Shan, Qichen Fu, R. Poston, Sam
Xu, Shuangning Liu, Sushma Rao, Tashweena Heeramun, Thomas Merth, Uday Rayala, Victor Cui,
Vivek Rangarajan Sridhar, Wencong Zhang, Wenqi Zhang, Wentao Wu, Xingyu Zhou, Xinwen Liu,
Yang Zhao, Yin Xia, Zhile Ren, and Zhongzheng Ren. Apple intelligence foundation language models,
[arXiv preprint arXiv:2407.21075, 2024. URL https://arxiv.org/abs/2407.21075v1.](https://arxiv.org/abs/2407.21075v1)


[358] Jiayan Guo, Lun Du, and Hengyu Liu. Gpt4graph: Can large language models understand graph
structured data ? an empirical evaluation and benchmarking, arXiv preprint arXiv:2305.15066,
[2023. URL https://arxiv.org/abs/2305.15066v2.](https://arxiv.org/abs/2305.15066v2)


[359] Jing Guo, Nan Li, Jianchuan Qi, Hang Yang, Ruiqiao Li, Yuzhen Feng, Si Zhang, and Ming Xu.
Empowering working memory for large language model agents, arXiv preprint arXiv:2312.17259,
[2024. URL https://arxiv.org/abs/2312.17259.](https://arxiv.org/abs/2312.17259)


[360] Taicheng Guo, Xiuying Chen, Yaqi Wang, Ruidi Chang, Shichao Pei, N. Chawla, Olaf Wiest, and
Xiangliang Zhang. Large language model based multi-agents: A survey of progress and challenges.
_International Joint Conference on Artificial Intelligence_, 2024.


[361] Xiaojun Guo, Ang Li, Yifei Wang, Stefanie Jegelka, and Yisen Wang. G1: Teaching llms to reason on
graphs with reinforcement learning. _arXiv preprint arXiv:2505.18499_, 2025.


[362] Yuan Guo, Tingjia Miao, Zheng Wu, Pengzhou Cheng, Ming Zhou, and Zhuosheng Zhang. Atomicto-compositional generalization for mobile agents with a new benchmark and scheduling system,
[arXiv preprint arXiv:2506.08972, 2025. URL https://arxiv.org/abs/2506.08972v1.](https://arxiv.org/abs/2506.08972v1)


[363] Zhicheng Guo, Sijie Cheng, Hao Wang, Shihao Liang, Yujia Qin, Peng Li, Zhiyuan Liu, Maosong Sun,
and Yang Liu. Stabletoolbench: Towards stable large-scale benchmarking on tool learning of large
language models. _Annual Meeting of the Association for Computational Linguistics_, 2024.


[364] Zirui Guo, Lianghao Xia, Yanhua Yu, Tu Ao, and Chao Huang. Lightrag: Simple and fast retrieval[augmented generation, arXiv preprint arXiv:2410.05779, 2024. URL https://arxiv.org/abs/](https://arxiv.org/abs/2410.05779v3)
[2410.05779v3.](https://arxiv.org/abs/2410.05779v3)


[365] Sharut Gupta, Chenyu Wang, Yifei Wang, T. Jaakkola, and Stefanie Jegelka. In-context symmetries:
Self-supervised learning through contextual world models. _Neural Information Processing Systems_,
2024.


86


[366] Tanmay Gupta, Luca Weihs, and Aniruddha Kembhavi. Codenav: Beyond tool-use to using real-world
[codebases with llm agents, arXiv preprint arXiv:2406.12276, 2024. URL https://arxiv.org/](https://arxiv.org/abs/2406.12276v1)
[abs/2406.12276v1.](https://arxiv.org/abs/2406.12276v1)


[367] I Gur, H Furuta, A Huang, M Safdari, and Y Matsuo.... A real-world webagent with planning, long
[context understanding, and program synthesis. 2023. URL https://arxiv.org/abs/2307.](https://arxiv.org/abs/2307.12856)
[12856.](https://arxiv.org/abs/2307.12856)


[368] Izzeddin Gur, Hiroki Furuta, Austin Huang, Mustafa Safdari, Yutaka Matsuo, D. Eck, and Aleksandra
Faust. A real-world webagent with planning, long context understanding, and program synthesis.
_International Conference on Learning Representations_, 2023.


[369] Ashwin Kumar Gururajan, Enrique Lopez-Cuena, Jordi Bayarri-Planas, Adrián Tormos, Daniel Hinjos,
Pablo Bernabeu Perez, Anna Arias-Duart, Pablo A. Martin-Torres, Lucia Urcelay-Ganzabal, Marta
Gonzalez-Mallo, S. Álvarez Napagao, Eduard Ayguad’e-Parra, and Ulises Cortés Dario Garcia-Gasulla.
Aloe: A family of fine-tuned open healthcare llms, arXiv preprint arXiv:2405.01886, 2024. URL
[https://arxiv.org/abs/2405.01886v1.](https://arxiv.org/abs/2405.01886v1)


[370] Bernal Jimenez Gutierrez, Yiheng Shu, Yu Gu, Michihiro Yasunaga, and Yu Su. Hipporag: Neurobiologically inspired long-term memory for large language models. _Neural Information Processing_
_Systems_, 2024.


[371] Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Ming-Wei Chang. Realm: Retrievalaugmented language model pre-training. _International Conference on Machine Learning_, 2020.


[372] K. Gödel, B. Meltzer, and R. Schlegel. On formally undecidable propositions of principia mathematica
and related systems. arXiv preprint, 1966.


[373] Idan Habler, Ken Huang, Vineeth Sai Narajala, and Prashant Kulkarni. Building a secure agentic
[ai application leveraging a2a protocol, arXiv preprint arXiv:2504.16902, 2025. URL https://](https://arxiv.org/abs/2504.16902v2)
[arxiv.org/abs/2504.16902v2.](https://arxiv.org/abs/2504.16902v2)


[374] John Halloran. Mcp safety training: Learning to refuse falsely benign mcp exploits using improved
[preference alignment, arXiv preprint arXiv:2505.23634, 2025. URL https://arxiv.org/abs/](https://arxiv.org/abs/2505.23634v1)
[2505.23634v1.](https://arxiv.org/abs/2505.23634v1)


[375] Tae Jun Ham, Yejin Lee, Seong Hoon Seo, Soo-Uck Kim, Hyunji Choi, Sungjun Jung, and Jae W.
Lee. Elsa: Hardware-software co-design for efficient, lightweight self-attention mechanism in neural
networks. _International Symposium on Computer Architecture_, 2021.


[376] Feijiang Han, Licheng Guo, Hengtao Cui, and Zhiyuan Lyu. Question tokens deserve more attention:
Enhancing large language models without training through step-by-step reading and question
[attention recalibration, arXiv preprint arXiv:2504.09402, 2025. URL https://arxiv.org/abs/](https://arxiv.org/abs/2504.09402v1)
[2504.09402v1.](https://arxiv.org/abs/2504.09402v1)


[377] Han Han, Tong Zhu, Xiang Zhang, Mengsong Wu, Hao Xiong, and Wenliang Chen. Nestools: A
dataset for evaluating nested tool learning abilities of large language models. _International Conference_
_on Computational Linguistics_, 2024.


[378] Haoyu Han, Yu Wang, Harry Shomer, Kai Guo, Jiayuan Ding, Yongjia Lei, Mahantesh Halappanavar,
Ryan A. Rossi, Subhabrata Mukherjee, Xianfeng Tang, Qi He, Zhigang Hua, Bo Long, Tong Zhao,


87


Neil Shah, Amin Javari, Yinglong Xia, and Jiliang Tang. Retrieval-augmented generation with
[graphs (graphrag), arXiv preprint arXiv:2501.00309, 2025. URL https://arxiv.org/abs/](https://arxiv.org/abs/2501.00309)
[2501.00309.](https://arxiv.org/abs/2501.00309)


[379] Tingxu Han, Zhenting Wang, Chunrong Fang, Shiyun Zhao, Shiqing Ma, and Zhenyu Chen. Token[budget-aware llm reasoning, arXiv preprint arXiv:2412.18547, 2024. URL https://arxiv.org/](https://arxiv.org/abs/2412.18547v5)
[abs/2412.18547v5.](https://arxiv.org/abs/2412.18547v5)


[380] Yuanning Han, Ziyi Qiu, Jiale Cheng, and Ray Lc. When teams embrace ai: Human collaboration
strategies in generative prompting in a creative design task. _International Conference on Human_
_Factors in Computing Systems_, 2024.


[381] R. Hankache, Kingsley Nketia Acheampong, Liang Song, Marek Brynda, Raad Khraishi, and Greig A.
Cowan. Evaluating the sensitivity of llms to prior context, arXiv preprint arXiv:2506.00069, 2025.
[URL https://arxiv.org/abs/2506.00069v1.](https://arxiv.org/abs/2506.00069v1)


[382] S Hao, T Liu, Z Wang, and Z Hu. Toolkengpt: Augmenting frozen language models with massive
[tools via tool embeddings. 2023. URL https://proceedings.neurips.cc/paper_files/](https://proceedings.neurips.cc/paper_files/paper/2023/hash/8fd1a81c882cd45f64958da6284f4a3f-Abstract-Conference.html)

[paper/2023/hash/8fd1a81c882cd45f64958da6284f4a3f-Abstract-Conference.](https://proceedings.neurips.cc/paper_files/paper/2023/hash/8fd1a81c882cd45f64958da6284f4a3f-Abstract-Conference.html)
[html.](https://proceedings.neurips.cc/paper_files/paper/2023/hash/8fd1a81c882cd45f64958da6284f4a3f-Abstract-Conference.html)


[383] Mohanakrishnan Hariharan. Semantic mastery: Enhancing llms with advanced natural language
[understanding, arXiv preprint arXiv:2504.00409, 2025. URL https://arxiv.org/abs/2504.](https://arxiv.org/abs/2504.00409v1)
[00409v1.](https://arxiv.org/abs/2504.00409v1)


[384] Mareike Hartmann and Alexander Koller. A survey on complex tasks for goal-directed interactive agents, arXiv preprint arXiv:2409.18538, 2024. [URL https://arxiv.org/abs/2409.](https://arxiv.org/abs/2409.18538v1)
[18538v1.](https://arxiv.org/abs/2409.18538v1)


[385] A. Hassani, A. Medvedev, P. D. Haghighi, Sea Ling, A. Zaslavsky, and P. Jayaraman. Context definition
and query language: Conceptual specification, implementation, and evaluation. _Italian National_
_Conference on Sensors_, 2019.


[386] Kostas Hatalis, Despina Christou, Joshua Myers, Steven Jones, Keith Lambert, Adam Amos-Binks,
Zohreh Dannenhauer, and Dustin Dannenhauer. Memory matters: The need to improve long-term
memory in llm-agents. _Proceedings of the AAAI Symposium Series_, 2024.


[387] Kostas Hatalis, Despina Christou, and Vyshnavi Kondapalli. Review of case-based reasoning for llm
agents: Theoretical foundations, architectural components, and cognitive integration, arXiv preprint
[arXiv:2504.06943, 2025. URL https://arxiv.org/abs/2504.06943v2.](https://arxiv.org/abs/2504.06943v2)


[388] Jacky He, Guiran Liu, Binrong Zhu, Hanlu Zhang, Hongye Zheng, and Xiaokai Wang. Context-guided
dynamic retrieval for improving generation quality in rag models, arXiv preprint arXiv:2504.19436,
[2025. URL https://arxiv.org/abs/2504.19436v1.](https://arxiv.org/abs/2504.19436v1)


[389] Jianben He, Xingbo Wang, Shiyi Liu, Guande Wu, Claudio Silva, and Huamin Qu. Poem: Interactive
prompt optimization for enhancing multimodal reasoning of large language models. _IEEE Pacific_
_Visualization Symposium_, 2024.


[390] Junqing He, Liang Zhu, Rui Wang, Xi Wang, Gholamreza Haffari, and Jiaxing Zhang. Madial-bench:
Towards real-world evaluation of memory-augmented dialogue generation. _North American Chapter_
_of the Association for Computational Linguistics_, 2024.


88


[391] Shawn He, Surangika Ranathunga, Stephen Cranefield, and B. Savarimuthu. Norm violation
detection in multi-agent systems using large language models: A pilot study. _COINE_, 2024.


[392] Shengtao He. Achieving tool calling functionality in llms using only prompt engineering with[out fine-tuning, arXiv preprint arXiv:2407.04997, 2024. URL https://arxiv.org/abs/2407.](https://arxiv.org/abs/2407.04997v1)
[04997v1.](https://arxiv.org/abs/2407.04997v1)

