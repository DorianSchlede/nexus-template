<!-- Source: 02-ContextSurvey-2507.13334.pdf | Chunk 16/26 -->

retrieval-augmented generation. arXiv preprint, 2025.


[538] Prannay Khosla, Piotr Teterwak, Chen Wang, Aaron Sarna, Yonglong Tian, Phillip Isola, Aaron
Maschinot, Ce Liu, and Dilip Krishnan. Supervised contrastive learning. _Neural Information Processing_
_Systems_, 2020.


[539] Tushar Khot, H. Trivedi, Matthew Finlayson, Yao Fu, Kyle Richardson, Peter Clark, and Ashish
Sabharwal. Decomposed prompting: A modular approach for solving complex tasks. _International_
_Conference on Learning Representations_, 2022.


[540] Sambhav Khurana, Xiner Li, Shurui Gui, and Shuiwang Ji. A hierarchical language model for
[interpretable graph reasoning, arXiv preprint arXiv:2410.22372, 2024. URL https://arxiv.](https://arxiv.org/abs/2410.22372v1)
[org/abs/2410.22372v1.](https://arxiv.org/abs/2410.22372v1)


[541] Daehee Kim, Deokhyung Kang, Sangwon Ryu, and Gary Geunbae Lee. Ontology-free general-domain
knowledge graph-to-text generation dataset synthesis using large language model, arXiv preprint
[arXiv:2409.07088, 2024. URL https://arxiv.org/abs/2409.07088v1.](https://arxiv.org/abs/2409.07088v1)


[542] Geunwoo Kim, P. Baldi, and S. McAleer. Language models can solve computer tasks. _Neural_
_Information Processing Systems_, 2023.


[543] Jaeyeon Kim, Injune Hwang, and Kyogu Lee. Learning semantic information from raw audio signal
using both contextual and phonetic representations. _IEEE International Conference on Acoustics,_
_Speech, and Signal Processing_, 2024.


99


[544] Jang-Hyun Kim, Junyoung Yeom, Sangdoo Yun, and Hyun Oh Song. Compressed context memory
for online language model interaction. _International Conference on Learning Representations_, 2023.


[545] Jiho Kim, Yeonsu Kwon, Yohan Jo, and Edward Choi. Kg-gpt: A general framework for reasoning
on knowledge graphs using large language models. _Conference on Empirical Methods in Natural_
_Language Processing_, 2023.


[546] Jiin Kim, Byeongjun Shin, Jinha Chung, and Minsoo Rhu. The cost of dynamic reasoning: Demystifying ai agents and test-time scaling from an ai infrastructure perspective, arXiv preprint
[arXiv:2506.04301, 2025. URL https://arxiv.org/abs/2506.04301v1.](https://arxiv.org/abs/2506.04301v1)


[547] Sehoon Kim, Suhong Moon, Ryan Tabrizi, Nicholas Lee, Michael W. Mahoney, Kurt Keutzer, and
A. Gholami. An llm compiler for parallel function calling. _International Conference on Machine_
_Learning_, 2023.


[548] Taewoon Kim, Michael Cochez, Vincent Francois-Lavet, Mark Neerincx, and Piek Vossen. A machine
with short-term, episodic, and semantic memory systems. _Proceedings of the AAAI Conference on_
_Artificial Intelligence_, 37(1):48–56, 2023. ISSN 2159-5399. doi: 10.1609/aaai.v37i1.25075. URL
[http://dx.doi.org/10.1609/aaai.v37i1.25075.](http://dx.doi.org/10.1609/aaai.v37i1.25075)


[549] Lukas Kirchdorfer, Robert Blümel, T. Kampik, Han van der Aa, and Heiner Stuckenschmidt. Discovering multi-agent systems for resource-centric business process simulation. _Process Science_,
2025.


[550] J. Kirkpatrick, Razvan Pascanu, Neil C. Rabinowitz, J. Veness, Guillaume Desjardins, Andrei A.
Rusu, Kieran Milan, John Quan, Tiago Ramalho, A. Grabska-Barwinska, D. Hassabis, C. Clopath,
D. Kumaran, and R. Hadsell. Overcoming catastrophic forgetting in neural networks. _Proceedings of_
_the National Academy of Sciences of the United States of America_, 2016.


[551] Louis Kirsch, James Harrison, Jascha Narain Sohl-Dickstein, and Luke Metz. General-purpose
in-context learning by meta-learning transformers, arXiv preprint arXiv:2212.04458, 2022. URL
[https://arxiv.org/abs/2212.04458v2.](https://arxiv.org/abs/2212.04458v2)


[552] Yuval Kirstain, Patrick Lewis, Sebastian Riedel, and Omer Levy. A few more examples may be worth
billions of parameters. _Conference on Empirical Methods in Natural Language Processing_, 2021.


[553] Andrew Kiruluta, Preethi Raju, and Priscilla Burity. Breaking quadratic barriers: A non-attention
[llm for ultra-long context horizons, arXiv preprint arXiv:2506.01963, 2025. URL https://arxiv.](https://arxiv.org/abs/2506.01963v1)
[org/abs/2506.01963v1.](https://arxiv.org/abs/2506.01963v1)


[554] Nikita Kitaev, Lukasz Kaiser, and Anselm Levskaya. Reformer: The efficient transformer. _International_
_Conference on Learning Representations_, 2020.


[555] Vincent Koc, Jacques Verre, Douglas Blank, and Abigail Morgan. Mind the metrics: Patterns for
telemetry-aware in-ide ai application development using the model context protocol (mcp), arXiv
[preprint arXiv:2506.11019, 2025. URL https://arxiv.org/abs/2506.11019v1.](https://arxiv.org/abs/2506.11019v1)


[556] Tomás Kociský, Jonathan Schwarz, Phil Blunsom, Chris Dyer, Karl Moritz Hermann, Gábor Melis,
and Edward Grefenstette. The narrativeqa reading comprehension challenge. _Transactions of the_
_Association for Computational Linguistics_, 2017.


100


[557] Jing Yu Koh, R. Salakhutdinov, and Daniel Fried. Grounding language models to images for
multimodal inputs and outputs. _International Conference on Machine Learning_, 2023.


[558] Jing Yu Koh, Robert Lo, Lawrence Jang, Vikram Duvvur, Ming Chong Lim, Po-Yu Huang, Graham Neubig, Shuyan Zhou, Ruslan Salakhutdinov, and Daniel Fried. Visualwebarena: Evaluating
multimodal agents on realistic visual web tasks, arXiv preprint arXiv:2401.13649, 2024. URL
[https://arxiv.org/abs/2401.13649v2.](https://arxiv.org/abs/2401.13649v2)


[559] Takeshi Kojima, S. Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. Large language models
are zero-shot reasoners. _Neural Information Processing Systems_, 2022.


[560] Yilun Kong, Jingqing Ruan, Yihong Chen, Bin Zhang, Tianpeng Bao, Shiwei Shi, Guoqing Du, Xiaoru
Hu, Hangyu Mao, Ziyue Li, Xingyu Zeng, and Rui Zhao. Tptu-v2: Boosting task planning and tool
usage of large language model-based agents in real-world systems, arXiv preprint arXiv:2311.11315,
[2023. URL https://arxiv.org/abs/2311.11315.](https://arxiv.org/abs/2311.11315)


[561] P. Korzyński, G. Mazurek, Pamela Krzypkowska, and Artur Kurasiński. Artificial intelligence prompt
engineering as a new digital competence: Analysis of generative ai technologies such as chatgpt.
_Entrepreneurial Business and Economics Review_, 2023.


[562] Oliver Kramer. Cognitive prompts using guilford’s structure of intellect model. arXiv preprint, 2025.


[563] Oliver Kramer. Conceptual metaphor theory as a prompting paradigm for large language models,
[arXiv preprint arXiv:2502.01901, 2025. URL https://arxiv.org/abs/2502.01901v1.](https://arxiv.org/abs/2502.01901v1)


[564] Oliver Kramer and Jill Baumann. Unlocking structured thinking in language models with cognitive
prompting. _ESANN 2025 proceesdings_, 2024.


[565] K. Kravari and Nick Bassiliades. A survey of agent platforms. _Journal of Artificial Societies and Social_
_Simulation_, 2015.


[566] Prashant Krishnan, Zilong Wang, Yangkun Wang, and Jingbo Shang. Towards few-shot entity
recognition in document images: A graph neural network approach robust to image manipulation.
_International Conference on Language Resources and Evaluation_, 2023.


[567] W. Kruijne, S. Bohté, P. Roelfsema, and C. Olivers. Flexible working memory through selective gating
and attentional tagging. _bioRxiv_, 2019.


[568] L. Krupp, Daniel Geissler, P. Lukowicz, and Jakob Karolus. Towards sustainable web agents: A plea
for transparency and dedicated metrics for energy consumption, arXiv preprint arXiv:2502.17903,
[2025. URL https://arxiv.org/abs/2502.17903v1.](https://arxiv.org/abs/2502.17903v1)


[569] M. Kuhail, Jose Berengueres, Fatma Taher, Sana Z. Khan, and Ansah Siddiqui. Designing a haptic
boot for space with prompt engineering: Process, insights, and implications. _IEEE Access_, 2024.


[570] Amandeep Kumar, Muzammal Naseer, Sanath Narayan, R. Anwer, Salman H. Khan, and
Hisham Cholakkal. Multi-modal generation via cross-modal in-context learning, arXiv preprint
[arXiv:2405.18304v1, 2024. URL https://arxiv.org/abs/2405.18304v1.](https://arxiv.org/abs/2405.18304v1)


[571] Rajeev Kumar, Harishankar Kumar, and Kumari Shalini. Detecting and mitigating bias in llms through
knowledge graph-augmented training. _2025 International Conference on Artificial Intelligence and_
_Data Engineering (AIDE)_, 2025.


101


[572] Taeyoon Kwon, Dongwook Choi, Sunghwan Kim, Hyojun Kim, Seungjun Moon, Beong woo Kwak,
Kuan-Hao Huang, and Jinyoung Yeo. Embodied agents meet personalization: Exploring memory
[utilization for personalized assistance, arXiv preprint arXiv:2505.16348, 2025. URL https://](https://arxiv.org/abs/2505.16348v1)
[arxiv.org/abs/2505.16348v1.](https://arxiv.org/abs/2505.16348v1)


[573] Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E.
Gonzalez, Haotong Zhang, and Ion Stoica. Efficient memory management for large language model
serving with pagedattention. _Symposium on Operating Systems Principles_, 2023.


[574] T. Lai, Quan Hung Tran, Trung Bui, and D. Kihara. A gated self-attention memory network for
answer selection. _Conference on Empirical Methods in Natural Language Processing_, 2019.


[575] Divya Lamba. The role of prompt engineering in improving language understanding and generation.
_International Journal For Multidisciplinary Research_, 2024.


[576] Xiaochong Lan, Jie Feng, Jia Lei, Xinlei Shi, and Yong Li. Benchmarking and advancing large
language models for local life services, arXiv preprint arXiv:2506.02720, 2025. [URL https:](https://arxiv.org/abs/2506.02720v1)
[//arxiv.org/abs/2506.02720v1.](https://arxiv.org/abs/2506.02720v1)


[[577] LangChain Team. Memory in langgraph. https://langchain-ai.github.io/langgraph/](https://langchain-ai.github.io/langgraph/concepts/memory/)
[concepts/memory/, 2025. Accessed: 2025-07-17.](https://langchain-ai.github.io/langgraph/concepts/memory/)


[578] Samuel T. Langlois, Oghenetekevwe Akoroda, Estefany Carrillo, J. Herrmann, S. Azarm, Huan Xu,
and Michael W. Otte. Metareasoning structures, problems, and modes for multiagent systems: A
survey. _IEEE Access_, 2020.


[579] B. Lattimer, Varun Gangal, Ryan McDonald, and Yi Yang. Sparse rewards can self-train dia[logue agents, arXiv preprint arXiv:2409.04617, 2024. URL https://arxiv.org/abs/2409.](https://arxiv.org/abs/2409.04617v2)
[04617v2.](https://arxiv.org/abs/2409.04617v2)


[580] Pak Kin Lau and Stuart Michael McManus. Mining asymmetric intertextuality, arXiv preprint
[arXiv:2410.15145, 2024. URL https://arxiv.org/abs/2410.15145v1.](https://arxiv.org/abs/2410.15145v1)


[581] Hung Le, T. Tran, and S. Venkatesh. Self-attentive associative memory. _International Conference on_
_Machine Learning_, 2020.


[582] Dohyun Lee, Seungil Chad Lee, Chanwoo Yang, Yujin Baek, and Jaegul Choo. Exploring in-context
[example generation for machine translation, arXiv preprint arXiv:2506.00507, 2025. URL https:](https://arxiv.org/abs/2506.00507v1)
[//arxiv.org/abs/2506.00507v1.](https://arxiv.org/abs/2506.00507v1)


[583] Dongyub Lee, Eunhwan Park, Hodong Lee, and Heuiseok Lim. Ask, assess, and refine: Rectifying
factual consistency and hallucination in llms with metric-guided feedback learning. _Conference of the_
_European Chapter of the Association for Computational Linguistics_, 2024.


[584] Eunhae Lee. Towards ethical personal ai applications: Practical considerations for ai assistants with
[long-term memory, arXiv preprint arXiv:2409.11192, 2024. URL https://arxiv.org/abs/](https://arxiv.org/abs/2409.11192v1)
[2409.11192v1.](https://arxiv.org/abs/2409.11192v1)


[585] Gibbeum Lee, Volker Hartmann, Jongho Park, Dimitris Papailiopoulos, and Kangwook Lee. Prompted
llms as chatbot modules for long open-domain conversation. In _Findings of the Association for_
_Computational Linguistics: ACL 2023_ . Association for Computational Linguistics, 2023. doi: 10.18653/
[v1/2023.findings-acl.277. URL http://dx.doi.org/10.18653/v1/2023.findings-acl.](http://dx.doi.org/10.18653/v1/2023.findings-acl.277)
[277.](http://dx.doi.org/10.18653/v1/2023.findings-acl.277)


102


[586] Heejun Lee, Geon Park, Youngwan Lee, Jina Kim, Wonyoung Jeong, Myeongjae Jeon, and Sung Ju
Hwang. A training-free sub-quadratic cost transformer model serving framework with hierarchically
[pruned attention, arXiv preprint arXiv:2406.09827, 2024. URL https://arxiv.org/abs/2406.](https://arxiv.org/abs/2406.09827v3)
[09827v3.](https://arxiv.org/abs/2406.09827v3)


[587] Ho-Jun Lee, Junho Kim, Hyunjun Kim, and Yonghyun Ro. Refocus: Reinforcement-guided frame
[optimization for contextual understanding, arXiv preprint arXiv:2506.01274v1, 2025. URL https:](https://arxiv.org/abs/2506.01274v1)
[//arxiv.org/abs/2506.01274v1.](https://arxiv.org/abs/2506.01274v1)


[588] Jonathan Lee, Annie Xie, Aldo Pacchiano, Yash Chandak, Chelsea Finn, Ofir Nachum, and E. Brunskill.
Supervised pretraining can learn in-context reinforcement learning. _Neural Information Processing_
_Systems_, 2023.


[589] Namkyeong Lee, E. Brouwer, Ehsan Hajiramezanali, Chanyoung Park, and Gabriele Scalia. Ragenhanced collaborative llm agents for drug discovery, arXiv preprint arXiv:2502.17506, 2025. URL
[https://arxiv.org/abs/2502.17506v2.](https://arxiv.org/abs/2502.17506v2)


[590] Shinbok Lee, Gaeun Seo, Daniel Lee, Byeongil Ko, Sunghee Jung, and M. Shin. Functionchat-bench:
Comprehensive evaluation of language models’ generative capabilities in korean tool-use dialogs.
arXiv preprint, 2024.


[591] Younghun Lee, Sungchul Kim, Ryan A. Rossi, Tong Yu, and Xiang Chen. Learning to reduce:
Towards improving performance of large language models on structured data, arXiv preprint
[arXiv:2407.02750, 2024. URL https://arxiv.org/abs/2407.02750v1.](https://arxiv.org/abs/2407.02750v1)


[592] Younghun Lee, Sungchul Kim, Tong Yu, Ryan A. Rossi, and Xiang Chen. Learning to reduce:
Optimal representations of structured data in prompting large language models, arXiv preprint
[arXiv:2402.14195, 2024. URL https://arxiv.org/abs/2402.14195v1.](https://arxiv.org/abs/2402.14195v1)


[593] Yu-Ting Lee, Hui-Ying Shih, Fu-Chieh Chang, and Pei-Yuan Wu. An explanation of intrinsic selfcorrection via linear representations and latent concepts, arXiv preprint arXiv:2505.11924, 2025.
[URL https://arxiv.org/abs/2505.11924v1.](https://arxiv.org/abs/2505.11924v1)


[594] Melissa Lehman and Kenneth J. Malmberg. A buffer model of memory encoding and temporal
correlations in retrieval. _Psychology Review_, 2013.


[595] Yiming Lei, Zhizheng Yang, Zeming Liu, Haitao Leng, Shaoguo Liu, Tingting Gao, Qingjie Liu,
and Yunhong Wang. Contextqformer: A new context modeling method for multi-turn multi-modal
[conversations, arXiv preprint arXiv:2505.23121v1, 2025. URL https://arxiv.org/abs/2505.](https://arxiv.org/abs/2505.23121v1)
[23121v1.](https://arxiv.org/abs/2505.23121v1)


[596] Brian Lester, Rami Al-Rfou, and Noah Constant. The power of scale for parameter-efficient prompt
tuning. _Conference on Empirical Methods in Natural Language Processing_, 2021.


[597] Patrick Lewis, Ethan Perez, Aleksandara Piktus, F. Petroni, Vladimir Karpukhin, Naman Goyal,
Heinrich Kuttler, M. Lewis, Wen tau Yih, Tim Rocktäschel, Sebastian Riedel, and Douwe Kiela.
Retrieval-augmented generation for knowledge-intensive nlp tasks. _Neural Information Processing_
_Systems_, 2020.


[598] Bohan Li, Yutai Hou, and Wanxiang Che. Data augmentation approaches in natural language
processing: A survey. _AI Open_, 2021.


103


[599] Chaozhuo Li, Bochen Pang, Yuming Liu, Hao Sun, Zheng Liu, Xing Xie, Tianqi Yang, Yanling Cui,
Liangjie Zhang, and Qi Zhang. Adsgnn: Behavior-graph augmented relevance modeling in sponsored
search. _Annual International ACM SIGIR Conference on Research and Development in Information_
_Retrieval_, 2021.


[600] Chengpeng Li, Zhengyang Tang, Ziniu Li, Mingfeng Xue, Keqin Bao, Tian Ding, Ruoyu Sun, Benyou
Wang, Xiang Wang, Junyang Lin, and Dayiheng Liu. Cort: Code-integrated reasoning within thinking,
[arXiv preprint arXiv:2506.09820, 2025. URL https://arxiv.org/abs/2506.09820v2.](https://arxiv.org/abs/2506.09820v2)


[601] Chengshu Li, Jacky Liang, Andy Zeng, Xinyun Chen, Karol Hausman, Dorsa Sadigh, Sergey Levine,
Fei-Fei Li, Fei Xia, and Brian Ichter. Chain of code: Reasoning with a language model-augmented
code emulator. _International Conference on Machine Learning_, 2023.


[602] Chuanhao Li, Runhan Yang, Tiankai Li, Milad Bafarassat, Kourosh Sharifi, Dirk Bergemann, and Zhuoran Yang. Stride: A tool-assisted llm agent framework for strategic and interactive decision-making,
[arXiv preprint arXiv:2405.16376, 2024. URL https://arxiv.org/abs/2405.16376v2.](https://arxiv.org/abs/2405.16376v2)


[603] Daliang Li, Ankit Singh Rawat, Manzil Zaheer, Xin Wang, Michal Lukasik, Andreas Veit, Felix
Yu, and Sanjiv Kumar. Large language models with controllable working memory, arXiv preprint
[arXiv:2211.05110, 2022. URL https://arxiv.org/abs/2211.05110.](https://arxiv.org/abs/2211.05110)


[604] Daniel Li and Lincoln Murr. Humaneval on latest gpt models - 2024. arXiv preprint, 2024.


[605] Fu Li, Xueying Wang, Bin Li, Yunlong Wu, Yanzhen Wang, and Xiaodong Yi. A study on training and
developing large language models for behavior tree generation, arXiv preprint arXiv:2401.08089,
[2024. URL https://arxiv.org/abs/2401.08089v1.](https://arxiv.org/abs/2401.08089v1)


[606] Guohao Li, Hasan Abed Al Kader Hammoud, Hani Itani, Dmitrii Khizbullin, and Bernard Ghanem.
Camel: Communicative agents for "mind" exploration of large language model society. In _Thirty-_
_seventh Conference on Neural Information Processing Systems_, 2023.


[607] Guozheng Li, Peng Wang, Jiajun Liu, Yikai Guo, Ke Ji, Ziyu Shang, and Zijie Xu. Meta in-context
learning makes large language models better zero and few-shot relation extractors. _International_
_Joint Conference on Artificial Intelligence_, 2024.


[608] Haoyang Li, Jing Zhang, Cuiping Li, and Hong Chen. Resdsql: Decoupling schema linking and
skeleton parsing for text-to-sql. _AAAI Conference on Artificial Intelligence_, 2023.


[609] Jia Li, Ge Li, Yongming Li, and Zhi Jin. Structured chain-of-thought prompting for code generation.
_ACM Transactions on Software Engineering and Methodology_, 2023.


[610] Jia Li, Xiangguo Sun, Yuhan Li, Zhixun Li, Hong Cheng, and Jeffrey Xu Yu. Graph intelligence with
large language models and prompt learning. _Knowledge Discovery and Data Mining_, 2024.


[611] Jiahao Nick Li, Yan Xu, Tovi Grossman, Stephanie Santosa, and Michelle Li. Omniactions: Predicting
digital actions in response to real-world multimodal sensory inputs with llms. _International Conference_
_on Human Factors in Computing Systems_, 2024.


[612] Jiarui Li, Ye Yuan, and Zehua Zhang. Enhancing llm factual accuracy with rag to counter hallucinations: A case study on domain-specific queries in private knowledge-bases, arXiv preprint
[arXiv:2403.10446, 2024. URL https://arxiv.org/abs/2403.10446v1.](https://arxiv.org/abs/2403.10446v1)


104


[613] Juncheng Li, Kaihang Pan, Zhiqi Ge, Minghe Gao, Hanwang Zhang, Wei Ji, Wenqiao Zhang, Tat
seng Chua, Siliang Tang, and Yueting Zhuang. Fine-tuning multimodal llms to follow zero-shot
demonstrative instructions. _International Conference on Learning Representations_, 2023.


[614] Junnan Li, Ramprasaath R. Selvaraju, Akhilesh Deepak Gotmare, Shafiq R. Joty, Caiming Xiong, and
S. Hoi. Align before fuse: Vision and language representation learning with momentum distillation.
_Neural Information Processing Systems_, 2021.


[615] Junnan Li, Dongxu Li, S. Savarese, and Steven C. H. Hoi. Blip-2: Bootstrapping language-image
pre-training with frozen image encoders and large language models. _International Conference on_
_Machine Learning_, 2023.


[616] Kun Li, Tianhua Zhang, Yunxiang Li, Hongyin Luo, Abdalla Moustafa, Xixin Wu, James Glass, and
Helen M. Meng. Generate, discriminate, evolve: Enhancing context faithfulness via fine-grained
[sentence-level self-evolution, arXiv preprint arXiv:2503.01695, 2025. URL https://arxiv.org/](https://arxiv.org/abs/2503.01695v1)
[abs/2503.01695v1.](https://arxiv.org/abs/2503.01695v1)


[617] Kunchang Li, Yinan He, Yi Wang, Yizhuo Li, Wen Wang, Ping Luo, Yali Wang, Limin Wang, and
Yu Qiao. Videochat: Chat-centric video understanding, arXiv preprint arXiv:2305.06355v2, 2023.
[URL https://arxiv.org/abs/2305.06355v2.](https://arxiv.org/abs/2305.06355v2)


[618] M Li, Y Zhao, B Yu, F Song, H Li, H Yu, and Z Li.... Api-bank: A comprehensive benchmark for
[tool-augmented llms. 2023. URL https://arxiv.org/abs/2304.08244.](https://arxiv.org/abs/2304.08244)


[619] Michelle M. Li, Ben Y. Reis, Adam Rodman, Tianxi Cai, Noa Dagan, Ran D. Balicer, Joseph Loscalzo,
Isaac S. Kohane, and M. Zitnik. One patient, many contexts: Scaling medical ai through contextual
[intelligence, arXiv preprint arXiv:2506.10157, 2025. URL https://arxiv.org/abs/2506.](https://arxiv.org/abs/2506.10157v1)
[10157v1.](https://arxiv.org/abs/2506.10157v1)


[620] Ming Li, Keyu Chen, Ziqian Bi, Ming Liu, Benji Peng, Qian Niu, Junyu Liu, Jinlang Wang, Sen
Zhang, Xuanhe Pan, Jiawei Xu, and Pohsun Feng. Surveying the mllm landscape: A meta-review of
[current surveys, arXiv preprint arXiv:2409.18991, 2024. URL https://arxiv.org/abs/2409.](https://arxiv.org/abs/2409.18991v1)
[18991v1.](https://arxiv.org/abs/2409.18991v1)


[621] Minghao Li, Feifan Song, Yu Bowen, Haiyang Yu, Zhoujun Li, Fei Huang, and Yongbin Li. Api-bank:
A comprehensive benchmark for tool-augmented llms. _Conference on Empirical Methods in Natural_
_Language Processing_, 2023.


[622] Qiaomu Li and Ying Xie. From glue-code to protocols: A critical analysis of a2a and mcp integration
[for scalable agent systems, arXiv preprint arXiv:2505.03864, 2025. URL https://arxiv.org/](https://arxiv.org/abs/2505.03864v1)
[abs/2505.03864v1.](https://arxiv.org/abs/2505.03864v1)


[623] Rongsheng Li, Jin Xu, Zhixiong Cao, Hai-Tao Zheng, and Hong-Gee Kim. Extending context window
in large language models with segmented base adjustment for rotary position embeddings. _Applied_
_Sciences_, 2024.


[624] Shuaike Li, Kai Zhang, Qi Liu, and Enhong Chen. Mindbridge: Scalable and cross-model knowledge
[editing via memory-augmented modality, arXiv preprint arXiv:2503.02701v1, 2025. URL https:](https://arxiv.org/abs/2503.02701v1)
[//arxiv.org/abs/2503.02701v1.](https://arxiv.org/abs/2503.02701v1)


105


[625] Shuaiyi Li, Zhisong Zhang, Yang Deng, Chenlong Deng, Tianqing Fang, Hongming Zhang, Haitao
Mi, Dong Yu, and Wai Lam. Incomes: Integrating compression and selection mechanisms into llms
[for efficient model editing, arXiv preprint arXiv:2505.22156, 2025. URL https://arxiv.org/](https://arxiv.org/abs/2505.22156v1)
[abs/2505.22156v1.](https://arxiv.org/abs/2505.22156v1)


[626] Siheng Li, Cheng Yang, Zesen Cheng, Lemao Liu, Mo Yu, Yujiu Yang, and Wai Lam. Large language
models can self-improve in long-context reasoning, arXiv preprint arXiv:2411.08147, 2024. URL
[https://arxiv.org/abs/2411.08147v1.](https://arxiv.org/abs/2411.08147v1)


[[627] X Li, H Zou, and P Liu. Torl: Scaling tool-integrated rl. 2025. URL https://arxiv.org/abs/](https://arxiv.org/abs/2503.23383)
[2503.23383.](https://arxiv.org/abs/2503.23383)


[628] Xiaopeng Li, Pengyue Jia, Derong Xu, Yi Wen, Yingyi Zhang, Wenlin Zhang, Wanyu Wang, Yichao
Wang, Zhaochen Du, Xiangyang Li, Yong Liu, Huifeng Guo, Ruiming Tang, and Xiangyu Zhao.
A survey of personalization: From rag to agent, arXiv preprint arXiv:2504.10147, 2025. URL
[https://arxiv.org/abs/2504.10147v1.](https://arxiv.org/abs/2504.10147v1)


[629] Xiaoxi Li, Jiajie Jin, Guanting Dong, Hongjin Qian, Yutao Zhu, Yongkang Wu, Ji-Rong Wen, and
Zhicheng Dou. Webthinker: Empowering large reasoning models with deep research capability,
[arXiv preprint arXiv:2504.21776, 2025. URL https://arxiv.org/abs/2504.21776v1.](https://arxiv.org/abs/2504.21776v1)


[630] Xin Li, Qizhi Chu, Yubin Chen, Yang Liu, Yaoqi Liu, Zekai Yu, Weize Chen, Cheng Qian, Chuan Shi,
and Cheng Yang. Graphteam: Facilitating large language model-based graph analysis via multi-agent
[collaboration, arXiv preprint arXiv:2410.18032v4, 2024. URL https://arxiv.org/abs/2410.](https://arxiv.org/abs/2410.18032v4)
[18032v4.](https://arxiv.org/abs/2410.18032v4)


[631] Xinyi Li, Sai Wang, Siqi Zeng, Yu Wu, and Yi Yang. A survey on llm-based multi-agent systems:
workflow, infrastructure, and challenges. _Vicinagearth_, 2024.


[632] Xinze Li, Zhenghao Liu, Chenyan Xiong, Shi Yu, Yu Gu, Zhiyuan Liu, and Ge Yu. Structure-aware
language model pretraining improves dense retrieval on structured data. _Annual Meeting of the_
_Association for Computational Linguistics_, 2023.


[633] Yang Li, Jiacong He, Xiaoxia Zhou, Yuan Zhang, and Jason Baldridge. Mapping natural language
instructions to mobile ui action sequences. _Annual Meeting of the Association for Computational_
_Linguistics_, 2020.


[634] Yinghao Li, R. Ramprasad, and Chao Zhang. A simple but effective approach to improve structured
language model output for information extraction. _Conference on Empirical Methods in Natural_
_Language Processing_, 2024.


[635] Yuan Li, Yixuan Zhang, and Lichao Sun. Metaagents: Simulating interactions of human behaviors for llm-based task-oriented coordination via collaborative generative agents, arXiv preprint
[arXiv:2310.06500, 2023. URL https://arxiv.org/abs/2310.06500.](https://arxiv.org/abs/2310.06500)


[636] Yucheng Li. Unlocking context constraints of llms: Enhancing context efficiency of llms with
[self-information-based content filtering, arXiv preprint arXiv:2304.12102, 2023. URL https:](https://arxiv.org/abs/2304.12102v1)
[//arxiv.org/abs/2304.12102v1.](https://arxiv.org/abs/2304.12102v1)

