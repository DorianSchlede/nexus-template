<!-- Source: 02-ContextSurvey-2507.13334.pdf | Chunk 11/26 -->

Zhao, Tianlu Mao, and Yucheng Zhang. Haif-gs: Hierarchical and induced flow-guided gaussian
splatting for dynamic scene. 2025.


[139] Jiaqi Chen, Xiaoye Zhu, Yue Wang, Tianyang Liu, Xinhui Chen, Ying Chen, Chak Tou Leong, Yifei
Ke, Joseph Liu, Yiwen Yuan, Julian McAuley, and Li jia Li. Symbolic representation for any-to-any
[generative tasks, arXiv preprint arXiv:2504.17261v1, 2025. URL https://arxiv.org/abs/](https://arxiv.org/abs/2504.17261v1)
[2504.17261v1.](https://arxiv.org/abs/2504.17261v1)


69


[140] Jiayi Chen, J. Ye, and Guiling Wang. From standalone llms to integrated intelligence: A survey of
[compound al systems, arXiv preprint arXiv:2506.04565, 2025. URL https://arxiv.org/abs/](https://arxiv.org/abs/2506.04565v1)
[2506.04565v1.](https://arxiv.org/abs/2506.04565v1)


[141] Jin Chen, Zheng Liu, Xu Huang, Chenwang Wu, Qi Liu, Gangwei Jiang, Yuanhao Pu, Yuxuan Lei,
Xiaolong Chen, Xingmei Wang, Defu Lian, and Enhong Chen. When large language models meet
personalization: Perspectives of challenges and opportunities. _World wide web (Bussum)_, 2023.


[142] Jiyu Chen, Shuang Peng, Daxiong Luo, Fan Yang, Renshou Wu, Fangyuan Li, and Xiaoxin Chen.
Edgeinfinite: A memory-efficient infinite-context transformer for edge devices, arXiv preprint
[arXiv:2503.22196, 2025. URL https://arxiv.org/abs/2503.22196v1.](https://arxiv.org/abs/2503.22196v1)


[143] Justin Chih-Yao Chen, Archiki Prasad, Swarnadeep Saha, Elias Stengel-Eskin, and Mohit Bansal. Magicore: Multi-agent, iterative, coarse-to-fine refinement for reasoning, arXiv preprint arXiv:2409.12147,
[2024. URL https://arxiv.org/abs/2409.12147v1.](https://arxiv.org/abs/2409.12147v1)


[144] Mingyang Chen, Haoze Sun, Tianpeng Li, Fan Yang, Hao Liang, Keer Lu, Bin Cui, Wentao Zhang,
Zenan Zhou, and Weipeng Chen. Facilitating multi-turn function calling for llms via compositional
instruction tuning. _International Conference on Learning Representations_, 2024.


[145] Nuo Chen, Yuhan Li, Jianheng Tang, and Jia Li. Graphwiz: An instruction-following language model
for graph computational problems. In _Proceedings of the 30th ACM SIGKDD Conference on Knowledge_
_Discovery and Data Mining_, pages 353–364, 2024.


[146] Nuo Chen, Zhiyuan Hu, Qingyun Zou, Jiaying Wu, Qian Wang, Bryan Hooi, and Bingsheng He.
[Judgelrm: Large reasoning models as a judge, arXiv preprint arXiv:2504.00050, 2025. URL https:](https://arxiv.org/abs/2504.00050v1)
[//arxiv.org/abs/2504.00050v1.](https://arxiv.org/abs/2504.00050v1)


[147] Qiguang Chen, Libo Qin, Jiaqi Wang, Jinxuan Zhou, and Wanxiang Che. Unlocking the capabilities
of thought: A reasoning boundary framework to quantify and optimize chain-of-thought, arXiv
[preprint arXiv:2410.05695, 2024. URL https://arxiv.org/abs/2410.05695.](https://arxiv.org/abs/2410.05695)


[148] Qiguang Chen, Libo Qin, Jinhao Liu, Dengyun Peng, Jiannan Guan, Peng Wang, Mengkang Hu,
Yuhang Zhou, Te Gao, and Wangxiang Che. Towards reasoning era: A survey of long chainof-thought for reasoning large language models, arXiv preprint arXiv:2503.09567, 2025. URL
[https://arxiv.org/abs/2503.09567v3.](https://arxiv.org/abs/2503.09567v3)


[149] Qiguang Chen, Libo Qin, Jinhao Liu, Dengyun Peng, Jiannan Guan, Peng Wang, Mengkang Hu,
Yuhang Zhou, Te Gao, and Wanxiang Che. Towards reasoning era: A survey of long chain-of[thought for reasoning large language models, arXiv preprint arXiv:2503.09567, 2025. URL https:](https://arxiv.org/abs/2503.09567)
[//arxiv.org/abs/2503.09567.](https://arxiv.org/abs/2503.09567)


[150] Qiguang Chen, Libo Qin, Jinhao Liu, Dengyun Peng, Jiaqi Wang, Mengkang Hu, Zhi Chen, Wanxiang
Che, and Ting Liu. Ecm: A unified electronic circuit model for explaining the emergence of in-context
learning and chain-of-thought in large language model, arXiv preprint arXiv:2502.03325, 2025.
[URL https://arxiv.org/abs/2502.03325.](https://arxiv.org/abs/2502.03325)


[151] Qiguang Chen, Mingda Yang, Libo Qin, Jinhao Liu, Zheng Yan, Jiannan Guan, Dengyun Peng, Yiyan
Ji, Hanjing Li, Mengkang Hu, Yimeng Zhang, Yihao Liang, Yuhang Zhou, Jiaqi Wang, Zhi Chen, and
Wanxiang Che. Ai4research: A survey of artificial intelligence for scientific research, arXiv preprint
[arXiv:2507.01903, 2025. URL https://arxiv.org/abs/2507.01903.](https://arxiv.org/abs/2507.01903)


70


[152] S Chen, Y Wang, YF Wu, and Q Chen.... Advancing tool-augmented large
language models: Integrating insights from errors in inference trees. 2024.
URL [https://proceedings.neurips.cc/paper_files/paper/2024/hash/](https://proceedings.neurips.cc/paper_files/paper/2024/hash/c0f7ee1901fef1da4dae2b88dfd43195-Abstract-Conference.html)
[c0f7ee1901fef1da4dae2b88dfd43195-Abstract-Conference.html.](https://proceedings.neurips.cc/paper_files/paper/2024/hash/c0f7ee1901fef1da4dae2b88dfd43195-Abstract-Conference.html)


[153] Shouyuan Chen, Sherman Wong, Liangjian Chen, and Yuandong Tian. Extending context window
of large language models via positional interpolation, arXiv preprint arXiv:2306.15595, 2023. URL
[https://arxiv.org/abs/2306.15595v2.](https://arxiv.org/abs/2306.15595v2)


[154] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey E. Hinton. A simple framework for
contrastive learning of visual representations. _International Conference on Machine Learning_, 2020.


[155] Wenhu Chen, Xueguang Ma, Xinyi Wang, and William W. Cohen. Program of thoughts prompting:
Disentangling computation from reasoning for numerical reasoning tasks. _Trans. Mach. Learn. Res._,
2022.


[156] Yanda Chen, Ruiqi Zhong, Sheng Zha, G. Karypis, and He He. Meta-learning via language model
in-context tuning. _Annual Meeting of the Association for Computational Linguistics_, 2021.


[157] Yi Chen, JiaHao Zhao, and HaoHao Han. A survey on collaborative mechanisms between large and
[small language models, arXiv preprint arXiv:2505.07460, 2025. URL https://arxiv.org/abs/](https://arxiv.org/abs/2505.07460v1)
[2505.07460v1.](https://arxiv.org/abs/2505.07460v1)


[158] Yixin Chen, Shuai Zhang, Boran Han, Tong He, and Bo Li. Camml: Context-aware multimodal
learner for large models. _Annual Meeting of the Association for Computational Linguistics_, 2024.


[159] Z Chen, K Zhou, B Zhang, Z Gong, and WX Zhao.... Chatcot: Tool-augmented chain-of-thought
[reasoning on chat-based large language models. 2023. URL https://arxiv.org/abs/2305.](https://arxiv.org/abs/2305.14323)
[14323.](https://arxiv.org/abs/2305.14323)


[160] Zehui Chen, Weihua Du, Wenwei Zhang, Kuikun Liu, Jiangning Liu, Miao Zheng, Jingming Zhuo,
Songyang Zhang, Dahua Lin, Kai Chen, et al. T-eval: Evaluating the tool utilization capability step
by step. _arXiv preprint arXiv:2312.14033_, 2023.


[161] Zehui Chen, Kuikun Liu, Qiuchen Wang, Jiangning Liu, Wenwei Zhang, Kai Chen, and Feng Zhao.
Mindsearch: Mimicking human minds elicits deep ai searcher, arXiv preprint arXiv:2407.20183,
[2024. URL https://arxiv.org/abs/2407.20183v1.](https://arxiv.org/abs/2407.20183v1)


[162] Zhi Chen, Qiguang Chen, Libo Qin, Qipeng Guo, Haijun Lv, Yicheng Zou, Wanxiang Che, Hang
Yan, Kai Chen, and Dahua Lin. What are the essential factors in crafting effective long context
multi-hop instruction datasets? insights and best practices, arXiv preprint arXiv:2409.01893, 2025.
[URL https://arxiv.org/abs/2409.01893.](https://arxiv.org/abs/2409.01893)


[163] Zhikai Chen, Haitao Mao, Hang Li, Wei Jin, Haifang Wen, Xiaochi Wei, Shuaiqiang Wang, Dawei
Yin, Wenqi Fan, Hui Liu, and Jiliang Tang. Exploring the potential of large language models (llms)in
learning on graphs. _SIGKDD Explorations_, 2023.


[164] Zihan Chen, Song Wang, Zhen Tan, Xingbo Fu, Zhenyu Lei, Peng Wang, Huan Liu, Cong Shen, and
Jundong Li. A survey of scaling in large language model reasoning, arXiv preprint arXiv:2504.02181,
[2025. URL https://arxiv.org/abs/2504.02181v1.](https://arxiv.org/abs/2504.02181v1)


71


[165] ZY Chen, S Shen, G Shen, and G Zhi.... Towards tool use alignment of large language models.
[2024. URL https://aclanthology.org/2024.emnlp-main.82/.](https://aclanthology.org/2024.emnlp-main.82/)


[166] Mingyue Cheng, Yucong Luo, Ouyang Jie, Qi Liu, Huijie Liu, Li Li, Shuo Yu, Bohou Zhang, Jiawei Cao,
Jie Ma, Daoyu Wang, and Enhong Chen. A survey on knowledge-oriented retrieval-augmented gener[ation, arXiv preprint arXiv:2503.10677, 2025. URL https://arxiv.org/abs/2503.10677v2.](https://arxiv.org/abs/2503.10677v2)


[167] Ning Cheng, Zhaohui Yan, Ziming Wang, Zhijie Li, Jiaming Yu, Zilong Zheng, Kewei Tu, Jinan Xu,
and Wenjuan Han. Potential and limitations of llms in capturing structured semantics: A case study
on srl. _International Conference on Intelligent Computing_, 2024.


[168] Sitao Cheng, Ziyuan Zhuang, Yong Xu, Fangkai Yang, Chaoyun Zhang, Xiaoting Qin, Xiang Huang,
Ling Chen, Qingwei Lin, Dongmei Zhang, et al. Call me when necessary: Llms can efficiently and
faithfully reason over structured environments. In _Association for Computational Linguistics 2024_,
pages 4275–4295, 2024.


[169] Xin Cheng, Di Luo, Xiuying Chen, Lemao Liu, Dongyan Zhao, and Rui Yan. Lift yourself up:
Retrieval-augmented text generation with self memory. _Neural Information Processing Systems_, 2023.


[170] Yao Cheng, Yibo Zhao, Jiapeng Zhu, Yao Liu, Xing Sun, and Xiang Li. Human cognition inspired rag
with knowledge graph for complex problem solving, arXiv preprint arXiv:2503.06567, 2025. URL
[https://arxiv.org/abs/2503.06567v1.](https://arxiv.org/abs/2503.06567v1)


[171] Yuheng Cheng, Ceyao Zhang, Zhengwen Zhang, Xiangrui Meng, Sirui Hong, Wenhao Li, Zihao
Wang, Zekai Wang, Feng Yin, Junhua Zhao, and Xiuqiang He. Exploring large language model based
intelligent agents: Definitions, methods, and prospects, arXiv preprint arXiv:2401.03428, 2024. URL
[https://arxiv.org/abs/2401.03428v1.](https://arxiv.org/abs/2401.03428v1)


[172] Egor Cherepanov, Nikita Kachaev, A. Kovalev, and Aleksandr I. Panov. Memory, benchmark & robots: A
benchmark for solving complex tasks with reinforcement learning, arXiv preprint arXiv:2502.10550,
[2025. URL https://arxiv.org/abs/2502.10550v2.](https://arxiv.org/abs/2502.10550v2)


[173] Prateek Chhikara, Dev Khant, Saket Aryan, Taranjeet Singh, and Deshraj Yadav. Mem0: Building
production-ready ai agents with scalable long-term memory, arXiv preprint arXiv:2504.19413, 2025.
[URL https://arxiv.org/abs/2504.19413.](https://arxiv.org/abs/2504.19413)


[174] Yew Ken Chia, Lidong Bing, Soujanya Poria, and Luo Si. Relationprompt: Leveraging prompts to
generate synthetic data for zero-shot relation triplet extraction. _Findings_, 2022.


[175] Jihye Choi, Nils Palumbo, P. Chalasani, Matthew M. Engelhard, Somesh Jha, Anivarya Kumar, and
David Page. Malade: Orchestration of llm-powered agents with retrieval augmented generation
[for pharmacovigilance, arXiv preprint arXiv:2408.01869, 2024. URL https://arxiv.org/abs/](https://arxiv.org/abs/2408.01869v1)
[2408.01869v1.](https://arxiv.org/abs/2408.01869v1)


[176] K. Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tamás Sarlós,
Peter Hawkins, Jared Davis, Afroz Mohiuddin, Lukasz Kaiser, David Belanger, Lucy J. Colwell,
and Adrian Weller. Rethinking attention with performers. _International Conference on Learning_
_Representations_, 2020.


[177] Zhendong Chu, Shen Wang, Jian Xie, Tinghui Zhu, Yibo Yan, Jinheng Ye, Aoxiao Zhong, Xuming Hu,
Jing Liang, Philip S. Yu, and Qingsong Wen. Llm agents for education: Advances and applications,
[arXiv preprint arXiv:2503.11733, 2025. URL https://arxiv.org/abs/2503.11733v1.](https://arxiv.org/abs/2503.11733v1)


72


[178] Zhixuan Chu, Huaiyu Guo, Xinyuan Zhou, Yijia Wang, Fei Yu, Hong Chen, Wanqing Xu, Xin Lu, Qing
Cui, Longfei Li, Junqing Zhou, and Sheng Li. Data-centric financial large language models, arXiv
[preprint arXiv:2310.17784, 2023. URL https://arxiv.org/abs/2310.17784v2.](https://arxiv.org/abs/2310.17784v2)


[179] Yun-Shiuan Chuang, Agam Goyal, Nikunj Harlalka, Siddharth Suresh, Robert Hawkins, Sijia Yang,
Dhavan Shah, Junjie Hu, and Timothy T. Rogers. Simulating opinion dynamics with networks of
[llm-based agents, arXiv preprint arXiv:2311.09618, 2024. URL https://arxiv.org/abs/2311.](https://arxiv.org/abs/2311.09618)
[09618.](https://arxiv.org/abs/2311.09618)


[180] Yung-Sung Chuang, Benjamin Cohen-Wang, Shannon Zejiang Shen, Zhaofeng Wu, Hu Xu, Xi Victoria
Lin, James Glass, Shang-Wen Li, and Wen tau Yih. Selfcite: Self-supervised alignment for context
[attribution in large language models, arXiv preprint arXiv:2502.09604, 2025. URL https://](https://arxiv.org/abs/2502.09604v3)
[arxiv.org/abs/2502.09604v3.](https://arxiv.org/abs/2502.09604v3)


[181] Julian Coda-Forno, Marcel Binz, Zeynep Akata, M. Botvinick, Jane X. Wang, and Eric Schulz.
Meta-in-context learning in large language models. _Neural Information Processing Systems_, 2023.


[182] Joao Coelho, Jingjie Ning, Jingyuan He, Kangrui Mao, A. Paladugu, Pranav Setlur, Jiahe Jin, James P.
Callan, João Magalhães, Bruno Martins, and Chenyan Xiong. Deepresearchgym: A free, transparent,
and reproducible evaluation sandbox for deep research, arXiv preprint arXiv:2505.19253, 2025.
[URL https://arxiv.org/abs/2505.19253v2.](https://arxiv.org/abs/2505.19253v2)


[183] Emile Contal and Garrin McGoldrick. Ragsys: Item-cold-start recommender as rag system. _IR-_
_RAG@SIGIR_, 2024.


[184] Erica Coppolillo. Injecting knowledge graphs into large language models, arXiv preprint
[arXiv:2505.07554, 2025. URL https://arxiv.org/abs/2505.07554v1.](https://arxiv.org/abs/2505.07554v1)


[185] R. P. Costa, R. Froemke, P. J. Sjöström, and Mark C. W. van Rossum. Unified pre- and postsynaptic
long-term plasticity enables reliable and flexible learning. _eLife_, 2015.


[186] Caia Costello, Simon Guo, Anna Goldie, and Azalia Mirhoseini. Think, prune, train, improve:
[Scaling reasoning without scaling models, arXiv preprint arXiv:2504.18116, 2025. URL https:](https://arxiv.org/abs/2504.18116v1)
[//arxiv.org/abs/2504.18116v1.](https://arxiv.org/abs/2504.18116v1)


[187] Michael Craig, Karla Butterworth, Jonna Nilsson, Colin J Hamilton, P. Gallagher, and T. Smulders.
How does intentionality of encoding affect memory for episodic information? _Learning & memory_
_(Cold Spring Harbor, N.Y.)_, 2016.


[[188] crewAI Inc. crewai: Framework for orchestrating role-playing, autonomous ai agents. https:](https://github.com/crewAIInc/crewAI)
[//github.com/crewAIInc/crewAI, 2024. [Online; accessed 17-July-2025].](https://github.com/crewAIInc/crewAI)


[189] A. Cruz, André V. dos Santos, R. Santiago, and B. Bedregal. A fuzzy semantic for bdi logic. _Fuzzy_
_Information and Engineering_, 2021.


[190] Florin Cuconasu, Giovanni Trappolini, F. Siciliano, Simone Filice, Cesare Campagnano, Y. Maarek,
Nicola Tonellotto, and Fabrizio Silvestri. The power of noise: Redefining retrieval for rag systems.
_Annual International ACM SIGIR Conference on Research and Development in Information Retrieval_,
2024.


73


[191] Kai Cui, Anam Tahir, Gizem Ekinci, Ahmed Elshamanhory, Yannick Eich, Mengguang Li, and
H. Koeppl. A survey on large-population systems and scalable multi-agent reinforcement learning,
[arXiv preprint arXiv:2209.03859, 2022. URL https://arxiv.org/abs/2209.03859v1.](https://arxiv.org/abs/2209.03859v1)


[192] Yuanning Cui, Zequn Sun, and Wei Hu. A prompt-based knowledge graph foundation model for
universal in-context reasoning. In _Advances in Neural Information Processing Systems_, 2024.


[193] Yue Cui, Liuyi Yao, Shuchang Tao, Weijie Shi, Yaliang Li, Bolin Ding, and Xiaofang Zhou. Enhancing tool learning in large language models with hierarchical error checklists, arXiv preprint
[arXiv:2506.00042, 2025. URL https://arxiv.org/abs/2506.00042v1.](https://arxiv.org/abs/2506.00042v1)


[194] C. Curto, A. Degeratu, and V. Itskov. Flexible memory networks. _Bulletin of Mathematical Biology_,
2010.


[195] Ruiting Dai, Yuqiao Tan, Lisi Mo, Shuang Liang, Guohao Huo, Jiayi Luo, and Yao Cheng. G-sap:
Graph-based structure-aware prompt learning over heterogeneous knowledge for commonsense
reasoning. _International Conference on Multimedia Retrieval_, 2024.


[196] Xinnan Dai, Haohao Qu, Yifen Shen, Bohang Zhang, Qihao Wen, Wenqi Fan, Dongsheng Li, Jiliang
Tang, and Caihua Shan. How do large language models understand graph patterns? a benchmark for
[graph pattern comprehension, arXiv preprint arXiv:2410.05298v2, 2024. URL https://arxiv.](https://arxiv.org/abs/2410.05298v2)
[org/abs/2410.05298v2.](https://arxiv.org/abs/2410.05298v2)


[197] Fatemeh Daneshfar and H. Bevrani. Multi-agent systems in control engineering: a survey. arXiv
preprint, 2009.


[198] Yufan Dang, Cheng Qian, Xueheng Luo, Jingru Fan, Zihao Xie, Ruijie Shi, Weize Chen, Cheng
Yang, Xiaoyin Che, Ye Tian, Xuantang Xiong, Lei Han, Zhiyuan Liu, and Maosong Sun. Multi-agent
[collaboration via evolving orchestration, arXiv preprint arXiv:2505.19591, 2025. URL https:](https://arxiv.org/abs/2505.19591v1)
[//arxiv.org/abs/2505.19591v1.](https://arxiv.org/abs/2505.19591v1)


[199] Tri Dao. Flashattention-2: Faster attention with better parallelism and work partitioning. _International_
_Conference on Learning Representations_, 2023.


[200] Tri Dao, Daniel Y. Fu, Stefano Ermon, A. Rudra, and Christopher R’e. Flashattention: Fast and
memory-efficient exact attention with io-awareness. _Neural Information Processing Systems_, 2022.


[201] D Das, D Banerjee, S Aditya, and A Kulkarni. Mathsensei: a tool-augmented large language model
[for mathematical reasoning. 2024. URL https://arxiv.org/abs/2402.17231.](https://arxiv.org/abs/2402.17231)


[202] Payel Das, Subhajit Chaudhury, Elliot Nelson, Igor Melnyk, Sarath Swaminathan, Sihui Dai, Aurélie
Lozano, Georgios Kollias, Vijil Chenthamarakshan, Jiří, Navrátil, Soham Dan, and Pin-Yu Chen.
Larimar: Large language models with episodic memory control, arXiv preprint arXiv:2403.11901,
[2024. URL https://arxiv.org/abs/2403.11901.](https://arxiv.org/abs/2403.11901)


[203] Adrian de Wynter, Xun Wang, Qilong Gu, and Si-Qing Chen. On meta-prompting, arXiv preprint
[arXiv:2312.06562, 2023. URL https://arxiv.org/abs/2312.06562v3.](https://arxiv.org/abs/2312.06562v3)


[204] Ramandeep Singh Dehal, Mehak Sharma, and Enayat Rajabi. Knowledge graphs and their reciprocal
relationship with large language models. _Machine Learning and Knowledge Extraction_, 2025.


74


[205] Mauricio R. Delgado, V. Stenger, and J. Fiez. Motivation-dependent responses in the human caudate
nucleus. _Cerebral Cortex_, 2004.


[206] Xiang Deng, Yu Gu, Boyuan Zheng, Shijie Chen, Samuel Stevens, Boshi Wang, Huan Sun, and Yu Su.
Mind2web: Towards a generalist agent for the web. _Neural Information Processing Systems_, 2023.


[207] Yang Deng, Wenqiang Lei, Hongru Wang, and Tat seng Chua. Prompting and evaluating large language models for proactive dialogues: Clarification, target-guided, and non-collaboration. _Conference_
_on Empirical Methods in Natural Language Processing_, 2023.


[208] Yang Deng, An Zhang, Yankai Lin, Xu Chen, Ji-Rong Wen, and Tat-Seng Chua. Large language
model powered agents in the web. _The Web Conference_, 2024.


[209] Yang Deng, Xuan Zhang, Wenxuan Zhang, Yifei Yuan, See-Kiong Ng, and Tat-Seng Chua. On the
multi-turn instruction following for conversational web agents. _Annual Meeting of the Association for_
_Computational Linguistics_, 2024.


[210] Brouillet Denis and Versace Rémy. The nature of the traces and the dynamics of memory. _Psychology_
_and Behavioral Sciences_, 2019.


[211] Mohammad Mahdi Derakhshani, Ivona Najdenkoska, Cees G. M. Snoek, M. Worring, and Yuki
Asano. Self-supervised open-ended classification with small visual language models, arXiv preprint
[arXiv:2310.00500, 2023. URL https://arxiv.org/abs/2310.00500v2.](https://arxiv.org/abs/2310.00500v2)


[212] Stefan Dernbach, Khushbu Agarwal, Alejandro Zuniga, Michael Henry, and Sutanay Choudhury.
Glam: Fine-tuning large language models for domain knowledge graph alignment via neighborhood
partitioning and generative subgraph encoding. _AAAI Spring Symposia_, 2024.


[213] Rushali Deshmukh, Rutuj Raut, Mayur Bhavsar, Sanika Gurav, and Y. Patil. Optimizing human-ai
interaction: Innovations in prompt engineering. _2025 3rd International Conference on Intelligent_
_Data Communication Technologies and Internet of Things (IDCIoT)_, 2025.


[214] Darshan Deshpande, Varun Gangal, Hersh Mehta, Jitin Krishnan, Anand Kannappan, and Rebecca
Qian. Trail: Trace reasoning and agentic issue localization, arXiv preprint arXiv:2505.08638, 2025.
[URL https://arxiv.org/abs/2505.08638v3.](https://arxiv.org/abs/2505.08638v3)


[215] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep
bidirectional transformers for language understanding. _North American Chapter of the Association_
_for Computational Linguistics_, 2019.


[216] Dhruv Dhamani and Mary Lou Maher. The tyranny of possibilities in the design of task-oriented llm
[systems: A scoping survey, arXiv preprint arXiv:2312.17601, 2023. URL https://arxiv.org/](https://arxiv.org/abs/2312.17601v1)
[abs/2312.17601v1.](https://arxiv.org/abs/2312.17601v1)


[217] Frederick Dillon, Gregor Halvorsen, Simon Tattershall, Magnus Rowntree, and Gareth Vanderpool.
Contextual memory reweaving in large language models using layered latent state reconstruction,
[arXiv preprint arXiv:2502.02046, 2025. URL https://arxiv.org/abs/2502.02046v2.](https://arxiv.org/abs/2502.02046v2)


[218] Hanxing Ding, Shuchang Tao, Liang Pang, Zihao Wei, Jinyang Gao, Bolin Ding, Huawei Shen,
and Xueqi Chen. Toolcoder: A systematic code-empowered tool learning framework for large
[language models, arXiv preprint arXiv:2502.11404, 2025. URL https://arxiv.org/abs/2502.](https://arxiv.org/abs/2502.11404v2)
[11404v2.](https://arxiv.org/abs/2502.11404v2)


75


[219] Hongxin Ding, Yue Fang, Runchuan Zhu, Xinke Jiang, Jinyang Zhang, Yongxin Xu, Xu Chu, Junfeng
Zhao, and Yasha Wang. 3ds: Decomposed difficulty data selection’s case study on llm medical
domain adaptation. 2024.


[220] Jiayu Ding, Shuming Ma, Li Dong, Xingxing Zhang, Shaohan Huang, Wenhui Wang, and Furu Wei.
Longnet: Scaling transformers to 1, 000, 000, 000 tokens. arXiv preprint, 2023.


[221] Tianyu Ding, Tianyi Chen, Haidong Zhu, Jiachen Jiang, Yiqi Zhong, Jinxin Zhou, Guangzhi Wang,
Zhihui Zhu, Ilya Zharkov, and Luming Liang. The efficiency spectrum of large language models:
[An algorithmic survey, arXiv preprint arXiv:2312.00678, 2023. URL https://arxiv.org/abs/](https://arxiv.org/abs/2312.00678v2)
[2312.00678v2.](https://arxiv.org/abs/2312.00678v2)


[222] Yiran Ding, L. Zhang, Chengruidong Zhang, Yuanyuan Xu, Ning Shang, Jiahang Xu, Fan Yang,
and Mao Yang. Longrope: Extending llm context window beyond 2 million tokens. _International_
_Conference on Machine Learning_, 2024.


[223] Yiwen Ding, Zhiheng Xi, Wei He, Zhuoyuan Li, Yitao Zhai, Xiaowei Shi, Xunliang Cai, Tao Gui,
Qi Zhang, and Xuanjing Huang. Mitigating tail narrowing in llm self-improvement via socratic-guided
sampling. _North American Chapter of the Association for Computational Linguistics_, 2024.


[224] Christian Djeffal. Reflexive prompt engineering: A framework for responsible prompt engineering
and ai interaction design. _Conference on Fairness, Accountability and Transparency_, 2025.


[225] G Dong, Y Chen, X Li, J Jin, H Qian, and Y Zhu.... Tool-star: Empowering llm-brained multi-tool
[reasoner via reinforcement learning. 2025. URL https://arxiv.org/abs/2505.16410.](https://arxiv.org/abs/2505.16410)


[226] Guanting Dong, Jinxu Zhao, Tingfeng Hui, Daichi Guo, Wenlong Wan, Boqi Feng, Yueyan Qiu,
Zhuoma Gongque, Keqing He, Zechen Wang, and Weiran Xu. Revisit input perturbation problems
for llms: A unified robustness evaluation framework for noisy slot filling task. _Natural Language_
_Processing and Chinese Computing_, 2023.


[227] Guanting Dong, Yifei Chen, Xiaoxi Li, Jiajie Jin, Hongjin Qian, Yutao Zhu, Hangyu Mao, Guorui
Zhou, Zhicheng Dou, and Ji-Rong Wen. Tool-star: Empowering llm-brained multi-tool reasoner via
reinforcement learning. arXiv preprint, 2025.


[228] Kaiwen Dong. Large language model applied in multi-agent systema survey. _Applied and Computa-_
_tional Engineering_, 2024.


[229] Peijie Dong, Zhenheng Tang, Xiang-Hong Liu, Lujun Li, Xiaowen Chu, and Bo Li. Can compressed
llms truly act? an empirical evaluation of agentic capabilities in llm compression, arXiv preprint
[arXiv:2505.19433, 2025. URL https://arxiv.org/abs/2505.19433v2.](https://arxiv.org/abs/2505.19433v2)


[230] Vicky Dong, Hao Yu, and Yao Chen. Graph-augmented relation extraction model with llms-generated
[support document, arXiv preprint arXiv:2410.23452, 2024. URL https://arxiv.org/abs/](https://arxiv.org/abs/2410.23452v1)
[2410.23452v1.](https://arxiv.org/abs/2410.23452v1)


[231] Xiangjue Dong, Maria Teleki, and James Caverlee. A survey on llm inference-time self-improvement,
[arXiv preprint arXiv:2412.14352, 2024. URL https://arxiv.org/abs/2412.14352v1.](https://arxiv.org/abs/2412.14352v1)


[232] Yuxin Dong, Shuo Wang, Hongye Zheng, Jiajing Chen, Zhenhong Zhang, and Chihang Wang.
Advanced rag models with graph structures: Optimizing complex knowledge reasoning and text generation. _2024 5th International Symposium on Computer Engineering and Intelligent Communications_
_(ISCEIC)_, 2024.


76


[233] Zican Dong, Junyi Li, Xin Men, Wayne Xin Zhao, Bingbing Wang, Zhen Tian, Weipeng Chen, and
Ji-Rong Wen. Exploring context window of large language models via decomposed positional vectors.
_Neural Information Processing Systems_, 2024.


[234] Ehsan Doostmohammadi and Marco Kuhlmann. Studying the role of input-neighbor overlap in
retrieval-augmented language models training efficiency, arXiv preprint arXiv:2505.14309, 2025.
[URL https://arxiv.org/abs/2505.14309v1.](https://arxiv.org/abs/2505.14309v1)


[235] Mohammadreza Doostmohammadian, Alireza Aghasi, Mohammad Pirani, Ehsan Nekouei, H. Zarrabi,
Reza Keypour, Apostolos I. Rikos, and K. H. Johansson. Survey of distributed algorithms for resource
[allocation over multi-agent systems, arXiv preprint arXiv:2401.15607, 2024. URL https://arxiv.](https://arxiv.org/abs/2401.15607v1)
[org/abs/2401.15607v1.](https://arxiv.org/abs/2401.15607v1)

