<!-- Source: 02-ContextSurvey-2507.13334.pdf | Chunk 15/26 -->

[454] Sirui Huang, Hanqian Li, Yanggan Gu, Xuming Hu, Qing Li, and Guandong Xu. Hyperg: Hypergraph[enhanced llms for structured knowledge, arXiv preprint arXiv:2502.18125, 2025. URL https:](https://arxiv.org/abs/2502.18125v1)
[//arxiv.org/abs/2502.18125v1.](https://arxiv.org/abs/2502.18125v1)


[455] Wenlong Huang, P. Abbeel, Deepak Pathak, and Igor Mordatch. Language models as zero-shot
planners: Extracting actionable knowledge for embodied agents. _International Conference on Machine_
_Learning_, 2022.


[456] Xu Huang, Jianxun Lian, Yuxuan Lei, Jing Yao, Defu Lian, and Xing Xie. Recommender ai agent:
Integrating large language models for interactive recommendations, arXiv preprint arXiv:2308.16505,
[2024. URL https://arxiv.org/abs/2308.16505.](https://arxiv.org/abs/2308.16505)


[457] Xu Huang, Weiwen Liu, Xiaolong Chen, Xingmei Wang, Hao Wang, Defu Lian, Yasheng Wang,
Ruiming Tang, and Enhong Chen. Understanding the planning of llm agents: A survey, arXiv preprint
[arXiv:2402.02716, 2024. URL https://arxiv.org/abs/2402.02716v1.](https://arxiv.org/abs/2402.02716v1)


[458] Y Huang, J Shi, Y Li, C Fan, S Wu, and Q Zhang.... Metatool benchmark for large language models:
[Deciding whether to use tools and which to use. 2023. URL https://arxiv.org/abs/2310.](https://arxiv.org/abs/2310.03128)
[03128.](https://arxiv.org/abs/2310.03128)


93


[459] Yunpeng Huang, Jingwei Xu, Zixu Jiang, Junyu Lai, Zenan Li, Yuan Yao, Taolue Chen, Lijuan
Yang, Zhou Xin, and Xiaoxing Ma. Advancing transformer architecture in long-context large
[language models: A comprehensive survey, arXiv preprint arXiv:2311.12351, 2023. URL https:](https://arxiv.org/abs/2311.12351v2)
[//arxiv.org/abs/2311.12351v2.](https://arxiv.org/abs/2311.12351v2)


[460] Zeyi Huang, Yuyang Ji, Anirudh Sundara Rajan, Zefan Cai, Wen Xiao, Junjie Hu, and Yong Jae Lee.
Visualtoolagent (vista): A reinforcement learning framework for visual tool selection, arXiv preprint
[arXiv:2505.20289, 2025. URL https://arxiv.org/abs/2505.20289v1.](https://arxiv.org/abs/2505.20289v1)


[461] Ziheng Huang, S. Gutierrez, Hemanth Kamana, and S. Macneil. Memory sandbox: Transparent
and interactive memory management for conversational agents. _ACM Symposium on User Interface_
_Software and Technology_, 2023.


[462] Ziheng Huang, Sebastian Gutierrez, Hemanth Kamana, and Stephen MacNeil. Memory sandbox: Transparent and interactive memory management for conversational agents, arXiv preprint
[arXiv:2308.01542, 2023. URL https://arxiv.org/abs/2308.01542.](https://arxiv.org/abs/2308.01542)


[463] Alexis Huet, Zied Ben-Houidi, and Dario Rossi. Episodic memories generation and evaluation
benchmark for large language models. _International Conference on Learning Representations_, 2025.


[464] Dom Huh and Prasant Mohapatra. Multi-agent reinforcement learning: A comprehensive survey,
[arXiv preprint arXiv:2312.10256, 2023. URL https://arxiv.org/abs/2312.10256v2.](https://arxiv.org/abs/2312.10256v2)


[465] Eunjeong Hwang, Yichao Zhou, James Bradley Wendt, Beliz Gunel, Nguyen Vo, Jing Xie, and Sandeep
Tata. Enhancing incremental summarization with structured representations. _Conference on Empirical_
_Methods in Natural Language Processing_, 2024.


[466] Thorsten Händler. Balancing autonomy and alignment: A multi-dimensional taxonomy for autonomous llm-powered multi-agent architectures. arXiv preprint, 2023.


[467] Michael Iannelli, Sneha Kuchipudi, and Vera Dvorak. Sla management in reconfigurable multi-agent
rag: A systems approach to question answering, arXiv preprint arXiv:2412.06832, 2024. URL
[https://arxiv.org/abs/2412.06832v2.](https://arxiv.org/abs/2412.06832v2)


[[468] IBM. What is agent communication protocol (acp)? https://www.ibm.com/think/topics/](https://www.ibm.com/think/topics/agent-communication-protocol)
[agent-communication-protocol, 2025. [Online; accessed 17-July-2025].](https://www.ibm.com/think/topics/agent-communication-protocol)


[469] T Inaba, H Kiyomaru, F Cheng, and S Kurohashi. Multitool-cot: Gpt-3 can use multiple external
[tools with chain of thought prompting. 2023. URL https://arxiv.org/abs/2305.16896.](https://arxiv.org/abs/2305.16896)


[470] G. Indiveri and Shih-Chii Liu. Memory and information processing in neuromorphic systems.
_Proceedings of the IEEE_, 2015.


[471] V. Ioannidis, Xiang Song, Da Zheng, Houyu Zhang, Jun Ma, Yi Xu, Belinda Zeng, Trishul M. Chilimbi,
and G. Karypis. Efficient and effective training of language and graph neural network models, arXiv
[preprint arXiv:2206.10781, 2022. URL https://arxiv.org/abs/2206.10781v1.](https://arxiv.org/abs/2206.10781v1)


[472] Yoichi Ishibashi, Taro Yano, and M. Oyamada. Can large language models invent algorithms to improve themselves?: Algorithm discovery for recursive self-improvement through reinforcement learn[ing, arXiv preprint arXiv:2410.15639, 2024. URL https://arxiv.org/abs/2410.15639v5.](https://arxiv.org/abs/2410.15639v5)


94


[473] Shadi Iskander, Nachshon Cohen, Zohar S. Karnin, Ori Shapira, and Sofia Tolmach. Quality matters:
Evaluating synthetic data for tool-using llms. _Conference on Empirical Methods in Natural Language_
_Processing_, 2024.


[474] Z. Ismail and N. Sariff. A survey and analysis of cooperative multi-agent robot systems: Challenges
and directions. _Applications of Mobile Robots_, 2018.


[475] Yusuf Izmirlioglu, Loc Pham, Tran Cao Son, and Enrico Pontelli. A survey of multi-agent systems for
smartgrids. _Energies_, 2024.


[[476] Jace.AI. Jace.ai web agent, 2024. URL https://www.jace.ai/. Accessed: 2025-07-14.](https://www.jace.ai/)


[477] Arthur Jacot, Franck Gabriel, and Clément Hongler. Neural tangent kernel: Convergence and
generalization in neural networks. _Neural Information Processing Systems_, 2018.


[478] Tejas Jade and Alex Yartsev. Chatgpt for automated grading of short answer questions in mechani[cal ventilation, arXiv preprint arXiv:2505.04645, 2025. URL https://arxiv.org/abs/2505.](https://arxiv.org/abs/2505.04645v1)
[04645v1.](https://arxiv.org/abs/2505.04645v1)


[479] A. Jafarpour, L. Fuentemilla, A. Horner, W. Penny, and E. Duzel. Replay of very early encoding
representations during recollection. _Journal of Neuroscience_, 2014.


[480] A. Jaiswal, Nurendra Choudhary, Ravinarayana Adkathimar, M. P. Alagappan, G. Hiranandani,
Ying Ding, Zhangyang Wang, E-Wen Huang, and Karthik Subbian. All against some: Efficient
integration of large language models for message passing in graph neural networks, arXiv preprint
[arXiv:2407.14996, 2024. URL https://arxiv.org/abs/2407.14996v1.](https://arxiv.org/abs/2407.14996v1)


[481] H. Jaleel, Jane J. Stephan, and Sinan Naji. Multi-agent systems: A review study. _Ibn AL- Haitham_
_Journal For Pure and Applied Sciences_, 2020.


[482] Lawrence Jang, Yinheng Li, Charles Ding, Justin Lin, Paul Pu Liang, Dan Zhao, Rogerio Bonatti, and
K. Koishida. Videowebarena: Evaluating long context multimodal agents with video understanding
web tasks. _International Conference on Learning Representations_, 2024.


[483] R. Janik. Aspects of human memory and large language models, arXiv preprint arXiv:2311.03839,
[2023. URL https://arxiv.org/abs/2311.03839v3.](https://arxiv.org/abs/2311.03839v3)


[484] Shumaila Javaid, Hamza Fahim, Bin He, and Nasir Saeed. Large language models for uavs: Current
state and pathways to the future. _IEEE Open Journal of Vehicular Technology_, 2024.


[485] T. S. Jayram, Younes Bouhadjar, Ryan L. McAvoy, Tomasz Kornuta, Alexis Asseman, K. Rocki, and
A. Ozcan. Learning to remember, forget and ignore using attention control in memory, arXiv preprint
[arXiv:1809.11087, 2018. URL https://arxiv.org/abs/1809.11087v1.](https://arxiv.org/abs/1809.11087v1)


[486] Cheonsu Jeong. A study on the mcp x a2a framework for enhancing interoperability of llm-based
[autonomous agents, arXiv preprint arXiv:2506.01804, 2025. URL https://arxiv.org/abs/](https://arxiv.org/abs/2506.01804v2)
[2506.01804v2.](https://arxiv.org/abs/2506.01804v2)


[487] Gang Ji and J. Bilmes. Multi-speaker language modeling. _North American Chapter of the Association_
_for Computational Linguistics_, 2004.


95


[488] Ke Ji, Junying Chen, Anningzhe Gao, Wenya Xie, Xiang Wan, and Benyou Wang. Llms could
autonomously learn without external supervision, arXiv preprint arXiv:2406.00606, 2024. URL
[https://arxiv.org/abs/2406.00606v2.](https://arxiv.org/abs/2406.00606v2)


[489] Shaoxiong Ji, Shirui Pan, E. Cambria, P. Marttinen, and Philip S. Yu. A survey on knowledge graphs:
Representation, acquisition, and applications. _IEEE Transactions on Neural Networks and Learning_
_Systems_, 2020.


[490] Bowen Jiang, Runchuan Zhu, Jiang Wu, Zinco Jiang, Yifan He, Junyuan Gao, Jia Yu, Rui Min, Yinfan
Wang, Haote Yang, et al. Evaluating large language model with knowledge oriented language
specific simple question answering. 2025.


[491] Caigao Jiang, Siqiao Xue, James Zhang, Lingyue Liu, Zhibo Zhu, and Hongyan Hao. Learning largescale universal user representation with sparse mixture of experts, arXiv preprint arXiv:2207.04648,
[2022. URL https://arxiv.org/abs/2207.04648v1.](https://arxiv.org/abs/2207.04648v1)


[492] Feibo Jiang, Li Dong, Yubo Peng, Kezhi Wang, Kun Yang, Cunhua Pan, D. Niyato, and O. Dobre. Large
language model enhanced multi-agent systems for 6g communications. _IEEE wireless communications_,
2023.


[493] J Jiang, K Zhou, WX Zhao, Y Song, and C Zhu.... Kg-agent: An efficient autonomous agent
[framework for complex reasoning over knowledge graph. 2024. URL https://arxiv.org/abs/](https://arxiv.org/abs/2402.11163)
[2402.11163.](https://arxiv.org/abs/2402.11163)


[494] Jinhao Jiang, Kun Zhou, Wayne Xin Zhao, and Ji rong Wen. Unikgqa: Unified retrieval and reasoning
for solving multi-hop question answering over knowledge graph. _International Conference on Learning_
_Representations_, 2022.


[495] Jinhao Jiang, Kun Zhou, Zican Dong, Keming Ye, Wayne Xin Zhao, and Ji rong Wen. Structgpt: A
general framework for large language model to reason over structured data. _Conference on Empirical_
_Methods in Natural Language Processing_, 2023.


[496] Song Jiang, Zahra Shakeri, Aaron Chan, Maziar Sanjabi, Hamed Firooz, Yinglong Xia, Bugra Akyildiz,
Yizhou Sun, Jinchao Li, Qifan Wang, and Asli Celikyilmaz. Resprompt: Residual connection prompting
advances multi-step reasoning in large language models. _North American Chapter of the Association_
_for Computational Linguistics_, 2023.


[497] Zhengbao Jiang, Frank F. Xu, Luyu Gao, Zhiqing Sun, Qian Liu, Jane Dwivedi-Yu, Yiming Yang,
Jamie Callan, and Graham Neubig. Active retrieval augmented generation. _Conference on Empirical_
_Methods in Natural Language Processing_, 2023.


[498] Zhonglin Jiang, Qian Tang, and Zequn Wang. Generative reliability-based design optimization using
in-context learning capabilities of large language models, arXiv preprint arXiv:2503.22401, 2025.
[URL https://arxiv.org/abs/2503.22401v1.](https://arxiv.org/abs/2503.22401v1)


[499] Zhuoxuan Jiang, Haoyuan Peng, Shanshan Feng, Fan Li, and Dongsheng Li. Llms can find mathematical reasoning mistakes by pedagogical chain-of-thought. _International Joint Conference on Artificial_
_Intelligence_, 2024.


[500] Carlos E. Jimenez, John Yang, Alexander Wettig, Shunyu Yao, Kexin Pei, Ofir Press, and Karthik
Narasimhan. Swe-bench: Can language models resolve real-world github issues?, arXiv preprint
[arXiv:2310.06770, 2024. URL https://arxiv.org/abs/2310.06770.](https://arxiv.org/abs/2310.06770)


96


[501] B Jin, C Xie, J Zhang, KK Roy, Y Zhang, and Z Li.... Graph chain-of-thought: Augmenting large
[language models by reasoning on graphs. 2024. URL https://arxiv.org/abs/2404.07103.](https://arxiv.org/abs/2404.07103)


[502] Bowen Jin, Yu Zhang, Qi Zhu, and Jiawei Han. Heterformer: Transformer-based deep node
representation learning on heterogeneous text-rich networks. _Knowledge Discovery and Data Mining_,
2022.


[503] Feihu Jin, Jiajun Zhang, and Chengqing Zong. Parameter-efficient tuning for large language model
without calculating its gradients. _Conference on Empirical Methods in Natural Language Processing_,
2023.


[504] Haolin Jin, Linghan Huang, Haipeng Cai, Jun Yan, Bo Li, and Huaming Chen. From llms to llmbased agents for software engineering: A survey of current, challenges and future, arXiv preprint
[arXiv:2408.02479, 2024. URL https://arxiv.org/abs/2408.02479v2.](https://arxiv.org/abs/2408.02479v2)


[505] Hongye Jin, Xiaotian Han, Jingfeng Yang, Zhimeng Jiang, Zirui Liu, Chia yuan Chang, Huiyuan
Chen, and Xia Hu. Llm maybe longlm: Self-extend llm context window without tuning. _International_
_Conference on Machine Learning_, 2024.


[506] Jiajie Jin, Yutao Zhu, Xinyu Yang, Chenghao Zhang, and Zhicheng Dou. Flashrag: A modular toolkit
for efficient retrieval-augmented generation research. _The Web Conference_, 2024.


[507] Qiao Jin, Yifan Yang, Qingyu Chen, and Zhiyong Lu. Genegpt: Augmenting large language models
with domain tools for improved access to biomedical information. _Bioinform._, 2023.


[508] Tian Jin, W. Yazar, Zifei Xu, Sayeh Sharify, and Xin Wang. Self-selected attention span for accelerating
[large language model inference, arXiv preprint arXiv:2404.09336, 2024. URL https://arxiv.](https://arxiv.org/abs/2404.09336v1)
[org/abs/2404.09336v1.](https://arxiv.org/abs/2404.09336v1)


[509] Weiqiang Jin, Hongyang Du, Biao Zhao, Xingwu Tian, Bohang Shi, and Guang Yang. A comprehensive survey on multi-agent cooperative decision-making: Scenarios, approaches, challenges and
perspectives. arXiv preprint, 2025.


[510] Yang Jin, Kun Xu, Kun Xu, Liwei Chen, Chao Liao, Jianchao Tan, Quzhe Huang, Bin Chen, Chenyi
Lei, An Liu, Chengru Song, Xiaoqiang Lei, Di Zhang, Wenwu Ou, Kun Gai, and Yadong Mu. Unified
language-vision pretraining in llm with dynamic discrete visual tokenization. _International Conference_
_on Learning Representations_, 2023.


[511] Yiqiao Jin, Minje Choi, Gaurav Verma, Jindong Wang, and Srijan Kumar. Mm-soc: Benchmarking
multimodal large language models in social media platforms. _Annual Meeting of the Association for_
_Computational Linguistics_, 2024.


[512] Yiyang Jin, Kunzhao Xu, Hang Li, Xueting Han, Yanmin Zhou, Cheng Li, and Jing Bai. Reveal:
Self-evolving code agents via iterative generation-verification, arXiv preprint arXiv:2506.11442,
[2025. URL https://arxiv.org/abs/2506.11442v1.](https://arxiv.org/abs/2506.11442v1)


[513] Jeff Johnson, Matthijs Douze, and H. Jégou. Billion-scale similarity search with gpus. _IEEE Transac-_
_tions on Big Data_, 2017.


[514] Jeff A. Johnson and Daniel H. Bullock. Fragility in ais using artificial neural networks. _Communications_
_of the ACM_, 2023.


97


[515] Zhao Kaiya, Michelangelo Naim, J. Kondic, Manuel Cortes, Jiaxin Ge, Shuying Luo, Guangyu Robert
Yang, and Andrew Ahn. Lyfe agents: Generative agents for low-cost real-time social interactions,
[arXiv preprint arXiv:2310.02172, 2023. URL https://arxiv.org/abs/2310.02172v1.](https://arxiv.org/abs/2310.02172v1)


[516] Kurmanbek Kaiyrbekov, Nic Dobbins, and Sean D. Mooney. Automated survey collection with llm[based conversational agents, arXiv preprint arXiv:2504.02891, 2025. URL https://arxiv.org/](https://arxiv.org/abs/2504.02891v1)
[abs/2504.02891v1.](https://arxiv.org/abs/2504.02891v1)


[517] A. Kakas, P. Mancarella, F. Sadri, Kostas Stathis, and Francesca Toni. Computational logic foundations
of kgp agents. _Journal of Artificial Intelligence Research_, 2008.


[518] Vikas Kamra, Lakshya Gupta, Dhruv Arora, and Ashwin Kumar Yadav. Enhancing document retrieval
using ai and graph-based rag techniques. _2024 5th International Conference on Communication,_
_Computing & Industry 6.0 (C2I6)_, 2024.


[519] Eser Kandogan, Nikita Bhutani, Dan Zhang, Rafael Li Chen, Sairam Gurajada, and Estevam R.
Hruschka. Orchestrating agents and data for enterprise: A blueprint architecture for compound ai,
[arXiv preprint arXiv:2504.08148, 2025. URL https://arxiv.org/abs/2504.08148v1.](https://arxiv.org/abs/2504.08148v1)


[520] Haoyu Kang, Yuzhou Zhu, Yukun Zhong, Ke Wang Central South University, Dalian University
of Technology, Nanjing University, and Xidian University. Sakr: Enhancing retrieval-augmented
generation via streaming algorithm and k-means clustering. arXiv preprint, 2024.


[521] Jiazheng Kang, Mingming Ji, Zhe Zhao, and Ting Bai. Memory os of ai agent, arXiv preprint
[arXiv:2506.06326, 2025. URL https://arxiv.org/abs/2506.06326v1.](https://arxiv.org/abs/2506.06326v1)


[522] Jikun Kang, Wenqi Wu, Filippos Christianos, Alex J. Chan, Fraser Greenlee, George Thomas, Marvin
Purtorab, and Andy Toulis. Lm2: Large memory models, arXiv preprint arXiv:2502.06049, 2025.
[URL https://arxiv.org/abs/2502.06049v1.](https://arxiv.org/abs/2502.06049v1)


[523] Sungmin Kang, Gabin An, and S. Yoo. A quantitative and qualitative evaluation of llm-based
explainable fault localization. _Proc. ACM Softw. Eng._, 2023.


[524] Guy Kaplan, Matanel Oren, Yuval Reif, and Roy Schwartz. From tokens to words: On the inner
lexicon of llms. _International Conference on Learning Representations_, 2024.


[525] Vladimir Karpukhin, Barlas Oğuz, Sewon Min, Patrick Lewis, Ledell Yu Wu, Sergey Edunov, Danqi
Chen, and Wen tau Yih. Dense passage retrieval for open-domain question answering. _Conference on_
_Empirical Methods in Natural Language Processing_, 2020.


[526] Zdeněk Kasner and Ondrej Dusek. Beyond traditional benchmarks: Analyzing behaviors of open llms
on data-to-text generation. _Annual Meeting of the Association for Computational Linguistics_, 2024.


[527] Kiran Kate, Tejaswini Pedapati, Kinjal Basu, Yara Rizk, Vijil Chenthamarakshan, Subhajit Chaudhury,
Mayank Agarwal, and Ibrahim Abdelaziz. Longfunceval: Measuring the effectiveness of long context
[models for function calling, arXiv preprint arXiv:2505.10570, 2025. URL https://arxiv.org/](https://arxiv.org/abs/2505.10570v1)
[abs/2505.10570v1.](https://arxiv.org/abs/2505.10570v1)


[528] Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and Franccois Fleuret. Transformers are
rnns: Fast autoregressive transformers with linear attention. _International Conference on Machine_
_Learning_, 2020.


98


[529] Richard Katrix, Quentin Carroway, Rowan Hawkesbury, and Matthias Heathfield. Context-aware
semantic recomposition mechanism for large language models, arXiv preprint arXiv:2501.17386,
[2025. URL https://arxiv.org/abs/2501.17386v2.](https://arxiv.org/abs/2501.17386v2)


[530] Amirhossein Kazemnejad, Inkit Padhi, K. Ramamurthy, Payel Das, and Siva Reddy. The impact of
positional encoding on length generalization in transformers. _Neural Information Processing Systems_,
2023.


[531] T. Kelley, R. Thomson, and Jonathan Milton. Standard model of mind: Episodic memory. _Biologically_
_Inspired Cognitive Architectures_, 2018.


[532] Daan Kepel and Konstantina Valogianni. Autonomous prompt engineering in large language models,
[arXiv preprint arXiv:2407.11000, 2024. URL https://arxiv.org/abs/2407.11000v1.](https://arxiv.org/abs/2407.11000v1)


[533] R. Kesner. Neurobiological foundations of an attribute model of memory. arXiv preprint, 2013.


[534] A. Khan, Md Toufique Hasan, Kai-Kristian Kemell, Jussi Rasku, and Pekka Abrahamsson. Developing
retrieval augmented generation (rag) based llm systems from pdfs: An experience report, arXiv
[preprint arXiv:2410.15944, 2024. URL https://arxiv.org/abs/2410.15944v1.](https://arxiv.org/abs/2410.15944v1)


[535] Muhammad Tayyab Khan, Lequn Chen, Ye Han Ng, Wenhe Feng, Nicholas Yew Jin Tan, and Seung Ki
Moon. Leveraging vision-language models for manufacturing feature recognition in cad designs,
[arXiv preprint arXiv:2411.02810, 2024. URL https://arxiv.org/abs/2411.02810v1.](https://arxiv.org/abs/2411.02810v1)


[536] Urvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke Zettlemoyer, and M. Lewis. Generalization
through memorization: Nearest neighbor language models. _International Conference on Learning_
_Representations_, 2019.


[537] Elahe Khatibi, Ziyu Wang, and Amir M. Rahmani. Cdf-rag: Causal dynamic feedback for adaptive
retrieval-augmented generation. arXiv preprint, 2025.


[538] Prannay Khosla, Piotr Teterwak, Chen Wang, Aaron Sarna, Yonglong Tian, Phillip Isola, Aaron
Maschinot, Ce Liu, and Dilip Krishnan. Supervised contrastive learning. _Neural Information Processing_
_Systems_, 2020.


[539] Tushar Khot, H. Trivedi, Matthew Finlayson, Yao Fu, Kyle Richardson, Peter Clark, and Ashish
Sabharwal. Decomposed prompting: A modular approach for solving complex tasks. _International_
_Conference on Learning Representations_, 2022.


[540] Sambhav Khurana, Xiner Li, Shurui Gui, and Shuiwang Ji. A hierarchical language model for
[interpretable graph reasoning, arXiv preprint arXiv:2410.22372, 2024. URL https://arxiv.](https://arxiv.org/abs/2410.22372v1)
[org/abs/2410.22372v1.](https://arxiv.org/abs/2410.22372v1)


[541] Daehee Kim, Deokhyung Kang, Sangwon Ryu, and Gary Geunbae Lee. Ontology-free general-domain
knowledge graph-to-text generation dataset synthesis using large language model, arXiv preprint
[arXiv:2409.07088, 2024. URL https://arxiv.org/abs/2409.07088v1.](https://arxiv.org/abs/2409.07088v1)


[542] Geunwoo Kim, P. Baldi, and S. McAleer. Language models can solve computer tasks. _Neural_
_Information Processing Systems_, 2023.


[543] Jaeyeon Kim, Injune Hwang, and Kyogu Lee. Learning semantic information from raw audio signal
using both contextual and phonetic representations. _IEEE International Conference on Acoustics,_
_Speech, and Signal Processing_, 2024.


99


[544] Jang-Hyun Kim, Junyoung Yeom, Sangdoo Yun, and Hyun Oh Song. Compressed context memory
for online language model interaction. _International Conference on Learning Representations_, 2023.


[545] Jiho Kim, Yeonsu Kwon, Yohan Jo, and Edward Choi. Kg-gpt: A general framework for reasoning
on knowledge graphs using large language models. _Conference on Empirical Methods in Natural_
_Language Processing_, 2023.


[546] Jiin Kim, Byeongjun Shin, Jinha Chung, and Minsoo Rhu. The cost of dynamic reasoning: Demystifying ai agents and test-time scaling from an ai infrastructure perspective, arXiv preprint
[arXiv:2506.04301, 2025. URL https://arxiv.org/abs/2506.04301v1.](https://arxiv.org/abs/2506.04301v1)


[547] Sehoon Kim, Suhong Moon, Ryan Tabrizi, Nicholas Lee, Michael W. Mahoney, Kurt Keutzer, and
A. Gholami. An llm compiler for parallel function calling. _International Conference on Machine_
_Learning_, 2023.


[548] Taewoon Kim, Michael Cochez, Vincent Francois-Lavet, Mark Neerincx, and Piek Vossen. A machine
with short-term, episodic, and semantic memory systems. _Proceedings of the AAAI Conference on_
_Artificial Intelligence_, 37(1):48–56, 2023. ISSN 2159-5399. doi: 10.1609/aaai.v37i1.25075. URL
[http://dx.doi.org/10.1609/aaai.v37i1.25075.](http://dx.doi.org/10.1609/aaai.v37i1.25075)


[549] Lukas Kirchdorfer, Robert Blümel, T. Kampik, Han van der Aa, and Heiner Stuckenschmidt. Discovering multi-agent systems for resource-centric business process simulation. _Process Science_,
2025.


[550] J. Kirkpatrick, Razvan Pascanu, Neil C. Rabinowitz, J. Veness, Guillaume Desjardins, Andrei A.
Rusu, Kieran Milan, John Quan, Tiago Ramalho, A. Grabska-Barwinska, D. Hassabis, C. Clopath,
D. Kumaran, and R. Hadsell. Overcoming catastrophic forgetting in neural networks. _Proceedings of_
_the National Academy of Sciences of the United States of America_, 2016.


[551] Louis Kirsch, James Harrison, Jascha Narain Sohl-Dickstein, and Luke Metz. General-purpose
in-context learning by meta-learning transformers, arXiv preprint arXiv:2212.04458, 2022. URL
[https://arxiv.org/abs/2212.04458v2.](https://arxiv.org/abs/2212.04458v2)


[552] Yuval Kirstain, Patrick Lewis, Sebastian Riedel, and Omer Levy. A few more examples may be worth
billions of parameters. _Conference on Empirical Methods in Natural Language Processing_, 2021.


[553] Andrew Kiruluta, Preethi Raju, and Priscilla Burity. Breaking quadratic barriers: A non-attention
[llm for ultra-long context horizons, arXiv preprint arXiv:2506.01963, 2025. URL https://arxiv.](https://arxiv.org/abs/2506.01963v1)
[org/abs/2506.01963v1.](https://arxiv.org/abs/2506.01963v1)


[554] Nikita Kitaev, Lukasz Kaiser, and Anselm Levskaya. Reformer: The efficient transformer. _International_
_Conference on Learning Representations_, 2020.


[555] Vincent Koc, Jacques Verre, Douglas Blank, and Abigail Morgan. Mind the metrics: Patterns for
telemetry-aware in-ide ai application development using the model context protocol (mcp), arXiv
[preprint arXiv:2506.11019, 2025. URL https://arxiv.org/abs/2506.11019v1.](https://arxiv.org/abs/2506.11019v1)


[556] Tomás Kociský, Jonathan Schwarz, Phil Blunsom, Chris Dyer, Karl Moritz Hermann, Gábor Melis,
and Edward Grefenstette. The narrativeqa reading comprehension challenge. _Transactions of the_
_Association for Computational Linguistics_, 2017.


100

