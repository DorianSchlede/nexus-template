<!-- Source: 02-ContextSurvey-2507.13334.pdf | Chunk 14/26 -->


[375] Tae Jun Ham, Yejin Lee, Seong Hoon Seo, Soo-Uck Kim, Hyunji Choi, Sungjun Jung, and Jae W.
Lee. Elsa: Hardware-software co-design for efficient, lightweight self-attention mechanism in neural
networks. _International Symposium on Computer Architecture_, 2021.


[376] Feijiang Han, Licheng Guo, Hengtao Cui, and Zhiyuan Lyu. Question tokens deserve more attention:
Enhancing large language models without training through step-by-step reading and question
[attention recalibration, arXiv preprint arXiv:2504.09402, 2025. URL https://arxiv.org/abs/](https://arxiv.org/abs/2504.09402v1)
[2504.09402v1.](https://arxiv.org/abs/2504.09402v1)


[377] Han Han, Tong Zhu, Xiang Zhang, Mengsong Wu, Hao Xiong, and Wenliang Chen. Nestools: A
dataset for evaluating nested tool learning abilities of large language models. _International Conference_
_on Computational Linguistics_, 2024.


[378] Haoyu Han, Yu Wang, Harry Shomer, Kai Guo, Jiayuan Ding, Yongjia Lei, Mahantesh Halappanavar,
Ryan A. Rossi, Subhabrata Mukherjee, Xianfeng Tang, Qi He, Zhigang Hua, Bo Long, Tong Zhao,


87


Neil Shah, Amin Javari, Yinglong Xia, and Jiliang Tang. Retrieval-augmented generation with
[graphs (graphrag), arXiv preprint arXiv:2501.00309, 2025. URL https://arxiv.org/abs/](https://arxiv.org/abs/2501.00309)
[2501.00309.](https://arxiv.org/abs/2501.00309)


[379] Tingxu Han, Zhenting Wang, Chunrong Fang, Shiyun Zhao, Shiqing Ma, and Zhenyu Chen. Token[budget-aware llm reasoning, arXiv preprint arXiv:2412.18547, 2024. URL https://arxiv.org/](https://arxiv.org/abs/2412.18547v5)
[abs/2412.18547v5.](https://arxiv.org/abs/2412.18547v5)


[380] Yuanning Han, Ziyi Qiu, Jiale Cheng, and Ray Lc. When teams embrace ai: Human collaboration
strategies in generative prompting in a creative design task. _International Conference on Human_
_Factors in Computing Systems_, 2024.


[381] R. Hankache, Kingsley Nketia Acheampong, Liang Song, Marek Brynda, Raad Khraishi, and Greig A.
Cowan. Evaluating the sensitivity of llms to prior context, arXiv preprint arXiv:2506.00069, 2025.
[URL https://arxiv.org/abs/2506.00069v1.](https://arxiv.org/abs/2506.00069v1)


[382] S Hao, T Liu, Z Wang, and Z Hu. Toolkengpt: Augmenting frozen language models with massive
[tools via tool embeddings. 2023. URL https://proceedings.neurips.cc/paper_files/](https://proceedings.neurips.cc/paper_files/paper/2023/hash/8fd1a81c882cd45f64958da6284f4a3f-Abstract-Conference.html)

[paper/2023/hash/8fd1a81c882cd45f64958da6284f4a3f-Abstract-Conference.](https://proceedings.neurips.cc/paper_files/paper/2023/hash/8fd1a81c882cd45f64958da6284f4a3f-Abstract-Conference.html)
[html.](https://proceedings.neurips.cc/paper_files/paper/2023/hash/8fd1a81c882cd45f64958da6284f4a3f-Abstract-Conference.html)


[383] Mohanakrishnan Hariharan. Semantic mastery: Enhancing llms with advanced natural language
[understanding, arXiv preprint arXiv:2504.00409, 2025. URL https://arxiv.org/abs/2504.](https://arxiv.org/abs/2504.00409v1)
[00409v1.](https://arxiv.org/abs/2504.00409v1)


[384] Mareike Hartmann and Alexander Koller. A survey on complex tasks for goal-directed interactive agents, arXiv preprint arXiv:2409.18538, 2024. [URL https://arxiv.org/abs/2409.](https://arxiv.org/abs/2409.18538v1)
[18538v1.](https://arxiv.org/abs/2409.18538v1)


[385] A. Hassani, A. Medvedev, P. D. Haghighi, Sea Ling, A. Zaslavsky, and P. Jayaraman. Context definition
and query language: Conceptual specification, implementation, and evaluation. _Italian National_
_Conference on Sensors_, 2019.


[386] Kostas Hatalis, Despina Christou, Joshua Myers, Steven Jones, Keith Lambert, Adam Amos-Binks,
Zohreh Dannenhauer, and Dustin Dannenhauer. Memory matters: The need to improve long-term
memory in llm-agents. _Proceedings of the AAAI Symposium Series_, 2024.


[387] Kostas Hatalis, Despina Christou, and Vyshnavi Kondapalli. Review of case-based reasoning for llm
agents: Theoretical foundations, architectural components, and cognitive integration, arXiv preprint
[arXiv:2504.06943, 2025. URL https://arxiv.org/abs/2504.06943v2.](https://arxiv.org/abs/2504.06943v2)


[388] Jacky He, Guiran Liu, Binrong Zhu, Hanlu Zhang, Hongye Zheng, and Xiaokai Wang. Context-guided
dynamic retrieval for improving generation quality in rag models, arXiv preprint arXiv:2504.19436,
[2025. URL https://arxiv.org/abs/2504.19436v1.](https://arxiv.org/abs/2504.19436v1)


[389] Jianben He, Xingbo Wang, Shiyi Liu, Guande Wu, Claudio Silva, and Huamin Qu. Poem: Interactive
prompt optimization for enhancing multimodal reasoning of large language models. _IEEE Pacific_
_Visualization Symposium_, 2024.


[390] Junqing He, Liang Zhu, Rui Wang, Xi Wang, Gholamreza Haffari, and Jiaxing Zhang. Madial-bench:
Towards real-world evaluation of memory-augmented dialogue generation. _North American Chapter_
_of the Association for Computational Linguistics_, 2024.


88


[391] Shawn He, Surangika Ranathunga, Stephen Cranefield, and B. Savarimuthu. Norm violation
detection in multi-agent systems using large language models: A pilot study. _COINE_, 2024.


[392] Shengtao He. Achieving tool calling functionality in llms using only prompt engineering with[out fine-tuning, arXiv preprint arXiv:2407.04997, 2024. URL https://arxiv.org/abs/2407.](https://arxiv.org/abs/2407.04997v1)
[04997v1.](https://arxiv.org/abs/2407.04997v1)


[393] Wenchong He, Liqian Peng, Zhe Jiang, and Alex Go. You only fine-tune once: Many-shot in[context fine-tuning for large language model, arXiv preprint arXiv:2506.11103, 2025. URL https:](https://arxiv.org/abs/2506.11103v1)
[//arxiv.org/abs/2506.11103v1.](https://arxiv.org/abs/2506.11103v1)


[394] Xu He, Di Wu, Yan Zhai, and Kun Sun. Sentinelagent: Graph-based anomaly detection in multi[agent systems, arXiv preprint arXiv:2505.24201, 2025. URL https://arxiv.org/abs/2505.](https://arxiv.org/abs/2505.24201v1)
[24201v1.](https://arxiv.org/abs/2505.24201v1)


[395] Yang He, Xiao Ding, Bibo Cai, Yufei Zhang, Kai Xiong, Zhouhao Sun, Bing Qin, and Ting Liu. Selfroute: Automatic mode switching via capability estimation for efficient reasoning. arXiv preprint,
2025.


[396] Yu He, Yingxi Li, Colin White, and Ellen Vitercik. Dsr-bench: Evaluating the structural reasoning
abilities of llms via data structures. arXiv preprint, 2025.


[397] Zexue He, Leonid Karlinsky, Donghyun Kim, Julian McAuley, Dmitry Krotov, and Rogério Feris.
Camelot: Towards large language models with training-free consolidated associative memory, arXiv
[preprint arXiv:2402.13449, 2024. URL https://arxiv.org/abs/2402.13449v1.](https://arxiv.org/abs/2402.13449v1)


[398] James B. Heald, M. Lengyel, and D. Wolpert. Contextual inference in learning and memory. _Trends_
_in Cognitive Sciences_, 2022.


[399] Shekoofeh Hedayati, Ryan E. O’Donnell, and Brad Wyble. A model of working memory for latent
representations. _Nature Human Behaviour_, 2021.


[400] Tooraj Helmi. Modeling response consistency in multi-agent llm systems: A comparative analysis
[of shared and separate context approaches, arXiv preprint arXiv:2504.07303, 2025. URL https:](https://arxiv.org/abs/2504.07303v1)
[//arxiv.org/abs/2504.07303v1.](https://arxiv.org/abs/2504.07303v1)


[401] Arshia Hemmat, Kianoosh Vadaei, Mohammad Hassan Heydari, and Afsaneh Fatemi. Leveraging
retrieval-augmented generation for persian university knowledge retrieval. _Conference on Information_
_and Knowledge Technology_, 2024.


[402] M. Herrera, Marco Pérez-Hernández, A. Kumar Parlikad, and J. Izquierdo. Multi-agent systems and
complex networks: Review and applications in systems engineering. _Processes_, 2020.


[403] Nora A. Herweg, A. Sharan, M. Sperling, A. Brandt, A. Schulze-Bonhage, and M. Kahana. Reactivated
spatial context guides episodic recall. _Journal of Neuroscience_, 2018.


[404] Thomas F. Heston and Charya Khun. Prompt engineering in medical education. _International Medical_
_Education_, 2023.


[405] Dollaya Hirunyasiri, Danielle R. Thomas, Jionghao Lin, K. Koedinger, and Vincent Aleven. Comparative analysis of gpt-4 and human graders in evaluating human tutors giving praise to students.
_Human-AI Math Tutoring@AIED_, 2023.


89


[406] Thomas Hoang. Gnn: Graph neural network and large language model for data discovery, arXiv
[preprint arXiv:2408.13609, 2024. URL https://arxiv.org/abs/2408.13609v2.](https://arxiv.org/abs/2408.13609v2)


[407] W. Hoek and M. Wooldridge. Towards a logic of rational agency. _Logic Journal of the IGPL_, 2003.


[408] Aidan Hogan, E. Blomqvist, Michael Cochez, C. d’Amato, Gerard de Melo, C. Gutierrez, J. E. L. Gayo,
S. Kirrane, S. Neumaier, A. Polleres, Roberto Navigli, A. Ngomo, S. M. Rashid, Anisa Rula, Lukas
Schmelzeisen, Juan Sequeda, Steffen Staab, and Antoine Zimmermann. Knowledge graphs. _ACM_
_Computing Surveys_, 2020.


[409] Nithin Holla, Pushkar Mishra, H. Yannakoudakis, and Ekaterina Shutova. Meta-learning with sparse
experience replay for lifelong language learning, arXiv preprint arXiv:2009.04891, 2020. URL
[https://arxiv.org/abs/2009.04891v2.](https://arxiv.org/abs/2009.04891v2)


[410] Chuanyang Hong and Qingyun He. Enhancing memory retrieval in generative agents through
llm-trained cross attention networks. _Frontiers in Psychology_, 2025.


[411] M. Hong, Sean M. Polyn, and Lisa K. Fazio. Examining the episodic context account: does retrieval
practice enhance memory for context? _Cognitive Research_, 2019.


[412] Sirui Hong, Xiawu Zheng, Jonathan P. Chen, Yuheng Cheng, Ceyao Zhang, Zili Wang, Steven
Ka Shing Yau, Z. Lin, Liyang Zhou, Chenyu Ran, Lingfeng Xiao, and Chenglin Wu. Metagpt: Meta
programming for multi-agent collaborative framework. arXiv preprint, 2023.


[413] Sirui Hong, Mingchen Zhuge, Jiaqi Chen, Xiawu Zheng, Yuheng Cheng, Ceyao Zhang, Jinlin Wang,
Zili Wang, Steven Ka Shing Yau, Zijuan Lin, Liyang Zhou, Chenyu Ran, Lingfeng Xiao, Chenglin Wu,
and Jürgen Schmidhuber. Metagpt: Meta programming for a multi-agent collaborative framework,
[arXiv preprint arXiv:2308.00352, 2024. URL https://arxiv.org/abs/2308.00352.](https://arxiv.org/abs/2308.00352)


[414] Wenyi Hong, Weihan Wang, Qingsong Lv, Jiazheng Xu, Wenmeng Yu, Junhui Ji, Yan Wang, Zihan
Wang, Yuxiao Dong, Ming Ding, and Jie Tang. Cogagent: A visual language model for gui agents.
_Computer Vision and Pattern Recognition_, 2023.


[415] Xiangyu Hong, Che Jiang, Biqing Qi, Fandong Meng, Mo Yu, Bowen Zhou, and Jie Zhou. On the
token distance modeling ability of higher rope attention dimension. _Conference on Empirical Methods_
_in Natural Language Processing_, 2024.


[416] Yubin Hong, Chaofan Li, Jingyi Zhang, and Yingxia Shao. Fg-rag: Enhancing query-focused summarization with context-aware fine-grained graph rag. arXiv preprint, 2025.


[417] Thanapapas Horsuwan, Piyawat Lertvittayakumjorn, Kasidis Kanwatchara, B. Kijsirikul, and P. Vateekul. Meta lifelong-learning with selective and task-aware adaptation. _IEEE Access_, 2024.


[418] A. N. Hoskin, A. Bornstein, K. Norman, and J. Cohen. Refresh my memory: Episodic memory reinstatements intrude on working memory maintenance. _Cognitive, Affective, & Behavioral Neuroscience_,
2017.


[419] Timothy M. Hospedales, Antreas Antoniou, P. Micaelli, and A. Storkey. Meta-learning in neural
networks: A survey. _IEEE Transactions on Pattern Analysis and Machine Intelligence_, 2020.


[420] Peyman Hosseini, Ignacio Castro, Iacopo Ghinassi, and Matthew Purver. Efficient solutions for an
intriguing failure of llms: Long context window does not mean llms can analyze long sequences
flawlessly. _International Conference on Computational Linguistics_, 2024.


90


[421] Haowen Hou, Fei Ma, Binwen Bai, Xinxin Zhu, and F. Yu. Enhancing and accelerating large language
models via instruction-aware contextual compression, arXiv preprint arXiv:2408.15491, 2024. URL
[https://arxiv.org/abs/2408.15491v1.](https://arxiv.org/abs/2408.15491v1)


[422] Wenjun Hou, Yi Cheng, Kaishuai Xu, Yan Hu, Wenjie Li, and Jiangming Liu. Memory-augmented
multimodal llms for surgical vqa via self-contained inquiry, arXiv preprint arXiv:2411.10937v1, 2024.
[URL https://arxiv.org/abs/2411.10937v1.](https://arxiv.org/abs/2411.10937v1)


[423] Xinyi Hou, Yanjie Zhao, Shenao Wang, and Haoyu Wang. Model context protocol (mcp): Landscape,
security threats, and future research directions, arXiv preprint arXiv:2503.23278, 2025. URL
[https://arxiv.org/abs/2503.23278v2.](https://arxiv.org/abs/2503.23278v2)


[424] Zejiang Hou, Julian Salazar, and George Polovets. Meta-learning the difference: Preparing large
language models for efficient adaptation. _Transactions of the Association for Computational Linguistics_,
2022.


[425] N. Houlsby, A. Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin de Laroussilhe, Andrea
Gesmundo, Mona Attariyan, and S. Gelly. Parameter-efficient transfer learning for nlp. _International_
_Conference on Machine Learning_, 2019.


[426] Marc W Howard and M. Kahana. A distributed representation of temporal context. arXiv preprint,
2002.


[427] Chenxu Hu, Jie Fu, Chenzhuang Du, Simian Luo, J. Zhao, and Hang Zhao. Chatdb: Augmenting
[llms with databases as their symbolic memory, arXiv preprint arXiv:2306.03901, 2023. URL https:](https://arxiv.org/abs/2306.03901v2)
[//arxiv.org/abs/2306.03901v2.](https://arxiv.org/abs/2306.03901v2)


[428] J. E. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, and Weizhu
Chen. Lora: Low-rank adaptation of large language models. _International Conference on Learning_
_Representations_, 2021.


[429] Junhao Hu, Wenrui Huang, Weidong Wang, Zhenwen Li, Tiancheng Hu, Zhixia Liu, XuSheng Chen,
Tao Xie, and Yizhou Shan. Raas: Reasoning-aware attention sparsity for efficient llm reasoning,
[arXiv preprint arXiv:2502.11147, 2025. URL https://arxiv.org/abs/2502.11147v2.](https://arxiv.org/abs/2502.11147v2)


[430] Junwei Hu, Weicheng Zheng, Yan Liu, and Yihan Liu. Optimizing token consumption in llms: A
nano surge approach for code reasoning efficiency, arXiv preprint arXiv:2504.15989, 2025. URL
[https://arxiv.org/abs/2504.15989v2.](https://arxiv.org/abs/2504.15989v2)


[431] Junyan Hu, Hanlin Niu, J. Carrasco, B. Lennox, and F. Arvin. Voronoi-based multi-robot autonomous
exploration in unknown environments via deep reinforcement learning. _IEEE Transactions on_
_Vehicular Technology_, 2020.


[432] Linmei Hu, Zeyi Liu, Ziwang Zhao, Lei Hou, Liqiang Nie, and Juanzi Li. A survey of knowledge
enhanced pre-trained language models. _IEEE Transactions on Knowledge and Data Engineering_, 2022.


[433] Mengkang Hu, Tianxing Chen, Qiguang Chen, Yao Mu, Wenqi Shao, and Ping Luo. Hiagent:
Hierarchical working memory management for solving long-horizon agent tasks with large language
[model, arXiv preprint arXiv:2408.09559, 2024. URL https://arxiv.org/abs/2408.09559.](https://arxiv.org/abs/2408.09559)


91


[434] Mengkang Hu, Yao Mu, Xinmiao Yu, Mingyu Ding, Shiguang Wu, Wenqi Shao, Qiguang Chen, Bin
Wang, Yu Qiao, and Ping Luo. Tree-planner: Efficient close-loop task planning with large language
[models, arXiv preprint arXiv:2310.08582, 2024. URL https://arxiv.org/abs/2310.08582.](https://arxiv.org/abs/2310.08582)


[435] Mengkang Hu, Yuhang Zhou, Wendong Fan, Yuzhou Nie, Bowei Xia, Tao Sun, Ziyu Ye, Zhaoxuan
Jin, Yingru Li, Qiguang Chen, Zeyu Zhang, Yifeng Wang, Qianshuo Ye, Bernard Ghanem, Ping Luo,
and Guohao Li. Owl: Optimized workforce learning for general multi-agent assistance in real-world
[task automation, arXiv preprint arXiv:2505.23885, 2025. URL https://arxiv.org/abs/2505.](https://arxiv.org/abs/2505.23885)
[23885.](https://arxiv.org/abs/2505.23885)


[436] Nathan J. Hu, E. Mitchell, Christopher D. Manning, and Chelsea Finn. Meta-learning online
adaptation of language models. _Conference on Empirical Methods in Natural Language Processing_,
2023.


[437] Shengxiang Hu, Guobing Zou, Song Yang, Yanglan Gan, Bofeng Zhang, and Yixin Chen. Large
language model meets graph neural network in knowledge distillation. _AAAI Conference on Artificial_
_Intelligence_, 2024.


[438] Siyuan Hu, Mingyu Ouyang, Difei Gao, and Mike Zheng Shou. The dawn of gui agent: A preliminary
case study with claude 3.5 computer use. arXiv preprint, 2024.


[439] Ting Hu, Christoph Meinel, and Haojin Yang. Scaled prompt-tuning for few-shot natural lan[guage generation, arXiv preprint arXiv:2309.06759, 2023. URL https://arxiv.org/abs/](https://arxiv.org/abs/2309.06759v1)
[2309.06759v1.](https://arxiv.org/abs/2309.06759v1)


[440] Xiang Hu, Hongyu Fu, Jinge Wang, Yifeng Wang, Zhikun Li, Renjun Xu, Yu Lu, Yaochu Jin, Lili Pan,
and Zhenzhong Lan. Nova: An iterative planning and search approach to enhance novelty and
[diversity of llm generated ideas, arXiv preprint arXiv:2410.14255, 2024. URL https://arxiv.](https://arxiv.org/abs/2410.14255v2)
[org/abs/2410.14255v2.](https://arxiv.org/abs/2410.14255v2)


[441] Yuntong Hu, Zhengwu Zhang, and Liang Zhao. Beyond text: A deep dive into large language
[models’ ability on understanding graph data, arXiv preprint arXiv:2310.04944, 2023. URL https:](https://arxiv.org/abs/2310.04944v1)
[//arxiv.org/abs/2310.04944v1.](https://arxiv.org/abs/2310.04944v1)


[442] Yilun Hua and Yoav Artzi. Talk less, interact better: Evaluating in-context conversational adaptation
[in multimodal llms, arXiv preprint arXiv:2408.01417v1, 2024. URL https://arxiv.org/abs/](https://arxiv.org/abs/2408.01417v1)
[2408.01417v1.](https://arxiv.org/abs/2408.01417v1)


[443] Brandon Huang, Chancharik Mitra, Assaf Arbelle, Leonid Karlinsky, Trevor Darrell, and Roei Herzig.
Multimodal task vectors enable many-shot multimodal in-context learning. _Neural Information_
_Processing Systems_, 2024.


[444] Chengkai Huang, Hongtao Huang, Tong Yu, Kaige Xie, Junda Wu, Shuai Zhang, Julian J. McAuley,
Dietmar Jannach, and Lina Yao. A survey of foundation model-powered recommender systems:
From feature-based, generative to agentic paradigms, arXiv preprint arXiv:2504.16420, 2025. URL
[https://arxiv.org/abs/2504.16420v1.](https://arxiv.org/abs/2504.16420v1)


[445] Chengkai Huang, Junda Wu, Yu Xia, Zixu Yu, Ruhan Wang, Tong Yu, Ruiyi Zhang, Ryan A. Rossi,
B. Kveton, Dongruo Zhou, Julian J. McAuley, and Lina Yao. Towards agentic recommender systems
in the era of multimodal large language models, arXiv preprint arXiv:2503.16734, 2025. URL
[https://arxiv.org/abs/2503.16734v1.](https://arxiv.org/abs/2503.16734v1)


92


[446] Chengrui Huang, Shen Gao, Zhengliang Shi, Dongsheng Wang, and Shuo Shang. Ttpa: Tokenlevel tool-use preference alignment training framework with fine-grained evaluation, arXiv preprint
[arXiv:2505.20016, 2025. URL https://arxiv.org/abs/2505.20016v1.](https://arxiv.org/abs/2505.20016v1)


[447] Chensen Huang, Guibo Zhu, Xuepeng Wang, Yifei Luo, Guojing Ge, Haoran Chen, Dong Yi, and
Jinqiao Wang. Recurrent context compression: Efficiently expanding the context window of llm,
[arXiv preprint arXiv:2406.06110, 2024. URL https://arxiv.org/abs/2406.06110v1.](https://arxiv.org/abs/2406.06110v1)


[448] Jing Huang, X. Ruan, Naigong Yu, Qingwu Fan, Jiaming Li, and Jianxian Cai. A cognitive model
based on neuromodulated plasticity. _Computational Intelligence and Neuroscience_, 2016.


[449] Ken Huang, Akram Sheriff, Vineeth Sai Narajala, and Idan Habler. Agent capability negotiation and
[binding protocol (acnbp), arXiv preprint arXiv:2506.13590, 2025. URL https://arxiv.org/](https://arxiv.org/abs/2506.13590v1)
[abs/2506.13590v1.](https://arxiv.org/abs/2506.13590v1)


[450] Le Huang, Hengzhi Lan, Zijun Sun, Chuan Shi, and Ting Bai. Emotional rag: Enhancing role-playing
agents through emotional retrieval. _2024 IEEE International Conference on Knowledge Graph (ICKG)_,
2024.


[451] Lisheng Huang, Yichen Liu, Jinhao Jiang, Rongxiang Zhang, Jiahao Yan, Junyi Li, and Wayne Xin
Zhao. Manusearch: Democratizing deep search in large language models with a transparent and
[open multi-agent framework, arXiv preprint arXiv:2505.18105, 2025. URL https://arxiv.org/](https://arxiv.org/abs/2505.18105v1)
[abs/2505.18105v1.](https://arxiv.org/abs/2505.18105v1)


[452] Shiting Huang, Zhen Fang, Zehui Chen, Siyu Yuan, Junjie Ye, Yu Zeng, Lin Chen, Qi Mao, and
Feng Zhao. Critictool: Evaluating self-critique capabilities of large language models in tool-calling
[error scenarios, arXiv preprint arXiv:2506.13977, 2025. URL https://arxiv.org/abs/2506.](https://arxiv.org/abs/2506.13977v1)
[13977v1.](https://arxiv.org/abs/2506.13977v1)


[453] Sirui Huang, Yanggan Gu, Xuming Hu, Zhonghao Li, Qing Li, and Guandong Xu. Reasoning factual
knowledge in structured data with large language models, arXiv preprint arXiv:2408.12188, 2024.
[URL https://arxiv.org/abs/2408.12188v1.](https://arxiv.org/abs/2408.12188v1)


[454] Sirui Huang, Hanqian Li, Yanggan Gu, Xuming Hu, Qing Li, and Guandong Xu. Hyperg: Hypergraph[enhanced llms for structured knowledge, arXiv preprint arXiv:2502.18125, 2025. URL https:](https://arxiv.org/abs/2502.18125v1)
[//arxiv.org/abs/2502.18125v1.](https://arxiv.org/abs/2502.18125v1)


[455] Wenlong Huang, P. Abbeel, Deepak Pathak, and Igor Mordatch. Language models as zero-shot
planners: Extracting actionable knowledge for embodied agents. _International Conference on Machine_
_Learning_, 2022.


[456] Xu Huang, Jianxun Lian, Yuxuan Lei, Jing Yao, Defu Lian, and Xing Xie. Recommender ai agent:
Integrating large language models for interactive recommendations, arXiv preprint arXiv:2308.16505,
[2024. URL https://arxiv.org/abs/2308.16505.](https://arxiv.org/abs/2308.16505)


[457] Xu Huang, Weiwen Liu, Xiaolong Chen, Xingmei Wang, Hao Wang, Defu Lian, Yasheng Wang,
Ruiming Tang, and Enhong Chen. Understanding the planning of llm agents: A survey, arXiv preprint
[arXiv:2402.02716, 2024. URL https://arxiv.org/abs/2402.02716v1.](https://arxiv.org/abs/2402.02716v1)


[458] Y Huang, J Shi, Y Li, C Fan, S Wu, and Q Zhang.... Metatool benchmark for large language models:
[Deciding whether to use tools and which to use. 2023. URL https://arxiv.org/abs/2310.](https://arxiv.org/abs/2310.03128)
[03128.](https://arxiv.org/abs/2310.03128)


93


[459] Yunpeng Huang, Jingwei Xu, Zixu Jiang, Junyu Lai, Zenan Li, Yuan Yao, Taolue Chen, Lijuan
Yang, Zhou Xin, and Xiaoxing Ma. Advancing transformer architecture in long-context large
[language models: A comprehensive survey, arXiv preprint arXiv:2311.12351, 2023. URL https:](https://arxiv.org/abs/2311.12351v2)
[//arxiv.org/abs/2311.12351v2.](https://arxiv.org/abs/2311.12351v2)


[460] Zeyi Huang, Yuyang Ji, Anirudh Sundara Rajan, Zefan Cai, Wen Xiao, Junjie Hu, and Yong Jae Lee.
Visualtoolagent (vista): A reinforcement learning framework for visual tool selection, arXiv preprint
[arXiv:2505.20289, 2025. URL https://arxiv.org/abs/2505.20289v1.](https://arxiv.org/abs/2505.20289v1)


[461] Ziheng Huang, S. Gutierrez, Hemanth Kamana, and S. Macneil. Memory sandbox: Transparent
and interactive memory management for conversational agents. _ACM Symposium on User Interface_
_Software and Technology_, 2023.


[462] Ziheng Huang, Sebastian Gutierrez, Hemanth Kamana, and Stephen MacNeil. Memory sandbox: Transparent and interactive memory management for conversational agents, arXiv preprint
[arXiv:2308.01542, 2023. URL https://arxiv.org/abs/2308.01542.](https://arxiv.org/abs/2308.01542)


[463] Alexis Huet, Zied Ben-Houidi, and Dario Rossi. Episodic memories generation and evaluation
benchmark for large language models. _International Conference on Learning Representations_, 2025.


[464] Dom Huh and Prasant Mohapatra. Multi-agent reinforcement learning: A comprehensive survey,
[arXiv preprint arXiv:2312.10256, 2023. URL https://arxiv.org/abs/2312.10256v2.](https://arxiv.org/abs/2312.10256v2)


[465] Eunjeong Hwang, Yichao Zhou, James Bradley Wendt, Beliz Gunel, Nguyen Vo, Jing Xie, and Sandeep
Tata. Enhancing incremental summarization with structured representations. _Conference on Empirical_
_Methods in Natural Language Processing_, 2024.


[466] Thorsten Händler. Balancing autonomy and alignment: A multi-dimensional taxonomy for autonomous llm-powered multi-agent architectures. arXiv preprint, 2023.


[467] Michael Iannelli, Sneha Kuchipudi, and Vera Dvorak. Sla management in reconfigurable multi-agent
rag: A systems approach to question answering, arXiv preprint arXiv:2412.06832, 2024. URL
[https://arxiv.org/abs/2412.06832v2.](https://arxiv.org/abs/2412.06832v2)


[[468] IBM. What is agent communication protocol (acp)? https://www.ibm.com/think/topics/](https://www.ibm.com/think/topics/agent-communication-protocol)
[agent-communication-protocol, 2025. [Online; accessed 17-July-2025].](https://www.ibm.com/think/topics/agent-communication-protocol)


[469] T Inaba, H Kiyomaru, F Cheng, and S Kurohashi. Multitool-cot: Gpt-3 can use multiple external
[tools with chain of thought prompting. 2023. URL https://arxiv.org/abs/2305.16896.](https://arxiv.org/abs/2305.16896)


[470] G. Indiveri and Shih-Chii Liu. Memory and information processing in neuromorphic systems.
_Proceedings of the IEEE_, 2015.


[471] V. Ioannidis, Xiang Song, Da Zheng, Houyu Zhang, Jun Ma, Yi Xu, Belinda Zeng, Trishul M. Chilimbi,
and G. Karypis. Efficient and effective training of language and graph neural network models, arXiv
[preprint arXiv:2206.10781, 2022. URL https://arxiv.org/abs/2206.10781v1.](https://arxiv.org/abs/2206.10781v1)


[472] Yoichi Ishibashi, Taro Yano, and M. Oyamada. Can large language models invent algorithms to improve themselves?: Algorithm discovery for recursive self-improvement through reinforcement learn[ing, arXiv preprint arXiv:2410.15639, 2024. URL https://arxiv.org/abs/2410.15639v5.](https://arxiv.org/abs/2410.15639v5)


94


[473] Shadi Iskander, Nachshon Cohen, Zohar S. Karnin, Ori Shapira, and Sofia Tolmach. Quality matters:
Evaluating synthetic data for tool-using llms. _Conference on Empirical Methods in Natural Language_
_Processing_, 2024.


[474] Z. Ismail and N. Sariff. A survey and analysis of cooperative multi-agent robot systems: Challenges
and directions. _Applications of Mobile Robots_, 2018.

