<!-- Source: 02-ContextSurvey-2507.13334.pdf | Chunk 20/26 -->



[856] Pranav Putta, Edmund Mills, Naman Garg, S. Motwani, Chelsea Finn, Divyansh Garg, and Rafael
Rafailov. Agent q: Advanced reasoning and learning for autonomous ai agents, arXiv preprint
[arXiv:2408.07199, 2024. URL https://arxiv.org/abs/2408.07199v1.](https://arxiv.org/abs/2408.07199v1)


[857] S. Qasim, Hassan Mahmood, and F. Shafait. Rethinking table recognition using graph neural networks.
_IEEE International Conference on Document Analysis and Recognition_, 2019.


[858] Peng Qi, Haejun Lee, OghenetegiriTGSido, and Christopher D. Manning. Answering open-domain
questions of varying reasoning steps from text. _Conference on Empirical Methods in Natural Language_
_Processing_, 2020.


[859] Yong Qi, Gabriel Kyebambo, Siyuan Xie, Wei Shen, Shenghui Wang, Bitao Xie, Bin He, Zhipeng
Wang, and Shuo Jiang. Safety control of service robots with llms and embodied knowledge graphs,
[arXiv preprint arXiv:2405.17846, 2024. URL https://arxiv.org/abs/2405.17846v1.](https://arxiv.org/abs/2405.17846v1)


[860] Zehan Qi, Xiao Liu, Iat Long Iong, Hanyu Lai, Xueqiao Sun, Xinyue Yang, Jiadai Sun, Yu Yang,
Shuntian Yao, Tianjie Zhang, Wei Xu, Jie Tang, and Yuxiao Dong. Webrl: Training llm web agents
via self-evolving online curriculum reinforcement learning, arXiv preprint arXiv:2411.02337, 2024.
[URL https://arxiv.org/abs/2411.02337v3.](https://arxiv.org/abs/2411.02337v3)


[861] Chen Qian, Wei Liu, Hongzhang Liu, Nuo Chen, Yufan Dang, Jiahao Li, Cheng Yang, Weize Chen,
Yusheng Su, Xin Cong, Juyuan Xu, Dahai Li, Zhiyuan Liu, and Maosong Sun. Chatdev: Communicative
[agents for software development, arXiv preprint arXiv:2307.07924, 2024. URL https://arxiv.](https://arxiv.org/abs/2307.07924)
[org/abs/2307.07924.](https://arxiv.org/abs/2307.07924)


[862] Cheng Qian, Chi Han, Y. Fung, Yujia Qin, Zhiyuan Liu, and Heng Ji. Creator: Tool creation for
disentangling abstract and concrete reasoning of large language models. _Conference on Empirical_
_Methods in Natural Language Processing_, 2023.


[863] Cheng Qian, Jiahao Li, Yufan Dang, Wei Liu, Yifei Wang, Zihao Xie, Weize Chen, Cheng Yang,
Yingli Zhang, Zhiyuan Liu, and Maosong Sun. Iterative experience refinement of software[developing agents, arXiv preprint arXiv:2405.04219, 2024. URL https://arxiv.org/abs/](https://arxiv.org/abs/2405.04219v1)
[2405.04219v1.](https://arxiv.org/abs/2405.04219v1)


[864] Cheng Qian, Emre Can Acikgoz, Qi He, Hongru Wang, Xiusi Chen, Dilek Hakkani-Tur, Gokhan Tur,
and Heng Ji. Toolrl: Reward is all tool learning needs, arXiv preprint arXiv:2504.13958, 2025. URL
[https://arxiv.org/abs/2504.13958v1.](https://arxiv.org/abs/2504.13958v1)


[865] Hongjin Qian, Zheng Liu, Peitian Zhang, Zhicheng Dou, and Defu Lian. Boosting long-context
management via query-guided activation refilling, arXiv preprint arXiv:2412.12486, 2024. URL
[https://arxiv.org/abs/2412.12486v3.](https://arxiv.org/abs/2412.12486v3)


123


[866] Hongjin Qian, Zheng Liu, Peitian Zhang, Kelong Mao, Defu Lian, Zhicheng Dou, and Tiejun Huang.
Memorag: Boosting long context processing with global memory-enhanced retrieval augmentation,
[arXiv preprint arXiv:2409.05591, 2025. URL https://arxiv.org/abs/2409.05591.](https://arxiv.org/abs/2409.05591)


[867] Kangan Qian, Sicong Jiang, Yang Zhong, Ziang Luo, Zilin Huang, Tianze Zhu, Kun Jiang, Mengmeng
Yang, Zheng Fu, Jinyu Miao, Yining Shi, He Zhe Lim, Li Liu, Tianbao Zhou, Hongyi Wang, Huang
Yu, Yifei Hu, Guang Li, Guangyao Chen, Hao Ye, Lijun Sun, and Diange Yang. Agentthink: A
unified framework for tool-augmented chain-of-thought reasoning in vision-language models for
[autonomous driving, arXiv preprint arXiv:2505.15298, 2025. URL https://arxiv.org/abs/](https://arxiv.org/abs/2505.15298v3)
[2505.15298v3.](https://arxiv.org/abs/2505.15298v3)


[868] Changze Qiao and Mingming Lu. Efficiently enhancing general agents with hierarchical-categorical
memory, arXiv preprint arXiv:2505.22006, 2025. [URL https://arxiv.org/abs/2505.](https://arxiv.org/abs/2505.22006v1)
[22006v1.](https://arxiv.org/abs/2505.22006v1)


[869] S Qiao, H Gui, C Lv, Q Jia, H Chen, and N Zhang. Making language models better tool learners with
[execution feedback. 2023. URL https://arxiv.org/abs/2305.13068.](https://arxiv.org/abs/2305.13068)


[870] Binjie Qin, Haohao Mao, Ruipeng Zhang, Y. Zhu, Song Ding, and Xu Chen. Working memory
inspired hierarchical video decomposition with transformative representations, arXiv preprint
[arXiv:2204.10105, 2022. URL https://arxiv.org/abs/2204.10105v3.](https://arxiv.org/abs/2204.10105v3)


[871] Bowen Qin, Binyuan Hui, Lihan Wang, Min Yang, Jinyang Li, Binhua Li, Ruiying Geng, Rongyu Cao,
Jian Sun, Luo Si, Fei Huang, and Yongbin Li. A survey on text-to-sql parsing: Concepts, methods,
[and future directions, arXiv preprint arXiv:2208.13629, 2022. URL https://arxiv.org/abs/](https://arxiv.org/abs/2208.13629v1)
[2208.13629v1.](https://arxiv.org/abs/2208.13629v1)


[872] Libo Qin, Qiguang Chen, Fuxuan Wei, Shijue Huang, and Wanxiang Che. Cross-lingual prompting:
Improving zero-shot chain-of-thought reasoning across languages, arXiv preprint arXiv:2310.14799,
[2023. URL https://arxiv.org/abs/2310.14799.](https://arxiv.org/abs/2310.14799)


[873] Libo Qin, Fuxuan Wei, Qiguang Chen, Jingxuan Zhou, Shijue Huang, Jiasheng Si, Wenpeng Lu,
and Wanxiang Che. Croprompt: Cross-task interactive prompting for zero-shot spoken language
[understanding, arXiv preprint arXiv:2406.10505, 2024. URL https://arxiv.org/abs/2406.](https://arxiv.org/abs/2406.10505)
[10505.](https://arxiv.org/abs/2406.10505)


[874] Yujia Qin, Shengding Hu, Yankai Lin, Weize Chen, Ning Ding, Ganqu Cui, Zheni Zeng, Yufei Huang,
Chaojun Xiao, Chi Han, Y. Fung, Yusheng Su, Huadong Wang, Cheng Qian, Runchu Tian, Kunlun
Zhu, Shi Liang, Xingyu Shen, Bokai Xu, Zhen Zhang, Yining Ye, Bo Li, Ziwei Tang, Jing Yi, Yu Zhu,
Zhenning Dai, Lan Yan, Xin Cong, Ya-Ting Lu, Weilin Zhao, Yuxiang Huang, Junxi Yan, Xu Han,
Xian Sun, Dahai Li, Jason Phang, Cheng Yang, Tongshuang Wu, Heng Ji, Zhiyuan Liu, and Maosong
Sun. Tool learning with foundation models. _ACM Computing Surveys_, 2023.


[875] Yujia Qin, Shi Liang, Yining Ye, Kunlun Zhu, Lan Yan, Ya-Ting Lu, Yankai Lin, Xin Cong, Xiangru Tang,
Bill Qian, Sihan Zhao, Runchu Tian, Ruobing Xie, Jie Zhou, Marc H. Gerstein, Dahai Li, Zhiyuan Liu,
and Maosong Sun. Toolllm: Facilitating large language models to master 16000+ real-world apis.
_International Conference on Learning Representations_, 2023.


[876] Zhen Qin and Yiran Zhong. Accelerating toeplitz neural network with constant-time inference
complexity. _Conference on Empirical Methods in Natural Language Processing_, 2023.


124


[877] Zhen Qin, Xiaodong Han, Weixuan Sun, Bowen He, Dong Li, Dongxu Li, Yuchao Dai, Lingpeng
Kong, and Yiran Zhong. Toeplitz neural network for sequence modeling. _International Conference on_
_Learning Representations_, 2023.


[878] Zhen Qin, Weigao Sun, Dong Li, Xuyang Shen, Weixuan Sun, and Yiran Zhong. Lightning attention-2:
A free lunch for handling unlimited sequence lengths in large language models. arXiv preprint, 2024.


[879] Jiahao Qiu, Xinzhe Juan, Yiming Wang, Ling Yang, Xuan Qi, Tongcheng Zhang, Jiacheng Guo, Yifu
Lu, Zixin Yao, Hongru Wang, Shilong Liu, Xun Jiang, Liu Leqi, and Mengdi Wang. Agentdistill:
Training-free agent distillation with generalizable mcp boxes, arXiv preprint arXiv:2506.14728,
[2025. URL https://arxiv.org/abs/2506.14728v1.](https://arxiv.org/abs/2506.14728v1)


[880] Jiahao Qiu, Fulian Xiao, Yiming Wang, Yuchen Mao, Yijia Chen, Xinzhe Juan, Siran Wang, Xuan
Qi, Tongcheng Zhang, Zixin Yao, Jiacheng Guo, Yifu Lu, Charles Argon, Jundi Cui, Daixin Chen,
Junran Zhou, Shuyao Zhou, Zhanpeng Zhou, Ling Yang, Shilong Liu, Hongru Wang, Kaixuan
Huang, Xun Jiang, Yuming Cao, Yue Chen, Yunfei Chen, Zhengyi Chen, Ruowei Dai, Mengqiu
Deng, Jiye Fu, Yu Gu, Zijie Guan, Zirui Huang, Xiaoyan Ji, Yumeng Jiang, Delong Kong, Haolong
Li, Jiaqi Li, Ruipeng Li, Tianze Li, Zhuo-Yang Li, Haixia Lian, Meng Lin, Xudong Liu, Jiayi Lu,
Jinghan Lu, Wanyu Luo, Ziyue Luo, Zihao Pu, Zhi Qiao, Rui-Fang Ren, Liang Wan, Ruixiang Wang,
Tianhui Wang, Yang Wang, Zeyu Wang, Zihua Wang, Yujia Wu, Zhaoyi Wu, Hao Xin, Weiao Xing,
Ruojun Xiong, Weijie Xu, Yao Shu, Xiao Yao, Xiaorui Yang, Yuchen Yang, Nan Yi, Jiadong Yu, Yang
Yu, Huiting Zeng, Danni Zhang, Yunjie Zhang, Zhaoyu Zhang, Zhiheng Zhang, Xiaofeng Zheng,
Peirong Zhou, Li-Ying Zhong, Xiaoyin Zong, Ying Zhao, Zhen Chen, Lin Ding, Xiaoyu Gao, Bingbing
Gong, Yichao Li, Yang Liao, Guang Ma, Tianyuan Ma, Xinrui Sun, Tianyi Wang, Han Xia, Ruobing
Xian, Gen Ye, Tengfei Yu, Wentao Zhang, Yuxi Wang, Xi Gao, and Mengdi Wang. On path to
multimodal historical reasoning: Histbench and histagent, arXiv preprint arXiv:2505.20246, 2025.
[URL https://arxiv.org/abs/2505.20246v3.](https://arxiv.org/abs/2505.20246v3)


[881] Ruidi Qiu, Grace Li Zhang, Rolf Drechsler, Ulf Schlichtmann, and Bing Li. Autobench: Automatic
testbench generation and evaluation using llms for hdl design. _Workshop on Machine Learning for_
_CAD_, 2024.


[882] Changle Qu, Sunhao Dai, Xiaochi Wei, Hengyi Cai, Shuaiqiang Wang, Dawei Yin, Jun Xu, and Jirong
Wen. Tool learning with large language models: A survey. _Frontiers Comput. Sci._, 2024.


[883] Xiaoye Qu, Yafu Li, Zhao yu Su, Weigao Sun, Jianhao Yan, Dongrui Liu, Ganqu Cui, Daizong
Liu, Shuxian Liang, Junxian He, Peng Li, Wei Wei, Jing Shao, Chaochao Lu, Yue Zhang, XianSheng Hua, Bowen Zhou, and Yu Cheng. A survey of efficient reasoning for large reasoning
models: Language, multimodality, and beyond, arXiv preprint arXiv:2503.21614, 2025. URL
[https://arxiv.org/abs/2503.21614v1.](https://arxiv.org/abs/2503.21614v1)


[884] Victor Quintanar-Zilinskas. A neuromimetic approach to the serial acquisition, long-term storage,
and selective utilization of overlapping memory engrams. _bioRxiv_, 2019.


[885] Stephan Raaijmakers, Roos Bakker, Anita Cremers, Roy de Kleijn, Tom Kouwenhoven, and Tessa
Verhoef. Memory-augmented generative adversarial transformers, arXiv preprint arXiv:2402.19218,
[2024. URL https://arxiv.org/abs/2402.19218.](https://arxiv.org/abs/2402.19218)


[886] Ella Rabinovich and Ateret Anaby-Tavor. On the robustness of agentic function calling. _Proceedings_
_of the 5th Workshop on Trustworthy NLP (TrustNLP 2025)_, 2025.


125


[887] Neil C. Rabinowitz, Frank Perbet, H. F. Song, Chiyuan Zhang, S. Eslami, and M. Botvinick. Machine
theory of mind. _International Conference on Machine Learning_, 2018.


[888] Zackary Rackauckas. Rag-fusion: a new take on retrieval-augmented generation. _International_
_Journal on Natural Language Computing_, 2024.


[889] Alec Radford, Jong Wook Kim, Chris Hallacy, A. Ramesh, Gabriel Goh, Sandhini Agarwal, Girish
Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and I. Sutskever. Learning
transferable visual models from natural language supervision. _International Conference on Machine_
_Learning_, 2021.


[890] Colin Raffel, Noam M. Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi
Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a unified text-to-text
transformer. _Journal of machine learning research_, 2019.


[891] Mohaimenul Azam Khan Raiaan, Md. Saddam Hossain Mukta, Kaniz Fatema, Nur Mohammad
Fahad, S. Sakib, Most. Marufatul Jannat Mim, Jubaer Ahmad, Mohammed Eunus Ali, and Sami
Azam. A review on large language models: Architectures, applications, taxonomies, open issues and
challenges. _IEEE Access_, 2024.


[892] Keshav Ramji, Young-Suk Lee, R. Astudillo, M. Sultan, Tahira Naseem, Asim Munawar, Radu Florian,
and S. Roukos. Self-refinement of language models from external proxy metrics feedback, arXiv
[preprint arXiv:2403.00827, 2024. URL https://arxiv.org/abs/2403.00827v1.](https://arxiv.org/abs/2403.00827v1)


[893] Sumedh Rasal. An artificial neuron for enhanced problem solving in large language models, arXiv
[preprint arXiv:2404.14222, 2024. URL https://arxiv.org/abs/2404.14222v1.](https://arxiv.org/abs/2404.14222v1)


[894] Shaina Raza, Ranjan Sapkota, Manoj Karkee, and Christos Emmanouilidis. Trism for agentic ai:
A review of trust, risk, and security management in llm-based agentic multi-agent systems, arXiv
[preprint arXiv:2506.04133, 2025. URL https://arxiv.org/abs/2506.04133v2.](https://arxiv.org/abs/2506.04133v2)


[895] Jing Ren and Feng Xia. Brain-inspired artificial intelligence: A comprehensive review, arXiv preprint
[arXiv:2408.14811, 2024. URL https://arxiv.org/abs/2408.14811v1.](https://arxiv.org/abs/2408.14811v1)


[896] Shuo Ren, Pu Jian, Zhenjiang Ren, Chunlin Leng, Can Xie, and Jiajun Zhang. Towards scientific
intelligence: A survey of llm-based scientific agents, arXiv preprint arXiv:2503.24047, 2025. URL
[https://arxiv.org/abs/2503.24047v2.](https://arxiv.org/abs/2503.24047v2)


[897] Xubin Ren, Jiabin Tang, Dawei Yin, Nitesh V. Chawla, and Chao Huang. A survey of large language
models for graphs. _Knowledge Discovery and Data Mining_, 2024.


[898] Yi Ren, Shangmin Guo, Linlu Qiu, Bailin Wang, and Danica J. Sutherland. Bias amplification in
language model evolution: An iterated learning perspective. _Neural Information Processing Systems_,
2024.


[899] Alireza Rezazadeh, Zichao Li, Wei Wei, and Yujia Bao. From isolated conversations to hierarchical
schemas: Dynamic tree memory representation for llms. _International Conference on Learning_
_Representations_, 2024.


[900] Marco Tulio Ribeiro, Carlos Guestrin, and Sameer Singh. Are red roses red? evaluating consistency
of question-answering models. _Annual Meeting of the Association for Computational Linguistics_, 2019.


126


[901] Yara Rizk, Abhishek Bhandwalder, S. Boag, Tathagata Chakraborti, Vatche Isahagian, Y. Khazaeni,
Falk Pollock, and Merve Unuvar. A unified conversational assistant framework for business process
[automation, arXiv preprint arXiv:2001.03543, 2020. URL https://arxiv.org/abs/2001.](https://arxiv.org/abs/2001.03543v1)
[03543v1.](https://arxiv.org/abs/2001.03543v1)


[902] Yara Rizk, Vatche Isahagian, S. Boag, Y. Khazaeni, Merve Unuvar, Vinod Muthusamy, and Rania Y.
Khalaf. A conversational digital assistant for intelligent process automation. _International Conference_
_on Business Process Management_, 2020.


[903] S. Rizvi, Nazreen Pallikkavaliyaveetil, David Zhang, Zhuoyang Lyu, Nhi Nguyen, Haoran Lyu,
B. Christensen, J. O. Caro, Antonio H. O. Fonseca, E. Zappala, Maryam Bagherian, Christopher
Averill, C. Abdallah, Amin Karbasi, Rex Ying, M. Brbic, R. M. Dhodapkar, and David van Dijk.
Fimp: Foundation model-informed message passing for graph neural networks, arXiv preprint
[arXiv:2210.09475v5, 2022. URL https://arxiv.org/abs/2210.09475v5.](https://arxiv.org/abs/2210.09475v5)


[904] Joshua Robinson, Christopher Rytting, and D. Wingate. Leveraging large language models for
multiple choice question answering. _International Conference on Learning Representations_, 2022.


[905] Juri Di Rocco, D. D. Ruscio, Claudio Di Sipio, P. T. Nguyen, and Riccardo Rubei. On the use of large
language models in model-driven engineering. _Journal of Software and Systems Modeling_, 2024.


[906] Juan David Salazar Rodriguez, Sam Conrad Joyce, and Julfendi Julfendi. Using customized gpt to
develop prompting proficiency in architectural ai-generated images, arXiv preprint arXiv:2504.13948,
[2025. URL https://arxiv.org/abs/2504.13948v2.](https://arxiv.org/abs/2504.13948v2)


[907] Albert Roethel, M. Ganzha, and Anna Wr’oblewska. Enriching language models with graph-based
context information to better understand textual data. _Electronics_, 2023.


[908] Reudismam Rolim, Gustavo Soares, Rohit Gheyi, and Loris D’antoni. Learning quick fixes from code
repositories. _Brazilian Symposium on Software Engineering_, 2018.


[909] Robin Rombach, A. Blattmann, Dominik Lorenz, Patrick Esser, and B. Ommer. High-resolution image
synthesis with latent diffusion models. _Computer Vision and Pattern Recognition_, 2021.


[910] Hayley Ross, A. Mahabaleshwarkar, and Yoshi Suhara. When2call: When (not) to call tools. _North_
_American Chapter of the Association for Computational Linguistics_, 2025.


[911] J. Rosser and Jakob N. Foerster. Agentbreeder: Mitigating the ai safety impact of multi-agent scaffolds,
[arXiv preprint arXiv:2502.00757, 2025. URL https://arxiv.org/abs/2502.00757v3.](https://arxiv.org/abs/2502.00757v3)


[912] Federico Rossi, Saptarshi Bandyopadhyay, Michael T. Wolf, and M. Pavone. Review of multi-agent
algorithms for collective behavior: a structural taxonomy, arXiv preprint arXiv:1803.05464, 2018.
[URL https://arxiv.org/abs/1803.05464v1.](https://arxiv.org/abs/1803.05464v1)


[913] Federico Rossi, Saptarshi Bandyopadhyay, Michael T. Wolf, and M. Pavone. Multi-agent algorithms
for collective behavior: A structural and application-focused atlas, arXiv preprint arXiv:2103.11067,
[2021. URL https://arxiv.org/abs/2103.11067v1.](https://arxiv.org/abs/2103.11067v1)


[914] Alex Roxin and Stefano Fusi. Efficient partitioning of memory systems and its importance for memory
consolidation. _PLoS Comput. Biol._, 2013.


127


[915] Kaushik Roy, Yuxin Zi, Vignesh Narayanan, Manas Gaur, and Amit P. Sheth. Knowledge-infused self
[attention transformers, arXiv preprint arXiv:2306.13501, 2023. URL https://arxiv.org/abs/](https://arxiv.org/abs/2306.13501v1)
[2306.13501v1.](https://arxiv.org/abs/2306.13501v1)


[916] Dongyu Ru, Lin Qiu, Xiangkun Hu, Tianhang Zhang, Peng Shi, Shuaichen Chang, Jiayang Cheng,
Cunxiang Wang, Shichao Sun, Huanyu Li, Zizhao Zhang, Binjie Wang, Jiarong Jiang, Tong He,
Zhiguo Wang, Pengfei Liu, Yue Zhang, and Zheng Zhang. Ragchecker: A fine-grained framework for
diagnosing retrieval-augmented generation. _Neural Information Processing Systems_, 2024.


[917] Jingqing Ruan, Yihong Chen, Bin Zhang, Zhiwei Xu, Tianpeng Bao, Guoqing Du, Shiwei Shi, Hangyu
Mao, Ziyue Li, Xingyu Zeng, and Rui Zhao. Tptu: Large language model-based ai agents for task
[planning and tool usage, arXiv preprint arXiv:2308.03427, 2023. URL https://arxiv.org/](https://arxiv.org/abs/2308.03427)
[abs/2308.03427.](https://arxiv.org/abs/2308.03427)


[918] M. Russak, Umar Jamil, Christopher Bryant, Kiran Kamble, Axel Magnuson, Mateusz Russak, and
Waseem Alshikh. Writing in the margins: Better inference pattern for long context retrieval, arXiv
[preprint arXiv:2408.14906, 2024. URL https://arxiv.org/abs/2408.14906v1.](https://arxiv.org/abs/2408.14906v1)


[919] Hyun Ryu and Eric Kim. Closer look at efficient inference methods: A survey of speculative decoding,
[arXiv preprint arXiv:2411.13157, 2024. URL https://arxiv.org/abs/2411.13157v2.](https://arxiv.org/abs/2411.13157v2)


[920] Iman Saberi and Fatemeh Fard. Context-augmented code generation using programming knowl[edge graphs, arXiv preprint arXiv:2410.18251, 2024. URL https://arxiv.org/abs/2410.](https://arxiv.org/abs/2410.18251v2)
[18251v2.](https://arxiv.org/abs/2410.18251v2)


[921] Abdulfattah Safa and Gözde Gül Sahin. A zero-shot open-vocabulary pipeline for dialogue understanding. _North American Chapter of the Association for Computational Linguistics_, 2024.


[922] Alireza Akhavan Safaei, Pegah Saboori, Reza Ramezani, and Mohammadali Nematbakhsh. Kglm-qa:
A novel approach for knowledge graph-enhanced large language models for question answering.
_Conference on Information and Knowledge Technology_, 2024.


[923] Avirup Saha, Lakshmi Mandal, Balaji Ganesan, Sambit Ghosh, Renuka Sindhgatta, Carlos Eberhardt,
Dan Debrunner, and Sameep Mehta. Sequential api function calling using graphql schema. _Conference_
_on Empirical Methods in Natural Language Processing_, 2024.


[924] Pranab Sahoo, Ayush Kumar Singh, Sriparna Saha, Vinija Jain, S. Mondal, and Aman Chadha. A
systematic survey of prompt engineering in large language models: Techniques and applications,
[arXiv preprint arXiv:2402.07927, 2024. URL https://arxiv.org/abs/2402.07927v2.](https://arxiv.org/abs/2402.07927v2)


[925] Rana Salama, Jason Cai, Michelle Yuan, Anna Currey, Monica Sunkara, Yi Zhang, and Yassine Benajiba. Meminsight: Autonomous memory augmentation for llm agents, arXiv preprint
[arXiv:2503.21760, 2025. URL https://arxiv.org/abs/2503.21760.](https://arxiv.org/abs/2503.21760)


[926] Jefferson Salan, Devyn E Smith, Erica S Shafer, and Rachel A Diana. Variation in encoding context
benefits item recognition. _Memory & Cognition_, 2024.


[927] Alaa Saleh, Sasu Tarkoma, Praveen Kumar Donta, Naser Hossein Motlagh, S. Dustdar, Susanna
Pirttikangas, and Lauri Lov’en. Usercentrix: An agentic memory-augmented ai framework for
[smart spaces, arXiv preprint arXiv:2505.00472, 2025. URL https://arxiv.org/abs/2505.](https://arxiv.org/abs/2505.00472v1)
[00472v1.](https://arxiv.org/abs/2505.00472v1)


128


[928] Leonard Salewski, Stephan Alaniz, Isabel Rio-Torto, Eric Schulz, and Zeynep Akata. In-context
impersonation reveals large language models’ strengths and biases. _Neural Information Processing_
_Systems_, 2023.


[929] A. Samsonovich. Toward a unified catalog of implemented cognitive architectures. _Biologically_
_Inspired Cognitive Architectures_, 2010.


[930] Narendra Reddy Sanikommu. Model context protocol: Enhancing llm performance for observability
and analytics. _European journal of computer science and information technology_, 2025.


[931] S. Santhanam. Context based text-generation using lstm networks, arXiv preprint arXiv:2005.00048,
[2020. URL https://arxiv.org/abs/2005.00048v1.](https://arxiv.org/abs/2005.00048v1)


[932] G. Santos, Rita Maria Silva Julia, and Marcelo Zanchetta do Nascimento. Diverse prompts: Illuminating the prompt space of large language models with map-elites. _IEEE Congress on Evolutionary_
_Computation_, 2025.


[933] Ranjan Sapkota, Konstantinos I. Roumeliotis, and Manoj Karkee. Ai agents vs. agentic ai: A
conceptual taxonomy, applications and challenges, arXiv preprint arXiv:2505.10468, 2025. URL
[https://arxiv.org/abs/2505.10468v4.](https://arxiv.org/abs/2505.10468v4)


[934] Anjana Sarkar and Soumyendu Sarkar. Survey of llm agent communication with mcp: A software
[design pattern centric review, arXiv preprint arXiv:2506.05364, 2025. URL https://arxiv.org/](https://arxiv.org/abs/2506.05364v1)
[abs/2506.05364v1.](https://arxiv.org/abs/2506.05364v1)


[935] Soumajyoti Sarkar and Leonard Lausen. Testing the limits of unified sequence to sequence llm
[pretraining on diverse table data tasks, arXiv preprint arXiv:2310.00789, 2023. URL https:](https://arxiv.org/abs/2310.00789v1)
[//arxiv.org/abs/2310.00789v1.](https://arxiv.org/abs/2310.00789v1)


[936] Parth Sarthi, Salman Abdullah, Aditi Tuli, Shubh Khanna, Anna Goldie, and Christopher D. Manning.
Raptor: Recursive abstractive processing for tree-organized retrieval. _International Conference on_
_Learning Representations_, 2024.


[937] Gabriele Sarti. Umberto-mtsa @ accompl-it: Improving complexity and acceptability prediction
with multi-task learning on self-supervised annotations (short paper). _International Workshop on_
_Evaluation of Natural Language and Speech Tools for Italian_, 2020.


[938] Apoorv Saxena, Adrian Kochsiek, and Rainer Gemulla. Sequence-to-sequence knowledge graph
completion and question answering. _Annual Meeting of the Association for Computational Linguistics_,
2022.


[939] Timo Schick, Jane Dwivedi-Yu, Roberto Dessì, R. Raileanu, M. Lomeli, Luke Zettlemoyer, Nicola
Cancedda, and Thomas Scialom. Toolformer: Language models can teach themselves to use tools.
_Neural Information Processing Systems_, 2023.


[940] Guido Schillaci, Uwe Schmidt, and Luis Miranda. Prediction error-driven memory consolidation for
continual learning: On the case of adaptive greenhouse models. _KI - Künstliche Intelligenz_, 35(1):
[71–80, 2021. ISSN 1610-1987. doi: 10.1007/s13218-020-00700-8. URL http://dx.doi.org/](http://dx.doi.org/10.1007/s13218-020-00700-8)
[10.1007/s13218-020-00700-8.](http://dx.doi.org/10.1007/s13218-020-00700-8)


129


[941] Florian Schneider, Narges Baba Ahmadi, Niloufar Baba Ahmadi, Iris Vogel, Martin Semmann, and
Christian Biemann. Collex - a multimodal agentic rag system enabling interactive exploration of
scientific collections. arXiv preprint, 2025.


[942] Sheila Schoepp, Masoud Jafaripour, Yingyue Cao, Tianpei Yang, Fatemeh Abdollahi, Shadan Golestan,
Zahin Sufiyan, Osmar R. Zaiane, and Matthew E. Taylor. The evolving landscape of llm- and vlmintegrated reinforcement learning. arXiv preprint, 2025.


[943] Lianlei Shan, Shixian Luo, Zezhou Zhu, Yu Yuan, and Yong Wu. Cognitive memory in large
[language models, arXiv preprint arXiv:2504.02441, 2025. URL https://arxiv.org/abs/2504.](https://arxiv.org/abs/2504.02441v2)
[02441v2.](https://arxiv.org/abs/2504.02441v2)


[944] Wenbo Shang and Xin Huang. A survey of large language models on generative graph analytics:
[Query, learning, and applications, arXiv preprint arXiv:2404.14809v2, 2024. URL https://arxiv.](https://arxiv.org/abs/2404.14809v2)
[org/abs/2404.14809v2.](https://arxiv.org/abs/2404.14809v2)


[945] Yunfan Shao, Linyang Li, Junqi Dai, and Xipeng Qiu. Character-llm: A trainable agent for role-playing,
[arXiv preprint arXiv:2310.10158, 2023. URL https://arxiv.org/abs/2310.10158.](https://arxiv.org/abs/2310.10158)


[946] Yutong Shao and N. Nakashole. On linearizing structured data in encoder-decoder language models:
Insights from text-to-sql. _North American Chapter of the Association for Computational Linguistics_,
2024.


[947] Zhihong Shao, Yeyun Gong, Yelong Shen, Minlie Huang, Nan Duan, and Weizhu Chen. Synthetic
prompting: Generating chain-of-thought demonstrations for large language models. _International_
_Conference on Machine Learning_, 2023.


[948] Peter Shaw, Mandar Joshi, James Cohan, Jonathan Berant, Panupong Pasupat, Hexiang Hu, Urvashi
Khandelwal, Kenton Lee, and Kristina Toutanova. From pixels to ui actions: Learning to follow
instructions via graphical user interfaces. _Neural Information Processing Systems_, 2023.


[949] Jonathan Shen, Ruoming Pang, Ron J. Weiss, M. Schuster, N. Jaitly, Zongheng Yang, Z. Chen,
Yu Zhang, Yuxuan Wang, R. Skerry-Ryan, R. Saurous, Yannis Agiomyrgiannakis, and Yonghui Wu.
Natural tts synthesis by conditioning wavenet on mel spectrogram predictions. _IEEE International_
_Conference on Acoustics, Speech, and Signal Processing_, 2017.


[950] Junhong Shen, Atishay Jain, Zedian Xiao, Ishan Amlekar, Mouad Hadji, Aaron Podolny, and Ameet
Talwalkar. Scribeagent: Towards specialized web agents using production-scale workflow data.
2024.

