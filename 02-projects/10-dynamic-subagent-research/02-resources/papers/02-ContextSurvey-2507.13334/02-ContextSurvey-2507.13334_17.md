<!-- Source: 02-ContextSurvey-2507.13334.pdf | Chunk 17/26 -->


[618] M Li, Y Zhao, B Yu, F Song, H Li, H Yu, and Z Li.... Api-bank: A comprehensive benchmark for
[tool-augmented llms. 2023. URL https://arxiv.org/abs/2304.08244.](https://arxiv.org/abs/2304.08244)


[619] Michelle M. Li, Ben Y. Reis, Adam Rodman, Tianxi Cai, Noa Dagan, Ran D. Balicer, Joseph Loscalzo,
Isaac S. Kohane, and M. Zitnik. One patient, many contexts: Scaling medical ai through contextual
[intelligence, arXiv preprint arXiv:2506.10157, 2025. URL https://arxiv.org/abs/2506.](https://arxiv.org/abs/2506.10157v1)
[10157v1.](https://arxiv.org/abs/2506.10157v1)


[620] Ming Li, Keyu Chen, Ziqian Bi, Ming Liu, Benji Peng, Qian Niu, Junyu Liu, Jinlang Wang, Sen
Zhang, Xuanhe Pan, Jiawei Xu, and Pohsun Feng. Surveying the mllm landscape: A meta-review of
[current surveys, arXiv preprint arXiv:2409.18991, 2024. URL https://arxiv.org/abs/2409.](https://arxiv.org/abs/2409.18991v1)
[18991v1.](https://arxiv.org/abs/2409.18991v1)


[621] Minghao Li, Feifan Song, Yu Bowen, Haiyang Yu, Zhoujun Li, Fei Huang, and Yongbin Li. Api-bank:
A comprehensive benchmark for tool-augmented llms. _Conference on Empirical Methods in Natural_
_Language Processing_, 2023.


[622] Qiaomu Li and Ying Xie. From glue-code to protocols: A critical analysis of a2a and mcp integration
[for scalable agent systems, arXiv preprint arXiv:2505.03864, 2025. URL https://arxiv.org/](https://arxiv.org/abs/2505.03864v1)
[abs/2505.03864v1.](https://arxiv.org/abs/2505.03864v1)


[623] Rongsheng Li, Jin Xu, Zhixiong Cao, Hai-Tao Zheng, and Hong-Gee Kim. Extending context window
in large language models with segmented base adjustment for rotary position embeddings. _Applied_
_Sciences_, 2024.


[624] Shuaike Li, Kai Zhang, Qi Liu, and Enhong Chen. Mindbridge: Scalable and cross-model knowledge
[editing via memory-augmented modality, arXiv preprint arXiv:2503.02701v1, 2025. URL https:](https://arxiv.org/abs/2503.02701v1)
[//arxiv.org/abs/2503.02701v1.](https://arxiv.org/abs/2503.02701v1)


105


[625] Shuaiyi Li, Zhisong Zhang, Yang Deng, Chenlong Deng, Tianqing Fang, Hongming Zhang, Haitao
Mi, Dong Yu, and Wai Lam. Incomes: Integrating compression and selection mechanisms into llms
[for efficient model editing, arXiv preprint arXiv:2505.22156, 2025. URL https://arxiv.org/](https://arxiv.org/abs/2505.22156v1)
[abs/2505.22156v1.](https://arxiv.org/abs/2505.22156v1)


[626] Siheng Li, Cheng Yang, Zesen Cheng, Lemao Liu, Mo Yu, Yujiu Yang, and Wai Lam. Large language
models can self-improve in long-context reasoning, arXiv preprint arXiv:2411.08147, 2024. URL
[https://arxiv.org/abs/2411.08147v1.](https://arxiv.org/abs/2411.08147v1)


[[627] X Li, H Zou, and P Liu. Torl: Scaling tool-integrated rl. 2025. URL https://arxiv.org/abs/](https://arxiv.org/abs/2503.23383)
[2503.23383.](https://arxiv.org/abs/2503.23383)


[628] Xiaopeng Li, Pengyue Jia, Derong Xu, Yi Wen, Yingyi Zhang, Wenlin Zhang, Wanyu Wang, Yichao
Wang, Zhaochen Du, Xiangyang Li, Yong Liu, Huifeng Guo, Ruiming Tang, and Xiangyu Zhao.
A survey of personalization: From rag to agent, arXiv preprint arXiv:2504.10147, 2025. URL
[https://arxiv.org/abs/2504.10147v1.](https://arxiv.org/abs/2504.10147v1)


[629] Xiaoxi Li, Jiajie Jin, Guanting Dong, Hongjin Qian, Yutao Zhu, Yongkang Wu, Ji-Rong Wen, and
Zhicheng Dou. Webthinker: Empowering large reasoning models with deep research capability,
[arXiv preprint arXiv:2504.21776, 2025. URL https://arxiv.org/abs/2504.21776v1.](https://arxiv.org/abs/2504.21776v1)


[630] Xin Li, Qizhi Chu, Yubin Chen, Yang Liu, Yaoqi Liu, Zekai Yu, Weize Chen, Cheng Qian, Chuan Shi,
and Cheng Yang. Graphteam: Facilitating large language model-based graph analysis via multi-agent
[collaboration, arXiv preprint arXiv:2410.18032v4, 2024. URL https://arxiv.org/abs/2410.](https://arxiv.org/abs/2410.18032v4)
[18032v4.](https://arxiv.org/abs/2410.18032v4)


[631] Xinyi Li, Sai Wang, Siqi Zeng, Yu Wu, and Yi Yang. A survey on llm-based multi-agent systems:
workflow, infrastructure, and challenges. _Vicinagearth_, 2024.


[632] Xinze Li, Zhenghao Liu, Chenyan Xiong, Shi Yu, Yu Gu, Zhiyuan Liu, and Ge Yu. Structure-aware
language model pretraining improves dense retrieval on structured data. _Annual Meeting of the_
_Association for Computational Linguistics_, 2023.


[633] Yang Li, Jiacong He, Xiaoxia Zhou, Yuan Zhang, and Jason Baldridge. Mapping natural language
instructions to mobile ui action sequences. _Annual Meeting of the Association for Computational_
_Linguistics_, 2020.


[634] Yinghao Li, R. Ramprasad, and Chao Zhang. A simple but effective approach to improve structured
language model output for information extraction. _Conference on Empirical Methods in Natural_
_Language Processing_, 2024.


[635] Yuan Li, Yixuan Zhang, and Lichao Sun. Metaagents: Simulating interactions of human behaviors for llm-based task-oriented coordination via collaborative generative agents, arXiv preprint
[arXiv:2310.06500, 2023. URL https://arxiv.org/abs/2310.06500.](https://arxiv.org/abs/2310.06500)


[636] Yucheng Li. Unlocking context constraints of llms: Enhancing context efficiency of llms with
[self-information-based content filtering, arXiv preprint arXiv:2304.12102, 2023. URL https:](https://arxiv.org/abs/2304.12102v1)
[//arxiv.org/abs/2304.12102v1.](https://arxiv.org/abs/2304.12102v1)


[637] Yucheng Li, Bo Dong, Chenghua Lin, and Frank Guerin. Compressing context to enhance inference
efficiency of large language models. _Conference on Empirical Methods in Natural Language Processing_,
2023.


106


[638] Zhaoxin Li, Xiaoming Zhang, Haifeng Zhang, and Chengxiang Liu. Refining interactions: Enhancing
anisotropy in graph neural networks with language semantics, arXiv preprint arXiv:2504.01429,
[2025. URL https://arxiv.org/abs/2504.01429v1.](https://arxiv.org/abs/2504.01429v1)


[639] Zhecheng Li, Yiwei Wang, Bryan Hooi, Yujun Cai, Naifan Cheung, Nanyun Peng, and Kai-Wei Chang.
Think carefully and check again! meta-generation unlocking llms for low-resource cross-lingual
summarization. 2024.


[640] Zhecheng Li, Yiwei Wang, Bryan Hooi, Yujun Cai, Nanyun Peng, and Kai-Wei Chang. Drs: Deep
question reformulation with structured output. In _Association for Computational Linguistics ACL,_
_2025._, 2024.


[641] Zhecheng Li, Yiwei Wang, Bryan Hooi, Yujun Cai, Zhen Xiong, Nanyun Peng, and Kai-Wei Chang.
Vulnerability of llms to vertically aligned text manipulations. In _Association for Computational_
_Linguistics ACL, 2025._, 2024.


[642] Zhecheng Li, Guoxian Song, Yujun Cai, Zhen Xiong, Junsong Yuan, and Yiwei Wang. Texture or
semantics? vision-language models get lost in font recognition. In _Conference on Language Modeling_
_COLM, 2025._, 2025.


[643] Zhiyu Li, Shichao Song, Hanyu Wang, Simin Niu, Ding Chen, Jiawei Yang, Chenyang Xi, Huayi Lai,
Jihao Zhao, Yezhaohui Wang, Junpeng Ren, Zehao Lin, Jiahao Huo, Tianyi Chen, Kai Chen, Ke-Rong
Li, Zhiqiang Yin, Qingchen Yu, Bo Tang, Hongkang Yang, Zhiyang Xu, and Feiyu Xiong. Memos: An
operating system for memory-augmented generation (mag) in large language models, arXiv preprint
[arXiv:2505.22101, 2025. URL https://arxiv.org/abs/2505.22101v1.](https://arxiv.org/abs/2505.22101v1)


[644] Zhong-Zhi Li, Ming-Liang Zhang, Fei Yin, and Cheng-Lin Liu. Lans: A layout-aware neural solver for
plane geometry problem. 2023.


[645] Zhong-Zhi Li, Ming-Liang Zhang, Fei Yin, Zhi-Long Ji, Jin-Feng Bai, Zhen-Ru Pan, Fan-Hu Zeng,
Jian Xu, Jia-Xin Zhang, and Cheng-Lin Liu. Cmmath: A chinese multi-modal math skill evaluation
benchmark for foundation models. 2024.


[646] Zhong-Zhi Li, Duzhen Zhang, Ming-Liang Zhang, Jiaxin Zhang, Zengyan Liu, Yuxuan Yao, Haotian
Xu, Junhao Zheng, Pei-Jie Wang, Xiuyi Chen, et al. From system 1 to system 2: A survey of reasoning
large language models. 2025.


[647] Zhuoqun Li, Xuanang Chen, Haiyang Yu, Hongyu Lin, Yaojie Lu, Qiaoyu Tang, Fei Huang, Xianpei
Han, Le Sun, and Yongbin Li. Structrag: Boosting knowledge intensive reasoning of llms via inferencetime hybrid information structurization. _International Conference on Learning Representations_, 2024.


[648] Zinuo Li, Xian Zhang, Yongxin Guo, Mohammed Bennamoun, F. Boussaid, Girish Dwivedi, Luqi Gong,
and Qiuhong Ke. Watch and listen: Understanding audio-visual-speech moments with multimodal
[llm, arXiv preprint arXiv:2505.18110v2, 2025. URL https://arxiv.org/abs/2505.18110v2.](https://arxiv.org/abs/2505.18110v2)


[649] Zixuan Li, Jing Xiong, Fanghua Ye, Chuanyang Zheng, Xun Wu, Jianqiao Lu, Zhongwei Wan, Xiaodan
Liang, Chengming Li, Zhenan Sun, Lingpeng Kong, and Ngai Wong. Uncertaintyrag: Span-level
uncertainty enhanced long-context modeling for retrieval-augmented generation, arXiv preprint
[arXiv:2410.02719, 2024. URL https://arxiv.org/abs/2410.02719v1.](https://arxiv.org/abs/2410.02719v1)


[650] Zonglin Li, Ruiqi Guo, and Surinder Kumar. Decoupled context processing for context augmented
language modeling. _Neural Information Processing Systems_, 2022.


107


[651] Zongqian Li, Yinhong Liu, Yixuan Su, and Nigel Collier. Prompt compression for large language
models: A survey. _North American Chapter of the Association for Computational Linguistics_, 2024.


[652] Wen li Yu and Junfeng Zhao. Quantum multi-agent reinforcement learning as an emerging ai
technology: A survey and future directions. _International Conferences on Computing Advancements_,
2023.


[653] Guannan Liang and Qianqian Tong. Llm-powered ai agent systems and their applications in industry,
[arXiv preprint arXiv:2505.16120, 2025. URL https://arxiv.org/abs/2505.16120v1.](https://arxiv.org/abs/2505.16120v1)


[654] Jintao Liang, Gang Su, Huifeng Lin, You Wu, Rui Zhao, and Ziyue Li. Reasoning rag via system 1 or
system 2: A survey on reasoning agentic retrieval-augmented generation for industry challenges,
[arXiv preprint arXiv:2506.10408, 2025. URL https://arxiv.org/abs/2506.10408v1.](https://arxiv.org/abs/2506.10408v1)


[655] Xinnian Liang, Bing Wang, Huijia Huang, Shuangzhi Wu, Peihao Wu, Lu Lu, Zejun Ma, and Zhoujun
Li. Scm: Enhancing large language model with self-controlled memory framework, arXiv preprint
[arXiv:2304.13343, 2023. URL https://arxiv.org/abs/2304.13343v4.](https://arxiv.org/abs/2304.13343v4)


[656] Xuechen Liang, Meiling Tao, Yinghui Xia, Tianyu Shi, Jun Wang, and Jingsong Yang. Self-evolving
agents with reflective and memory-augmented abilities, arXiv preprint arXiv:2409.00872, 2024.
[URL https://arxiv.org/abs/2409.00872v2.](https://arxiv.org/abs/2409.00872v2)


[657] Xuechen Liang, Meiling Tao, Yinghui Xia, Jianhui Wang, Kun Li, Yijin Wang, Jingsong Yang, Tianyu
Shi, Yuantao Wang, Miao Zhang, and Xueqian Wang. Mars: Memory-enhanced agents with reflective
[self-improvement, arXiv preprint arXiv:2503.19271, 2025. URL https://arxiv.org/abs/2503.](https://arxiv.org/abs/2503.19271v2)
[19271v2.](https://arxiv.org/abs/2503.19271v2)


[658] Yanbiao Liang, Huihong Shi, Haikuo Shao, and Zhongfeng Wang. Accllm: Accelerating long-context
llm inference via algorithm-hardware co-design, arXiv preprint arXiv:2505.03745, 2025. URL
[https://arxiv.org/abs/2505.03745v1.](https://arxiv.org/abs/2505.03745v1)


[659] Yaobo Liang, Chenfei Wu, Ting Song, Wenshan Wu, Yan Xia, Yu Liu, Yangyiwen Ou, Shuai Lu, Lei Ji,
Shaoguang Mao, Yun Wang, Linjun Shou, Ming Gong, and Nan Duan. Taskmatrix.ai: Completing
tasks by connecting foundation models with millions of apis. _Intelligent Computing_, 2023.


[660] Yiming Liang, Ge Zhang, Xingwei Qu, Tianyu Zheng, Jiawei Guo, Xinrun Du, Zhenzhu Yang, Jiaheng
Liu, Chenghua Lin, Lei Ma, Wenhao Huang, and Jiajun Zhang. I-sheep: Self-alignment of llm from
scratch through an iterative self-enhancement paradigm. arXiv preprint, 2024.


[661] Bingli Liao and Danilo Vasconcellos Vargas. Beyond kv caching: Shared attention for efficient llms.
_Neurocomputing_, 2024.


[662] Xiaoxuan Liao, Binrong Zhu, Jacky He, Guiran Liu, Hongye Zheng, and Jia Gao. A fine-tuning
approach for t5 using knowledge graphs to address complex tasks, arXiv preprint arXiv:2502.16484,
[2025. URL https://arxiv.org/abs/2502.16484v1.](https://arxiv.org/abs/2502.16484v1)


[663] David Lillis. Internalising interaction protocols as first-class programming elements in multi agent sys[tems, arXiv preprint arXiv:1711.02634, 2017. URL https://arxiv.org/abs/1711.02634v1.](https://arxiv.org/abs/1711.02634v1)


[664] Bill Yuchen Lin, Xinyue Chen, Jamin Chen, and Xiang Ren. Kagnet: Knowledge-aware graph networks
for commonsense reasoning. _Conference on Empirical Methods in Natural Language Processing_, 2019.


108


[665] Bill Yuchen Lin, Seyeon Lee, Xiaoyang Qiao, and Xiang Ren. Common sense beyond english:
Evaluating and improving multilingual language models for commonsense reasoning. _Annual_
_Meeting of the Association for Computational Linguistics_, 2021.


[666] Bin Lin, Chen Zhang, Tao Peng, Hanyu Zhao, Wencong Xiao, Minmin Sun, Anmin Liu, Zhipeng
Zhang, Lanbo Li, Xiafei Qiu, Shen Li, Zhigang Ji, Tao Xie, Yong Li, and Wei Lin. Infinite-llm:
Efficient llm service for long context with distattention and distributed kvcache, arXiv preprint
[arXiv:2401.02669, 2024. URL https://arxiv.org/abs/2401.02669.](https://arxiv.org/abs/2401.02669)


[667] Jianhao Lin, Lexuan Sun, and Yixin Yan. Simulating macroeconomic expectations using llm agents,
[arXiv preprint arXiv:2505.17648, 2025. URL https://arxiv.org/abs/2505.17648v2.](https://arxiv.org/abs/2505.17648v2)


[668] Lei Lin, Jiayi Fu, Pengli Liu, Qingyang Li, Yan Gong, Junchen Wan, Fuzheng Zhang, Zhongyuan
Wang, Di Zhang, and Kun Gai. Just ask one more time! self-agreement improves reasoning of
language models in (almost) all scenarios. _Annual Meeting of the Association for Computational_
_Linguistics_, 2023.


[669] Matthieu Lin, Jenny Sheng, Andrew Zhao, Shenzhi Wang, Yang Yue, Yiran Wu, Huan Liu, Jun Liu, Gao
Huang, and Yong-Jin Liu. Training of scaffolded language models with language supervision: A survey,
[arXiv preprint arXiv:2410.16392, 2024. URL https://arxiv.org/abs/2410.16392v2.](https://arxiv.org/abs/2410.16392v2)


[670] Qiqiang Lin, Muning Wen, Qiuying Peng, Guanyu Nie, Junwei Liao, Jun Wang, Xiaoyun Mo, Jiamu
Zhou, Cheng Cheng, Yin Zhao, and Weinan Zhang. Hammer: Robust function-calling for on[device language models via function masking, arXiv preprint arXiv:2410.04587, 2024. URL https:](https://arxiv.org/abs/2410.04587v2)
[//arxiv.org/abs/2410.04587v2.](https://arxiv.org/abs/2410.04587v2)


[671] Yu-Chen Lin, Akhilesh Kumar, Norman Chang, Wen-Liang Zhang, Muhammad Zakir, Rucha Apte,
Haiyang He, Chao Wang, and Jyh-Shing Roger Jang. Novel preprocessing technique for data
embedding in engineering code generation using large language model. _2024 IEEE LLM Aided Design_
_Workshop (LAD)_, 2023.


[672] Yu-Hsuan Lin, Qian-Hui Chen, Yi-Jie Cheng, Jia-Ren Zhang, Yi-Hung Liu, Liang-Yu Hsia, and
Yun-Nung Chen. Llm inference enhanced by external knowledge: A survey, arXiv preprint
[arXiv:2505.24377, 2025. URL https://arxiv.org/abs/2505.24377v1.](https://arxiv.org/abs/2505.24377v1)


[673] Jack W Lindsey and Ashok Litwin-Kumar. Selective consolidation of learning and memory via
recall-gated plasticity. _bioRxiv_, 2024.


[674] Tal Linzen, Emmanuel Dupoux, and Yoav Goldberg. Assessing the ability of lstms to learn syntaxsensitive dependencies. _Transactions of the Association for Computational Linguistics_, 2016.


[675] Gili Lior, Yuval Shalev, Gabriel Stanovsky, and Ariel Goldstein. Computation or weight adaptation?
rethinking the role of plasticity in learning. _bioRxiv_, 2024.


[676] Bingyang Liu, Haoyi Zhang, Xiaohan Gao, Zichen Kong, Xiyuan Tang, Yibo Lin, Runsheng Wang,
and Ru Huang. Layoutcopilot: An llm-powered multi-agent collaborative framework for interactive
analog layout design. _IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems_,
2024.


[677] E. Liu, Kelvin Guu, Panupong Pasupat, Tianlin Shi, and Percy Liang. Reinforcement learning on web
interfaces using workflow-guided exploration. _International Conference on Learning Representations_,
2018.


109


[678] Guang-Da Liu, Haitao Mao, Bochuan Cao, Zhiyu Xue, K. Johnson, Jiliang Tang, and Rongrong Wang.
On the intrinsic self-correction capability of llms: Uncertainty and latent concept, arXiv preprint
[arXiv:2406.02378, 2024. URL https://arxiv.org/abs/2406.02378v2.](https://arxiv.org/abs/2406.02378v2)


[679] Guangyi Liu, Yongqi Zhang, Yong Li, and Quanming Yao. Dual reasoning: A gnn-llm collaborative
framework for knowledge graph question answering, arXiv preprint arXiv:2406.01145, 2024. URL
[https://arxiv.org/abs/2406.01145v2.](https://arxiv.org/abs/2406.01145v2)


[680] Guangyi Liu, Pengxiang Zhao, Liang Liu, Yaxuan Guo, Han Xiao, Weifeng Lin, Yuxiang Chai, Yue Han,
Shuai Ren, Hao Wang, Xiaoyu Liang, Wenhao Wang, Tianze Wu, Linghao Li, Guanjing Xiong, Yong Liu,
and Hongsheng Li. Llm-powered gui agents in phone automation: Surveying progress and prospects,
[arXiv preprint arXiv:2504.19838, 2025. URL https://arxiv.org/abs/2504.19838v2.](https://arxiv.org/abs/2504.19838v2)


[681] Hanchao Liu, Rong-Zhi Li, Weimin Xiong, Ziyu Zhou, and Wei Peng. Workteam: Constructing
workflows from natural language with multi-agents. _North American Chapter of the Association for_
_Computational Linguistics_, 2025.


[682] Hao Liu, Matei Zaharia, and Pieter Abbeel. Ring attention with blockwise transformers for nearinfinite context. _International Conference on Learning Representations_, 2023.


[683] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. _Neural_
_Information Processing Systems_, 2023.


[684] Jie Liu, Pan Zhou, Yingjun Du, Ah-Hwee Tan, Cees G. M. Snoek, J. Sonke, and E. Gavves. Capo: Cooperative plan optimization for efficient embodied multi-agent cooperation. _International Conference_
_on Learning Representations_, 2024.


[685] Jun Liu, Ke Yu, Keliang Chen, Ke Li, Yuxinyue Qian, Xiaolian Guo, Haozhe Song, and Yinming Li.
Acps: Agent collaboration protocols for the internet of agents, arXiv preprint arXiv:2505.13523,
[2025. URL https://arxiv.org/abs/2505.13523v1.](https://arxiv.org/abs/2505.13523v1)


[686] Junwei Liu, Kaixin Wang, Yixuan Chen, Xin Peng, Zhenpeng Chen, Lingming Zhang, and Yiling Lou. Large language model-based agents for software engineering: A survey, arXiv preprint
[arXiv:2409.02977, 2024. URL https://arxiv.org/abs/2409.02977v1.](https://arxiv.org/abs/2409.02977v1)


[687] Kai Liu, Zhihang Fu, Chao Chen, Wei Zhang, Rongxin Jiang, Fan Zhou, Yao-Shen Chen, Yue Wu,
and Jieping Ye. Enhancing llm’s cognition via structurization. _Neural Information Processing Systems_,
2024.


[688] Lei Liu, Xiaoyan Yang, Yue Shen, Binbin Hu, Zhiqiang Zhang, Jinjie Gu, and Guannan Zhang.
Think-in-memory: Recalling and post-thinking enable llms with long-term memory. arXiv preprint,
2023.


[689] Lei Liu, Xiaoyan Yang, Yue Shen, Binbin Hu, Zhiqiang Zhang, Jinjie Gu, and Guannan Zhang.
Think-in-memory: Recalling and post-thinking enable llms with long-term memory, arXiv preprint
[arXiv:2311.08719, 2023. URL https://arxiv.org/abs/2311.08719.](https://arxiv.org/abs/2311.08719)


[690] Na Liu, Liangyu Chen, Xiaoyu Tian, Wei Zou, Kaijiang Chen, and Ming Cui. From llm to conversational
agent: A memory enhanced architecture with fine-tuning of large language models, arXiv preprint
[arXiv:2401.02777, 2024. URL https://arxiv.org/abs/2401.02777v2.](https://arxiv.org/abs/2401.02777v2)


110


[691] Nelson F. Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, F. Petroni, and Percy
Liang. Lost in the middle: How language models use long contexts. _Transactions of the Association_
_for Computational Linguistics_, 2023.


[692] Pei Liu, Xin Liu, Ruoyu Yao, Junming Liu, Siyuan Meng, Ding Wang, and Jun Ma. Hm-rag: Hierarchical multi-agent multimodal retrieval augmented generation. arXiv preprint, 2025.


[693] Shicheng Liu, Jialiang Xu, Wesley Tjangnaka, Sina J. Semnani, Chen Jie Yu, Gui D’avid, and Monica S.
Lam. Suql: Conversational search over structured and unstructured data with large language models.
_NAACL-HLT_, 2023.


[694] Shiyu Liu, Yucheng Han, Peng Xing, Fukun Yin, Rui Wang, Wei Cheng, Jiaqi Liao, Yingming Wang,
Honghao Fu, Chunrui Han, et al. Step1x-edit: A practical framework for general image editing.
2025.


[695] W Liu, X Huang, X Zeng, X Hao, S Yu, and D Li.... Toolace: Winning the points of llm function
[calling. 2024. URL https://arxiv.org/abs/2409.00920.](https://arxiv.org/abs/2409.00920)


[696] Weijie Liu, Peng Zhou, Zhe Zhao, Zhiruo Wang, Qi Ju, Haotang Deng, and Ping Wang. K-bert:
Enabling language representation with knowledge graph. _AAAI Conference on Artificial Intelligence_,
2019.


[697] Weiwen Liu, Xu Huang, Xingshan Zeng, Xinlong Hao, Shuai Yu, Dexun Li, Shuai Wang, Weinan Gan,
Zhengying Liu, Yuanqing Yu, Zezhong Wang, Yuxian Wang, Wu Ning, Yutai Hou, Bin Wang, Chuhan
Wu, Xinzhi Wang, Yong Liu, Yasheng Wang, Duyu Tang, Dandan Tu, Lifeng Shang, Xin Jiang, Ruiming
Tang, Defu Lian, Qun Liu, and Enhong Chen. Toolace: Winning the points of llm function calling,
[arXiv preprint arXiv:2409.00920, 2024. URL https://arxiv.org/abs/2409.00920v1.](https://arxiv.org/abs/2409.00920v1)


[698] Weiwen Liu, Jiarui Qin, Xu Huang, Xingshan Zeng, Yunjia Xi, Jianghao Lin, Chuhan Wu, Yasheng
Wang, Lifeng Shang, Ruiming Tang, Defu Lian, Yong Yu, and Weinan Zhang. The real barrier to llm
[agent usability is agentic roi, arXiv preprint arXiv:2505.17767, 2025. URL https://arxiv.org/](https://arxiv.org/abs/2505.17767v1)
[abs/2505.17767v1.](https://arxiv.org/abs/2505.17767v1)


[699] Wentao Liu, Hanglei Hu, Jie Zhou, Yuyang Ding, Junsong Li, Jiayi Zeng, Mengliang He, Qin Chen,
Bo Jiang, Aimin Zhou, and Liang He. Mathematical language models: A survey, arXiv preprint
[arXiv:2312.07622, 2023. URL https://arxiv.org/abs/2312.07622v4.](https://arxiv.org/abs/2312.07622v4)


[700] Wentao Liu, Ruohua Zhang, Aimin Zhou, Feng Gao, and JiaLi Liu. Echo: A large language model
[with temporal episodic memory, arXiv preprint arXiv:2502.16090, 2025. URL https://arxiv.](https://arxiv.org/abs/2502.16090v1)
[org/abs/2502.16090v1.](https://arxiv.org/abs/2502.16090v1)


[701] Xu Liu, S. Ramirez, Petti T. Pang, C. Puryear, A. Govindarajan, K. Deisseroth, and S. Tonegawa.
Optogenetic stimulation of a hippocampal engram activates fear memory recall. _Nature_, 2012.


[702] Yang Liu, Xiaobin Tian, Zequn Sun, and Wei Hu. Finetuning generative large language models
with discrimination instructions for knowledge graph completion. In _International Semantic Web_
_Conference_, 2024.


[703] Yue Liu, Jiaying Wu, Yufei He, Hongcheng Gao, Hongyu Chen, Baolong Bi, Jiaheng Zhang, Zhiqi
Huang, and Bryan Hooi. Efficient inference for large reasoning models: A survey, arXiv preprint
[arXiv:2503.23077, 2025. URL https://arxiv.org/abs/2503.23077v2.](https://arxiv.org/abs/2503.23077v2)


111


[704] Zijun Liu, Yanzhe Zhang, Peng Li, Yang Liu, and Diyi Yang. A dynamic llm-powered agent network for
[task-oriented agent collaboration, arXiv preprint arXiv:2310.02170, 2023. URL https://arxiv.](https://arxiv.org/abs/2310.02170v2)
[org/abs/2310.02170v2.](https://arxiv.org/abs/2310.02170v2)


[705] Zijun Liu, Zhennan Wan, Peng Li, Ming Yan, Ji Zhang, Fei Huang, and Yang Liu. Scaling external
knowledge input beyond context windows of llms via multi-agent collaboration, arXiv preprint
[arXiv:2505.21471, 2025. URL https://arxiv.org/abs/2505.21471v1.](https://arxiv.org/abs/2505.21471v1)


[706] Zinan Liu, Haoran Li, Jingyi Lu, Gaoyuan Ma, Xu Hong, Giovanni Iacca, Arvind Kumar, Shaojun
Tang, and Lin Wang. Nature’s insight: A novel framework and comprehensive analysis of agentic
reasoning through the lens of neuroscience. arXiv preprint, 2025.


[707] Zuxin Liu, Thai Hoang, Jianguo Zhang, Ming Zhu, Tian Lan, Shirley Kokane, Juntao Tan, Weiran
Yao, Zhiwei Liu, Yihao Feng, Rithesh Murthy, Liangwei Yang, Silvio Savarese, Juan Carlos Niebles,
Huan Wang, Shelby Heinecke, and Caiming Xiong. Apigen: Automated pipeline for generating
verifiable and diverse function-calling datasets. _Neural Information Processing Systems_, 2024.


[708] Leo S. Lo. The art and science of prompt engineering: A new literacy in the information age. _Internet_
_Reference Services Quarterly_, 2023.


[709] Joseph R. Loffredo and Suyeol Yun. Agent-enhanced large language models for researching political
institutions, arXiv preprint arXiv:2503.13524, 2025. [URL https://arxiv.org/abs/2503.](https://arxiv.org/abs/2503.13524v1)
[13524v1.](https://arxiv.org/abs/2503.13524v1)


[710] Jianqiao Lu, Wanjun Zhong, Wenyong Huang, Yufei Wang, Fei Mi, Baojun Wang, Weichao
Wang, Lifeng Shang, and Qun Liu. Self: Self-evolution with language feedback, arXiv preprint
[arXiv:2310.00533, 2023. URL https://arxiv.org/abs/2310.00533v4.](https://arxiv.org/abs/2310.00533v4)


[711] Junru Lu, Siyu An, Mingbao Lin, Gabriele Pergola, Yulan He, Di Yin, Xing Sun, and Yunsheng Wu.
Memochat: Tuning llms to use memos for consistent long-range open-domain conversation, arXiv
[preprint arXiv:2308.08239, 2023. URL https://arxiv.org/abs/2308.08239.](https://arxiv.org/abs/2308.08239)


[712] Junting Lu, Zhiyang Zhang, Fangkai Yang, Jue Zhang, Lu Wang, Chao Du, Qingwei Lin, Saravan
Rajmohan, Dongmei Zhang, and Qi Zhang. Axis: Efficient human-agent-computer interaction with
[api-first llm-based agents, arXiv preprint arXiv:2409.17140, 2025. URL https://arxiv.org/](https://arxiv.org/abs/2409.17140)
[abs/2409.17140.](https://arxiv.org/abs/2409.17140)


[713] Keer Lu, Xiaonan Nie, Zheng Liang, Da Pan, Shusen Zhang, Keshi Zhao, Weipeng Chen, Zenan
Zhou, Guosheng Dong, Bin Cui, and Wentao Zhang. Datasculpt: Crafting data landscapes for
long-context llms through multi-objective partitioning, arXiv preprint arXiv:2409.00997, 2024. URL
[https://arxiv.org/abs/2409.00997v2.](https://arxiv.org/abs/2409.00997v2)


[714] Liqiang Lu, Yicheng Jin, Hangrui Bi, Zizhang Luo, Peng Li, Tao Wang, and Yun Liang. Sanger: A
co-design framework for enabling sparse attention using reconfigurable architecture. _Micro_, 2021.

