<!-- Source: 02-ContextSurvey-2507.13334.pdf | Chunk 23/26 -->

Sule Bai, Zijian Kang, Jiashi Feng, et al. Traceable evidence enhanced visual grounded reasoning:
Evaluation and methodology. _arXiv preprint arXiv:2507.07999_, 2025.


[1091] Haochun Wang, Chi Liu, Nuwa Xi, Zewen Qiang, Sendong Zhao, Bing Qin, and Ting Liu. Huatuo:
Tuning llama model with chinese medical knowledge, arXiv preprint arXiv:2304.06975, 2023. URL
[https://arxiv.org/abs/2304.06975.](https://arxiv.org/abs/2304.06975)


[1092] Haoyu Wang, Tong Teng, Tianyu Guo, An Xiao, Duyu Tang, Hanting Chen, and Yunhe Wang.
Unshackling context length: An efficient selective attention approach through query-key compression,
[arXiv preprint arXiv:2502.14477, 2025. URL https://arxiv.org/abs/2502.14477v1.](https://arxiv.org/abs/2502.14477v1)


[1093] Heng Wang, Shangbin Feng, Tianxing He, Zhaoxuan Tan, Xiaochuang Han, and Yulia Tsvetkov. Can
language models solve graph problems in natural language? _Neural Information Processing Systems_,
2023.


[1094] Hengyi Wang, Haizhou Shi, Shiwei Tan, Weiyi Qin, Wenyuan Wang, Tunyu Zhang, A. Nambi,
T. Ganu, and Hao Wang. Multimodal needle in a haystack: Benchmarking long-context capability of
multimodal large language models. _North American Chapter of the Association for Computational_
_Linguistics_, 2024.


[1095] Hongru Wang, Cheng Qian, Manling Li, Jiahao Qiu, Boyang Xue, Mengdi Wang, Heng Ji, and Kam-Fai
Wong. Toward a theory of agents as tool-use decision-makers, arXiv preprint arXiv:2506.00886,
[2025. URL https://arxiv.org/abs/2506.00886v1.](https://arxiv.org/abs/2506.00886v1)


[1096] Jingjin Wang. Proprag: Guiding retrieval with beam search over proposition paths, arXiv preprint
[arXiv:2504.18070, 2025. URL https://arxiv.org/abs/2504.18070v1.](https://arxiv.org/abs/2504.18070v1)


[1097] Jingyu Wang, Lu Zhang, Xueqing Li, Huazhong Yang, and Yongpan Liu. Ulseq-ta: Ultra-long sequence
attention fusion transformer accelerator supporting grouped sparse softmax and dual-path sparse
layernorm. _IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems_, 2024.


[1098] Jize Wang, Zerun Ma, Yining Li, Songyang Zhang, Cailian Chen, Kai Chen, and Xinyi Le. Gta: A
benchmark for general tool agents. _Neural Information Processing Systems_, 2024.


[1099] Lei Wang, Chengbang Ma, Xueyang Feng, Zeyu Zhang, Hao ran Yang, Jingsen Zhang, Zhi-Yang
Chen, Jiakai Tang, Xu Chen, Yankai Lin, Wayne Xin Zhao, Zhewei Wei, and Ji rong Wen. A survey
on large language model based autonomous agents. _Frontiers Comput. Sci._, 2023.


141


[1100] Lei Wang, Wanyu Xu, Yihuai Lan, Zhiqiang Hu, Yunshi Lan, Roy Ka-Wei Lee, and Ee-Peng Lim. Planand-solve prompting: Improving zero-shot chain-of-thought reasoning by large language models.
_Annual Meeting of the Association for Computational Linguistics_, 2023.


[1101] Lei Wang, Jingsen Zhang, Hao Yang, Zhiyuan Chen, Jiakai Tang, Zeyu Zhang, Xu Chen, Yankai Lin,
Ruihua Song, Wayne Xin Zhao, et al. When large language model based agent meets user behavior
analysis: A novel user simulation paradigm. 2023.


[1102] Libo Wang. Towards humanoid robot autonomy: A dynamic architecture integrating continuous
thought machines (ctm) and model context protocol (mcp), arXiv preprint arXiv:2505.19339, 2025.
[URL https://arxiv.org/abs/2505.19339v1.](https://arxiv.org/abs/2505.19339v1)


[1103] Liya Wang, Jason Chou, Xin Zhou, A. Tien, and Diane M. Baumgartner. Aviationgpt: A large
[language model for the aviation domain, arXiv preprint arXiv:2311.17686, 2023. URL https:](https://arxiv.org/abs/2311.17686v1)
[//arxiv.org/abs/2311.17686v1.](https://arxiv.org/abs/2311.17686v1)


[1104] Liyuan Wang, Bo Lei, Qian Li, Hang Su, Jun Zhu, and Yi Zhong. Triple-memory networks: A
brain-inspired method for continual learning. _IEEE Transactions on Neural Networks and Learning_
_Systems_, 2020.


[1105] Lu Wang, Fangkai Yang, Chaoyun Zhang, Junting Lu, Jiaxu Qian, Shilin He, Pu Zhao, Bo Qiao, Ray
Huang, Si Qin, Qisheng Su, Jiayi Ye, Yudi Zhang, Jian-Guang Lou, Qingwei Lin, Saravan Rajmohan,
Dongmei Zhang, and Qi Zhang. Large action models: From inception to implementation, arXiv
[preprint arXiv:2412.10047, 2025. URL https://arxiv.org/abs/2412.10047.](https://arxiv.org/abs/2412.10047)


[1106] Peijie Wang, Zhong-Zhi Li, Fei Yin, Xin Yang, Dekang Ran, and Cheng-Lin Liu. Mv-math: Evaluating
multimodal math reasoning in multi-visual contexts. 2025.


[1107] Qineng Wang, Zihao Wang, Ying Su, and Yangqiu Song. On the discussion of large language models:
Symmetry of agents and interplay with prompts, arXiv preprint arXiv:2311.07076, 2023. URL
[https://arxiv.org/abs/2311.07076v1.](https://arxiv.org/abs/2311.07076v1)


[1108] Qingyue Wang, Liang Ding, Yanan Cao, Yibing Zhan, Zheng Lin, Shi Wang, Dacheng Tao, and Li Guo.
Divide, conquer, and combine: Mixture of semantic-independent experts for zero-shot dialogue state
[tracking, arXiv preprint arXiv:2306.00434, 2023. URL https://arxiv.org/abs/2306.00434.](https://arxiv.org/abs/2306.00434)


[1109] Rongzheng Wang, Shuang Liang, Qizhi Chen, Jiasheng Zhang, and Ke Qin. Graphtool-instruction:
Revolutionizing graph reasoning in llms through decomposed subtask instruction. _Knowledge Discovery_
_and Data Mining_, 2024.


[1110] Shengnan Wang, Youhui Bai, Lin Zhang, Pingyi Zhou, Shixiong Zhao, Gong Zhang, Sen Wang,
Renhai Chen, Hua Xu, and Hongwei Sun. Xl3m: A training-free framework for llm length extension
[based on segment-wise inference, arXiv preprint arXiv:2405.17755, 2024. URL https://arxiv.](https://arxiv.org/abs/2405.17755v1)
[org/abs/2405.17755v1.](https://arxiv.org/abs/2405.17755v1)


[1111] Song Wang, Yaochen Zhu, Haochen Liu, Zaiyi Zheng, Chen Chen, and Jundong Li. Knowledge
editing for large language models: A survey. _ACM Computing Surveys_, 2023.


[1112] Song Wang, Junhong Lin, Xiaojie Guo, Julian Shun, Jundong Li, and Yada Zhu. Reasoning of large
language models over knowledge graphs with super-relations. _International Conference on Learning_
_Representations_, 2025.


142


[1113] Tiannan Wang, Jiamin Chen, Qingrui Jia, Shuai Wang, Ruoyu Fang, Huilin Wang, Zhaowei Gao,
Chunzhao Xie, Chuou Xu, Jihong Dai, Yibin Liu, Jialong Wu, Shengwei Ding, Long Li, Zhiwei Huang,
Xinle Deng, Teng Yu, Gangan Ma, Han Xiao, Z. Chen, Danjun Xiang, Yunxia Wang, Yuanyuan
Zhu, Yichen Xiao, Jing Wang, Yiru Wang, Siran Ding, Jiayang Huang, Jiayi Xu, Yilihamujiang
Tayier, Zhenyu Hu, Yuan Gao, Chengfeng Zheng, Yu-Jie Ye, Yihan Li, Lei Wan, Xinyue Jiang,
Yujie Wang, Siyuan Cheng, Zhule Song, Xiangru Tang, Xiaohua Xu, Ningyu Zhang, Huajun Chen,
Y. Jiang, and Wangchunshu Zhou. Weaver: Foundation models for creative writing, arXiv preprint
[arXiv:2401.17268, 2024. URL https://arxiv.org/abs/2401.17268v1.](https://arxiv.org/abs/2401.17268v1)


[1114] Weizhi Wang, Li Dong, Hao Cheng, Haoyu Song, Xiaodong Liu, Xifeng Yan, Jianfeng Gao, and Furu
Wei. Visually-augmented language modeling. _International Conference on Learning Representations_,
2022.


[1115] Xiang Wang, Tinglin Huang, Dingxian Wang, Yancheng Yuan, Zhenguang Liu, Xiangnan He, and Tat
seng Chua. Learning intents behind interactions with knowledge graph for recommendation. _The_
_Web Conference_, 2021.


[1116] Xiao Wang, Isaac Lyngaas, A. Tsaris, Peng Chen, Sajal Dash, Mayanka Chandra Shekar, Tao Luo,
Hong-Jun Yoon, M. Wahib, and J. Gounley. Ultra-long sequence distributed transformer, arXiv
[preprint arXiv:2311.02382, 2023. URL https://arxiv.org/abs/2311.02382v2.](https://arxiv.org/abs/2311.02382v2)


[1117] Xiaohan Wang, Yuhui Zhang, Orr Zohar, and S. Yeung-Levy. Videoagent: Long-form video understanding with large language model as agent. _European Conference on Computer Vision_, 2024.


[1118] Xiaolong Wang, Zhaolu Kang, Wangyuxuan Zhai, Xinyue Lou, Yunghwei Lai, Ziyue Wang,
Yawen Wang, Kaiyu Huang, Yile Wang, Peng Li, and Yang Liu. Mucar: Benchmarking multilingual cross-modal ambiguity resolution for multimodal large language models, arXiv preprint
[arXiv:2506.17046v1, 2025. URL https://arxiv.org/abs/2506.17046v1.](https://arxiv.org/abs/2506.17046v1)


[1119] Xiaoqiang Wang, Suyuchen Wang, Yun Zhu, and Bang Liu. R3mem: Bridging memory retention and
retrieval via reversible compression. arXiv preprint, 2025.


[1120] Xiaoyang Wang, Pavan Kapanipathi, Ryan Musa, Mo Yu, Kartik Talamadupula, I. Abdelaziz, Maria
Chang, Achille Fokoue, B. Makni, Nicholas Mattei, and M. Witbrock. Improving natural language
inference using external knowledge in the science questions domain. _AAAI Conference on Artificial_
_Intelligence_, 2018.


[1121] Xindi Wang, Mahsa Salmani, Parsa Omidi, Xiangyu Ren, Mehdi Rezagholizadeh, and A. Eshaghi.
Beyond the limits: A survey of techniques to extend the context length in large language models.
_International Joint Conference on Artificial Intelligence_, 2024.


[1122] Xingyao Wang, Yangyi Chen, Lifan Yuan, Yizhe Zhang, Yunzhu Li, Hao Peng, and Heng Ji. Executable
code actions elicit better llm agents. _International Conference on Machine Learning_, 2024.


[1123] Xuezhi Wang, Jason Wei, D. Schuurmans, Quoc Le, Ed H. Chi, and Denny Zhou. Self-consistency
improves chain of thought reasoning in language models. _International Conference on Learning_
_Representations_, 2022.


[1124] Yancheng Wang, Ziyan Jiang, Zheng Chen, Fan Yang, Yingxue Zhou, Eunah Cho, Xing Fan, Xiaojiang
Huang, Yanbin Lu, and Yingzhen Yang. Recmind: Large language model powered agent for rec[ommendation, arXiv preprint arXiv:2308.14296, 2024. URL https://arxiv.org/abs/2308.](https://arxiv.org/abs/2308.14296)
[14296.](https://arxiv.org/abs/2308.14296)


143


[1125] Yani Wang. Application of large language models based on knowledge graphs in question-answering
systems: A review. _Applied and Computational Engineering_, 2024.


[1126] Yanlin Wang, Wanjun Zhong, Yanxian Huang, Ensheng Shi, Min Yang, Jiachi Chen, Hui Li, Yuchi Ma,
Qianxiang Wang, and Zibin Zheng. Agents in software engineering: Survey, landscape, and vision,
[arXiv preprint arXiv:2409.09030, 2024. URL https://arxiv.org/abs/2409.09030v2.](https://arxiv.org/abs/2409.09030v2)


[1127] Yaqi Wang and Haipei Xu. Srsa: A cost-efficient strategy-router search agent for real-world humanmachine interactions. _2024 IEEE International Conference on Data Mining Workshops (ICDMW)_,
2024.


[1128] Yi Wang, Xinhao Li, Ziang Yan, Yinan He, Jiashuo Yu, Xiangyun Zeng, Chenting Wang, Changlian
Ma, Haian Huang, Jianfei Gao, Min Dou, Kaiming Chen, Wenhai Wang, Yu Qiao, Yali Wang, and
Limin Wang. Internvideo2.5: Empowering video mllms with long and rich context modeling. arXiv
preprint, 2025.


[1129] Yiming Wang, Zhuosheng Zhang, and Rui Wang. Element-aware summarization with large language
models: Expert-aligned evaluation and chain-of-thought method. _Annual Meeting of the Association_
_for Computational Linguistics_, 2023.


[1130] Yingming Wang and Pepa Atanasova. Self-critique and refinement for faithful natural language
[explanations, arXiv preprint arXiv:2505.22823, 2025. URL https://arxiv.org/abs/2505.](https://arxiv.org/abs/2505.22823v1)
[22823v1.](https://arxiv.org/abs/2505.22823v1)


[1131] Yiwei Wang, Wei Wang, Yuxuan Liang, Yujun Cai, and Bryan Hooi. Mixup for node and graph
classification. In _Proceedings of the Web Conference 2021_, pages 3663–3674, 2021.


[1132] Yu Wang, Yifan Gao, Xiusi Chen, Haoming Jiang, Shiyang Li, Jingfeng Yang, Qingyu Yin, Zheng Li,
Xian Li, Bing Yin, Jingbo Shang, and Julian McAuley. Memoryllm: Towards self-updatable large
[language models, arXiv preprint arXiv:2402.04624, 2024. URL https://arxiv.org/abs/2402.](https://arxiv.org/abs/2402.04624)
[04624.](https://arxiv.org/abs/2402.04624)


[1133] Yu Wang, Dmitry Krotov, Yuanzhe Hu, Yifan Gao, Wangchunshu Zhou, Julian McAuley, Dan Gutfreund, Rogério Feris, and Zexue He. M+: Extending memoryllm with scalable long-term memory.
arXiv preprint, 2025.


[1134] Yubin Wang, Xinyang Jiang, De Cheng, Wenli Sun, Dongsheng Li, and Cairong Zhao. Hpt++:
Hierarchically prompting vision-language models with multi-granularity knowledge generation and
improved structure modeling. arXiv preprint, 2024.


[1135] Yujie Wang, Shiju Wang, Shenhan Zhu, Fangcheng Fu, Xinyi Liu, Xuefeng Xiao, Huixia Li, Jiashi Li,
Faming Wu, and Bin Cui. Flexsp: Accelerating large language model training via flexible sequence
parallelism. _International Conference on Architectural Support for Programming Languages and_
_Operating Systems_, 2024.


[1136] Yuntao Wang, Yanghe Pan, Zhou Su, Yi Deng, Quan Zhao, L. Du, Tom H. Luan, Jiawen Kang, and
D. Niyato. Large model based agents: State-of-the-art, cooperation paradigms, security and privacy,
and future trends. _IEEE Communications Surveys & Tutorials_, 2024.


[1137] Yuntao Wang, Shaolong Guo, Yanghe Pan, Zhou Su, Fahao Chen, Tom H. Luan, Peng Li, Jiawen Kang,
and Dusit Niyato. Internet of agents: Fundamentals, applications, and challenges, arXiv preprint
[arXiv:2505.07176, 2025. URL https://arxiv.org/abs/2505.07176v1.](https://arxiv.org/abs/2505.07176v1)


144


[1138] Yuxiang Wang, Xinnan Dai, Wenqi Fan, and Yao Ma. Exploring graph tasks with pure llms: A
comprehensive benchmark and investigation, arXiv preprint arXiv:2502.18771v1, 2025. URL
[https://arxiv.org/abs/2502.18771v1.](https://arxiv.org/abs/2502.18771v1)


[1139] Z. Wang, King Zhu, Chunpu Xu, Wangchunshu Zhou, Jiaheng Liu, Yibo Zhang, Jiashuo Wang, Ning
Shi, Siyu Li, Yizhi Li, Haoran Que, Zhaoxiang Zhang, Yuanxing Zhang, Ge Zhang, Ke Xu, Jie Fu, and
Wenhao Huang. Mio: A foundation model on multimodal tokens, arXiv preprint arXiv:2409.17692v3,
[2024. URL https://arxiv.org/abs/2409.17692v3.](https://arxiv.org/abs/2409.17692v3)


[1140] Zheng Wang, Shu Xian Teo, Jieer Ouyang, Yongjun Xu, and Wei Shi. M-rag: Reinforcing large
language model performance through retrieval-augmented generation with multiple partitions.
_Annual Meeting of the Association for Computational Linguistics_, 2024.


[1141] Zhiruo Wang, Zhoujun Cheng, Hao Zhu, Daniel Fried, and Graham Neubig. What are tools anyway?
a survey from the language model perspective, arXiv preprint arXiv:2403.15452, 2024. URL
[https://arxiv.org/abs/2403.15452v1.](https://arxiv.org/abs/2403.15452v1)


[1142] Ziyang Wang, Jianzhou You, Haining Wang, Tianwei Yuan, Shichao Lv, Yang Wang, and Limin Sun.
Honeygpt: Breaking the trilemma in terminal honeypots with large language model, arXiv preprint
[arXiv:2406.01882, 2024. URL https://arxiv.org/abs/2406.01882v2.](https://arxiv.org/abs/2406.01882v2)


[1143] Ziyue Wang, Chi Chen, Yiqi Zhu, Fuwen Luo, Peng Li, Ming Yan, Ji Zhang, Fei Huang, Maosong Sun,
and Yang Liu. Browse and concentrate: Comprehending multimodal content via prior-llm context
fusion. _Annual Meeting of the Association for Computational Linguistics_, 2024.


[1144] Zora Zhiruo Wang, Jiayuan Mao, Daniel Fried, and Graham Neubig. Agent workflow memory, arXiv
[preprint arXiv:2409.07429, 2024. URL https://arxiv.org/abs/2409.07429.](https://arxiv.org/abs/2409.07429)


[1145] Irene Weber. Large language models are pattern matchers: Editing semi-structured and structured
documents with chatgpt. _AKWI Jahrestagung_, 2024.


[1146] Hui Wei, Chenyue Feng, and Jianning Zhang. Modeling of memory mechanisms in cerebral cortex
[and simulation of storage performance, arXiv preprint arXiv:2401.00381, 2023. URL https:](https://arxiv.org/abs/2401.00381v2)
[//arxiv.org/abs/2401.00381v2.](https://arxiv.org/abs/2401.00381v2)


[1147] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed H. Chi, F. Xia, Quoc Le, and Denny
Zhou. Chain of thought prompting elicits reasoning in large language models. _Neural Information_
_Processing Systems_, 2022.


[1148] Jerry W. Wei, Le Hou, Andrew Kyle Lampinen, Xiangning Chen, Da Huang, Yi Tay, Xinyun Chen,
Yifeng Lu, Denny Zhou, Tengyu Ma, and Quoc V. Le. Symbol tuning improves in-context learning in
language models. _Conference on Empirical Methods in Natural Language Processing_, 2023.


[1149] Shaopeng Wei, Yu Zhao, Xingyan Chen, Qing Li, Fuzhen Zhuang, Ji Liu, and Gang Kou. Graph
learning and its advancements on large language models: A holistic survey, arXiv preprint
[arXiv:2212.08966, 2022. URL https://arxiv.org/abs/2212.08966v5.](https://arxiv.org/abs/2212.08966v5)


[1150] Zhepei Wei, Wenlin Yao, Yao Liu, Weizhi Zhang, Qin Lu, Liang Qiu, Changlong Yu, Puyang Xu, Chao
Zhang, Bing Yin, Hyokun Yun, and Lihong Li. Webagent-r1: Training web agents via end-to-end
multi-turn reinforcement learning. arXiv preprint, 2025.


145


[1151] Zhiyuan Wei, Jing Sun, Zijian Zhang, and Xianhao Zhang. Llm-smartaudit: Advanced smart contract
vulnerability detection. arXiv preprint, 2024.


[1152] Rebecca Westhäußer, Frederik Berenz, Wolfgang Minker, and Sebastian Zepf. Caim: Development
and evaluation of a cognitive ai memory framework for long-term interaction with intelligent agents.
arXiv preprint, 2025.


[1153] Danny Weyns and F. Oquendo. An architectural style for self-adaptive multi-agent systems, arXiv
[preprint arXiv:1909.03475, 2019. URL https://arxiv.org/abs/1909.03475v1.](https://arxiv.org/abs/1909.03475v1)


[1154] Erik Wijmans, Brody Huval, Alexander Hertzberg, V. Koltun, and Philipp Krähenbühl. Cut your losses
in large-vocabulary language models. _International Conference on Learning Representations_, 2024.


[1155] Wikipedia contributors. Agent communications language — Wikipedia, the free encyclopedia,
[2025. URL https://en.wikipedia.org/wiki/Agent_Communications_Language. [On-](https://en.wikipedia.org/wiki/Agent_Communications_Language)
line; accessed 17-July-2025].


[1156] Genta Indra Winata, Lingjue Xie, Karthik Radhakrishnan, Shijie Wu, Xisen Jin, Pengxiang Cheng,
Mayank Kulkarni, and Daniel Preotiuc-Pietro. Overcoming catastrophic forgetting in massively
[multilingual continual learning, arXiv preprint arXiv:2305.16252, 2023. URL https://arxiv.](https://arxiv.org/abs/2305.16252)
[org/abs/2305.16252.](https://arxiv.org/abs/2305.16252)


[1157] Beong woo Kwak, Minju Kim, Dongha Lim, Hyungjoo Chae, Dongjin Kang, Sunghwan Kim, Dongil
Yang, and Jinyoung Yeo. Toolhaystack: Stress-testing tool-augmented language models in realistic
[long-term interactions, arXiv preprint arXiv:2505.23662, 2025. URL https://arxiv.org/abs/](https://arxiv.org/abs/2505.23662v1)
[2505.23662v1.](https://arxiv.org/abs/2505.23662v1)


[1158] Biao Wu, Yanda Li, Meng Fang, Zirui Song, Zhiwei Zhang, Yunchao Wei, and Ling Chen. Foundations
and recent trends in multimodal mobile agents: A survey, arXiv preprint arXiv:2411.02006, 2024.
[URL https://arxiv.org/abs/2411.02006v2.](https://arxiv.org/abs/2411.02006v2)


[1159] Cheng-Kuang Wu, Zhi Rui Tam, Chieh-Yen Lin, Yun-Nung Chen, and Hung yi Lee. Streambench:
Towards benchmarking continuous improvement of language agents. _Neural Information Processing_
_Systems_, 2024.


[1160] Jialong Wu, Baixuan Li, Runnan Fang, Wenbiao Yin, Liwen Zhang, Zhengwei Tao, Dingchu Zhang,
Zekun Xi, Yong Jiang, Pengjun Xie, Fei Huang, and Jingren Zhou. Webdancer: Towards autonomous
[information seeking agency, arXiv preprint arXiv:2505.22648, 2025. URL https://arxiv.org/](https://arxiv.org/abs/2505.22648v2)
[abs/2505.22648v2.](https://arxiv.org/abs/2505.22648v2)


[1161] Jialong Wu, Wenbiao Yin, Yong Jiang, Zhenglin Wang, Zekun Xi, Runnan Fang, Deyu Zhou,
Pengjun Xie, and Fei Huang. Webwalker: Benchmarking llms in web traversal, arXiv preprint
[arXiv:2501.07572, 2025. URL https://arxiv.org/abs/2501.07572v2.](https://arxiv.org/abs/2501.07572v2)


[1162] Junde Wu, Jiayuan Zhu, and Yuyuan Liu. Agentic reasoning: Reasoning llms with tools for the
[deep research, arXiv preprint arXiv:2502.04644, 2025. URL https://arxiv.org/abs/2502.](https://arxiv.org/abs/2502.04644v1)
[04644v1.](https://arxiv.org/abs/2502.04644v1)


[1163] Likang Wu, Zhilan Zheng, Zhaopeng Qiu, Hao Wang, Hongchao Gu, Tingjia Shen, Chuan Qin, Chen
Zhu, Hengshu Zhu, Qi Liu, Hui Xiong, and Enhong Chen. A survey on large language models for
recommendation. _World wide web (Bussum)_, 2023.


146


[1164] M Wu, J Yang, J Jiang, M Li, K Yan, and H Yu.... Vtool-r1: Vlms learn to think with images via
[reinforcement learning on multimodal tool use. 2025. URL https://arxiv.org/abs/2505.](https://arxiv.org/abs/2505.19255)
[19255.](https://arxiv.org/abs/2505.19255)


[1165] Mengsong Wu, Tong Zhu, Han Han, Chuanyuan Tan, Xiang Zhang, and Wenliang Chen. Seal-tools:
Self-instruct tool learning dataset for agent tuning and detailed benchmark. _Natural Language_
_Processing and Chinese Computing_, 2024.


[1166] Panlong Wu, Ting Wang, Yifei Zhong, Haoqi Zhang, Zitong Wang, and Fangxin Wang. Deepform: Reasoning large language model for communication system formulation, arXiv preprint
[arXiv:2506.08551, 2025. URL https://arxiv.org/abs/2506.08551v2.](https://arxiv.org/abs/2506.08551v2)


[1167] Qingyun Wu, Gagan Bansal, Jieyu Zhang, Yiran Wu, Beibin Li, Erkang Zhu, Li Jiang, Xiaoyun Zhang,
Shaokun Zhang, Jiale Liu, A. Awadallah, Ryen W. White, Doug Burger, and Chi Wang. Autogen:
Enabling next-gen llm applications via multi-agent conversation, arXiv preprint arXiv:2308.08155,
[2023. URL https://arxiv.org/abs/2308.08155v2.](https://arxiv.org/abs/2308.08155v2)


[1168] Ruofan Wu, Youngwon Lee, Fan Shu, Danmei Xu, Seung won Hwang, Zhewei Yao, Yuxiong He,
and Feng Yan. Composerag: A modular and composable rag for corpus-grounded multi-hop ques[tion answering, arXiv preprint arXiv:2506.00232, 2025. URL https://arxiv.org/abs/2506.](https://arxiv.org/abs/2506.00232v1)
[00232v1.](https://arxiv.org/abs/2506.00232v1)


[1169] Shangyu Wu, Ying Xiong, Yufei Cui, Haolun Wu, Can Chen, Ye Yuan, Lianming Huang, Xue Liu,
Tei-Wei Kuo, Nan Guan, and C. Xue. Retrieval-augmented generation for natural language process[ing: A survey, arXiv preprint arXiv:2407.13193, 2024. URL https://arxiv.org/abs/2407.](https://arxiv.org/abs/2407.13193v3)
[13193v3.](https://arxiv.org/abs/2407.13193v3)


[1170] Shirley Wu, Shiyu Zhao, Qian Huang, Kexin Huang, Michihiro Yasunaga, V. Ioannidis, Karthik
Subbian, J. Leskovec, and James Zou. Avatar: Optimizing llm agents for tool usage via contrastive
reasoning. _Neural Information Processing Systems_, 2024.


[1171] Suhang Wu, Minlong Peng, Yue Chen, Jinsong Su, and Mingming Sun. Eva-kellm: A new benchmark
[for evaluating knowledge editing of llms, arXiv preprint arXiv:2308.09954, 2023. URL https:](https://arxiv.org/abs/2308.09954)
[//arxiv.org/abs/2308.09954.](https://arxiv.org/abs/2308.09954)


[1172] Tianhao Wu, Weizhe Yuan, Olga Golovneva, Jing Xu, Yuandong Tian, Jiantao Jiao, Jason E Weston,
and Sainbayar Sukhbaatar. Meta-rewarding language models: Self-improving alignment with
llm-as-a-meta-judge. arXiv preprint, 2024.


[1173] Tong Wu, Chong Xiang, Jiachen T. Wang, and Prateek Mittal. Effectively controlling reasoning
[models through thinking intervention, arXiv preprint arXiv:2503.24370, 2025. URL https://](https://arxiv.org/abs/2503.24370v3)
[arxiv.org/abs/2503.24370v3.](https://arxiv.org/abs/2503.24370v3)


[1174] Xinbo Wu and L. Varshney. A meta-learning perspective on transformers for causal language modeling.
_Annual Meeting of the Association for Computational Linguistics_, 2023.


[1175] Xue Wu and Kostas Tsioutsiouliklis. Thinking with knowledge graphs: Enhancing llm reasoning
[through structured data, arXiv preprint arXiv:2412.10654, 2024. URL https://arxiv.org/](https://arxiv.org/abs/2412.10654v1)
[abs/2412.10654v1.](https://arxiv.org/abs/2412.10654v1)


147


[1176] Yaxiong Wu, Sheng Liang, Chen Zhang, Yichao Wang, Yongyue Zhang, Huifeng Guo, Ruiming Tang,
and Yong Liu. From human memory to ai memory: A survey on memory mechanisms in the era of
[llms, arXiv preprint arXiv:2504.15965, 2025. URL https://arxiv.org/abs/2504.15965v2.](https://arxiv.org/abs/2504.15965v2)


[1177] Zengqing Wu and Takayuki Ito. The hidden strength of disagreement: Unraveling the consensusdiversity tradeoff in adaptive multi-agent systems, arXiv preprint arXiv:2502.16565, 2025. URL
[https://arxiv.org/abs/2502.16565v2.](https://arxiv.org/abs/2502.16565v2)


[1178] Zihao Wu, Lu Zhang, Chao-Yang Cao, Xiao-Xing Yu, Haixing Dai, Chong-Yi Ma, Zheng Liu, Lin Zhao,
Gang Li, Wei Liu, Quanzheng Li, Dinggang Shen, Xiang Li, Dajiang Zhu, and Tianming Liu. Exploring
the trade-offs: Unified large language models vs local fine-tuned models for highly-specific radiology
nli task. _IEEE Transactions on Big Data_, 2023.


[1179] Zhiheng Xi, Wenxiang Chen, Xin Guo, Wei He, Yiwen Ding, Boyang Hong, Ming Zhang, Junzhe
Wang, Senjie Jin, Enyu Zhou, Rui Zheng, Xiaoran Fan, Xiao Wang, Limao Xiong, Qin Liu, Yuhao
Zhou, Weiran Wang, Changhao Jiang, Yicheng Zou, Xiangyang Liu, Zhangyue Yin, Shihan Dou,
Rongxiang Weng, Wensen Cheng, Qi Zhang, Wenjuan Qin, Yongyan Zheng, Xipeng Qiu, Xuanjing
Huan, and Tao Gui. The rise and potential of large language model based agents: A survey, arXiv
[preprint arXiv:2309.07864, 2023. URL https://arxiv.org/abs/2309.07864v3.](https://arxiv.org/abs/2309.07864v3)


[1180] Menglin Xia, Victor Ruehle, Saravan Rajmohan, and Reza Shokri. Minerva: A programmable
[memory test benchmark for language models, arXiv preprint arXiv:2502.03358, 2025. URL https:](https://arxiv.org/abs/2502.03358v2)
[//arxiv.org/abs/2502.03358v2.](https://arxiv.org/abs/2502.03358v2)


[1181] Yuchen Xia, Manthan Shenoy, N. Jazdi, and M. Weyrich. Towards autonomous system: flexible modular production system enhanced with large language model agents. _IEEE International Conference_
_on Emerging Technologies and Factory Automation_, 2023.


[1182] Yutong Xia, Ao Qu, Yunhan Zheng, Yihong Tang, Dingyi Zhuang, Yuxuan Liang, Cathy Wu, Roger
Zimmermann, and Jinhua Zhao. Reimagining urban science: Scaling causal inference with large
[language models, arXiv preprint arXiv:2504.12345v3, 2025. URL https://arxiv.org/abs/](https://arxiv.org/abs/2504.12345v3)
[2504.12345v3.](https://arxiv.org/abs/2504.12345v3)


[1183] Zhishang Xiang, Chuanjie Wu, Qinggang Zhang, Shengyuan Chen, Zijin Hong, Xiao Huang, and
Jinsong Su. When to use graphs in rag: A comprehensive analysis for graph retrieval-augmented gener[ation, arXiv preprint arXiv:2506.05690, 2025. URL https://arxiv.org/abs/2506.05690v1.](https://arxiv.org/abs/2506.05690v1)


[1184] Chaojun Xiao, Pengle Zhang, Xu Han, Guangxuan Xiao, Yankai Lin, Zhengyan Zhang, Zhiyuan Liu,
Song Han, and Maosong Sun. Inf **l** m: Training-free long-context extrapolation for llms with an
efficient context memory. _Neural Information Processing Systems_, 2024.


[1185] Guangxuan Xiao, Yuandong Tian, Beidi Chen, Song Han, and Mike Lewis. Efficient streaming
language models with attention sinks. _International Conference on Learning Representations_, 2023.

