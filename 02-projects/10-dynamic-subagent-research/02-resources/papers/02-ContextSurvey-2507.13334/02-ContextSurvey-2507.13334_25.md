<!-- Source: 02-ContextSurvey-2507.13334.pdf | Chunk 25/26 -->

[1244] Yingxuan Yang, Huacan Chai, Yuanyi Song, Siyuan Qi, Muning Wen, Ning Li, Junwei Liao, Haoyi
Hu, Jianghao Lin, Gaowei Chang, Weiwen Liu, Ying Wen, Yong Yu, and Weinan Zhang. A survey
[of ai agent protocols, arXiv preprint arXiv:2504.16736, 2025. URL https://arxiv.org/abs/](https://arxiv.org/abs/2504.16736v3)
[2504.16736v3.](https://arxiv.org/abs/2504.16736v3)


[1245] Yuan Yang, Siheng Xiong, Ehsan Shareghi, and F. Fekri. The compressor-retriever architecture for
[language model os, arXiv preprint arXiv:2409.01495, 2024. URL https://arxiv.org/abs/](https://arxiv.org/abs/2409.01495v1)
[2409.01495v1.](https://arxiv.org/abs/2409.01495v1)


[1246] Yuxin Yang, Haoyang Wu, Tao Wang, Jia Yang, Hao Ma, and Guojie Luo. Pseudo-knowledge graph:
Meta-path guided retrieval and in-graph text for rag-equipped llm, arXiv preprint arXiv:2503.00309,
[2025. URL https://arxiv.org/abs/2503.00309v1.](https://arxiv.org/abs/2503.00309v1)


[1247] Zhen Yang, Fang Liu, Zhongxing Yu, J. Keung, Jia Li, Shuo Liu, Yifan Hong, Xiaoxue Ma, Zhi Jin, and
Ge Li. Exploring and unleashing the power of large language models in automated code translation.
_Proc. ACM Softw. Eng._, 2024.


[1248] Chengyuan Yao and Satoshi Fujita. Adaptive control of retrieval-augmented generation for large
language models through reflective tags. _Electronics_, 2024.


[1249] Huaiyuan Yao, Longchao Da, Vishnu Nandam, J. Turnau, Zhiwei Liu, Linsey Pang, and Hua Wei.
Comal: Collaborative multi-agent large language models for mixed-autonomy traffic, arXiv preprint
[arXiv:2410.14368, 2024. URL https://arxiv.org/abs/2410.14368v2.](https://arxiv.org/abs/2410.14368v2)


[1250] Jiayu Yao, Shenghua Liu, Yiwei Wang, Lingrui Mei, Baolong Bi, Yuyao Ge, Zhecheng Li, and Xueqi
Cheng. Who is in the spotlight: The hidden bias undermining multimodal retrieval-augmented
generation. 2025.


[1251] Jinghan Yao, Sam Ade Jacobs, Masahiro Tanaka, Olatunji Ruwase, A. Shafi, H. Subramoni, and
Dhabaleswar K. Panda. Training ultra long context language model with fully pipelined distributed
[transformer, arXiv preprint arXiv:2408.16978, 2024. URL https://arxiv.org/abs/2408.](https://arxiv.org/abs/2408.16978v2)
[16978v2.](https://arxiv.org/abs/2408.16978v2)


153


[1252] Liang Yao, Chengsheng Mao, and Yuan Luo. Graph convolutional networks for text classification.
_AAAI Conference on Artificial Intelligence_, 2018.


[1253] Shunyu Yao, Howard Chen, John Yang, and Karthik Narasimhan. Webshop: Towards scalable
real-world web interaction with grounded language agents. _Neural Information Processing Systems_,
2022.


[1254] Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao.
React: Synergizing reasoning and acting in language models. _International Conference on Learning_
_Representations_, 2022.


[1255] Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, T. Griffiths, Yuan Cao, and Karthik Narasimhan.
Tree of thoughts: Deliberate problem solving with large language models. _Neural Information_
_Processing Systems_, 2023.


[1256] Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao.
React: Synergizing reasoning and acting in language models, arXiv preprint arXiv:2210.03629, 2023.
[URL https://arxiv.org/abs/2210.03629.](https://arxiv.org/abs/2210.03629)


[1257] Shunyu Yao, Noah Shinn, P. Razavi, and Karthik Narasimhan. _τ_ -bench: A benchmark for tool-agentuser interaction in real-world domains. arXiv preprint, 2024.


[1258] Weiran Yao, Shelby Heinecke, Juan Carlos Niebles, Zhiwei Liu, Yihao Feng, Le Xue, Rithesh Murthy,
Zeyuan Chen, Jianguo Zhang, Devansh Arpit, Ran Xu, Phil Mui, Huan Wang, Caiming Xiong, and
Silvio Savarese. Retroformer: Retrospective large language agents with policy gradient optimization,
[arXiv preprint arXiv:2308.02151, 2024. URL https://arxiv.org/abs/2308.02151.](https://arxiv.org/abs/2308.02151)


[1259] Michihiro Yasunaga, Hongyu Ren, Antoine Bosselut, Percy Liang, and J. Leskovec. Qa-gnn: Reasoning
with language models and knowledge graphs for question answering. _North American Chapter of the_
_Association for Computational Linguistics_, 2021.


[1260] Michihiro Yasunaga, Antoine Bosselut, Hongyu Ren, Xikun Zhang, Christopher D. Manning, Percy
Liang, and J. Leskovec. Deep bidirectional language-knowledge graph pretraining. _Neural Information_
_Processing Systems_, 2022.


[1261] Fahd Yazin, Moumita Das, A. Banerjee, and Dipanjan Roy. Contextual prediction errors reorganize
naturalistic episodic memories in time. _Scientific Reports_, 2021.


[1262] J Ye, G Li, S Gao, C Huang, Y Wu, S Li, and X Fan.... Tooleyes: Fine-grained evaluation for
[tool learning capabilities of large language models in real-world scenarios. 2024. URL https:](https://arxiv.org/abs/2401.00741)
[//arxiv.org/abs/2401.00741.](https://arxiv.org/abs/2401.00741)


[1263] J Ye, S Li, G Li, C Huang, S Gao, and Y Wu.... Toolsword: Unveiling safety issues of large language
[models in tool learning across three stages. 2024. URL https://arxiv.org/abs/2402.10753.](https://arxiv.org/abs/2402.10753)


[1264] Junjie Ye, Zhengyin Du, Xuesong Yao, Weijian Lin, Yufei Xu, Zehui Chen, Zaiyuan Wang, Sining
Zhu, Zhiheng Xi, Siyu Yuan, Tao Gui, Qi Zhang, Xuanjing Huang, and Jiechao Chen. Toolhop: A
query-driven benchmark for evaluating large language models in multi-hop tool use, arXiv preprint
[arXiv:2501.02506, 2025. URL https://arxiv.org/abs/2501.02506v4.](https://arxiv.org/abs/2501.02506v4)


[1265] Qinyuan Ye, Maxamed Axmed, Reid Pryzant, and Fereshte Khani. Prompt engineering a prompt
engineer. _Annual Meeting of the Association for Computational Linguistics_, 2023.


154


[1266] Sixiang Ye, Zeyu Sun, Guoqing Wang, Liwei Guo, Qing-Lin Liang, Zheng Li, and Yong Liu.
Prompt alchemy: Automatic prompt refinement for enhancing code generation, arXiv preprint
[arXiv:2503.11085, 2025. URL https://arxiv.org/abs/2503.11085v1.](https://arxiv.org/abs/2503.11085v1)


[1267] Zhifan Ye, Kejing Xia, Yonggan Fu, Xin Dong, Jihoon Hong, Xiangchi Yuan, Shizhe Diao, Jan
Kautz, Pavlo Molchanov, and Y. Lin. Longmamba: Enhancing mamba’s long-context capabilities
via training-free receptive field enlargement. _International Conference on Learning Representations_,
2025.


[1268] Asaf Yehudai, Lilach Eden, Alan Li, Guy Uziel, Yilun Zhao, Roy Bar-Haim, Arman Cohan, and Michal
Shmueli-Scheuer. Survey on evaluation of llm-based agents, arXiv preprint arXiv:2503.16416, 2025.
[URL https://arxiv.org/abs/2503.16416v1.](https://arxiv.org/abs/2503.16416v1)


[1269] Peiling Yi and Yuhan Xia. Irony detection, reasoning and understanding in zero-shot learning. _IEEE_
_Transactions on Artificial Intelligence_, 2025.


[1270] Da Yin, Faeze Brahman, Abhilasha Ravichander, Khyathi Raghavi Chandu, Kai-Wei Chang, Yejin Choi,
and Bill Yuchen Lin. Agent lumos: Unified and modular training for open-source language agents.
_Annual Meeting of the Association for Computational Linguistics_, 2023.


[1271] Fan Yin, Zifeng Wang, I-Hung Hsu, Jun Yan, Ke Jiang, Yanfei Chen, Jindong Gu, Long T. Le,
Kai-Wei Chang, Chen-Yu Lee, Hamid Palangi, and Tomas Pfister. Magnet: Multi-turn tool-use
data synthesis and distillation via graph translation, arXiv preprint arXiv:2503.07826, 2025. URL
[https://arxiv.org/abs/2503.07826v1.](https://arxiv.org/abs/2503.07826v1)


[1272] Guoli Yin, Haoping Bai, Shuang Ma, Feng Nan, Yanchao Sun, Zhaoyang Xu, Shen Ma, Jiarui Lu,
Xiang Kong, Aonan Zhang, Dian Ang Yap, Yizhe Zhang, K. Ahnert, Vik Kamath, Mathias Berglund,
Dominic Walsh, Tobias Gindele, Juergen Wiest, Zhengfeng Lai, Xiaoming Wang, Jiulong Shan, Meng
Cao, Ruoming Pang, and Zirui Wang. Mmau: A holistic benchmark of agent capabilities across
diverse domains. _North American Chapter of the Association for Computational Linguistics_, 2024.


[1273] Pengcheng Yin, Graham Neubig, Wen tau Yih, and Sebastian Riedel. Tabert: Pretraining for joint
understanding of textual and tabular data. _Annual Meeting of the Association for Computational_
_Linguistics_, 2020.


[1274] S Yin, W You, Z Ji, G Zhong, and J Bai. Mumath-code: Combining tool-use large language models
[with multi-perspective data augmentation for mathematical reasoning. 2024. URL https://](https://arxiv.org/abs/2405.07551)
[arxiv.org/abs/2405.07551.](https://arxiv.org/abs/2405.07551)


[1275] Gunwoo Yong, Kahyun Jeon, Daeyoung Gil, and Ghang Lee. Prompt engineering for zero-shot and
few-shot defect detection and classification using a visual-language pretrained model. _Comput. Aided_
_Civ. Infrastructure Eng._, 2022.


[1276] Aspen H. Yoo and A. Collins. How working memory and reinforcement learning are intertwined: A
cognitive, neural, and computational perspective. _Journal of Cognitive Neuroscience_, 2021.


[1277] Chanwoong Yoon, Taewhoo Lee, Hyeon Hwang, Minbyul Jeong, and Jaewoo Kang. Compact:
Compressing retrieved documents actively for question answering. _Conference on Empirical Methods_
_in Natural Language Processing_, 2024.


155


[1278] Jiaxuan You, Mingjie Liu, Shrimai Prabhumoye, M. Patwary, M. Shoeybi, and Bryan Catanzaro.
Llm-evolve: Evaluation for llm’s evolving capability on benchmarks. _Conference on Empirical Methods_
_in Natural Language Processing_, 2024.


[1279] Yuxin You, Zhen Liu, Xiangchao Wen, Yongtao Zhang, and Wei Ai. Large language models meet
graph neural networks: A perspective of graph mining. _Mathematics_, 2024.


[1280] Dian Yu, Yuheng Zhang, Jiahao Xu, Tian Liang, Linfeng Song, Zhaopeng Tu, Haitao Mi, and
[Dong Yu. Teaching llms to refine with tools, arXiv preprint arXiv:2412.16871, 2024. URL https:](https://arxiv.org/abs/2412.16871v1)
[//arxiv.org/abs/2412.16871v1.](https://arxiv.org/abs/2412.16871v1)


[1281] Miao Yu, Fanci Meng, Xinyun Zhou, Shilong Wang, Junyuan Mao, Linsey Pang, Tianlong Chen,
Kun Wang, Xinfeng Li, Yongfeng Zhang, Bo An, and Qingsong Wen. A survey on trustworthy
[llm agents: Threats and countermeasures, arXiv preprint arXiv:2503.09648, 2025. URL https:](https://arxiv.org/abs/2503.09648v1)
[//arxiv.org/abs/2503.09648v1.](https://arxiv.org/abs/2503.09648v1)


[1282] Ye Yu, Yaoning Yu, and Haohan Wang. Premise: Scalable and strategic prompt optimization for
efficient mathematical reasoning in large models, arXiv preprint arXiv:2506.10716, 2025. URL
[https://arxiv.org/abs/2506.10716v1.](https://arxiv.org/abs/2506.10716v1)


[1283] Zeping Yu and Sophia Ananiadou. Understanding multimodal llms: the mechanistic interpretability
[of llava in visual question answering, arXiv preprint arXiv:2411.10950v2, 2024. URL https:](https://arxiv.org/abs/2411.10950v2)
[//arxiv.org/abs/2411.10950v2.](https://arxiv.org/abs/2411.10950v2)


[1284] Zishun Yu, Tengyu Xu, Di Jin, Karthik Abinav Sankararaman, Yun He, Wenxuan Zhou, Zhouhao
Zeng, Eryk Helenowski, Chen Zhu, Si-Yuan Wang, Hao Ma, and Han Fang. Think smarter not harder:
Adaptive reasoning with inference aware optimization, arXiv preprint arXiv:2501.17974, 2025. URL
[https://arxiv.org/abs/2501.17974v2.](https://arxiv.org/abs/2501.17974v2)


[1285] Zhao yu Su, Linjie Li, Mingyang Song, Yunzhuo Hao, Zhengyuan Yang, Jun Zhang, Guanjie Chen,
Jiawei Gu, Juntao Li, Xiaoye Qu, and Yu Cheng. Openthinkimg: Learning to think with images via
[visual tool reinforcement learning, arXiv preprint arXiv:2505.08617, 2025. URL https://arxiv.](https://arxiv.org/abs/2505.08617v1)
[org/abs/2505.08617v1.](https://arxiv.org/abs/2505.08617v1)


[1286] Siyu Yuan, Zehui Chen, Zhiheng Xi, Junjie Ye, Zhengyin Du, and Jiecao Chen. Agent-r: Training
language model agents to reflect via iterative self-training. arXiv preprint, 2025.


[1287] Weizhe Yuan, Richard Yuanzhe Pang, Kyunghyun Cho, Sainbayar Sukhbaatar, Jing Xu, and Jason E
Weston. Self-rewarding language models. _International Conference on Machine Learning_, 2024.


[1288] Xiaowei Yuan, Zhao Yang, Ziyang Huang, Yequan Wang, Siqi Fan, Yiming Ju, Jun Zhao, and Kang
Liu. Exploiting contextual knowledge in llms through v-usable information based layer enhancement.
arXiv preprint, 2025.


[1289] Xinbin Yuan, Jian Zhang, Kaixin Li, Zhuoxuan Cai, Lujian Yao, Jie Chen, Enguang Wang, Qibin
Hou, Jinwei Chen, Peng-Tao Jiang, and Bo Li. Enhancing visual grounding for gui agents via
[self-evolutionary reinforcement learning, arXiv preprint arXiv:2505.12370, 2025. URL https:](https://arxiv.org/abs/2505.12370v2)
[//arxiv.org/abs/2505.12370v2.](https://arxiv.org/abs/2505.12370v2)


[1290] Murong Yue. A survey of large language model agents for question answering, arXiv preprint
[arXiv:2503.19213, 2025. URL https://arxiv.org/abs/2503.19213v1.](https://arxiv.org/abs/2503.19213v1)


156


[1291] Xihang Yue, Linchao Zhu, and Yi Yang. Fragrel: Exploiting fragment-level relations in the external
memory of large language models. _Annual Meeting of the Association for Computational Linguistics_,
2024.


[1292] Seongjun Yun, Minbyul Jeong, Raehyun Kim, Jaewoo Kang, and Hyunwoo J. Kim. Graph transformer
networks. _Neural Information Processing Systems_, 2019.


[1293] Ge Yuyao, Cheng Yiting, Wang Jia, Zhou Hanlin, and Chen Lizhe. Vision transformer based on
knowledge distillation in tcm image classification. In _2022 IEEE 5th International Conference on_
_Computer and Communication Engineering Technology (CCET)_, pages 120–125. IEEE, 2022.


[1294] M. Zaheer, Guru Guruganesh, Kumar Avinava Dubey, J. Ainslie, Chris Alberti, Santiago Ontañón,
Philip Pham, Anirudh Ravula, Qifan Wang, Li Yang, and Amr Ahmed. Big bird: Transformers for
longer sequences. _Neural Information Processing Systems_, 2020.


[1295] Yuhang Zang, Wei Li, Jun Han, Kaiyang Zhou, and Chen Change Loy. Contextual object detection
with multimodal large language models. _International Journal of Computer Vision_, 2023.


[1296] E. Zelikman, Yuhuai Wu, and Noah D. Goodman. Star: Bootstrapping reasoning with reasoning,
[arXiv preprint arXiv:2203.14465, 2022. URL https://arxiv.org/abs/2203.14465v2.](https://arxiv.org/abs/2203.14465v2)


[1297] E. Zelikman, Eliana Lorch, Lester Mackey, and A. Kalai. Self-taught optimizer (stop): Recursively
[self-improving code generation, arXiv preprint arXiv:2310.02304, 2023. URL https://arxiv.](https://arxiv.org/abs/2310.02304v3)
[org/abs/2310.02304v3.](https://arxiv.org/abs/2310.02304v3)


[1298] Pai Zeng, Zhenyu Ning, Jieru Zhao, Weihao Cui, Mengwei Xu, Liwei Guo, XuSheng Chen, and Yizhou
Shan. The cap principle for llm serving: A survey of long-context large language model serving,
[arXiv preprint arXiv:2405.11299, 2024. URL https://arxiv.org/abs/2405.11299v2.](https://arxiv.org/abs/2405.11299v2)


[1299] Ruihong Zeng, Jinyuan Fang, Siwei Liu, and Zaiqiao Meng. On the structural memory of llm agents,
[arXiv preprint arXiv:2412.15266, 2024. URL https://arxiv.org/abs/2412.15266v1.](https://arxiv.org/abs/2412.15266v1)


[1300] Yirong Zeng, Xiao Ding, Yuxian Wang, Weiwen Liu, Wu Ning, Yutai Hou, Xu Huang, Bing Qin, and
Ting Liu. itool: Reinforced fine-tuning with dynamic deficiency calibration for advanced tool use,
[arXiv preprint arXiv:2501.09766, 2025. URL https://arxiv.org/abs/2501.09766v4.](https://arxiv.org/abs/2501.09766v4)


[1301] Yongcheng Zeng, Xinyu Cui, Xuanfa Jin, Guoqing Liu, Zexu Sun, Dong Li, Ning Yang, Jianye Hao,
Haifeng Zhang, and Jun Wang. Evolving llms’ self-refinement capability via iterative preference
[optimization, arXiv preprint arXiv:2502.05605, 2025. URL https://arxiv.org/abs/2502.](https://arxiv.org/abs/2502.05605v3)
[05605v3.](https://arxiv.org/abs/2502.05605v3)


[1302] An Zhang, Leheng Sheng, Yuxin Chen, Hao Li, Yang Deng, Xiang Wang, and Tat-Seng Chua. On
generative agents in recommendation. _Annual International ACM SIGIR Conference on Research and_
_Development in Information Retrieval_, 2023.


[1303] B Zhang, K Zhou, X Wei, and X Zhao.... Evaluating and improving tool-augmented computation[intensive math reasoning. 2023. URL https://proceedings.neurips.cc/paper_files/](https://proceedings.neurips.cc/paper_files/paper/2023/hash/4a47dd69242d5af908cdd5d51c971cbf-Abstract-Datasets_and_Benchmarks.html)

[paper/2023/hash/4a47dd69242d5af908cdd5d51c971cbf-Abstract-Datasets_and_](https://proceedings.neurips.cc/paper_files/paper/2023/hash/4a47dd69242d5af908cdd5d51c971cbf-Abstract-Datasets_and_Benchmarks.html)
[Benchmarks.html.](https://proceedings.neurips.cc/paper_files/paper/2023/hash/4a47dd69242d5af908cdd5d51c971cbf-Abstract-Datasets_and_Benchmarks.html)


157


[1304] Chao Zhang, Haoxin Zhang, Shiwei Wu, Di Wu, Tong Xu, Xiangyu Zhao, Yan Gao, Yao Hu, and
Enhong Chen. Notellm-2: Multimodal large representation models for recommendation. _Knowledge_
_Discovery and Data Mining_, 2024.


[1305] Chaoyun Zhang, He Huang, Chiming Ni, Jian Mu, Si Qin, Shilin He, Lu Wang, Fangkai Yang, Pu Zhao,
Chao Du, et al. Ufo2: The desktop agentos. _arXiv preprint arXiv:2504.14603_, 2025.


[1306] Chi Zhang, Yujun Cai, Guosheng Lin, and Chunhua Shen. Deepemd: Few-shot image classification
with differentiable earth mover’s distance and structured classifiers. In _Proceedings of the IEEE/CVF_
_conference on computer vision and pattern recognition_, pages 12203–12213, 2020.


[1307] Dan Zhang, G. Feng, Yang Shi, and D. Srinivasan. Physical safety and cyber security analysis of
multi-agent systems: A survey of recent advances. _IEEE/CAA Journal of Automatica Sinica_, 2021.


[1308] Danyang Zhang, Lu Chen, Situo Zhang, Hongshen Xu, Zihan Zhao, and Kai Yu. Large language
models are semi-parametric reinforcement learning agents. _Neural Information Processing Systems_,
2023.


[1309] Daoan Zhang, Weitong Zhang, Bing He, Jiang Zhang, Chenchen Qin, and Jianhua Yao. Dnagpt: A
generalized pre-trained tool for multiple dna sequence analysis tasks. _bioRxiv_, 2024.


[1310] Duzhen Zhang, Yahan Yu, Jiahua Dong, Chenxing Li, Dan Su, Chenhui Chu, and Dong Yu. Mmllms: Recent advances in multimodal large language models. In _Findings of the Association for_
_Computational Linguistics ACL 2024_, pages 12401–12430, 2024.


[1311] Duzhen Zhang, Yong Ren, Zhong-Zhi Li, Yahan Yu, Jiahua Dong, Chenxing Li, Zhilong Ji, and Jinfeng
Bai. Enhancing multimodal continual instruction tuning with branchlora. In _Proceedings of the 63rd_
_Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)_, 2025.


[1312] Han Zhang, Langshi Zhou, and Hanfang Yang. Learning to retrieve and reason on knowledge graph
[through active self-reflection, arXiv preprint arXiv:2502.14932, 2025. URL https://arxiv.org/](https://arxiv.org/abs/2502.14932v1)
[abs/2502.14932v1.](https://arxiv.org/abs/2502.14932v1)


[1313] Hengyu Zhang. Sinklora: Enhanced efficiency and chat capabilities for long-context large lan[guage models, arXiv preprint arXiv:2406.05678, 2024. URL https://arxiv.org/abs/2406.](https://arxiv.org/abs/2406.05678v1)
[05678v1.](https://arxiv.org/abs/2406.05678v1)


[1314] Jiaxin Zhang, Zhongzhi Li, Mingliang Zhang, Fei Yin, Chenglin Liu, and Yashar Moshfeghi. Geoeval:
benchmark for evaluating llms and multi-modal models on geometry problem-solving. 2024.


[1315] Jing Zhang, Xiaokang Zhang, Jifan Yu, Jian Tang, Jie Tang, Cuiping Li, and Hong Chen. Subgraph
retrieval enhanced model for multi-hop knowledge base question answering. _Annual Meeting of the_
_Association for Computational Linguistics_, 2022.


[1316] Kai Zhang, Fubang Zhao, Yangyang Kang, and Xiaozhong Liu. Llm-based medical assistant personalization with short- and long-term memory coordination. _North American Chapter of the Association_
_for Computational Linguistics_, 2023.


[1317] Kai Zhang, Yejin Kim, and Xiaozhong Liu. Personalized llm response generation with parameterized
[memory injection, arXiv preprint arXiv:2404.03565, 2025. URL https://arxiv.org/abs/2404.](https://arxiv.org/abs/2404.03565)
[03565.](https://arxiv.org/abs/2404.03565)


158


[1318] Kechi Zhang, Zhuo Li, Jia Li, Ge Li, and Zhi Jin. Self-edit: Fault-aware code editor for code generation.
_Annual Meeting of the Association for Computational Linguistics_, 2023.


[1319] Kechi Zhang, Jia Li, Ge Li, Xianjie Shi, and Zhi Jin. Codeagent: Enhancing code generation with
tool-integrated agent systems for real-world repo-level coding challenges. _Annual Meeting of the_
_Association for Computational Linguistics_, 2024.


[1320] Kechi Zhang, Ge Li, Jia Li, Huangzhao Zhang, Jingjing Xu, Hao Zhu, Lecheng Wang, Yihong Dong,
Jing Mai, Bin Gu, and Zhi Jin. Computational thinking reasoning in large language models, arXiv
[preprint arXiv:2506.02658, 2025. URL https://arxiv.org/abs/2506.02658v2.](https://arxiv.org/abs/2506.02658v2)


[1321] Ming-Liang Zhang, Zhong-Zhi Li, Fei Yin, Liang Lin, and Cheng-Lin Liu. Fuse, reason and verify:
Geometry problem solving with parsed clauses from diagram. 2024.


[1322] Qiyuan Zhang, Fuyuan Lyu, Zexu Sun, Lei Wang, Weixu Zhang, Zhihan Guo, Yufei Wang, Irwin
King, Xue Liu, and Chen Ma. A survey on test-time scaling in large language models: What, how,
[where, and how well?, arXiv preprint arXiv:2503.24235, 2025. URL https://arxiv.org/abs/](https://arxiv.org/abs/2503.24235v3)
[2503.24235v3.](https://arxiv.org/abs/2503.24235v3)


[1323] Ruichen Zhang, Mufan Qiu, Zhen Tan, Mohan Zhang, Vincent Lu, Jie Peng, Kaidi Xu, Leandro Z.
Agudelo, Peter Qian, and Tianlong Chen. Symbiotic cooperation for web agents: Harnessing
complementary strengths of large and small llms, arXiv preprint arXiv:2502.07942, 2025. URL
[https://arxiv.org/abs/2502.07942v2.](https://arxiv.org/abs/2502.07942v2)


[1324] Tengchao Zhang, Yonglin Tian, Fei Lin, Jun Huang, Patrik P. Süli, Rui Qin, and Fei-Yue Wang.
Coordfield: Coordination field for agentic uav task allocation in low-altitude urban scenarios, arXiv
[preprint arXiv:2505.00091, 2025. URL https://arxiv.org/abs/2505.00091v3.](https://arxiv.org/abs/2505.00091v3)


[1325] Weizhi Zhang, Xinyang Zhang, Chenwei Zhang, Liangwei Yang, Jingbo Shang, Zhepei Wei,
Henry Peng Zou, Zijie Huang, Zhengyang Wang, Yifan Gao, Xiaoman Pan, Lian Xiong, Jingguo
Liu, Philip S. Yu, and Xian Li. Personaagent: When large language model agents meet personaliza[tion at test time, arXiv preprint arXiv:2506.06254, 2025. URL https://arxiv.org/abs/2506.](https://arxiv.org/abs/2506.06254v1)
[06254v1.](https://arxiv.org/abs/2506.06254v1)


[1326] Wen Zhang, Long Jin, Yushan Zhu, Jiaoyan Chen, Zhiwei Huang, Junjie Wang, Yin Hua, Lei Liang,
and Hua zeng Chen. Trustuqa: A trustful framework for unified structured data question answering.
_AAAI Conference on Artificial Intelligence_, 2024.


[1327] Wenlin Zhang, Xiangyang Li, Kuicai Dong, Yichao Wang, Pengyue Jia, Xiaopeng Li, Yingyi Zhang,
Derong Xu, Zhaochen Du, Huifeng Guo, Ruiming Tang, and Xiangyu Zhao. Process vs. outcome
reward: Which is better for agentic rag reinforcement learning, arXiv preprint arXiv:2505.14069,
[2025. URL https://arxiv.org/abs/2505.14069v2.](https://arxiv.org/abs/2505.14069v2)


[1328] Wentao Zhang, Ce Cui, Yilei Zhao, Rui Hu, Yang Liu, Yahui Zhou, and Bo An. Agentorchestra: A hierarchical multi-agent framework for general-purpose task solving, arXiv preprint arXiv:2506.12508,
[2025. URL https://arxiv.org/abs/2506.12508v2.](https://arxiv.org/abs/2506.12508v2)


[1329] Xiaoyi Zhang, Zhaoyang Jia, Zongyu Guo, Jiahao Li, Bin Li, Houqiang Li, and Yan Lu. Deep
video discovery: Agentic search with tool use for long-form video understanding, arXiv preprint
[arXiv:2505.18079, 2025. URL https://arxiv.org/abs/2505.18079v2.](https://arxiv.org/abs/2505.18079v2)


159


[1330] Xikun Zhang, Antoine Bosselut, Michihiro Yasunaga, Hongyu Ren, Percy Liang, Christopher D.
Manning, and J. Leskovec. Greaselm: Graph reasoning enhanced language models for question
answering. _International Conference on Learning Representations_, 2022.


[1331] Yao Zhang, Zijian Ma, Yunpu Ma, Zhen Han, Yu Wu, and Volker Tresp. Webpilot: A versatile and
autonomous multi-agent system for web task execution with strategic exploration, arXiv preprint
[arXiv:2408.15978, 2024. URL https://arxiv.org/abs/2408.15978.](https://arxiv.org/abs/2408.15978)


[1332] Yinger Zhang, Hui Cai, Yicheng Chen, Rui Sun, and Jing Zheng. Reverse chain: A generic-rule for
[llms to master multi-api planning, arXiv preprint arXiv:2310.04474, 2023. URL https://arxiv.](https://arxiv.org/abs/2310.04474v3)
[org/abs/2310.04474v3.](https://arxiv.org/abs/2310.04474v3)


[1333] Yongheng Zhang, Qiguang Chen, Jingxuan Zhou, Peng Wang, Jiasheng Si, Jin Wang, Wenpeng
Lu, and Libo Qin. Wrong-of-thought: An integrated reasoning framework with multi-perspective
[verification and wrong information, arXiv preprint arXiv:2410.04463, 2024. URL https://arxiv.](https://arxiv.org/abs/2410.04463)
[org/abs/2410.04463.](https://arxiv.org/abs/2410.04463)


[1334] Youjia Zhang, Jin Wang, Liang-Chih Yu, and Xuejie Zhang. Ma-bert: Learning representation by
incorporating multi-attribute knowledge in transformers. _Findings_, 2021.


[1335] Yu Zhang, Jinlong Ma, Yongshuai Hou, Xuefeng Bai, Kehai Chen, Yang Xiang, Jun Yu, and Min
Zhang. Evaluating and steering modality preferences in multimodal large language model, arXiv
[preprint arXiv:2505.20977v1, 2025. URL https://arxiv.org/abs/2505.20977v1.](https://arxiv.org/abs/2505.20977v1)


[1336] Yunyi Zhang, Ming Zhong, Siru Ouyang, Yizhu Jiao, Sizhe Zhou, Linyi Ding, and Jiawei Han.
Automated mining of structured knowledge from text in the era of large language models. _Knowledge_
_Discovery and Data Mining_, 2024.


[1337] Yusen Zhang, Ruoxi Sun, Yanfei Chen, Tomas Pfister, Rui Zhang, and Sercan Ö. Arik. Chain of agents:
Large language models collaborating on long-context tasks. _Neural Information Processing Systems_,
2024.


[1338] Yuxiang Zhang, Yuqi Yang, Jiangming Shu, Xinyan Wen, and Jitao Sang. Agent models: Internalizing
chain-of-action generation into reasoning models, arXiv preprint arXiv:2503.06580, 2025. URL
[https://arxiv.org/abs/2503.06580v1.](https://arxiv.org/abs/2503.06580v1)


[1339] Zeyu Zhang, Xiaohe Bo, Chen Ma, Rui Li, Xu Chen, Quanyu Dai, Jieming Zhu, Zhenhua Dong, and
Ji-Rong Wen. A survey on the memory mechanism of large language model based agents, arXiv
[preprint arXiv:2404.13501, 2024. URL https://arxiv.org/abs/2404.13501v1.](https://arxiv.org/abs/2404.13501v1)


[1340] Zeyu Zhang, Quanyu Dai, Luyu Chen, Zeren Jiang, Rui Li, Jieming Zhu, Xu Chen, Yi Xie, Zhenhua
Dong, and Ji-Rong Wen. Memsim: A bayesian simulator for evaluating memory of llm-based
[personal assistants, arXiv preprint arXiv:2409.20163, 2024. URL https://arxiv.org/abs/](https://arxiv.org/abs/2409.20163v1)
[2409.20163v1.](https://arxiv.org/abs/2409.20163v1)

