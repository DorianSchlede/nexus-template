<!-- Source: 15-AgentSurvey-2503.21460.pdf | Chunk 9/10 -->

[[174] “MCP Agent,” 2 2025. [Online]. Available: https://github.com/](https://github.com/lastmile-ai/mcp-agent)
[lastmile-ai/mcp-agent](https://github.com/lastmile-ai/mcp-agent)

[175] A. Li, Y. Zhou, V. C. Raghuram, T. Goldstein, and M. Goldblum,
“Commercial llm agents are already vulnerable to simple yet
dangerous attacks,” _arXiv preprint arXiv:2502.08586_, 2025.

[176] W. Zhang, K. Tang, H. Wu, M. Wang, Y. Shen, G. Hou, Z. Tan,
P. Li, Y. Zhuang, and W. Lu, “Agent-pro: Learning to evolve
via policy-level reflection and optimization,” in _ACL_, 2024, pp.
5348–5375.

[177] L. Mo, Z. Liao, B. Zheng, Y. Su, C. Xiao, and H. Sun, “A trembling
house of cards? mapping adversarial attacks against language
agents,” _arXiv preprint arXiv:2402.10196_, 2024.

[178] E. Debenedetti, J. Zhang, M. Balunovic, L. Beurer-Kellner, M. Fischer, and F. Tramer, “Agentdojo: A dynamic environment to
evaluate prompt injection attacks and defenses for llm agents,” in
_NeurIPS_, vol. 37, 2024, pp. 82 895–82 920.

[179] C. H. Wu, J. Y. Koh, R. Salakhutdinov, D. Fried, and A. Raghunathan, “Adversarial attacks on multimodal agents,” _arXiv preprint_
_arXiv:2406.12814_, 2024.

[180] L.-b. Ning, S. Wang, W. Fan, Q. Li, X. Xu, H. Chen, and F. Huang,
“Cheatagent: Attacking llm-empowered recommender systems via
llm agent,” in _KDD_, 2024, pp. 2284–2295.

[181] W. Yu, K. Hu, T. Pang, C. Du, M. Lin, and M. Fredrikson,
“Infecting llm agents via generalizable adversarial attack,” in
_NeurIPS Workshop_, 2024.

[182] G. Lin and Q. Zhao, “Large language model sentinel: Llm agent
for adversarial purification,” _arXiv preprint arXiv:2405.20770_, 2024.

[183] S. Chern, Z. Fan, and A. Liu, “Combating adversarial attacks with
multi-agent debate,” _arXiv preprint arXiv:2401.05998_, 2024.

[184] X. Wang, J. Peng, K. Xu, H. Yao, and T. Chen, “Reinforcement
learning-driven llm agent for automated attacks on llms,” in _ACL_
_Findings_, 2024, pp. 170–177.

[185] Y. Dong, Z. Li, X. Meng, N. Yu, and S. Guo, “Jailbreaking
text-to-image models with llm-based agents,” _arXiv preprint_
_arXiv:2408.00523_, 2024.



23


[186] X. Chen, Y. Nie, W. Guo, and X. Zhang, “When llm meets
drl: Advancing jailbreaking efficiency via drl-guided search,” in
_NeurIPS_, 2024.

[187] Z. Lin, W. Ma, M. Zhou, Y. Zhao, H. Wang, Y. Liu, J. Wang, and
L. Li, “Pathseeker: Exploring llm security vulnerabilities with a
reinforcement learning-based jailbreak approach,” _arXiv preprint_
_arXiv:2409.14177_, 2024.

[188] Y. Zeng, Y. Wu, X. Zhang, H. Wang, and Q. Wu, “Autodefense:
Multi-agent llm defense against jailbreak attacks,” _arXiv preprint_
_arXiv:2403.04783_, 2024.

[189] S. Barua, M. Rahman, M. J. Sadek, R. Islam, S. Khaled, and A. Kabir,
“Guardians of the agentic system: Preventing many shots jailbreak
with agentic system,” _arXiv preprint arXiv:2502.16750_, 2025.

[190] Z. Ni, H. Wang, and H. Wang, “Shieldlearner: A new paradigm for
jailbreak attack defense in llms,” _arXiv preprint arXiv:2502.13162_,
2025.

[191] P. Zhu, Z. Zhou, Y. Zhang, S. Yan, K. Wang, and S. Su, “Demonagent: Dynamically encrypted multi-backdoor implantation attack
on llm-based agent,” _arXiv preprint arXiv:2502.12575_, 2025.

[192] W. Yang, X. Bi, Y. Lin, S. Chen, J. Zhou, and X. Sun, “Watch out for
your agents! investigating backdoor threats to llm-based agents,”
_NeurIPS_, vol. 37, pp. 100 938–100 964, 2025.

[193] Y. Wang, D. Xue, S. Zhang, and S. Qian, “Badagent: Inserting
and activating backdoor attacks in llm agents,” in _ACL_, 2024, pp.
9811–9827.

[194] T. Tong, F. Wang, Z. Zhao, and M. Chen, “Badjudge: Backdoor
vulnerabilities of llm-as-a-judge,” in _ICLR_, 2025.

[195] Z. Guo and R. Tourani, “Darkmind: Latent chain-of-thought
backdoor in customized llms,” _arXiv preprint arXiv:2501.18617_,
2025.

[196] Z. Zhou, Z. Li, J. Zhang, Y. Zhang, K. Wang, Y. Liu, and
Q. Guo, “Corba: Contagious recursive blocking attacks on multiagent systems based on large language models,” _arXiv preprint_
_arXiv:2502.14529_, 2025.

[197] P. He, Y. Lin, S. Dong, H. Xu, Y. Xing, and H. Liu, “Red-teaming llm
multi-agent systems via communication attacks,” _arXiv preprint_
_arXiv:2502.14847_, 2025.

[198] M. Yu, S. Wang, G. Zhang, J. Mao, C. Yin, Q. Liu, Q. Wen, K. Wang,
and Y. Wang, “Netsafe: Exploring the topological safety of multiagent networks,” _arXiv preprint arXiv:2410.15686_, 2024.

[199] S. Wang, G. Zhang, M. Yu, G. Wan, F. Meng, C. Guo, K. Wang,
and Y. Wang, “G-safeguard: A topology-guided security lens
and treatment on llm-based multi-agent systems,” _arXiv preprint_
_arXiv:2502.11127_, 2025.

[200] W. Hua, X. Yang, M. Jin, Z. Li, W. Cheng, R. Tang, and Y. Zhang,
“Trustagent: Towards safe and trustworthy llm-based agents
through agent constitution,” in _EMNLP Findings_, 2024.

[201] Z. Zhang, Y. Zhang, L. Li, H. Gao, L. Wang, H. Lu, F. Zhao,
Y. Qiao, and J. Shao, “Psysafe: A comprehensive framework for
psychological-based attack, defense, and evaluation of multi-agent
system safety,” _arXiv preprint arXiv:2401.11880_, 2024.

[202] Z. Deng, Y. Guo, C. Han, W. Ma, J. Xiong, S. Wen, and Y. Xiang,
“Ai agents under threat: A survey of key security challenges and
future pathways,” _ACM Computing Surveys_, 2024.

[203] E. Debenedetti, J. Zhang, M. Balunovic, L. Beurer-Kellner, M. Fischer, and F. Tramer, “Agentdojo: A dynamic environment to`
evaluate prompt injection attacks and defenses for llm agents,”
_NeurIPS_, vol. 37, pp. 82 895–82 920, 2025.

[204] X. Li, Z. Li, Y. Kosuga, Y. Yoshida, and V. Bian, “Targeting the
core: A simple and effective method to attack rag-based agents via
direct llm manipulation,” _arXiv preprint arXiv:2412.04415_, 2024.

[205] Q. Zhan, Z. Liang, Z. Ying, and D. Kang, “Injecagent: Benchmarking indirect prompt injections in tool-integrated large language
model agents,” _arXiv preprint arXiv:2403.02691_, 2024.

[206] D. Pasquini, E. M. Kornaropoulos, and G. Ateniese, “Hacking back
the ai-hacker: Prompt injection as a defense against llm-driven
cyberattacks,” _arXiv preprint arXiv:2410.20911_, 2024.

[207] S. Abdelnabi, A. Gomaa, E. Bagdasarian, P. O. Kristensson, and
R. Shokri, “Firewalls to secure dynamic llm agentic networks,”
_arXiv preprint arXiv:2502.01822_, 2025.

[208] P. Y. Zhong, S. Chen, R. Wang, M. McCall, B. L. Titzer, and
H. Miller, “Rtbas: Defending llm agents against prompt injection
and privacy leakage,” _arXiv preprint arXiv:2502.08966_, 2025.

[209] F. Jia, T. Wu, X. Qin, and A. Squicciarini, “The task shield:
Enforcing task alignment to defend against indirect prompt
injection in llm agents,” _arXiv preprint arXiv:2412.16682_, 2024.


[210] Y. Tian, X. Yang, J. Zhang, Y. Dong, and H. Su, “Evil geniuses:
Delving into the safety of llm-based agents,” _arXiv preprint_
_arXiv:2311.11855_, 2023.

[211] C. Wang, Q. Long, X. Meng, X. Cai, C. Wu, Z. Meng, X. Wang, and
Y. Zhou, “Biorag: A rag-llm framework for biological question
reasoning,” _arXiv preprint arXiv:2408.01107_, 2024.

[212] Y. Gan, Y. Yang, Z. Ma, P. He, R. Zeng, Y. Wang, Q. Li, C. Zhou,
S. Li, T. Wang _et al._, “Navigating the risks: A survey of security,
privacy, and ethics threats in llm-based agents,” _arXiv preprint_
_arXiv:2411.09523_, 2024.

[213] Z. Xiang, Y. Zeng, M. Kang, C. Xu, J. Zhang, Z. Yuan, Z. Chen,
C. Xie, F. Jiang, M. Pan _et al._, “Clas 2024: The competition for llm
and agent safety,” in _NeurIPS Workshop_, 2024.

[214] F. Wu, S. Wu, Y. Cao, and C. Xiao, “Wipi: A new web threat for
llm-driven web agents,” _arXiv preprint arXiv:2402.16965_, 2024.

[215] I. Nakash, G. Kour, G. Uziel, and A. Anaby-Tavor, “Breaking react
agents: Foot-in-the-door attack will get you in,” _arXiv preprint_
_arXiv:2410.16950_, 2024.

[216] Z. Chen, Z. Xiang, C. Xiao, D. Song, and B. Li, “Agentpoison:
Red-teaming llm agents via poisoning memory or knowledge
bases,” _NeurIPS_, vol. 37, pp. 130 185–130 213, 2025.

[217] B. Wang, W. He, P. He, S. Zeng, Z. Xiang, Y. Xing, and J. Tang,
“Unveiling privacy risks in llm agent memory,” _arXiv preprint_
_arXiv:2502.13172_, 2025.

[218] E. T. Red, “Malicious chatgpt agents: How gpts can quietly grab
your data (demo),” _Embrace The Red_, 2023.

[219] Y. Li, H. Wen, W. Wang, X. Li, Y. Yuan, G. Liu, J. Liu, W. Xu,
X. Wang, Y. Sun _et al._, “Personal llm agents: Insights and survey
about the capability, efficiency and security,” _arXiv preprint_
_arXiv:2401.05459_, 2024.

[220] X. Gu, X. Zheng, T. Pang, C. Du, Q. Liu, Y. Wang, J. Jiang,
and M. Lin, “Agent smith: A single image can jailbreak one
million multimodal llm agents exponentially fast,” _arXiv preprint_
_arXiv:2402.08567_, 2024.

[221] D. Lee and M. Tiwari, “Prompt infection: Llm-to-llm prompt injection within multi-agent systems,” _arXiv preprint arXiv:2410.07283_,
2024.

[222] B. Chen, G. Li, X. Lin, Z. Wang, and J. Li, “Blockagents: Towards byzantine-robust llm-based multi-agent coordination via
blockchain,” in _ACM Turing Award Celebration Conference_, 2024, pp.
187–192.

[223] M. Andriushchenko, A. Souly, M. Dziemian, D. Duenas, M. Lin,
J. Wang, D. Hendrycks, A. Zou, Z. Kolter, M. Fredrikson _et al._,
“Agentharm: A benchmark for measuring harmfulness of llm
agents,” _arXiv preprint arXiv:2410.09024_, 2024.

[224] N. Carlini, F. Tramer, E. Wallace, M. Jagielski, A. Herbert-Voss,
K. Lee, A. Roberts, T. Brown, D. Song, U. Erlingsson _et al._,
“Extracting training data from large language models,” in _USENIX_,
2021, pp. 2633–2650.

[225] N. Carlini, D. Ippolito, M. Jagielski, K. Lee, F. Tramer, and
C. Zhang, “Quantifying memorization across neural language
models,” in _ICLR_, 2022.

[226] J. Huang, H. Shao, and K. C.-C. Chang, “Are large pre-trained
language models leaking your personal information?” _arXiv_
_preprint arXiv:2205.12628_, 2022.

[227] F. Mireshghallah, K. Goyal, A. Uniyal, T. Berg-Kirkpatrick,
and R. Shokri, “Quantifying privacy risks of masked language
models using membership inference attacks,” _arXiv preprint_
_arXiv:2203.03929_, 2022.

[228] W. Fu, H. Wang, C. Gao, G. Liu, Y. Li, and T. Jiang, “Practical membership inference attacks against fine-tuned large language models
via self-prompt calibration,” _arXiv preprint arXiv:2311.06062_, 2023.

[229] S. Hoory, A. Feder, A. Tendler, S. Erell, A. Peled-Cohen, I. Laish,
H. Nakhost, U. Stemmer, A. Benjamini, A. Hassidim _et al._, “Learning and evaluating a differentially private pre-trained language
model,” in _EMNLP Findings_, 2021, pp. 1178–1189.

[230] M. Kang, S. Lee, J. Baek, K. Kawaguchi, and S. J. Hwang,
“Knowledge-augmented reasoning distillation for small language
models in knowledge-intensive tasks,” _NeurIPS_, vol. 36, pp. 48 573–
48 602, 2023.

[231] X. Pan, M. Zhang, S. Ji, and M. Yang, “Privacy risks of generalpurpose language models,” in _IEEE Symposium on Security and_
_Privacy (SP)_ . IEEE, 2020, pp. 1314–1331.

[232] L. Wang, J. Wang, J. Wan, L. Long, Z. Yang, and Z. Qin, “Property
existence inference against generative models,” in _USENIX_, 2024,
pp. 2423–2440.



24


[233] N. Kandpal, E. Wallace, and C. Raffel, “Deduplicating training
data mitigates privacy risks in language models,” in _ICML_ . PMLR,
2022, pp. 10 697–10 707.

[234] S. Kim, S. Yun, H. Lee, M. Gubri, S. Yoon, and S. J. Oh, “Propile:
Probing privacy leakage in large language models,” _NeurIPS_,
vol. 36, pp. 20 750–20 762, 2023.

[235] K. Krishna, G. S. Tomar, A. P. Parikh, N. Papernot, and M. Iyyer,
“Thieves on sesame street! model extraction of bert-based apis,”
_arXiv preprint arXiv:1910.12366_, 2019.

[236] A. Naseh, K. Krishna, M. Iyyer, and A. Houmansadr, “Stealing
the decoding algorithms of language models,” in _ACM SIGSAC_,
2023, pp. 1835–1849.

[237] Z. Li, C. Wang, P. Ma, C. Liu, S. Wang, D. Wu, C. Gao, and Y. Liu,
“On extracting specialized code abilities from large language
models: A feasibility study,” in _ICSE_, 2024, pp. 1–13.

[238] J. Kirchenbauer, J. Geiping, Y. Wen, J. Katz, I. Miers, and T. Goldstein, “A watermark for large language models,” in _ICML_ . PMLR,
2023, pp. 17 061–17 084.

[239] Y. Lin, Z. Gao, H. Du, D. Niyato, J. Kang, Z. Xiong, and Z. Zheng,
“Blockchain-based efficient and trustworthy aigc services in metaverse,” _IEEE Transactions on Services Computing_, 2024.

[240] X. Shen, Y. Qu, M. Backes, and Y. Zhang, “Prompt stealing attacks
against _{_ Text-to-Image _}_ generation models,” in _USENIX_, 2024,
pp. 5823–5840.

[241] Z. Sha and Y. Zhang, “Prompt stealing attacks against large
language models,” _arXiv preprint arXiv:2402.12959_, 2024.

[242] B. Hui, H. Yuan, N. Gong, P. Burlina, and Y. Cao, “Pleak: Prompt
leaking attacks against large language model applications,” in
_ACM SIGSAC_, 2024, pp. 3600–3614.

[243] R. Bommasani, D. A. Hudson, E. Adeli, R. Altman, S. Arora, S. von
Arx, M. S. Bernstein, J. Bohg, A. Bosselut, E. Brunskill _et al._, “On
the Opportunities and Risks of Foundation Models,” _arXiv preprint_
_arXiv:2108.07258_, 2021.

[244] L. Floridi and M. Chiriatti, “GPT-3: Its Nature, Scope, Limits, and
Consequences,” _Minds and Machines_, vol. 30, pp. 681–694, 2020.

[245] H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux,
T. Lacroix, B. Roziere, N. Goyal, E. Hambro, F. Azhar` _et al._,
“LLaMA: Open and Efficient Foundation Language Models,” _arXiv_
_preprint arXiv:2302.13971_, 2023.

[246] P. Tadas and S. Agarmore, “Redefining Work in the Age of AI:
Challenges and Pathways to Opportunities,” in _SPICES_ . IEEE,
2024, pp. 1–5.

[247] S. Moore, R. Tong, A. Singh, Z. Liu, X. Hu, Y. Lu, J. Liang, C. Cao,
H. Khosravi, P. Denny _et al._, “Empowering Education with LLMs The Next-Gen Interface and Content Generation,” in _International_
_Conference on Artificial Intelligence in Education_ . Springer, 2023, pp.
32–37.

[248] S. Liu, Y. Jin, C. Li, D. F. Wong, Q. Wen, L. Sun, H. Chen, X. Xie,
and J. Wang, “Culturevlm: Characterizing and improving cultural
understanding of vision-language models for over 100 countries,”
_arXiv:2501.01282_, 2025.

[249] P. Henderson, X. Li, D. Jurafsky, T. Hashimoto, M. A. Lemley, and
P. Liang, “Foundation Models and Fair Use,” _JMLR_, vol. 24, no.
400, pp. 1–79, 2023.

[250] M. A. Lemley and B. Casey, “Fair Learning,” _Tex. L. Rev._, vol. 99,
p. 743, 2020.

[251] S. Oh, Y. Jin, M. Sharma, D. Kim, E. Ma, G. Verma, and S. Kumar,
“Uniguard: Towards universal safety guardrails for jailbreak
attacks on multimodal large language models,” _arXiv:2411.01703_,
2024.

[252] E. M. Bender, T. Gebru, A. McMillan-Major, and S. Shmitchell,
“On the Dangers of Stochastic Parrots: Can Language Models Be
Too Big?” in _FAccT_, 2021, pp. 610–623.

[253] M. Brundage, S. Avin, J. Wang, H. Belfield, G. Krueger, G. Hadfield,
H. Khlaaf, J. Yang, H. Toner, R. Fong _et al._, “Toward Trustworthy
AI Development: Mechanisms for Supporting Verifiable Claims,”
_arXiv preprint arXiv:2004.07213_, 2020.

[254] D. Ganguli, D. Hernandez, L. Lovitt, A. Askell, Y. Bai, A. Chen,
T. Conerly, N. Dassarma, D. Drain, N. Elhage _et al._, “Predictability
and Surprise in Large Generative Models,” in _FAccT_, 2022, pp.
1747–1764.

[255] C. Deng, Y. Duan, X. Jin, H. Chang, Y. Tian, H. Liu, H. P. Zou,
Y. Jin, Y. Xiao, Y. Wang _et al._, “Deconstructing The Ethics of Large
Language Models from Long-standing Issues to New-emerging
Dilemmas: A Survey,” _arXiv e-prints_, pp. arXiv–2406, 2024.


[256] I. Shumailov, Z. Shumaylov, Y. Zhao, N. Papernot, R. Anderson,
and Y. Gal, “AI models collapse when trained on recursively
generated data,” _Nature_, vol. 631, no. 8022, pp. 755–759, 2024.

[257] L. Weidinger, J. Mellor, M. Rauh, C. Griffin, J. Uesato, P.-S. Huang,
M. Cheng, M. Glaese, B. Balle, A. Kasirzadeh _et al._, “Ethical
and social risks of harm from Language Models,” _arXiv preprint_
_arXiv:2112.04359_, 2021.

[258] Y. Xiao, Y. Jin, Y. Bai, Y. Wu, X. Yang, X. Luo, W. Yu, X. Zhao,
Y. Liu, Q. Gu _et al._, “Large language models can be contextual
privacy protection learners,” in _EMNLP_, 2024, pp. 14 179–14 201.

[259] D. A. Alber, Z. Yang, A. Alyakin, E. Yang, S. Rai, A. A. Valliani,
J. Zhang, G. R. Rosenbaum, A. K. Amend-Thomas, D. B. Kurland
_et al._, “Medical large language models are vulnerable to datapoisoning attacks,” _Nature Medicine_, pp. 1–9, 2025.

[260] Y. Jin, X. Wang, R. Yang, Y. Sun, W. Wang, H. Liao, and X. Xie,
“Towards fine-grained reasoning for fake news detection,” in _AAAI_,
vol. 36, no. 5, 2022, pp. 5746–5754.

[261] T. Shen, R. Jin, Y. Huang, C. Liu, W. Dong, Z. Guo, X. Wu, Y. Liu,
and D. Xiong, “Large Language Model Alignment: A Survey,”
_arXiv preprint arXiv:2309.15025_, 2023.

[262] A. S. Luccioni, S. Viguier, and A.-L. Ligozat, “Estimating the
Carbon Footprint of BLOOM, a 176B Parameter Language Model,”
_JMLR_, vol. 24, no. 253, pp. 1–15, 2023.

[263] E. Strubell, A. Ganesh, and A. McCallum, “Energy and Policy
Considerations for Deep Learning in NLP,” in _AAAI_, vol. 34,
no. 09, 2020, pp. 13 693–13 696.

[264] J. Zhou, “Awesome ai agents for scientific discovery,” [https://github.com/zhoujieli/](https://github.com/zhoujieli/Awesome-LLM-Agents-Scientific-Discovery)
[Awesome-LLM-Agents-Scientific-Discovery, 2024.](https://github.com/zhoujieli/Awesome-LLM-Agents-Scientific-Discovery)

[265] AAAI, “Aaai 2025 presidential panel: Future of ai research,” 2025.

[[Online]. Available: https://aaai.org/wp-content/uploads/2025/](https://aaai.org/wp-content/uploads/2025/03/AAAI-2025-PresPanel-Report-FINAL.pdf)
[03/AAAI-2025-PresPanel-Report-FINAL.pdf](https://aaai.org/wp-content/uploads/2025/03/AAAI-2025-PresPanel-Report-FINAL.pdf)

[266] A. Ghafarollahi and M. J. Buehler, “Sciagents: Automating
scientific discovery through bioinspired multi-agent intelligent
graph reasoning,” _Advanced Materials_, vol. n/a, no. n/a, p.
[2413523. [Online]. Available: https://advanced.onlinelibrary.](https://advanced.onlinelibrary.wiley.com/doi/abs/10.1002/adma.202413523)
[wiley.com/doi/abs/10.1002/adma.202413523](https://advanced.onlinelibrary.wiley.com/doi/abs/10.1002/adma.202413523)

[267] P. T. J. Kon, J. Liu, Q. Ding, Y. Qiu, Z. Yang, Y. Huang, J. Srinivasa,
M. Lee, M. Chowdhury, and A. Chen, “Curie: Toward rigorous
and automated scientific experimentation with ai agents,” 2025.

[[Online]. Available: https://arxiv.org/abs/2502.16069](https://arxiv.org/abs/2502.16069)

[268] Y. Jin, Q. Zhao, Y. Wang, H. Chen, K. Zhu, Y. Xiao, and J. Wang,
“Agentreview: Exploring peer review dynamics with llm agents,”
in _EMNLP_, 2024, pp. 1208–1226.

[269] A. M. Bran, S. Cox, O. Schilter, C. Baldassari, A. D. White,
and P. Schwaller, “Chemcrow: Augmenting large-language
models with chemistry tools,” 2023. [Online]. Available:
[https://arxiv.org/abs/2304.05376](https://arxiv.org/abs/2304.05376)

[270] A. Ghafarollahi and M. J. Buehler, “Atomagents: Alloy
design and discovery through physics-aware multi-modal
multi-agent artificial intelligence,” 2024. [Online]. Available:
[https://arxiv.org/abs/2407.10022](https://arxiv.org/abs/2407.10022)

[271] D. Kostunin, V. Sotnikov, S. Golovachev, and A. Strube, “Ai
agents for ground-based gamma astronomy,” 2025. [Online].
[Available: https://arxiv.org/abs/2503.00821](https://arxiv.org/abs/2503.00821)

[272] B. Qi, K. Zhang, K. Tian, H. Li, Z.-R. Chen, S. Zeng, E. Hua,
H. Jinfang, and B. Zhou, “Large language models as biomedical
hypothesis generators: A comprehensive evaluation,” 2024.

[[Online]. Available: https://arxiv.org/abs/2407.08940](https://arxiv.org/abs/2407.08940)

[273] Y. Roohani, A. Lee, Q. Huang, J. Vora, Z. Steinhart, K. Huang,
A. Marson, P. Liang, and J. Leskovec, “Biodiscoveryagent: An
ai agent for designing genetic perturbation experiments,” _arXiv_
_preprint arXiv:2405.17631_, 2024.

[274] Z. Wang, Q. Jin, C.-H. Wei, S. Tian, P.-T. Lai, Q. Zhu, C.-P. Day,
C. Ross, and Z. Lu, “Geneagent: Self-verification language agent
for gene set knowledge discovery using domain databases,” 2024.

[[Online]. Available: https://arxiv.org/abs/2405.16205](https://arxiv.org/abs/2405.16205)

[275] M. Xiao, W. Zhang, X. Huang, H. Zhu, M. Wu, X. Li, and Y. Zhou,
“Knowledge-guided biomarker identification for label-free singlecell rna-seq data: A reinforcement learning perspective,” _arXiv_
_preprint arXiv:2501.04718_, 2025.

[276] Y. Sun, Y. Zhang, Y. Si, C. Zhu, Z. Shui, K. Zhang, J. Li,
X. Lyu, T. Lin, and L. Yang, “Pathgen-1.6m: 1.6 million pathology
image-text pairs generation through multi-agent collaboration,”
[2024. [Online]. Available: https://arxiv.org/abs/2407.00203](https://arxiv.org/abs/2407.00203)



25


[277] X. Cai, C. Wang, Q. Long, Y. Zhou, and M. Xiao, “Knowledge hierarchy guided biological-medical dataset distillation for domain
llm training,” _arXiv preprint arXiv:2501.15108_, 2025.

[278] Z. Chen, C. Hu, M. Wu, Q. Long, X. Wang, Y. Zhou, and
M. Xiao, “Genesum: Large language model-based gene summary
extraction,” in _2024 IEEE International Conference on Bioinformatics_
_and Biomedicine (BIBM)_ . IEEE, 2024, pp. 1438–1443.

[279] K. Keshavjee, J. Bosomworth, J. Copen, J. Lai, B. Kucukyazici,
R. Lilani, and A. M. Holbrook, “Best practices in emr implementation: a systematic review,” in _AMIA Annual Symposium Proceedings_,
vol. 2006, 2006, p. 982.

[280] X. Ye, M. Xiao, Z. Ning, W. Dai, W. Cui, Y. Du, and Y. Zhou,
“Needed: Introducing hierarchical transformer to eye diseases
diagnosis,” in _Proceedings of the 2023 SIAM International Conference_
_on Data Mining (SDM)_ . SIAM, 2023, pp. 667–675.

[281] J. Li, Y. Lai, W. Li, J. Ren, M. Zhang, X. Kang, S. Wang, P. Li, Y.-Q.
Zhang, W. Ma _et al._, “Agent hospital: A simulacrum of hospital
with evolvable medical agents,” _arXiv preprint arXiv:2405.02957_,
2024.

[282] W. Yan, H. Liu, T. Wu, Q. Chen, W. Wang, H. Chai, J. Wang,
W. Zhao, Y. Zhang, R. Zhang _et al._, “Clinicallab: Aligning agents
for multi-departmental clinical diagnostics in the real world,”
_arXiv preprint arXiv:2406.13890_, 2024.

[283] H. Yu, J. Zhou, L. Li, S. Chen, J. Gallifant, A. Shi, X. Li,
W. Hua, M. Jin, G. Chen, Y. Zhou, Z. Li, T. Gupte, M.-L. Chen,
Z. Azizi, Y. Zhang, T. L. Assimes, X. Ma, D. S. Bitterman,
L. Lu, and L. Fan, “Aipatient: Simulating patients with ehrs
and llm powered agentic workflow,” 2024. [Online]. Available:
[https://arxiv.org/abs/2409.18924](https://arxiv.org/abs/2409.18924)

[284] N. Sharma, “Cxr-agent: Vision-language models for chest x-ray
interpretation with uncertainty aware radiology reporting,” _arXiv_
_preprint arXiv:2407.08811_, 2024.

[285] A. Fallahpour, J. Ma, A. Munim, H. Lyu, and B. Wang, “Medrax:
Medical reasoning agent for chest x-ray,” 2025. [Online]. Available:
[https://arxiv.org/abs/2502.02673](https://arxiv.org/abs/2502.02673)

[286] R. W. Lee, K. H. Lee, J. S. Yun, M. S. Kim, and H. S. Choi,
“Comparative analysis of m4cxr, an llm-based chest x-ray report
generation model, and chatgpt in radiological interpretation,”
_Journal of Clinical Medicine_, vol. 13, no. 23, p. 7057, 2024.

[287] X. Feng, Y. Luo, Z. Wang, H. Tang, M. Yang, K. Shao, D. Mguni,
Y. Du, and J. Wang, “Chessgpt: Bridging policy learning and
language modeling,” in _NeurIPS_, 2023, pp. 7216–7262.

[288] T. Carta, C. Romac, T. Wolf, S. Lamprier, O. Sigaud, and P.Y. Oudeyer, “Grounding large language models in interactive
environments with online reinforcement learning,” in _ICML_, 2023,
pp. 3676–3713.
