<!-- Source: 10-TalkHier-2502.11098.pdf | Chunk 3/5 -->

8


**Limitations**


One of the main limitations of _TalkHier_ is the relatively high API cost associated with the experiments (see Appendix A for details). This is a tradeoff due to the design of _TalkHier_, where multiple
agents collaborate hierarchically using a specifically designed communication protocol. While this
structured interaction enhances reasoning and coordination, it also increases computational expenses.
This raises broader concerns about the accessibility and democratization of LLM research, as such
costs may pose barriers for researchers with limited
resources. Future work could explore more costefficient generation strategies while preserving the
benefits of multi-agent collaboration.


**References**


Anthony Brohan et al. 2022. Code as policies:
Language model-driven robotics. _arXiv preprint_
_arXiv:2209.07753_ .


Mark Chen et al. 2021. Evaluating large language models trained on code. _arXiv preprint_
_arXiv:2107.03374_ .


Pei Chen, Boran Han, and Shuai Zhang. 2024.
Comm: Collaborative multi-agent, multi-reasoningpath prompting for complex problem solving. _arXiv_
_preprint arXiv:2404.17729_ .


Yuheng Cheng, Ceyao Zhang, Zhengwen Zhang, Xiangrui Meng, Sirui Hong, Wenhao Li, Zihao Wang,
Zekai Wang, Feng Yin, Junhua Zhao, and Xiuqiang
[He. 2024. Exploring large language model based in-](https://arxiv.org/abs/2401.03428)
[telligent agents: Definitions, methods, and prospects.](https://arxiv.org/abs/2401.03428)
_CoRR_, abs/2401.03428.


Eva Eigner and Thorsten Händler. 2024. Determinants
of llm-assisted decision-making. _arXiv preprint_
_arXiv:2402.17385_ .


Federico Errica, Giuseppe Siracusano, Davide Sanvito, and Roberto Bifulco. 2024. What did i
do wrong? quantifying llms’ sensitivity and consistency to prompt engineering. _arXiv preprint_
_arXiv:2406.12334_ .


Jiangnan Fang, Cheng-Tse Liu, Jieun Kim, Yash
Bhedaru, Ethan Liu, Nikhil Singh, Nedim Lipka,
Puneet Mathur, Nesreen K Ahmed, Franck Dernoncourt, et al. 2024. Multi-llm text summarization.
_arXiv preprint arXiv:2412.15487_ .


Chen Gao, Xiaochong Lan, Nian Li, Yuan Yuan, Jingtao
Ding, Zhilun Zhou, Fengli Xu, and Yong Li. 2023.
[Large language models empowered agent-based mod-](https://arxiv.org/abs/2312.11970)
[eling and simulation: A survey and perspectives.](https://arxiv.org/abs/2312.11970)
_CoRR_, abs/2312.11970.



[Significant Gravitas. 2023. Autogpt: An experimental](https://github.com/Torantulino/Auto-GPT)
[open-source application.](https://github.com/Torantulino/Auto-GPT)


Taicheng Guo, Xiuying Chen, Yaqi Wang, Ruidi Chang,
Shichao Pei, Nitesh V Chawla, Olaf Wiest, and Xiangliang Zhang. 2024. Large language model based
multi-agents: A survey of progress and challenges.
_arXiv preprint arXiv:2402.01680_ .


Shiyang Han, Qian Zhang, Yue Yao, Wenhao Jin, Zhen
Xu, and Cheng He. 2024. Llm multi-agent systems: Challenges and open problems. _arXiv preprint_
_arXiv:2402.03578_ .


Chengbo He, Bochao Zou, Xin Li, Jiansheng Chen,
Junliang Xing, and Huimin Ma. 2024. Enhancing llm
reasoning with multi-path collaborative reactive and
reflection agents. _arXiv preprint arXiv:2501.00430_ .


Dan Hendrycks, Colin Burns, Samuel Basart, Chia Zou,
[David Song, and Thomas G. Dietterich. 2021. Mea-](https://arxiv.org/abs/2110.08307)
[suring massive multitask language understanding.](https://arxiv.org/abs/2110.08307)
_arXiv preprint arXiv:2110.08307_ .


Sirui Hong, Xiawu Zheng, Jonathan Chen, Yuheng
Cheng, Jinlin Wang, Ceyao Zhang, Zili Wang, Steven
Ka Shing Yau, Zijuan Lin, Liyang Zhou, et al. 2023.
Metagpt: Meta programming for multi-agent collaborative framework. _arXiv preprint arXiv:2308.00352_ .


Xiaoyu Li, Shuang Wang, Shaohui Zeng, Yucheng Wu,
and Yue Yang. 2024. A survey on llm-based multiagent systems: Workflow, infrastructure, and challenges. _Vicinagearth_, 1(9).


Chin-Yew Lin. 2004. Rouge: A package for automatic
evaluation of summaries. In _Text summarization_
_branches out_, pages 74–81.


Qun Ma, Xiao Xue, Deyu Zhou, Xiangning Yu,
Donghua Liu, Xuwen Zhang, Zihan Zhao, Yifan
Shen, Peilin Ji, Juanjuan Li, Gang Wang, and Wan[peng Ma. 2024. Computational experiments meet](https://arxiv.org/abs/2402.00262)
[large language model based agents: A survey and](https://arxiv.org/abs/2402.00262)
[perspective.](https://arxiv.org/abs/2402.00262) _CoRR_, abs/2402.00262.


Aman Madaan, Niket Tandon, Doug Downey, and
Shrimai Han. 2023. Self-refine: Iteratively improving text via self-feedback. _arXiv preprint_
_arXiv:2303.17651_ .


Masato Mita, Soichiro Murakami, Akihiko Kato, and
Peinan Zhang. 2024. Striking gold in advertising:
Standardization and exploration of ad text generation.
In _Proceedings of the 62nd Annual Meeting of the_
_Association for Computational Linguistics_ .


[OpenAI. 2024a. Hello gpt-4o.](https://openai.com/index/hello-gpt-4o/)


[OpenAI. 2024b. Introducing openai o1.](https://openai.com/index/introducing-openai-o1-preview/)


[OpenBMB. 2023. Agentverse: Facilitating multi-agent](https://github.com/OpenBMB/AgentVerse)
[collaboration.](https://github.com/OpenBMB/AgentVerse) _AgentVerse GitHub_ .



9


Pouya Pezeshkpour, Eser Kandogan, Nikita Bhutani,
Sajjadur Rahman, Tom Mitchell, and Estevam Hr[uschka. 2024. Reasoning capacity in multi-agent sys-](https://arxiv.org/abs/2402.01108)
[tems: Limitations, challenges and human-centered](https://arxiv.org/abs/2402.01108)
[solutions.](https://arxiv.org/abs/2402.01108) _CoRR_, abs/2402.01108.


Chen Qian, Zihao Xie, Yifei Wang, Wei Liu, Yufan Dang, Zhuoyun Du, Weize Chen, Cheng Yang,
Zhiyuan Liu, and Maosong Sun. 2024. Scaling
large-language-model-based multi-agent collaboration. _arXiv preprint arXiv:2406.07155_ .


Xue Qiu, Hongyu Wang, Xiaoyun Tan, Chengyi Qu,
Yifan Xiong, Yang Cheng, Yichao Xu, Wei Chu,
and Yiming Qi. 2024. Towards collaborative intelligence: Propagating intentions and reasoning for
multi-agent coordination with large language models.
_arXiv preprint arXiv:2407.12532_ .


Sudhir Rasal. 2024. Llm harmony: Multi-agent communication for problem solving. _arXiv preprint_
_arXiv:2401.01312_ .


Shivam Shah et al. 2023. Fingpt: An open-source
financial large language model. _arXiv preprint_
_arXiv:2306.03026_ .


Wenjun Shen, Cheng Li, Hui Chen, Meng Yan, Xuesong
Quan, Hao Chen, Jian Zhang, and Fangyu Huang.
2024. Small llms are weak tool learners: A multi-llm
agent. _arXiv preprint arXiv:2401.07324_ .


Yashar Talebirad and Amir Nadiri. 2023. Multi-agent
collaboration: Harnessing the power of intelligent
llm agents. _arXiv preprint arXiv:2306.03314_ .


Lei Wang, Chen Ma, Xueyang Feng, Zeyu Zhang, Hao
Yang, Jingsen Zhang, Zhiyuan Chen, Jiakai Tang,
Xu Chen, Yankai Lin, Wayne Xin Zhao, Zhewei Wei,
[and Ji-Rong Wen. 2024a. A survey on large language](https://doi.org/10.1007/s11704-024-40231-1)
[model based autonomous agents.](https://doi.org/10.1007/s11704-024-40231-1) _Front. Comput._
_Sci._, 18.


Qineng Wang, Zihao Wang, Ying Su, Hanghang Tong,
and Yangqiu Song. 2024b. Rethinking the bounds of
llm reasoning: Are multi-agent discussions the key?
_arXiv preprint arXiv:2402.18272_ .


Xiaoyu Wang, Yuanhao Liu, and Hao Zhang. 2023. Coeval: A framework for collaborative human and machine evaluation. _arXiv preprint arXiv:2310.19740_ .


Zhao Wang, Briti Gangopadhyay, Mengjie Zhao, and
Shingo Takamatsu. 2025. OKG: On-the-fly keyword
generation in sponsored search advertising. In _Pro-_
_ceedings of the 31st International Conference on_
_Computational Linguistics: Industry Track_, pages
115–127. Association for Computational Linguistics.


Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten
Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, and
Denny Zhou. 2022. Chain-of-thought prompting elicits reasoning in large language models.



Zhiheng Xi, Wenxiang Chen, Xin Guo, Wei He, Yiwen Ding, Boyang Hong, Ming Zhang, Junzhe
Wang, Senjie Jin, Enyu Zhou, Rui Zheng, Xiaoran Fan, Xiao Wang, Limao Xiong, Yuhao Zhou,
Weiran Wang, Changhao Jiang, Yicheng Zou, Xiangyang Liu, Zhangyue Yin, Shihan Dou, Rongxiang Weng, Wensen Cheng, Qi Zhang, Wenjuan Qin,
Yongyan Zheng, Xipeng Qiu, Xuanjing Huan, and
[Tao Gui. 2023. The rise and potential of large lan-](https://doi.org/10.48550/arXiv.2309.07864)
[guage model based agents: A survey.](https://doi.org/10.48550/arXiv.2309.07864) _arxiv preprint_,
abs/2309.07864.


Li Xu, Qiang Sun, and Hui Zhao. 2024. Cooperative
evaluation in large language model refinement. _arXiv_
_preprint arXiv:2401.10234_ .


Wen-tau Yang, Wen-tau Yih, Chris Meek, Alec Barnes,
Zhiyuan Zhang, and Hannaneh Hajishirzi. 2017.
Wikiqa: A challenge dataset for open domain question answering. In _Proceedings of the 55th Annual_
_Meeting of the Association for Computational Lin-_
_guistics (Volume 1: Long Papers)_, pages 814–818.
Association for Computational Linguistics.


Yuanhao Yang, Qingqing Peng, Jian Wang, and Wenbo
Zhang. 2024. Multi-llm-agent systems: Techniques and business perspectives. _arXiv preprint_
_arXiv:2411.14033_ .


Zhengyuan Yang, Jianfeng Wang, Linjie Li, Kevin Lin,
Chung-Ching Lin, Zicheng Liu, and Lijuan Wang.
2023. Idea2img: Iterative self-refinement with gpt4v (ision) for automatic image design and generation.
_arXiv preprint arXiv:2310.08541_ .


Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak
Shafran, Thomas L. Griffiths, Yuan Cao, and
Karthik Narasimhan. 2023. Tree of thoughts:
Deliberate problem solving with large language models. Code repo with all prompts:
https://github.com/ysymyth/tree-of-thought- llm.


Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak
Shafran, Karthik Narasimhan, and Yuan Cao. 2022.
React: Synergizing reasoning and acting in language
models. _arXiv preprint arXiv:2210.03629_ .


Guibin Zhang, Yanwei Yue, Zhixun Li, Sukwon Yun,
Guancheng Wan, Kun Wang, Dawei Cheng, Jeffrey Xu Yu, and Tianlong Chen. 2024a. Cut the
crap: An economical communication pipeline for
llm-based multi-agent systems. _arXiv preprint_
_arXiv:2410.02506_ .


Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q Weinberger, and Yoav Artzi. 2020. Bertscore: Evaluating
text generation with bert. In _International Confer-_
_ence on Learning Representations (ICLR)_ .


Wentao Zhang, Lingxuan Zhao, Haochong Xia, Shuo
Sun, Jiaze Sun, Molei Qin, Xinyi Li, Yuqing Zhao,
Yilei Zhao, Xinyu Cai, et al. 2024b. Finagent: A
multimodal foundation agent for financial trading:
Tool-augmented, diversified, and generalist. _arXiv_
_preprint arXiv:2402.18485_ .



10


Qinlin Zhao, Jindong Wang, Yixuan Zhang, Yiqiao Jin,
Kaijie Zhu, Hao Chen, and Xing Xie. 2024. Competeai: Understanding the competition dynamics in
large language model-based agents. In _Proceedings_
_of the 41st International Conference on Machine_
_Learning (ICML)_ .


Mingchen Zhuge, Wenyi Wang, Louis Kirsch,
Francesco Faccio, Dmitrii Khizbullin, and Jürgen
Schmidhuber. 2024. Gptswarm: Language agents
as optimizable graphs. In _Forty-first International_
_Conference on Machine Learning_ .



11


**A** **Cost Analysis for Experiments**


The total expenditure for the experiments across the MMLU dataset, WikiQA, and Camera (Japanese Ad
Text Generation) tasks was approximately **$2,100 USD** . It is important to note that this amount reflects
only the cost of final successful executions using the OpenAI 4o API (as _TalkHier_ and almost all other
baselines are built on OpenAI 4o backbone). Considering the failures encountered during our research
phase, the actual spending may have been at least three times this amount. Below is a detailed breakdown
of costs and task-specific details.


**A.1** **MMLU Dataset (1,450 USD)**


The **MMLU** dataset comprises approximately 16,000 multiple-choice questions across 57 subjects. For
our experiments, we focused on five specific domains:


**A.1.1** **Cost Analysis for the Moral Scenario Task and Baselines**

The **Moral Scenario** task involved generating and evaluating responses for various moral dilemma
scenarios using OpenAI’s GPT-4o model. Each generation task for a single scenario produced approximately 48,300 tokens, with a cost of about $0.17 per task. Given a total of 895 tasks, the overall token
consumption and cost were:


0 _._ 17 _×_ 895 = 152 _._ 15 USD (1)


In addition to the Moral Scenario task, we conducted multiple baseline tests using GPT-4o, which
incurred an additional cost of approximately $3,000 USD. Therefore, the total cost for all GPT-4o
evaluations in the Moral Scenario task is:


152 _._ 15 + 900 = 1052 _._ 15 USD (2)


**A.1.2** **Cost Analysis for Other Tasks**

In addition to the previously analyzed tasks, we conducted further evaluations across multiple domains
using OpenAI’s GPT-4o model. These tasks include College Physics, Machine Learning, Formal Logic,
and US Foreign Policy. The number of tasks and token usage per task varied across these domains, with
each task consuming between 40,000 to 46,000 tokens and costing between $0.14 to $0.15 per task.


  - **College Physics** : 101 tasks, each generating 40,000 tokens.

  - **Machine Learning** : 111 tasks, each generating 40,000 tokens.

  - **Formal Logic** : 125 tasks, each generating 46,000 tokens.

  - **US Foreign Policy** : 100 tasks, each generating 45,000 tokens.
The total expenditure for these tasks amounted to $63.43 USD. and we also did experiments for various
baseline, it cost around 320 usd. totally it is 383.43. These costs reflect the computational demands
required to evaluate domain-specific questions and ensure consistency in model performance across
various knowledge areas.
The total expenditure for these tasks amounted to $63.43 USD. Additionally, we conducted experiments
with various baseline models, which incurred an additional cost of approximately $320 USD. In total,
the overall expenditure was **$383.43 USD** . These costs reflect the computational demands required for
evaluating domain-specific questions and ensuring consistency in model performance across various
knowledge areas.


**A.2** **WikiQA Dataset (1,191.49 USD)**


The WikiQA dataset comprises 3,047 questions and 29,258 sentences, of which 1,473 sentences are
labeled as answers to their corresponding questions. Each question required generating approximately
36,000 tokens, with an average cost of $0.13 per question. Given this setup, the total expenditure for the
WikiQA task was:


0 _._ 13 _×_ 1 _,_ 473 = 191 _._ 49 USD (3)


12


In addition to the execution of _TalkHier_, we conducted multiple baseline tests using GPT-4o as their
backbones, which incurred an additional cost of approximately $1,000 USD. Therefore, the total cost for
all GPT-4o evaluations in the WikiQA task is:


191 _._ 49 + 1000 = 1191 _._ 49 USD (4)


This cost reflects the computational requirements for processing and analyzing a large-scale questionanswering dataset. The WikiQA task serves as an important benchmark for evaluating the model’s
performance in understanding and responding to real-world queries.


**A.3** **Camera Dataset (400.56 USD)**


The **Camera** dataset task involved generating and evaluating ad headlines for 872 different test sets
using OpenAI’s GPT-4o backbone. Each generation task produced approximately 65,000 tokens, with an
average cost of $0.23 per task. Given this setup, the total expenditure for the Camera dataset task was:


0 _._ 23 _×_ 872 = 200 _._ 56 USD (5)


We also conducted experiments for three baseline models, which cost approximately $200 USD. In
total, the expenditure amounted to $400.56 USD. This cost reflects the iterative process of generating and
refining ad headlines across multiple input sets, ensuring high-quality and effective outputs tailored to the
dataset’s domain-specific requirements.


13


**B** **Prompt Design and Work Flow for Tasks in MMLU**


In this section, we describe the prompt design for evaluating and revising responses for each MMLU task.
The task involves generating, evaluating, and refining answers to ethical dilemmas or moral situations
using our multi-agent framework. Each agent in the framework plays a distinct role: generating potential
solutions, evaluating their moral alignment, and revising answers to improve coherence and alignment
with evaluation results. The prompts used for each agent are detailed below.


**B.1** **Initial Prompt**


The following is the prompt given to the supervisor at the beginning.



**B.2** **Answer Generator**


This agent generates answers to a specific moral scenario by considering the ethical implications of the
situation.



**B.3** **Answer Evaluator**


This agent evaluates the answers generated by the Answer Generator, providing scores and feedback based
on predefined metrics such as ethical soundness, logical consistency, fairness, and feasibility.







**B.4** **Answer Revisor**


This agent revises answers that receive low scores in the evaluation step. Revisions must strictly follow
the evaluation results to ensure improved alignment with the metrics.


14


**B.5** **Settings for each Task**


**B.5.1** **Evaluator Types**

**B.5.2** **Tools**

To enhance the evaluation capabilities of each agent, we have deployed tools for each evaluator to use.
The tools are listed as follows:


  - **Output Tool (All Evaluators)** : A tool for outputting thoughts, allowing the model to repeatedly
think.

  - **Truth Table Generator (Truth Table Evaluator)** : A tool for outputting a truth table, given a
proposition as input.

  - **Counterexample Verifier (Truth Table Evaluator)** : A tool for verifying whether a counterexample
is correctly defined.
Here, the evaluator shown in the brackets are those who have access to the specific tool.


**B.6** **Good Revision Example for Moral Scenarios Task**


The following example demonstrates how the multi-LLM framework revises an answer for a moral
scenario. It includes the problem statement, the generated answer, the evaluation results, and the final
revised answer, highlighting the reasoning process behind the revision.





15


Task Metric Description



Moral Scenarios



Intent Evaluates the intentions behind actions.
Normality Evaluates how normal the action is.
Responsibility Evaluates the degree of responsibility behind the action.
Well-being Evaluates whether the action promotes well-being.
