<!-- Source: 10-TalkHier-2502.11098.pdf | Chunk 5/5 -->

Job change and employment with Baitoru For career change and employment, use
NEXT Baitoru NEXT
Aim to debut with Baitoru NEXT Start your career with Baitoru NEXT
Start your job search Take the first step in your career
Find a new workplace Discover new job opportunities
Opportunity to aim for a debut Opportunities for a successful debut


_NEXT"_ was revised to _"Find your ideal job with Baitoru NEXT"_, making the headline more appealing by
highlighting personalization and career goals.
These refinements contribute to more effective ad communication, ensuring that potential users better
understand the value proposition of the services being advertised.


**D.6** **An example of Hierarchical Refinement with Faithfulness, Fluency, Attractiveness**


_TalkHier_ employs a hierarchical refinement process where evaluators independently assess content
(faithfulness, fluency, and attractiveness) and report their findings to an evaluation team supervisor. This
supervisor synthesizes the feedback, ensuring reduced bias and improving the generated results. Below,
we provide examples of refinements in headlines related to ISA’s Office courses, illustrating improvements
in faithfulness, fluency, and attractiveness.
**Faithfulness Refinement:** Initial headline:


_Fastest qualification with ISA courses._


This headline lacked specificity and could mislead users. After refinement:


_Achieve qualification in two weeks with ISA courses._


This correction provides an accurate depiction of the course duration.


**Fluency Refinement:** Initial headline:


_ISA courses: beginner friendly._


While understandable, the phrase was somewhat unnatural. After refinement:


_Beginner-friendly ISA courses._


21


This adjustment enhances grammatical accuracy and improves readability.


**Attractiveness Refinement:** Initial headline:


_Boost skills with ISA Office courses._


This headline, though factual, lacked emotional appeal. After refinement:


_Advance your career with ISA Office courses._


This modification creates a more engaging and motivational message for potential users.


22


**E** **Subjective Experiment for the Rating in** _**TalkHier**_


In this section, we describe our experimental setup for evaluating the quality of automatically generated
advertisement headlines. Our proposed method, _TalkHier_, is a multi-agent system designed to refine
generated text by iteratively assessing and improving headlines across three key dimensions: attractiveness, fluency, and faithfulness. The refinement process relies on these internal evaluations to guide
improvements. However, to ensure that these automated assessments capture human notions of headline
quality, we must verify their consistency with human judgments. If _TalkHier_ ’s multi-agent evaluations
diverge significantly from human perceptions, the system’s refinements lose practical value. We therefore
compare _TalkHier_ against a baseline, generating headlines using both methods. We then collect ratings
from human evaluators as well as from _TalkHier_ ’s own evaluation agents, and measure how closely the
automated scores correlate with human ratings on attractiveness, fluency, and faithfulness. Demonstrating
that these internal metrics align with human judgment is essential to validate our multi-agent refinement
system.


**E.1** **Setup and Data Collection**


We selected five distinct products, each of which serves as a target for generating advertisement headlines.
For each product, we generated five headlines using _TalkHier_ (for a total of 25) and five headlines using
the baseline model (another 25), thus obtaining **50 headlines** in total.
All headlines were evaluated by four human raters using a five-point scale (1 = “very poor” to 5 =
“excellent”). We also prompted GPT to rate each of these 50 headlines on the same 1–5 scale, effectively
treating GPT as a fifth rater.


**E.2** **Data Example**


Table 9 provides a small subset of our dataset to illustrate how the information is organized. Each row
corresponds to one generated headline and includes _(i)_ the product name or headline identifier, _(ii)_ the
method that generated it, _(iii)_ the generated text, and _(iv)_ the ratings assigned by a subset of the human
evaluators and _TalkHier_ . [†]


Table 9: A sample of 10 headlines for the “credit card” product (LifeCard). Five are generated by _TalkHier_, and five
by the baseline ReAct. We show partial ratings (three of the four human raters plus the _TalkHier_ evaluation team) to
illustrate how _TalkHier_ generally receives higher scores than the Baseline.


**Headline** **Method** **Generated Headline (English)** **Human1** **Human2** **Human...** _**TalkHier**_


H1_card _TalkHier_ LifeCard with No Annual Fee 4.33 4.33 ... 5
H2_card _TalkHier_ Receive Your Card in Two Business Days 5 4.66 ... 4
H3_card _TalkHier_ Earn Points for Every ¥100 You Spend 4.33 5 ... 4.33
H4_card _TalkHier_ Triple Points on Your Birthday Month 4.33 4.33 ... 5
H5_card _TalkHier_ A Card That Fits Your Lifestyle 2.33 4 ... 4


H6_card ReAct Full of Benefits, LifeCard is Here 3.66 3 ... 3
H7_card ReAct Start a New Life with LifeCard 2.33 3.66 ... 2.33
H8_card ReAct Save Smartly with LifeCard 3.66 4.33 ... 3
H9_card ReAct Shop with LifeCard 3.66 3.66 ... 3
H10_card ReAct Trusted and Reliable Life Card 3.66 4 ... 3.66


... _(remaining headlines not shown)_


As shown in Table 9, each headline in the dataset includes:


  - **Headline ID** : A unique identifier (e.g., “H1_favs”) that can encode product information.

  - **Method** : Either _TalkHier_ (proposed method) or “Baseline” (GPT-4.0 or other reference model).

  - **Generated Headline** : The actual text shown to human raters.

  - **Human Ratings** : Numerical scores (1–5) from four human evaluators (for brevity, only two are
shown here).

  - _**TalkHier**_ **Rating** : _TalkHier_ ’s rating, also on a 1–5 scale.


†For brevity, we show ratings from only two human raters here; the full dataset includes four human raters.


23


**E.3** **Evaluation Metrics**


To determine whether _TalkHier_ evaluates headlines similarly to human raters, we compute both **(i)** the
correlation (Pearson and Spearman) between _TalkHier_ ’s ratings and the average human ratings, and **(ii)**
the Intraclass Correlation Coefficient (ICC), treating _TalkHier_ as an additional rater alongside the four
humans. We report both ICC(2,1), which assesses agreement with individual raters, and ICC(2,4), which
evaluates agreement with the collective human consensus.


**E.4** **Evaluation Results**


We quantitatively assessed how closely _TalkHier_ ’s ratings align with the human evaluations using both
**(i)** correlations (Pearson and Spearman) between _TalkHier_ ’s ratings and the _average_ ratings of the four
human evaluators, and **(ii)** the Intraclass Correlation Coefficient (ICC) treating _TalkHier_ as an additional
rater. Table 10 summarizes our main findings.


Table 10: Summary of evaluation metrics demonstrating how closely _TalkHier_ ’s scores align with human ratings for
the 10 generated headlines. Confidence intervals (CIs) are not reported due to the small sample size.


**Metric** **Value** **p-value**


Pearson Correlation 0.67 0.036
Spearman Correlation 0.68 0.030
ICC (2,1) 0.23               ICC (2,4) 0.33               

**Correlation Analysis.** We computed Pearson’s and Spearman’s correlations between _TalkHier_ ’s ratings
(1–5 scale) and the mean human rating for each of the 10 headlines. Both correlation coefficients, shown
in Table 10, indicate a moderate positive relationship (Pearson: 0 _._ 67, Spearman: 0 _._ 68), and both are
statistically significant ( _p <_ 0 _._ 05).
**Intraclass Correlation (ICC).** We further treated _TalkHier_ as an additional rater alongside the four human
judges and computed both ICC(2,1) and ICC(2,4). As reported in Table 10, ICC(2,1) is 0 _._ 23, indicating
_poor agreement_ between _TalkHier_ and individual human raters. However, ICC(2,4) is higher at 0 _._ 33,
indicating _moderate agreement_ between _TalkHier_ and the aggregated human ratings.
**Why ICC(2,4) is higher than ICC(2,1)?** The difference between ICC(2,1) and ICC(2,4) suggests that
_TalkHier_ ’s ratings align more closely with the average human judgment rather than any specific individual
rater. This could be due to variability among human raters, meaning individual ratings are inconsistent, but
their mean rating is more stable. Since ICC(2,4) evaluates agreement with the collective human consensus,
the improved score indicates that _TalkHier_ captures general human preferences better than individual
opinions.
**Overall Implications.** These results suggest that while _TalkHier_ does not perfectly replicate individual
human ratings, it effectively captures a broader human consensus. Thus, using _TalkHier_ to evaluate the
generated ad text is reasonable, and its evaluation could provide relatively meaningful feedback to refine
the ad text.


24


