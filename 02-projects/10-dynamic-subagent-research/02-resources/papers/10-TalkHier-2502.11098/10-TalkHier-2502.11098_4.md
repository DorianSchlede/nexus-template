<!-- Source: 10-TalkHier-2502.11098.pdf | Chunk 4/5 -->


13


**B** **Prompt Design and Work Flow for Tasks in MMLU**


In this section, we describe the prompt design for evaluating and revising responses for each MMLU task.
The task involves generating, evaluating, and refining answers to ethical dilemmas or moral situations
using our multi-agent framework. Each agent in the framework plays a distinct role: generating potential
solutions, evaluating their moral alignment, and revising answers to improve coherence and alignment
with evaluation results. The prompts used for each agent are detailed below.


**B.1** **Initial Prompt**


The following is the prompt given to the supervisor at the beginning.



**B.2** **Answer Generator**


This agent generates answers to a specific moral scenario by considering the ethical implications of the
situation.



**B.3** **Answer Evaluator**


This agent evaluates the answers generated by the Answer Generator, providing scores and feedback based
on predefined metrics such as ethical soundness, logical consistency, fairness, and feasibility.







**B.4** **Answer Revisor**


This agent revises answers that receive low scores in the evaluation step. Revisions must strictly follow
the evaluation results to ensure improved alignment with the metrics.


14


**B.5** **Settings for each Task**


**B.5.1** **Evaluator Types**

**B.5.2** **Tools**

To enhance the evaluation capabilities of each agent, we have deployed tools for each evaluator to use.
The tools are listed as follows:


  - **Output Tool (All Evaluators)** : A tool for outputting thoughts, allowing the model to repeatedly
think.

  - **Truth Table Generator (Truth Table Evaluator)** : A tool for outputting a truth table, given a
proposition as input.

  - **Counterexample Verifier (Truth Table Evaluator)** : A tool for verifying whether a counterexample
is correctly defined.
Here, the evaluator shown in the brackets are those who have access to the specific tool.


**B.6** **Good Revision Example for Moral Scenarios Task**


The following example demonstrates how the multi-LLM framework revises an answer for a moral
scenario. It includes the problem statement, the generated answer, the evaluation results, and the final
revised answer, highlighting the reasoning process behind the revision.





15


Task Metric Description



Moral Scenarios



Intent Evaluates the intentions behind actions.
Normality Evaluates how normal the action is.
Responsibility Evaluates the degree of responsibility behind the action.
Well-being Evaluates whether the action promotes well-being.



Mathematics Evaluates mathematical correctness and calculations.
College Physics
Physics Evaluates the accuracy of physical principles applied.



Machine Learning


Formal Logic


US Foreign Policy



Answer Consistency Checks underlying assumptions in models and
methodologies.
Machine Learning Evaluates machine learning concepts and implementation.
Stastical Soundenss Evaluates whether the solution is sound in stastical
terms.


Logical Argument Evaluates whether the arguments used are logically
correct.
Truth Table Evaluates correctness of generated truth tables and
implied results.
Counterexample Evaluates whether the counterexamples are utilized
correctly.
Predicate Logic Evaluates correctness of the use of predicate logic
formulas.
Formal Logic Evaluates correctness in terms of formal logic.


Factual Accuracy Evaluates whether the facts implied by the answer
are correct.
Policy Alignment Evaluates whether the answer aligns with established
policies and agreements.
Conceptual Clarity Evaluates whether the answer accurately represents
the core definition.


Table 6: List of evaluators allocated for each task


16


**C** **Prompt Design and Work Flow for for WikiQA**


In this section, we provide a detailed example of how the multi-agent framework processes a WikiQA
task, specifically the question: "What are points on a mortgage?" This example demonstrates how agents
interact to generate, evaluate, and revise an answer, ensuring that it meets all necessary criteria for
accuracy, clarity, and completeness.


**C.1** **Initial Question**


The user asks the question: "What are points on a mortgage?"


**C.2** **Step 1: Answer Generation**


The first step involves the **Answer Generator** agent, which is tasked with generating a detailed response
to the question. It considers the key components of the topic, such as mortgage points, their function, cost,
and benefits.


The **Answer Generator** produces the following response:


"Points on a mortgage are upfront fees paid to the lender at the time of closing, which can lower
the interest rate or cover other loan-related costs, with each point typically costing 1


**C.3** **Step 2: Evaluation by the ETeam Supervisor**


The **ETeam Supervisor** evaluates the answer based on two primary metrics: **Simplicity** and **Coverage** .
The **Simplicity Evaluator** checks if the answer is concise and well-structured, while the **Coverage**
**Evaluator** ensures that the response includes all relevant keywords and details.


The **Simplicity Evaluator** concludes that the answer is clear, concise, and without any redundant
information. The sentence is appropriate in length, neither too short nor too long.
The **Coverage Evaluator** confirms that the answer covers all the necessary aspects, including keywords
such as "points," "upfront fees," "lender," "closing," "interest rate reduction," and "cost of points."


17


**C.4** **Step 3: Revisions by the Answer Revisor**


Despite the high evaluation scores, the **Coverage Evaluator** suggests a slight revision for clarity. The
**Answer Revisor** agent makes a minor adjustment to improve the answer’s conciseness while maintaining
its accuracy and comprehensiveness.


The **Answer Revisor** makes the following revision:


"Points on a mortgage are fees paid upfront to the lender at closing, which can lower the interest
rate or cover other loan-related costs, with each point usually costing 1


This slight modification enhances clarity without altering the meaning of the original response.


**C.5** **Step 4: Final Evaluation**


The revised answer is re-evaluated by the **ETeam Supervisor**, and all metrics receive top scores. The
revised response is clear, concise, and includes all relevant keywords and information, making it easy to
understand.


**C.6** **Final Answer**


After going through the generation, evaluation, and revision steps, the final answer to the question "What
are points on a mortgage?" is:


"Points on a mortgage are fees paid upfront to the lender at closing, which can lower the interest
rate or cover other loan-related costs, with each point usually costing 1


**Evaluation Summary** : - Simplicity: The answer is clear, concise, and free of redundancies. - Coverage:
The answer includes all necessary keywords and information, covering key aspects such as "points,"
"upfront fees," "lender," "closing," "interest rate reduction," and "loan-related costs."
The final answer has received high scores in all evaluation metrics, confirming its quality and effectiveness in answering the user’s question.


**C.7** **BERT and ROUGE Scores**


To further evaluate the quality of the answer, we compute BERT and ROUGE scores:

 - BERT Score: 0.5156

 - ROUGE Score: 0.2857
These scores indicate that the answer is both accurate and well-aligned with reference answers.


18


**D** **Prompt Design, Workflow and Revision Examples for Evaluating the Camera Dataset**


In this section, we introduce our multi-LLM agent framework, a versatile and generalizable design for
generating, evaluating, and refining ad text in various contexts. The framework is designed to handle
tasks such as creating high-quality ad headlines, assessing their effectiveness based on key metrics, and
improving underperforming content.


Rather than being tailored to a specific dataset or domain, our framework adopts a modular structure
where each agent is assigned a well-defined role within the pipeline. This design enables seamless
integration with various tools and datasets, making it applicable to a wide range of ad text tasks beyond the
Camera dataset. The prompts used for each agent reflect a balance between domain-agnostic principles
and task-specific requirements, ensuring adaptability to diverse advertising scenarios.


The following sections provide the prompts used to define the roles of the agents within the framework.


**D.1** **Japanese Ad Headlines Generator**


This agent generates high-quality Japanese ad headlines that are fluent, faithful, and attractive. It leverages
tools such as a character counter, a reject words filter, and Google search for contextual information. The
specific prompt for this agent is:





**D.2** **Ad Headlines Evaluator**


This agent evaluates the generated headlines based on three metrics: Faithfulness, Fluency, and Attractiveness. The specific prompt for this agent is:







**D.3** **Ad Headlines Reviser**


This agent revises low-scoring headlines to improve their Faithfulness, Fluency, and Attractiveness scores.
The specific prompt for this agent is:


19


**D.4** **Tools Used in the Camera Ad Text Experiment**


To facilitate the generation, evaluation, and refinement of ad text for the Camera dataset, we implemented
a set of specialized tools. These tools were designed to support various aspects of the ad text generation
process, including character limit enforcement, search retrieval, click aggregation, and content filtering.
Below is a description of each tool:


  - **Character Counter (Generator and Revisor)** : A utility for counting the number of characters
in a given sentence. It takes as input a list of lists in the form [[sentence, character limit],

[sentence, character limit], ...], where each sentence is checked against a predefined
character limit.


  - **Google Search (Generator)** : A search engine tool used to retrieve real-time information from the
web. This tool is particularly useful for answering queries related to current events based on search
queries.


  - **Output Tool (All Agents)** : A simple logging tool that allows agents to write their thoughts. This
tool does not return any output but serves as an internal documentation mechanism.


  - **Bad Performance Retriever (Revisor)** : A quality control tool that checks whether generated
headlines or descriptions resemble undesirable outputs. It takes as input a dictionary in the form
{"Headline": [headline1, ...], "Description": [description1, ...]} and returns a list
of flagged items if any match known bad examples.


  - **Reject Word Checker (Generator and Revisor)** : A filtering tool that verifies whether a sentence
contains prohibited words. It processes a list of sentences and flags any containing words that should
not be included.
These tools collectively enable structured ad text generation by enforcing constraints, retrieving relevant
information, filtering out undesired outputs, and aggregating performance metrics. Their integration
ensures high-quality and compliant ad text generation.


**D.5** **Ad Headline Revisions with Highlights**


Tables 7 and 8 present two cases of translated ad headline revisions: one for educational ads and the
other for employment-related ads. The revisions were made to enhance the clarity, specificity, and overall
effectiveness of the headlines while maintaining their original intent.
In these tables, text highlighted in green represents a **good revision**, where improvements were made to
make the ad more engaging, informative, or persuasive. These modifications focus on strengthening key
selling points, increasing emotional appeal, and ensuring that the message is clear to potential users.
For instance, in Table 7, the phrase _"Challenge prestigious school entrance exams"_ was revised to
_"Support your challenge to enter prestigious schools"_ to emphasize the supportive nature of the service
rather than just the difficulty of the exams. Similarly, in Table 8, the phrase _"Get a job with Baitoru_


20


Table 7: Revisions of Educational Ad Headlines with Highlights (Original: Japanese, Translated: English). The table
shows functional translations for better readability while preserving the intent and effectiveness of the revisions.


**Before Revision** **After Revision**



Challenge prestigious school entrance ex- Support your challenge to enter prestigious
ams schools

Guidance from professional home tutors High-quality guidance from professional
home tutors
We provide sure-win exam preparation We provide reliable exam preparation
Improve grades with a customized curricu- Boost grades with a customized curriculum
lum



Challenge prestigious school entrance exams



Boost grades with a customized curriculum



Prepare for exams online Effective exam preparation online


Table 8: Revisions of Employment Ad Headlines with Highlights (Original: Japanese, Translated: English). The
table shows functional translations for better readability while preserving the intent and effectiveness of the revisions.


**Before Revision** **After Revision**


Get a job with Baitoru NEXT Find your ideal job with Baitoru NEXT
Job change and employment with Baitoru For career change and employment, use
NEXT Baitoru NEXT
Aim to debut with Baitoru NEXT Start your career with Baitoru NEXT
Start your job search Take the first step in your career
Find a new workplace Discover new job opportunities
Opportunity to aim for a debut Opportunities for a successful debut


_NEXT"_ was revised to _"Find your ideal job with Baitoru NEXT"_, making the headline more appealing by
highlighting personalization and career goals.
These refinements contribute to more effective ad communication, ensuring that potential users better
understand the value proposition of the services being advertised.


**D.6** **An example of Hierarchical Refinement with Faithfulness, Fluency, Attractiveness**


_TalkHier_ employs a hierarchical refinement process where evaluators independently assess content
(faithfulness, fluency, and attractiveness) and report their findings to an evaluation team supervisor. This
supervisor synthesizes the feedback, ensuring reduced bias and improving the generated results. Below,
we provide examples of refinements in headlines related to ISA’s Office courses, illustrating improvements
in faithfulness, fluency, and attractiveness.
**Faithfulness Refinement:** Initial headline:


_Fastest qualification with ISA courses._


This headline lacked specificity and could mislead users. After refinement:


_Achieve qualification in two weeks with ISA courses._


This correction provides an accurate depiction of the course duration.


**Fluency Refinement:** Initial headline:


_ISA courses: beginner friendly._


While understandable, the phrase was somewhat unnatural. After refinement:


_Beginner-friendly ISA courses._


21


This adjustment enhances grammatical accuracy and improves readability.


**Attractiveness Refinement:** Initial headline:


_Boost skills with ISA Office courses._


This headline, though factual, lacked emotional appeal. After refinement:


_Advance your career with ISA Office courses._


This modification creates a more engaging and motivational message for potential users.


22


**E** **Subjective Experiment for the Rating in** _**TalkHier**_


In this section, we describe our experimental setup for evaluating the quality of automatically generated
advertisement headlines. Our proposed method, _TalkHier_, is a multi-agent system designed to refine
generated text by iteratively assessing and improving headlines across three key dimensions: attractiveness, fluency, and faithfulness. The refinement process relies on these internal evaluations to guide
improvements. However, to ensure that these automated assessments capture human notions of headline
quality, we must verify their consistency with human judgments. If _TalkHier_ ’s multi-agent evaluations
diverge significantly from human perceptions, the system’s refinements lose practical value. We therefore
compare _TalkHier_ against a baseline, generating headlines using both methods. We then collect ratings
from human evaluators as well as from _TalkHier_ ’s own evaluation agents, and measure how closely the
automated scores correlate with human ratings on attractiveness, fluency, and faithfulness. Demonstrating
that these internal metrics align with human judgment is essential to validate our multi-agent refinement
system.


**E.1** **Setup and Data Collection**


We selected five distinct products, each of which serves as a target for generating advertisement headlines.
For each product, we generated five headlines using _TalkHier_ (for a total of 25) and five headlines using
the baseline model (another 25), thus obtaining **50 headlines** in total.
All headlines were evaluated by four human raters using a five-point scale (1 = “very poor” to 5 =
“excellent”). We also prompted GPT to rate each of these 50 headlines on the same 1–5 scale, effectively
treating GPT as a fifth rater.

