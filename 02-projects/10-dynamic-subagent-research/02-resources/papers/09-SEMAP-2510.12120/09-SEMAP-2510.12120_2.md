<!-- Source: 09-SEMAP-2510.12120.pdf | Chunk 2/2 -->

This paper presents SEMAP, a protocol-layer methodology
for addressing common failures in multi-agent LLMs, including under-specification, coordination misalignment, and inadequate verification. SEMAP instantiates three SE-informed
principles, behavioral contracts, structured messaging, and
lifecycle-guided execution with verification, supporting both
centralized and decentralized workflows. Empirical results
across diverse SE tasks demonstrate that SEMAP substantially
improves system robustness. In function-level and deploymentlevel code development, it achieves up to 69.6% and 56.7%
reductions in total failures, with the greatest gains observed
in mitigating under-specification and coordination issues. In
vulnerability detection, SEMAP consistently reduces failure
rates across Python and C/C++ tasks up to 47.4%, while
promoting better agent alignment in decentralized workflows.
To strengthen validity, future experiments will be scaled
to larger datasets, agent populations, and longer workflows,
and compared against more baselines, including single-agent
LLMs and domain-specific detectors. Ablation studies will isolate the impact of contracts, messaging, and lifecycle control.
Future work also includes measuring resource overhead, enabling cross-agent tool use, adding formal protocol correctness
verification, and releasing artifacts for reproducibility.


REFERENCES


[1] SlashData, “Global developer population trends 2025: How
many developers are there?” 2025, accessed: 202505-15. [Online]. Available: [https://www.slashdata.co/post/](https://www.slashdata.co/post/global-developer-population-trends-2025-how-many-developers-are-there)
[global-developer-population-trends-2025-how-many-developers-are-there](https://www.slashdata.co/post/global-developer-population-trends-2025-how-many-developers-are-there)

[2] J. Cai _et al._, “Exploring the improvement of evolutionary computation
via large language models,” in _Proceedings of the Genetic and Evolu-_
_tionary Computation Conference Companion_, 2024, pp. 83–84.

[3] J. Fan _et al._, “Coding latent concepts: a human and llm-coordinated
content analysis procedure,” _Communication Research Reports_, vol. 41,
no. 5, pp. 324–334, 2024.

[4] J. Li _et al._, “Generative ai for self-adaptive systems: State of the art and
research roadmap,” _ACM Transactions on Autonomous and Adaptive_
_Systems_, vol. 19, no. 3, pp. 1–60, 2024.

[5] Y. Sun _et al._, “Semirald: A semi-supervised hybrid language model
for robust anomalous log detection,” _Information_ _and_ _Software_
_Technology_ [, vol. 183, p. 107743, 2025. [Online]. Available: https:](https://www.sciencedirect.com/science/article/pii/S0950584925000825)
[//www.sciencedirect.com/science/article/pii/S0950584925000825](https://www.sciencedirect.com/science/article/pii/S0950584925000825)

[6] Z. Yang _et al._, “Exploring and unleashing the power of large language
models in automated code translation,” _Proc. ACM Softw. Eng._, vol. 1,
[no. FSE, Jul. 2024. [Online]. Available: https://doi.org/10.1145/3660778](https://doi.org/10.1145/3660778)

[7] Z. Mao _et_ _al._, “Hybrid privacy policy-code consistency check
using knowledge graphs and llms,” 2025. [Online]. Available:
[https://arxiv.org/abs/2505.11502](https://arxiv.org/abs/2505.11502)


TABLE I: Results of RQ1: Development Tasks


HumanEval ProgramDev
Failure Category GPT-4.1-nano Deepseek-V3 GPT-4.1-nano Deepseek-V3
Baseline SEMAP ∆% Baseline SEMAP ∆% Baseline SEMAP ∆% Baseline SEMAP ∆%
Under-specification 137 39 71.5 63 **17** 73.0 48 46 4.1 39 **18** 53.8
Inter-Agent Misalignment 27 5 81.5 10 **3** 70.0 9 9 0.0 8 **0** 100.0
Task Verification 92 48 47.8 39 **14** 64.1 46 35 23.9 20 **11** 45.0
Total 256 92 64.1 112 **34** 69.6 103 90 12.6 67 **29** 56.7


TABLE II: Results of RQ1: Vulnerability Detection Tasks


devign100 (C/C++) vudenc100 (Python)
Failure Category GPT-4.1-nano Deepseek-V3 GPT-4.1-nano Deepseek-V3
Baseline SEMAP ∆% Baseline SEMAP ∆% Baseline SEMAP ∆% Baseline SEMAP ∆%
Under-specification 12 9 25.0 5 **4** 20.0 4 **3** 25.0 16 15 6.3
Inter-Agent Misalignment 33 24 27.2 20 **18** 10.0 14 **11** 21.3 16 11 31.3
Task Verification 33 23 30.3 23 **20** 13.0 10 **6** 40.0 23 20 13.0
Total 78 56 28.2 48 **44** 8.3 38 **20** 47.4 55 46 16.4


(a) HumanEval: Under-Specification (b) HumanEval: Inter-Agent Misalignment (c) HumanEval: Task Verification


(d) ProgramDev: Under-Specification (e) ProgramDev: Inter-Agent Misalignment (f) ProgramDev: Task Verification


Fig. 2: Results of RQ2: Failures v.s. Rounds in Development Tasks



i


i



i


i



i


i



i


i



i


i


[8] J. He _et al._, “Llm-based multi-agent systems for software engineering:
Literature review, vision and the road ahead,” _ACM Transactions on_
_Software Engineering and Methodology_, 2024.

[9] D. Jin _et al._, “Mare: Multi-agents collaboration framework for requirements engineering,” _arXiv preprint arXiv:2405.03256_, 2024.

[10] M. Ataei _et al._, “Elicitron: An llm agent-based simulation framework
for design requirements elicitation,” 2024. [Online]. Available:
[https://arxiv.org/abs/2404.16045](https://arxiv.org/abs/2404.16045)

[11] H. Zhang _et al._, “A pair programming framework for code generation via
multi-plan exploration and feedback-driven refinement,” in _Proceedings_
_of the 39th IEEE/ACM International Conference on Automated Software_
_Engineering_, 2024, pp. 1319–1331.

[12] D. Zan _et al._, “Codes: Natural language to code repository via
[multi-layer sketch,” 2024. [Online]. Available: https://arxiv.org/abs/](https://arxiv.org/abs/2403.16443)
[2403.16443](https://arxiv.org/abs/2403.16443)

[13] Z. Mao _et al._, “Multi-role consensus through llms discussions for
vulnerability detection,” in _2024 IEEE 24th International Conference on_
_Software Quality, Reliability, and Security Companion (QRS-C)_ . IEEE,
2024, pp. 1318–1319.

[14] S. Hu _et_ _al._, “Large language model-powered smart contract
vulnerability detection: New perspectives,” 2023. [Online]. Available:
[https://arxiv.org/abs/2310.01152](https://arxiv.org/abs/2310.01152)

[15] H. Wang _et al._, “Intervenor: Prompting the coding ability of large



i


i



i


i



i


i


language models with the interactive chain of repair,” _arXiv preprint_
_arXiv:2311.09868_, 2023.

[16] W. Tao _et al._, “Magis: Llm-based multi-agent framework for github issue
[resolution,” 2024. [Online]. Available: https://arxiv.org/abs/2403.17927](https://arxiv.org/abs/2403.17927)

[17] D. Cohen _et al._, “An introduction to agile methods,” _Advances in_
_computers_, vol. 62, no. 03, pp. 1–66, 2004.

[18] K. Petersen _et al._, “The waterfall model in large-scale development,”
in _International Conference on Product-Focused Software Process Im-_
_provement_ . Springer, 2009, pp. 386–400.

[19] M. Cemri _et al._, “Why do multi-agent llm systems fail?” _arXiv preprint_
_arXiv:2503.13657_, 2025.

[20] Y. Yang _et al._, “A survey of ai agent protocols,” _arXiv preprint_
_arXiv:2504.16736_, 2025.

[21] M. Chen _et al._, “Evaluating large language models trained on code,”
_arXiv preprint arXiv:2107.03374_, 2021.

[22] Y. Zhou _et al._, “Devign: Effective vulnerability identification by learning comprehensive program semantics via graph neural networks,” in
_Advances in Neural Information Processing Systems (NeurIPS)_, vol. 32,
2019, pp. 10 197–10 207.

[23] H.-C. Tran _et al._, “Detectvul: A statement-level code vulnerability
detection for python,” _Future Generation Computer Systems_, p.
107504, 2024. [Online]. Available: [https://www.sciencedirect.com/](https://www.sciencedirect.com/science/article/pii/S0167739X24004680)
[science/article/pii/S0167739X24004680](https://www.sciencedirect.com/science/article/pii/S0167739X24004680)


