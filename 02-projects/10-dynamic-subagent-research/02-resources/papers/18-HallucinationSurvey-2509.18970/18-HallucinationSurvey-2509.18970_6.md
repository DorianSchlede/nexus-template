<!-- Source: 18-HallucinationSurvey-2509.18970.pdf | Chunk 6/8 -->


[113] Ziheng Huang, Sebastian Gutierrez, Hemanth Kamana, and Stephen
MacNeil. Memory sandbox: Transparent and interactive memory
management for conversational agents. In _Adjunct Proceedings of_
_the 36th Annual ACM Symposium on User Interface Software and_
_Technology_, pages 1–3, 2023.

[114] Johnny Li, Saksham Consul, Eda Zhou, James Wong, Naila Farooqui,
Yuxin Ye, Nithyashree Manohar, Zhuxiaona Wei, Tian Wu, Ben Echols,
et al. Banishing llm hallucinations requires rethinking generalization.
_arXiv preprint arXiv:2406.17642_, 2024.

[115] Aditi Singh, Abul Ehtesham, Saket Kumar, and Tala Talaei Khoei.
Agentic retrieval-augmented generation: A survey on agentic rag. _arXiv_
_preprint arXiv:2501.09136_, 2025.

[116] Karan Singhal, Tao Tu, Juraj Gottweis, Rory Sayres, Ellery Wulczyn,
Mohamed Amin, Le Hou, Kevin Clark, Stephen R Pfohl, Heather ColeLewis, et al. Toward expert-level medical question answering with large
language models. _Nature Medicine_, pages 1–8, 2025.

[117] Yiteng Tu, Weihang Su, Yujia Zhou, Yiqun Liu, and Qingyao Ai. Rbft:
Robust fine-tuning for retrieval-augmented generation against retrieval
defects. _arXiv preprint arXiv:2501.18365_, 2025.

[118] Garima Agrawal, Tharindu Kumarage, Zeyad Alghamdi, and Huanmin
Liu. Mindful-rag: A study of points of failure in retrieval augmented
generation. _2024 2nd International Conference on Foundation and_
_Large Language Models (FLLM)_, pages 607–611, 2024.

[119] Xiaoxin He, Yijun Tian, Yifei Sun, Nitesh Chawla, Thomas Laurent,
Yann LeCun, Xavier Bresson, and Bryan Hooi. G-retriever: Retrievalaugmented generation for textual graph understanding and question
answering. _Advances in Neural Information Processing Systems_,
37:132876–132907, 2024.


JOURNAL OF L [A] TEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 17




[120] Ali Modarressi, Ayyoob Imani, Mohsen Fayyaz, and Hinrich Sch¨utze.
Ret-llm: Towards a general read-write memory for large language
models. _ArXiv_, abs/2305.14322, 2023.

[121] Wan Zhang and Jing Zhang. Hallucination mitigation for retrievalaugmented large language models: a review. _Mathematics_, 13(5):856,
2025.

[122] Chanwoong Yoon, Gangwoo Kim, Byeongguk Jeon, Sungdong Kim,
Yohan Jo, and Jaewoo Kang. Ask optimal questions: Aligning large
language models with retriever’s preference in conversation. In _Find-_
_ings of the Association for Computational Linguistics: NAACL 2025_,
pages 5899–5921, 2025.

[123] Lingrui Mei, Jiayu Yao, Yuyao Ge, Yiwei Wang, Baolong Bi, Yujun
Cai, Jiazhi Liu, Mingyu Li, Zhong-Zhi Li, Duzhen Zhang, et al. A
survey of context engineering for large language models. _arXiv preprint_
_arXiv:2507.13334_, 2025.

[124] Zhuoran Jin, Pengfei Cao, Yubo Chen, Kang Liu, Xiaojian Jiang,
Jiexin Xu, Qiuxia Li, and Jun Zhao. Tug-of-war between knowledge:
Exploring and resolving knowledge conflicts in retrieval-augmented
language models. _arXiv preprint arXiv:2402.14409_, 2024.

[125] Hanxing Ding, Liang Pang, Zihao Wei, Huawei Shen, and Xueqi
Cheng. Retrieve only when it needs: Adaptive retrieval augmentation
for hallucination mitigation in large language models. _arXiv preprint_
_arXiv:2402.10612_, 2024.

[126] Jiatai Wang, Zhiwei Xu, Di Jin, Xuewen Yang, and Tao Li. Accommodate knowledge conflicts in retrieval-augmented llms: Towards reliable
response generation in the wild. _arXiv preprint arXiv:2504.12982_,
2025.

[127] Kevin Wu, Eric Wu, and James Zou. How faithful are rag models?
quantifying the tug-of-war between rag and llms’ internal prior. _arXiv_
_e-prints_, pages arXiv–2404, 2024.

[128] Rongwu Xu, Zehan Qi, Zhijiang Guo, Cunxiang Wang, Hongru Wang,
Yue Zhang, and Wei Xu. Knowledge conflicts for llms: A survey. _arXiv_
_preprint arXiv:2403.08319_, 2024.

[129] Wujiang Xu, Zujie Liang, Kai Mei, Hang Gao, Juntao Tan, and
Yongfeng Zhang. A-mem: Agentic memory for llm agents. _arXiv_
_preprint arXiv:2502.12110_, 2025.

[130] Qitao Qin, Yucong Luo, Yihang Lu, Zhibo Chu, and Xianwei Meng.
Towards adaptive memory-based optimization for enhanced retrievalaugmented generation. _arXiv preprint arXiv:2504.05312_, 2025.

[131] Qingyue Wang, Yanhe Fu, Yanan Cao, Shuai Wang, Zhiliang Tian,
and Liang Ding. Recursively summarizing enables long-term dialogue
memory in large language models. _Neurocomputing_, 639:130193, 2025.

[132] Jinyao Guo, Chengpeng Wang, Xiangzhe Xu, Zian Su, and Xiangyu
Zhang. Repoaudit: An autonomous llm-agent for repository-level code
auditing. _ArXiv_, abs/2501.18160, 2025.

[133] Ye Ye. Task memory engine (tme): Enhancing state awareness for
multi-step llm agent tasks. _arXiv preprint arXiv:2504.08525_, 2025.

[134] Yu Wang and Xi Chen. Mirix: Multi-agent memory system for llmbased agents. _arXiv preprint arXiv:2507.07957_, 2025.

[135] Zeyu Zhang, Xiaohe Bo, Chen Ma, Rui Li, Xu Chen, Quanyu Dai,
Jieming Zhu, Zhenhua Dong, and Ji-Rong Wen. A survey on the
memory mechanism of large language model based agents, 2024.

[136] Khanh-Tung Tran, Dung Dao, Minh-Duong Nguyen, Quoc-Viet Pham,
Barry O’Sullivan, and Hoang D Nguyen. Multi-agent collaboration
mechanisms: A survey of llms. _arXiv preprint arXiv:2501.06322_, 2025.

[137] Bingyu Yan, Xiaoming Zhang, Litian Zhang, Lian Zhang, Ziyi
Zhou, Dezhuang Miao, and Chaozhuo Li. Beyond self-talk: A
communication-centric survey of llm-based multi-agent systems. _arXiv_
_preprint arXiv:2502.14321_, 2025.

[138] Giorgio Piatti, Zhijing Jin, Max Kleiman-Weiner, Bernhard Sch¨olkopf,
Mrinmaya Sachan, and Rada Mihalcea. Cooperate or collapse: Emergence of sustainable cooperation in a society of llm agents. _Advances_
_in Neural Information Processing Systems_, 37:111715–111759, 2024.

[139] Yuheng Cheng, Ceyao Zhang, Zhengwen Zhang, Xiangrui Meng,
Sirui Hong, Wenhao Li, Zihao Wang, Zekai Wang, Feng Yin, Junhua
Zhao, et al. Exploring large language model based intelligent agents:
Definitions, methods, and prospects. _arXiv preprint arXiv:2401.03428_,
2024.

[140] Luke Yoffe, Alfonso Amayuelas, and William Yang Wang. Debunc:
mitigating hallucinations in large language model agent communication
with uncertainty estimations. _arXiv preprint arXiv:2407.06426_, 2024.

[141] Mert Cemri, Melissa Z Pan, Shuyi Yang, Lakshya A Agrawal, Bhavya
Chopra, Rishabh Tiwari, Kurt Keutzer, Aditya Parameswaran, Dan
Klein, Kannan Ramchandran, et al. Why do multi-agent llm systems
fail? _arXiv preprint arXiv:2503.13657_, 2025.

[142] Shaokun Zhang, Ming Yin, Jieyu Zhang, Jiale Liu, Zhiguang Han,
Jingyang Zhang, Beibin Li, Chi Wang, Huazheng Wang, Yiran Chen,



et al. Which agent causes task failures and when? on automated failure attribution of llm multi-agent systems. _arXiv preprint_
_arXiv:2505.00212_, 2025.

[143] Guibin Zhang, Yanwei Yue, Zhixun Li, Sukwon Yun, Guancheng Wan,
Kun Wang, Dawei Cheng, Jeffrey Xu Yu, and Tianlong Chen. Cut the
crap: An economical communication pipeline for LLM-based multiagent systems. In _The Thirteenth International Conference on Learning_
_Representations_, 2025.

[144] Zhexuan Wang, Yutong Wang, Xuebo Liu, Liang Ding, Miao Zhang,
Jie Liu, and Min Zhang. Agentdropout: Dynamic agent elimination for
token-efficient and high-performance llm-based multi-agent collaboration. _arXiv preprint arXiv:2503.18891_, 2025.

[145] Wei Liu, Chenxi Wang, YiFei Wang, Zihao Xie, Rennai Qiu, Yufan
Dang, Zhuoyun Du, Weize Chen, Cheng Yang, and Chen Qian.
Autonomous agents for collaborative task under information asymmetry. In _The Thirty-eighth Annual Conference on Neural Information_
_Processing Systems_, 2024.

[146] Haotian Wang, Xiyuan Du, Weijiang Yu, Qianglong Chen, Kun Zhu,
Zheng Chu, Lian Yan, and Yi Guan. Learning to break: Knowledgeenhanced reasoning in multi-agent debate system. _Neurocomputing_,
618:129063, 2025.

[147] Abul Ehtesham, Aditi Singh, Gaurav Kumar Gupta, and Saket Kumar. A survey of agent interoperability protocols: Model context
protocol (mcp), agent communication protocol (acp), agent-to-agent
protocol (a2a), and agent network protocol (anp). _arXiv preprint_
_arXiv:2505.02279_, 2025.

[148] Yingxuan Yang, Huacan Chai, Yuanyi Song, Siyuan Qi, Muning Wen,
Ning Li, Junwei Liao, Haoyi Hu, Jianghao Lin, Gaowei Chang, et al. A
survey of ai agent protocols. _arXiv preprint arXiv:2504.16736_, 2025.

[149] In Gim, Seung-seob Lee, and Lin Zhong. Asynchronous llm function
calling. _arXiv preprint arXiv:2412.07017_, 2024.

[150] Gonzalo Gonzalez-Pumariega, Leong Su Yean, Neha Sunkara, and
Sanjiban Choudhury. Robotouille: An asynchronous planning benchmark for LLM agents. In _The Thirteenth International Conference on_
_Learning Representations_, 2025.

[151] Gagan Bansal, Jennifer Wortman Vaughan, Saleema Amershi, Eric
Horvitz, Adam Fourney, Hussein Mozannar, Victor Dibia, and Daniel S
Weld. Challenges in human-agent communication. _arXiv preprint_
_arXiv:2412.10380_, 2024.

[152] Yingxuan Yang, Qiuying Peng, Jun Wang, and Weinan Zhang. Multillm-agent systems: Techniques and business perspectives. _arXiv_
_preprint arXiv:2411.14033_, 2024.

[153] Zhao Wang, Sota Moriyama, Wei-Yao Wang, Briti Gangopadhyay,
and Shingo Takamatsu. Talk structurally, act hierarchically: A collaborative framework for llm multi-agent systems. _arXiv preprint_
_arXiv:2502.11098_, 2025.

[154] Zijun Liu, Yanzhe Zhang, Peng Li, Yang Liu, and Diyi Yang. A
dynamic LLM-powered agent network for task-oriented agent collaboration. In _First Conference on Language Modeling_, 2024.

[155] Dae Cheol Kwon and Xinyu Zhang. Cp-agentnet: Autonomous and
explainable communication protocol design using generative agents.
_arXiv preprint arXiv:2503.17850_, 2025.

[156] Weize Chen, Yusheng Su, Jingwei Zuo, Cheng Yang, Chenfei Yuan,
Chi-Min Chan, Heyang Yu, Yaxi Lu, Yi-Hsin Hung, Chen Qian, Yujia
Qin, Xin Cong, Ruobing Xie, Zhiyuan Liu, Maosong Sun, and Jie
Zhou. Agentverse: Facilitating multi-agent collaboration and exploring
emergent behaviors. In _The Twelfth International Conference on_
_Learning Representations_, 2024.

[157] Mingchen Zhuge, Wenyi Wang, Louis Kirsch, Francesco Faccio,
Dmitrii Khizbullin, and J¨urgen Schmidhuber. GPTSwarm: Language
agents as optimizable graphs. In _Forty-first International Conference_
_on Machine Learning_, 2024.

[158] Shoucheng Song, Youfang Lin, Sheng Han, Chang Yao, Hao Wu, Shuo
Wang, and Kai Lv. Code: Communication delay-tolerant multi-agent
collaboration via dual alignment of intent and timeliness. _arXiv preprint_
_arXiv:2501.05207_, 2025.

[159] RM Aratchige and WMKS Ilmini. Llms working in harmony: A survey
on the technological aspects of building effective llm-based multi agent
systems. _arXiv preprint arXiv:2504.01963_, 2025.

[160] Kaiqu Liang, Zixu Zhang, and Jaime F Fisac. Introspective planning:
Aligning robots’ uncertainty with inherent task ambiguity. _Advances_
_in Neural Information Processing Systems_, 37:71998–72031, 2024.

[161] Kelin Fu and Kaigui Bian. Knowmap: Efficient knowledge-driven task
adaptation for llms. _arXiv preprint arXiv:2506.19527_, 2025.

[162] Siyu Yuan, Kaitao Song, Jiangjie Chen, Xu Tan, Yongliang Shen, Ren
Kan, Dongsheng Li, and Deqing Yang. Easytool: Enhancing llm-based


JOURNAL OF L [A] TEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 18



agents with concise tool instruction. _arXiv preprint arXiv:2401.06201_,
2024.

[163] Cheng-Yu Hsieh, Si-An Chen, Chun-Liang Li, Yasuhisa Fujii, Alexander Ratner, Chen-Yu Lee, Ranjay Krishna, and Tomas Pfister. Tool
documentation enables zero-shot tool-usage with large language models. _arXiv preprint arXiv:2308.00675_, 2023.

[164] Ibrahim Abdelaziz, Kinjal Basu, Mayank Agarwal, Sadhana Kumaravel, Matthew Stallone, Rameswar Panda, Yara Rizk, GP Shrivatsa
Bhargav, Maxwell Crouse, Chulaka Gunasekara, et al. Granite-function
calling model: Introducing function calling abilities via multi-task
learning of granular tasks. In _Proceedings of the 2024 Conference on_
_Empirical Methods in Natural Language Processing: Industry Track_,
pages 1131–1139, 2024.

[165] Yuhao Dong, Zuyan Liu, Hai-Long Sun, Jingkang Yang, Winston Hu,
Yongming Rao, and Ziwei Liu. Insight-v: Exploring long-chain visual
reasoning with multimodal large language models. In _Proceedings of_
_the Computer Vision and Pattern Recognition Conference_, pages 9062–
9072, 2025.

[166] Kun Huang, Weikai Xu, Yuxuan Liu, Quandong Wang, Pengzhi Gao,
Wei Liu, Jian Luan, Bin Wang, and Bo An. Enhance mobile agents
thinking process via iterative preference learning. _arXiv preprint_
_arXiv:2505.12299_, 2025.

[167] Xiaoxiao Long, Qingrui Zhao, Kaiwen Zhang, Zihao Zhang, Dingrui
Wang, Yumeng Liu, Zhengjie Shu, Yi Lu, Shouzheng Wang, Xinzhe
Wei, et al. A survey: Learning embodied intelligence from physical
simulators and world models. _arXiv preprint arXiv:2507.00917_, 2025.

[168] Jonathan Richens, David Abel, Alexis Bellot, and Tom Everitt. General
agents need world models. _arXiv preprint arXiv:2506.01622_, 2025.

[169] Zhiting Hu and Tianmin Shu. Language models, agent models, and
world models: The law for machine reasoning and planning. _arXiv_
_preprint arXiv:2312.05230_, 2023.

[170] Hongxin Zhang, Zeyuan Wang, Qiushi Lyu, Zheyuan Zhang, Sunli
Chen, Tianmin Shu, Behzad Dariush, Kwonjoon Lee, Yilun Du, and
Chuang Gan. Combo: compositional world models for embodied multiagent cooperation. _arXiv preprint arXiv:2404.10775_, 2024.

[171] Zhihua Duan and Jialin Wang. Enhancing multi-agent consensus
through third-party llm integration: Analyzing uncertainty and mitigating hallucinations in large language models. _arXiv preprint_
_arXiv:2411.16189_, 2024.

[172] Hyungjoo Chae, Namyoung Kim, Kai Tzu-iunn Ong, Minju Gwak,
Gwanwoo Song, Jihoon Kim, Sunghwan Kim, Dongha Lee, and
Jinyoung Yeo. Web agents with world models: Learning and leveraging environment dynamics in web navigation. _arXiv preprint_
_arXiv:2410.13232_, 2024.

[173] Xuan Yao, Junyu Gao, and Changsheng Xu. Navmorph: A selfevolving world model for vision-and-language navigation in continuous
environments. _arXiv preprint arXiv:2506.23468_, 2025.

[174] Shuofei Qiao, Runnan Fang, Ningyu Zhang, Yuqi Zhu, Xiang Chen,
Shumin Deng, Yong Jiang, Pengjun Xie, Fei Huang, and Huajun Chen.
Agent planning with world knowledge model. _Advances in Neural_
_Information Processing Systems_, 37:114843–114871, 2024.

[175] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei
Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. Chain-of-thought
prompting elicits reasoning in large language models. _Advances in_
_neural information processing systems_, 35:24824–24837, 2022.

[176] Kumar Manas, Stefan Zwicklbauer, and Adrian Paschke. Cot-tl: Lowresource temporal knowledge representation of planning instructions
using chain-of-thought reasoning. In _2024 IEEE/RSJ International_
_Conference on Intelligent Robots and Systems (IROS)_, pages 9636–
9643. IEEE, 2024.

[177] Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan
Narang, Aakanksha Chowdhery, and Denny Zhou. Self-consistency
improves chain of thought reasoning in language models. _arXiv_
_preprint arXiv:2203.11171_, 2022.

[178] Ishika Singh, Valts Blukis, Arsalan Mousavian, Ankit Goyal, Danfei
Xu, Jonathan Tremblay, Dieter Fox, Jesse Thomason, and Animesh
Garg. Progprompt: Generating situated robot task plans using large
language models. In _2023 IEEE International Conference on Robotics_
_and Automation (ICRA)_, pages 11523–11530. IEEE, 2023.

[179] Nikolai Goncharov and Donald Dansereau. Segment anything in
light fields for real-time applications via constrained prompting. In
_Proceedings of the Winter Conference on Applications of Computer_
_Vision_, pages 1490–1496, 2025.

[180] Shangfeng Chen, Xiayang Shi, Pu Li, Yinlin Li, and Jingjing Liu.
Refining translations with llms: A constraint-aware iterative prompting
approach. _arXiv preprint arXiv:2411.08348_, 2024.




[181] Jiarui Wu, Zhuo Liu, and Hangfeng He. Mitigating hallucinations in
multimodal spatial relations through constraint-aware prompting. _arXiv_
_preprint arXiv:2502.08317_, 2025.

[182] Yunzhi Yao, Peng Wang, Bozhong Tian, Siyuan Cheng, Zhoubo Li,
Shumin Deng, Huajun Chen, and Ningyu Zhang. Editing large
language models: Problems, methods, and opportunities. In _The 2023_
_Conference on Empirical Methods in Natural Language Processing_ .

[183] Xiaopeng Li, Shasha Li, Shezheng Song, Jing Yang, Jun Ma, and
Jie Yu. Pmet: Precise model editing in a transformer. _ArXiv_,
abs/2308.08742, 2023.

[184] Nicola De Cao, Wilker Aziz, and Ivan Titov. Editing factual knowledge
in language models. In _Proceedings of the 2021 Conference on_
_Empirical Methods in Natural Language Processing_, pages 6491–6506,
2021.

[185] Nianwen Si, Hao Zhang, Heyu Chang, Wenlin Zhang, Dan Qu, and
Weiqiang Zhang. Knowledge unlearning for llms: Tasks, methods, and
challenges. _arXiv preprint arXiv:2311.15766_, 2023.

[186] Guangzhi Sun, Potsawee Manakul, Xiao Zhan, and Mark Gales.
Unlearning vs. obfuscation: Are we truly removing knowledge? _arXiv_
_preprint arXiv:2505.02884_, 2025.

[187] Jiaao Chen and Diyi Yang. Unlearn what you want to forget: Efficient
unlearning for llms. In _The 2023 Conference on Empirical Methods in_
_Natural Language Processing_ .

[188] Gabriel Ilharco, Marco Tulio Ribeiro, Mitchell Wortsman, Ludwig
Schmidt, Hannaneh Hajishirzi, and Ali Farhadi. Editing models with
task arithmetic. In _The Eleventh International Conference on Learning_
_Representations_ .

[189] Phuc H Le-Khac, Graham Healy, and Alan F Smeaton. Contrastive representation learning: A framework and review. _Ieee Access_, 8:193907–
193934, 2020.

[190] Jiangxia Cao, Xixun Lin, Shu Guo, Luchen Liu, Tingwen Liu, and Bin
Wang. Bipartite graph embedding via mutual information maximization. In _Proceedings of the 14th ACM international conference on web_
_search and data mining_, pages 635–643, 2021.

[191] Xixun Lin, Rui Liu, Yanan Cao, Lixin Zou, Qian Li, Yongxuan Wu,
Yang Liu, Dawei Yin, and Guandong Xu. Contrastive modalitydisentangled learning for multimodal recommendation. _ACM Trans-_
_actions on Information Systems_, 43(3):1–31, 2025.

[192] Yew Ken Chia, Guizhen Chen, Luu Anh Tuan, Soujanya Poria, and
Lidong Bing. Contrastive chain-of-thought prompting. _arXiv preprint_
_arXiv:2311.09277_, 2023.

[193] Yinghui Li, Haojing Huang, Jiayi Kuang, Yangning Li, Shu-Yu Guo,
Chao Qu, Xiaoyu Tan, Hai-Tao Zheng, Ying Shen, and Philip S Yu.
Refine knowledge of large language models via adaptive contrastive
learning. _arXiv preprint arXiv:2502.07184_, 2025.

[194] Chaoya Jiang, Haiyang Xu, Mengfan Dong, Jiaxing Chen, Wei Ye,
Ming Yan, Qinghao Ye, Ji Zhang, Fei Huang, and Shikun Zhang.
Hallucination augmented contrastive learning for multimodal large
language model. In _Proceedings of the Computer Vision and Pattern_
_Recognition Conference_, pages 27036–27046, 2024.

[195] Hourui Deng, Hongjie Zhang, Jie Ou, and Chaosheng Feng. Can
llm be a good path planner based on prompt engineering? mitigating
the hallucination for path planning. _arXiv preprint arXiv:2408.13184_,
2024.

[196] Zhiheng Xi, Wenxiang Chen, Boyang Hong, Senjie Jin, Rui Zheng, Wei
He, Yiwen Ding, Shichun Liu, Xin Guo, Junzhe Wang, et al. Training large language models for reasoning through reverse curriculum
reinforcement learning. _arXiv preprint arXiv:2402.05808_, 2024.

[197] Leslie Pack Kaelbling, Michael L Littman, and Andrew W Moore.
Reinforcement learning: A survey. _Journal of artificial intelligence_
_research_, 4:237–285, 1996.

[198] Weimin Xiong, Yifan Song, Qingxiu Dong, Bingchan Zhao, Feifan
Song, Xun Wang, and Sujian Li. Mpo: Boosting llm agents with meta
plan optimization. _arXiv preprint arXiv:2503.02682_, 2025.

[199] Maohao Shen, Guangtao Zeng, Zhenting Qi, Zhang-Wei Hong, Zhenfang Chen, Wei Lu, Gregory Wornell, Subhro Das, David Cox, and
Chuang Gan. Satori: Reinforcement learning with chain-of-actionthought enhances llm reasoning via autoregressive search. _arXiv_
_preprint arXiv:2502.02508_, 2025.

[200] Qiancheng Xu, Yongqi Li, Heming Xia, and Wenjie Li. Enhancing tool
retrieval with iterative feedback from large language models. In Yaser
Al-Onaizan, Mohit Bansal, and Yun-Nung Chen, editors, _Findings of_
_the Association for Computational Linguistics: EMNLP_, pages 9609–
9619, 2024.

[201] Assaf Ben-Kish, Moran Yanuka, Morris Alper, Raja Giryes, and
Hadar Averbuch-Elor. Mocha: Multi-objective reinforcement mitigating
caption hallucinations. _arXiv preprint arXiv:2312.03631_, 2, 2023.


JOURNAL OF L [A] TEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 19




[202] Tianyu Yu, Yuan Yao, Haoye Zhang, Taiwen He, Yifeng Han, Ganqu
Cui, Jinyi Hu, Zhiyuan Liu, Hai-Tao Zheng, Maosong Sun, et al. Rlhfv: Towards trustworthy mllms via behavior alignment from fine-grained
correctional human feedback. In _Proceedings of the Computer Vision_
_and Pattern Recognition Conference_, pages 13807–13816, 2024.

[203] Jiajie Zhang, Zhongni Hou, Xin Lv, Shulin Cao, Zhenyu Hou, Yilin
Niu, Lei Hou, Yuxiao Dong, Ling Feng, and Juanzi Li. Longreward:
Improving long-context large language models with ai feedback. _arXiv_
_preprint arXiv:2410.21252_, 2024.

[204] Judea Pearl. Causal inference in statistics: An overview. 2009.

[205] Xixun Lin, Qing Yu, Yanan Cao, Lixin Zou, Chuan Zhou, Jia Wu,
Chenliang Li, Peng Zhang, and Shirui Pan. Generative causality-driven
network for graph multi-task learning. _IEEE transactions on pattern_
_analysis and machine intelligence_, 2025.

[206] Shawn Li, Jiashu Qu, Yuxiao Zhou, Yuehan Qin, Tiankai Yang, and Yue
Zhao. Treble counterfactual vlms: A causal approach to hallucination.
_arXiv preprint arXiv:2503.06169_, 2025.

[207] Xinmiao Hu, Chun Wang, Ruihe An, ChenYu Shao, Xiaojun Ye, Sheng
Zhou, and Liangcheng Li. Causal-llava: Causal disentanglement for
mitigating hallucination in multimodal large language models. _arXiv_
_preprint arXiv:2505.19474_, 2025.

[208] Kangsheng Wang, Xiao Zhang, Zizheng Guo, Tianyu Hu, and Huimin
Ma. Csce: Boosting llm reasoning by simultaneous enhancing of casual
significance and consistency. _arXiv preprint arXiv:2409.17174_, 2024.

[209] Ziyi Tang, Ruilin Wang, Weixing Chen, Keze Wang, Yang Liu, Tianshui Chen, and Liang Lin. Towards causalgpt: A multi-agent approach
for faithful knowledge reasoning via promoting causal consistency in
llms. _arXiv preprint arXiv:2308.11914_, 2023.

[210] Feng Xia, Ke Sun, Shuo Yu, Abdul Aziz, Liangtian Wan, Shirui Pan,
and Huan Liu. Graph learning: A survey. _IEEE Transactions on_
_Artificial Intelligence_, 2(2):109–127, 2021.

[211] Xixun Lin, Wenxiao Zhang, Fengzhao Shi, Chuan Zhou, Lixin Zou,
Xiangyu Zhao, Dawei Yin, Shirui Pan, and Yanan Cao. Graph neural
stochastic diffusion for estimating uncertainty in node classification. In
_41st International Conference on Machine Learning (PMLR)_ . MLResearchPress, 2024.

[212] Shichao Zhu, Mufan Li, Guangmou Pan, and Xixun Lin. Ttgl: Largescale multi-scenario universal graph learning at tiktok. In _Proceedings_
_of the 31st ACM SIGKDD Conference on Knowledge Discovery and_
_Data Mining V. 2_, pages 5249–5259, 2025.

[213] Yuanchen Bei, Weizhi Zhang, Siwen Wang, Weizhi Chen, Sheng Zhou,
Hao Chen, Yong Li, Jiajun Bu, Shirui Pan, Yizhou Yu, et al. Graphs
meet ai agents: Taxonomy, progress, and future opportunities. _arXiv_
_preprint arXiv:2506.18019_, 2025.

[214] Yixin Liu, Guibin Zhang, Kun Wang, Shiyuan Li, and Shirui Pan.
Graph-augmented large language model agents: Current progress and
future prospects. _arXiv preprint arXiv:2507.21407_, 2025.

[215] Zhaoyang Liu, Zeqiang Lai, Zhangwei Gao, Erfei Cui, Zhiheng Li,
Xizhou Zhu, Lewei Lu, Qifeng Chen, Yu Qiao, Jifeng Dai, and Wenhai
Wang. Controlllm: Augment language models with tools by searching
on graphs. _arXiv preprint arXiv:2305.10601_, 2023.

[216] Yingxuan Yang, Huacan Chai, Shuai Shao, Yuanyi Song, Siyuan Qi,
Renting Rui, and Weinan Zhang. Agentnet: Decentralized evolutionary coordination for llm-based multi-agent systems. _arXiv preprint_
_arXiv:2504.00587_, 2025.

[217] Weize Chen, Ziming You, Ran Li, Yitong Guan, Chen Qian, Chenyang
Zhao, Cheng Yang, Ruobing Xie, Zhiyuan Liu, and Maosong Sun.
Internet of agents: Weaving a web of heterogeneous agents for collaborative intelligence. _arXiv preprint arXiv:2407.07061_, 2024.

[218] Chenxi Wang, Xiang Chen, Ningyu Zhang, Bozhong Tian, Haoming
Xu, Shumin Deng, and Huajun Chen. Mllm can see? dynamic
correction decoding for hallucination mitigation. _arXiv preprint_
_arXiv:2410.11779_, 2024.

[219] Feilong Tang, Chengzhi Liu, Zhongxing Xu, Ming Hu, Zile Huang,
Haochen Xue, Ziyang Chen, Zelin Peng, Zhiwei Yang, Sijin Zhou,
et al. Seeing far and clearly: Mitigating hallucinations in mllms with
attention causal decoding. In _Proceedings of the Computer Vision and_
_Pattern Recognition Conference_, pages 26147–26159, 2025.
