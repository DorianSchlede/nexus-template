<!-- Source: 18-HallucinationSurvey-2509.18970.pdf | Chunk 2/8 -->

JOURNAL OF L [A] TEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 4




_•_ **Belief Update** : Based on the above decision-making
process and feedback, the agent refines _bt_ as follows,


_bt_ +1 = _LB_ ( _bt, mt_ +1 _, at, rt, ot_ +1) _._ (7)


The agent ultimately receives a cumulative discounted reward
(i.e., [�] _t_ _[∞]_ =0 _[γ][t][r][t]_ [) as a measure of goal achievement.]
Different from the single-agent setting, in the LLM-based
MAS, each agent must communicate with other agents to
accomplish goals. To model these processes, a communication structure _Gt_ is introduced among _N_ agents. Furthermore, _Gt_ would evolve with time to align with the dynamic
adjustments of MAS. Therefore, compared with the above
loop, the loop of LLM-based MAS is given as “Reasoning–
Execution– **Broadcasting** –Feedback–Environment Transition–
Perception–Memorization–Belief Update– **Structure Evolu-**
**tion** ”. This loop includes two additional procedures: Broadcasting and Structure Evolution. Broadcasting means that the
agent broadcasts its message to neighboring nodes according
to its plan, while structure evolution indicates that the communication structure _Gt_ can be updated in each iteration. The
complete description of the loop of MAS is given in Appendix
A.



_A. Reasoning Hallucinations_


Reasoning serves as the cornerstone of an agent’s functionality, influencing behavior analysis and decision-making [40],

[41], [42]. Upon receiving a specific goal _g_, the agent first
leverages its own reasoning capabilities to perform goal understanding for inferring the user’s true intention:


_•_ **Goal Understanding** : This phase occurs before the agent
executes multiple loops:


_I_ = Understand( _b_ 0 _, g_ ) _,_ (8)


where _I_ denotes the inferred intention, and _b_ 0 represents
the initial belief state. When _I_ is complex and relatively
difficult to execute, the agent would perform the intention
decomposition to decompose _I_ into a series of manageable sub-intentions _{It}_ _[n]_ _t_ =0 [.]

_•_ **Intention Decomposition** : Typically, there are two decomposition methods: pre-defined decomposition and
dynamic decomposition. In the first one, these subintentions are specified in advance:


_{It}_ _[n]_ _t_ =0 [=][ Pre-decompose][(] _[b]_ [0] _[,][ I]_ [)] _[.]_ (9)


In the second one, the sub-intention for each loop is
generated by


_It_ = Dyn-decompose( _bt, I_ ) _._ (10)


The decomposition results are dynamically optimized
based on the current belief state _bt_, enabling timely and
continuous refinement for the subsequent workflow [43].

_•_ **Planning Generation** : Each sub-intention _It_ is then
mapped to a concrete plan _pt_ for the current loop:


_pt_ = Plan( _bt, It_ ) _._ (11)



III. TAXONOMY OF AGENT HALLUCINATIONS


We first provide a formal definition of agent hallucinations.
Based on this definition, we then introduce the new taxonomy
that includes the following five types of agent hallucinations:
Reasoning Hallucinations, Execution Hallucinations, Perception hallucinations, Memorization Hallucinations, and Communication Hallucinations, as illustrated in Fig. 2. Detailed
descriptions of each type of agent hallucinations are provided
in the following sub-sections. Representative hallucination
examples are presented in Appendix B.



The subsequent sub-sections detail the triggering causes that
give rise to these three types of reasoning hallucinations.
_1) Problematic Objective Expression._ Understanding the user’s
true intention from _g_ is the first and essential step for effective




JOURNAL OF L [A] TEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 5























Fig. 2. A taxonomy of agent hallucinations. It includes five hallucination types and nine hallucination sub-types with corresponding triggering causes.



reasoning [44]. Objectively, when the expression of goal information carries a certain degree of semantic vagueness, it can
easily lead to erroneous parsing of user intention and induce
reasoning hallucinations [45], [46]. This semantic vagueness
can be mainly attributed to two issues: **Incomplete Goal**
**Specification** and **Ambiguous Content Presence** [47], [48].
To address semantic vagueness in user input, a plausible and
effective approach is to endow the agent with the capability
of active clarification [49], [50]. By engaging in appropriate
proactive interactions with the user, the agent can gain a deeper
understanding of the user’s true needs and intentions.

_2) Inadequate Subjective Comprehension._ In addition to above
objective issues, there also exist subjective limitations in the
agent’s comprehension capability. These are mainly manifested as **Instruction-following Deviation** [51], as well as
**Long-range Contextual Misuse** [52]. Specifically, instruction
following refers to an agent’s ability to adhere to userprovided instructions in order to accurately comprehend intentions [53]. When this procedure breaks down, the agent
struggles to segment critical fields and extract key information from instructions, leading to distorted understanding and
reasoning hallucinations [54], [55], [56]. Additionally, when
agents handle excessively long input goals, they may fail to
capture long-range contexts and instead over-rely on the most
recent tokens, resulting in contextual misuse [57], [58], [59].
Such deficiencies cause agents to violate explicitly stated user
preferences [60].



_3) Deficient Dependency Modeling._ Intention decomposition
is a crucial phase where agents determine the successive
reasoning process via the decomposed sub-intentions [61],

[62], [63]. These sub-intentions typically form a sequentially
dependent chain, where the completion of each sub-intention
pre-supposes the successful fulfillment of its predecessors.
Inadequate modeling of dependency relationships among these
sub-intentions can give rise to three types of errors: **Sub-**
**intention Omission**, **Sub-intention Redundancy** [64], [65]
and **Sub-intentions Disorder** . All three severely compromise
reasoning integrity and efficiency, ultimately leading to reasoning hallucinations [66]. Sub-intention omission occurs when
agents fail to identify essential sub-intentions, leading to the
absence of critical reasoning steps [67]. Sub-intention redundancy arises when agents erroneously introduce task-irrelevant
sub-intentions, causing reasoning processes to deviate from the
core intention and generating hallucinatory content [68], [69],

[67]. Sub-intention disorder refers to the phenomenon in which
an agent arranges sub-intentions with a sequential relationship
in the wrong order during reasoning, causing it to fail to obtain
the results of prerequisite sub-intentions, thereby missing key
information and further generating hallucinations [70].

_4) Planning Information Misinterpretation._ In Eq.(11), the
agent relies on two key sources of information to generate
sub-intention plans: **Operable Objects** and **Self-knowledge** .
For operable objects, the agent must recognize both explicit
and implicit relational connections between entities (e.g.,


JOURNAL OF L [A] TEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 6



spatial, temporal, or social relationships) [71], [72], [73]. The
misinterpreting information related to these operable objects
can lead the agent to construct plans based on incorrect
assumptions, thereby inducing planning generation hallucinations [74]. With respect to self-knowledge, the agent is
expected to plan reasonably within the bound of its own
self-knowledge to avoid planning errors. When confronted
with planning problems beyond its knowledge boundary, the
agent tends to respond with excessive confidence, generating
answers that sound certain but are actually incorrect [75].
_5) Planning Information Misapplication._ Even when an agent
correctly identifies the operable objects within its knowledge boundary, its ability to effectively utilize them remains
constrained by its **Logical Reasoning** and **Introspection**
**Capacity** [76]. Logical fallacies primarily manifest as rule
conflicts and causal misattributions [77]. Similar to humans,
when agents encounter issues such as rule conflicts, causal
inversion, or misattribution during causal reasoning [78], they
often fail to complete the reasoning process or resolve underlying logical paradoxes, ultimately leading to erroneous
planning [79]. Additionally, introspection is one of the key
mechanisms by which agents achieve learning, cognition, and
knowledge updating [15], agents that lack sufficient introspection capacity are unable to effectively improve their performance by revisiting logical fallacies in their reasoning [80].
Such agents may misinterpret feedback signals and accurately
identify the sources of error [81], thereby making misguided
adjustments to their reasoning processes [82]. Even when
feedback is correctly interpreted, the introspection process may
still be flawed: agents might wrongly judge their reasoning as
consistent despite existing logical contradictions, or mistakenly believe they have considered all relevant factors while
overlooking critical variables [83].


_B. Execution Hallucinations_


The execution phase is where the agent translates its deliberated plan _pt_ into a concrete, executable action _at_ . _at_
typically involves invoking external tools, either as a single
operation or through multiple parallel instances [84]. Broadly,
this execution process in Eq.(2) can be further decomposed
into two sub-stages [85]:


_•_ **Tool Selection** : Given _pt_ and a set of candidate tools
_T_ cand, the agent must first select the appropriate tool _Ts ∈_



_T_ cand as follows [3],


_Ts_ = Select ( _bt, pt, T_ cand) _._ (12)


As there are a large number of tools, _T_ cand is typically
retrieved from a full tool set _T_ to narrow the selection
scope [86]:
_T_ cand = Retr ( _bt, pt, T_ ) _._ (13)


_•_ **Tool Calling** : Once _Ts_ is selected, the agent then needs
to populate _Ts_ with the tool parameters derived from _bt_
and _pt_ to form the final executable action _at_ :


_at_ = Call ( _bt, pt, Ts_ ) _._ (14)


The underlying causes behind these two execution hallucinations are given in the following sub-sections.
_1) Tool Documentation Limitation._ Execution hallucinations
often arise when the agent’s internal representation of tool
behaviors diverges from the actual functionality of tools, frequently due to limitation in **Tool Documentation** [87]. These
hallucinations occur because the agent believes it has correctly
selected or invoked a tool, even though its decision is based
on misleading documentation. Such deficiencies may include
redundant information [88], [89], incomplete or inaccurate
descriptions [90], [91], or a lack of standardization [92], [93],
all of which impair the agent’s ability to properly use tools.
_2) Shallow Pattern Understanding._ The agent’s shallow understanding of **Tool Patterns** may lead to execution hallucinations
wherein the agent confidently invokes invalid or outdated
tools, mistakenly assuming successful execution. In fact, effective tool usage requires LLM-based agents to develop deep
understanding of tool patterns. This requirement primarily
stems from two key factors: First, tools often provide multiple
functionalities that can be applied across diverse scenarios,
rather than being limited to a single task-specific use case [94];
Second, tools frequently exhibit complex collaborative patterns, such as sequential dependencies and the need for nested
calls [95]. However, LLM-based agents are typically trained
with insufficient exposure to diverse and complex tool-use
scenarios [96], [97], [98]. This limitation makes agents prone
to hallucinating tool invocations that may appear plausible
or confident, but in fact violate expected patterns or omit
essential details, especially when dealing with complex and
novel tasks [99].
_3) Weak Dynamic Adaptability._ LLM-based agents are typically trained on relatively static datasets, with their tool-use
knowledge embedded in fixed model parameters [97], [98].
However, the execution environment of tools is inherently
dynamic and continuously evolving. Tool functionalities may
evolve, API interfaces may be modified, and in some cases,
tools may be deprecated or replaced [99], [100], [101]. When
an agent lacks sufficient adaptability to these **Tool Dynamics**,
its tool-use behavior becomes misaligned with the actual
environment, thus leading to execution hallucinations wherein
the agent confidently invokes outdated or invalid tools while
incorrectly assuming successful execution.


3 _Ts_ may denote multiple tools that are invoked in parallel to accelerate
execution.


JOURNAL OF L [A] TEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 7



_4) Lack of Solvability Awareness._ **Tool Solvability** refers to
whether the current plan _pt_ can be successfully executed under
existing conditions. It is primarily related to two factors: a)
the availability of suitable tools in _T_ ; b) the clarity and completeness of _pt_ . A lack of solvability awareness in LLM-based
agents can also lead to execution hallucinations, where the
agent mistakenly assumes that _pt_ is solvable and proceeds with
unjustified confidence. For example, if no suitable tools are
available, any tool retrieved from _T_ is likely to be irrelevant or
even fabricated and non-executable, resulting in tool selection
hallucinations [102]. Furthermore, even when a suitable tool
exists and is correctly selected, an unclear or incomplete plan
_pt_ can prevent the agent from correctly deriving the required
parameters for tool invocation [103]. Consequently, the agent
may omit required parameters or fabricate parameter values,
falsely believing that the tool call is valid.


_C. Perception Hallucinations_

Analogous to how humans depend on sensory organs like
eyes and ears to acquire information from the external world
and convert it into neural signals, LLM-based agents also
rely on a perception module to facilitate interaction with
the learning environment. The perception module serves as
a fundamental interface, extending its perception into a multimodal space that includes textual, auditory, and visual modalities. As illustrated in Eq.(5), the agent receives the external
information (e.g., _st_ +1) and transforms this information into
internal observations (e.g., _ot_ +1) via the perception module.
Here we analyze the causes of perception hallucinations.



extract the key information of individual modality from _st_ +1.
This limitation arises from multiple factors, potentially including insufficient training data quality [105], the Transformer
architecture’s tendency [10] to overlook local details [106],
and training conflicts between the pre-training and fine-tuning
stages [107]. **b) Weak Cross-modal Collaboration** : Agents
lack an effective mechanism to integrate semantic associations across different modalities, resulting in the incomplete
representation of _st_ +1. When faced with a complex learning
environment, LLM-based agents typically perform the procedures of modality alignment and joint encoding to integrate
different multi-modal information [108]. However, during this
process, LLM-based agents may fail to align modalities due to
insufficient cross-modal annotations and imbalanced modality
data [109]. Furthermore, in the generation stage, agents may
be dominated by the language prior, gradually neglecting nontextual information [110].







_1) Environmental Sensor Malfunction._ Agents typically rely
on environmental sensors to collect external data and convert
it into digital signals [104]. Common sensor types include
visual sensors (e.g., cameras), auditory sensors (e.g., microphones), tactile sensors (e.g., pressure-sensitive pads), and
inertial measurement units (e.g., accelerometers) [30]. When
environmental sensor malfunctions occur, such as lens distortion in cameras or signal drift in inertial measurement units,
agents may fail to accurately receive raw input information,
potentially resulting in perception hallucinations.
_2) Limited Encoding Capability._ After receiving _st_ +1, LLMbased agents typically employ a specific encoding module to
generate _ot_ +1, which serves as a high-level representation of
the received signals such as images and sound waves. However, this encoding process encounters two critical limitations:
**a) Insufficient Unimodal Representation** : Agents struggle to



_D. Memorization Hallucinations_


The memory module is a core component of LLM-based
agents, tasked with storing and managing information to
facilitate subsequent decision-making [111], [112], [113]. It
primarily owns two operations: **Memory Retrieval** : Extracting
and integrating relevant information from stored memory to
support the current decision process; **Memory Update** : revising and removing existing memory based on newly acquired
information or feedback to ensure its accuracy and timeliness.
Subsequently, we analyze the underlying causes that trigger
the above two memorization hallucinations.
_1) Flawed Memory Initialization._ The **Initial Memory** _m_ 0
serves as the foundation of memorization in LLM-based
agents, and its quality directly influences the reliability of
subsequent memory processes. Biased or incomplete content in
_m_ 0 can introduce initialization deficiencies, leading to memorization hallucinations before deployment. In particular, certain
biases, particularly those related to gender and nationality, are
inherently linked to hallucination issues [34]. Moreover, when
critical information is missing (e.g., the time or location of
a historical event) in _m_ 0, agents may rely on inherent biases
or assumptions to fill in the gaps, resulting in inaccurate or


JOURNAL OF L [A] TEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 8



fabricated responses in many real-world scenarios [114], [115],

[116].
_2) Sub-optimal Retrieval Mechanism._ First, poor **Ranking**
**Strategies** may lead the agent to retrieve memory content
that only superficially ”appears similar” but lacks true relevance [117], [118]. Additionally, deficiencies in the memory
**Indexing Structure** [119], [120], such as inappropriate index
granularity or delayed index updates, can further exacerbate
this problem, potentially resulting in information loss and the
retrieval of outdated memory. Finally, insufficient understanding of **Query Semantics** is also an important problem. If the
agent misinterprets the query’s intent or overlooks specific task
requirements [121], [122], it may fail to capture contextual
cues necessary for accurate retrieval.
_3) Imperfect Priority Assignment._ This issue mainly manifests
in two aspects: **Forgetting Priority** and **Merging Priority** .
For memory forgetting, poorly assigned priorities can result
in the elimination of important information or the retention of
irrelevant content, ultimately compromising the accuracy of
subsequent decisions [112], [123]. Moreover, when the agent
merges multiple memory fragments, failure to correctly assess
their priorities [124], [125] may result in the merged memory
containing inherent conflicts [126], [127]. Some of these
conflicts are implicit [128]: Although memory fragments may
appear similar, they differ significantly in key semantic details,
thereby increasing the difficulty of priority determination.
_4) Inaccurate Information Writing._ When performing longterm tasks and engaging in multi-turn interactions, agents
must summarize, structure, and store relevant historical information to the memory module. However, this process is
susceptible to **Information Compression** issues, where the
generated summaries may be overly general, omit crucial
details [129], [130], [131], or introduce distortions due to
imperfect abstraction [132], [133], [111]. Additionally, nonstandardized **Memory Formats** with disorganized structures
can hinder writing efficiency [111], [134]. Memory **Capac-**
**ity Constraints** exacerbate these challenges, because limited
storage may necessitate the selective retention of information [112], [135], increasing the risk that salient but less
frequently accessed information is discarded.


_E. Communication Hallucinations_



Compared with the independent operations of a single agent,
LLM-based MAS emphasizes collaboration and coordination
to harness collective intelligence for solving more complex
and demanding tasks [136]. In such systems, effective interagent communication serves as an essential requirement [137],
facilitating the exchange of ideas and the coordination of plans
among agents [138].
The following sub-sections provide a detailed account of
triggering causes underlying communication hallucinations.
_1) Erroneous Message Propagation._ In LLM-based MAS,
agents fundamentally rely on LLMs [38], [139], [30] to generate messages for exchanging information with other agents.
However, since LLMs are prone to the well-known **Factuality**
**and Faithfulness Hallucinations** [34], some agents may produce messages containing inaccurate facts, misinterpretations
of shared knowledge, or misleading inferences [140], [141],

[142], thereby giving rise to communication hallucinations.
Beyond this, **Content Redundancy** is also an important cause,
where agents generate unnecessary or repetitive content that
obscures critical signals, increases cognitive load, and sometimes leads to redundant task execution steps that manifest as
logical errors [143], [144], [141]. **Information Asymmetry**
can further exacerbate this issue. Because agents own different
roles and positions within MAS, the information accessible to
them varies, and such asymmetric settings may yield vague or
incomplete instructions that hinder task comprehension [145]
and amplify the risk of biased decisions [146].
_2) Uncoordinated Communication Protocols._ Communication
protocols govern how agents exchange messages, directly
determining the efficiency, reliability, and coordination of
their interactions [147], [137], [148]. Without a unified and
effective protocol, agents may ”talk past each other”, leading to communication hallucinations. First, LLM-based MAS
usually follows a manner of **Asynchronous Scheduling** [22],
so when receiving and processing instructions, agents may
encounter issues of information loss and information overload.
Such temporal discrepancies can result in information errors,
thereby increasing the risk of hallucinatory outputs [149],

[150]. Second, communication protocols define the **Message**
**formats** . Current LLM-based agents predominantly rely on the
format of natural language [151] which often introduces instruction ambiguity. Adopting structured formats (e.g., JSON)
can improve clarity and rigor of expression, which mitigates
the risk of miscommunication [152], [153]. Finally, LLMbased MAS demands a robust **Fault-tolerant Design**, incorporating confirmation conditions and synchronization constraints
to avoid erroneous decisions caused by message loss or delays
in dynamic or noisy environments [154], [155].
_3) Ineffective Network Updates._ Network topology defines how
agents are interconnected, determining who communicates
with whom and how frequently [137], [20], [156]. As discussed in Appendix A, the network topology is continuously
evolving, and network updates reshape the propagation paths
of messages within MAS. When network updates are ineffective, they can induce communication hallucinations due to
inconsistent or outdated inter-agent connections [141], [142],

[157]. Although recently proposed strategies improve the
flexibility and responsiveness of MAS, they often suffer from


JOURNAL OF L [A] TEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 9



delayed updates and poor coordination [20], [158], [159]. If
the updated network fails to accurately reflect agents’ current
relevance or expertise, messages may be routed to inappropriate recipients, leading to misunderstandings or redundant
reasoning in LLM-based MAS.


IV. SYSTEMATIC METHODOLOGY

