<!-- Source: 18-HallucinationSurvey-2509.18970.pdf | Chunk 5/8 -->

of llm agents. _arXiv preprint arXiv:2503.01935_, 2025.

[27] Zhexin Zhang, Shiyao Cui, Yida Lu, Jingzhuo Zhou, Junxiao Yang,
Hongning Wang, and Minlie Huang. Agent-safetybench: Evaluating
the safety of llm agents. _arXiv preprint arXiv:2412.14470_, 2024.

[28] Tongxin Yuan, Zhiwei He, Lingzhong Dong, Yiming Wang, Ruijie
Zhao, Tian Xia, Lizhen Xu, Binglin Zhou, Fangqi Li, Zhuosheng
Zhang, et al. R-judge: Benchmarking safety risk awareness for llm
agents. _arXiv preprint arXiv:2401.10019_, 2024.

[29] Yu Tian, Xiao Yang, Jingyuan Zhang, Yinpeng Dong, and Hang Su.
Evil geniuses: Delving into the safety of llm-based agents. _arXiv_
_preprint arXiv:2311.11855_, 2023.

[30] Bang Liu, Xinfeng Li, Jiayi Zhang, Jinlin Wang, Tanjin He, Sirui
Hong, Hongzhang Liu, Shaokun Zhang, Kaitao Song, Kunlun Zhu,
et al. Advances and challenges in foundation agents: From braininspired intelligence to evolutionary, collaborative, and safe systems.
_arXiv preprint arXiv:2504.01990_, 2025.

[31] Yuxiang Zhang, Jing Chen, Junjie Wang, Yaxin Liu, Cheng Yang,
Chufan Shi, Xinyu Zhu, Zihao Lin, Hanwen Wan, Yujiu Yang, Tetsuya
Sakai, Tian Feng, and Hayato Yamana. Toolbehonest: A multi-level
hallucination diagnostic benchmark for tool-augmented large language
models. In _Proceedings of the 2024 Conference on Empirical Methods_
_in Natural Language Processing, EMNLP 2024, Miami, FL, USA,_
_November 12-16, 2024_, pages 11388–11422, 2024.

[32] Zehang Deng, Yongjian Guo, Changzhou Han, Wanlun Ma, Junwu
Xiong, Sheng Wen, and Yang Xiang. Ai agents under threat: A survey
of key security challenges and future pathways. _ACM Computing_
_Surveys_, 57(7):1–36, 2025.

[33] Ziwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan Su, Yan Xu,
Etsuko Ishii, Ye Jin Bang, Andrea Madotto, and Pascale Fung. Survey
of hallucination in natural language generation. _ACM computing_
_surveys_, 55(12):1–38, 2023.

[34] Lei Huang, Weijiang Yu, Weitao Ma, Weihong Zhong, Zhangyin Feng,
Haotian Wang, Qianglong Chen, Weihua Peng, Xiaocheng Feng, Bing
Qin, et al. A survey on hallucination in large language models: Principles, taxonomy, challenges, and open questions. _ACM Transactions_
_on Information Systems_, 43(2):1–55, 2025.

[35] Sewon Min, Kalpesh Krishna, Xinxi Lyu, Mike Lewis, Wen-tau Yih,
Pang Wei Koh, Mohit Iyyer, Luke Zettlemoyer, and Hannaneh Hajishirzi. Factscore: Fine-grained atomic evaluation of factual precision
in long form text generation. _arXiv preprint arXiv:2305.14251_, 2023.

[36] Alexander R Fabbri, Chien-Sheng Wu, Wenhao Liu, and Caiming
Xiong. Qafacteval: Improved qa-based factual consistency evaluation
for summarization. _arXiv preprint arXiv:2112.08542_, 2021.

[37] Sheng Liu, Haotian Ye, and James Zou. Reducing hallucinations in
large vision-language models via latent space steering. In _The Thir-_
_teenth International Conference on Learning Representations_, 2025.

[38] Zhiheng Xi, Wenxiang Chen, Xin Guo, Wei He, Yiwen Ding, Boyang
Hong, Ming Zhang, Junzhe Wang, Senjie Jin, Enyu Zhou, et al. The
rise and potential of large language model based agents: A survey.
_Science China Information Sciences_, 68(2):121101, 2025.

[39] Karl Johan ˚Astr¨om. Optimal control of markov processes with
incomplete state information i. _Journal of mathematical analysis and_
_applications_, 10:174–205, 1965.

[40] Pengfei Cao, Tianyi Men, Wencan Liu, Jingwen Zhang, Xuzhao Li,
Xixun Lin, Dianbo Sui, Yanan Cao, Kang Liu, and Jun Zhao. Large


JOURNAL OF L [A] TEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 15



language models for planning: A comprehensive and systematic survey.
_arXiv preprint arXiv:2505.19683_, 2025.

[41] Gaole He, Gianluca Demartini, and Ujwal Gadiraju. Plan-then-execute:
An empirical study of user trust and team performance when using llm
agents as a daily assistant. In _Proceedings of the 2025 CHI Conference_
_on Human Factors in Computing Systems_, pages 1–22, 2025.

[42] Zhuosheng Zhang, Yao Yao, Aston Zhang, Xiangru Tang, Xinbei Ma,
Zhiwei He, Yiming Wang, Mark Gerstein, Rui Wang, Gongshen Liu,
et al. Igniting language intelligence: The hitchhiker’s guide from chainof-thought reasoning to language agents. _ACM Computing Surveys_,
57(8):1–39, 2025.

[43] Ranjan Sapkota, Konstantinos I Roumeliotis, and Manoj Karkee. Ai
agents vs. agentic ai: A conceptual taxonomy, applications and challenge. _arXiv preprint arXiv:2505.10468_, 2025.

[44] Xuanming Zhang, Yuxuan Chen, Min-Hsuan Yeh, and Yixuan Li.
Metamind: Modeling human social thoughts with metacognitive multiagent systems. _arXiv preprint arXiv:2505.18943_, 2025.

[45] Shen Zheng, Jie Huang, and Kevin Chen-Chuan Chang. Why does
chatgpt fall short in providing truthful answers? _arXiv preprint_
_arXiv:2304.10513_, 2023.

[46] Sergey Linok, Tatiana Zemskova, Svetlana Ladanova, Roman Titkov,
Dmitry Yudin, Maxim Monastyrny, and Aleksei Valenkov. Beyond bare
queries: Open-vocabulary object grounding with 3d scene graph. _arXiv_
_preprint arXiv:2406.07113_, 2024.

[47] Alisa Liu, Zhaofeng Wu, Julian Michael, Alane Suhr, Peter West,
Alexander Koller, Swabha Swayamdipta, Noah Smith, and Yejin Choi.
We’re afraid language models aren’t modeling ambiguity. In _Pro-_
_ceedings of the 2023 Conference on Empirical Methods in Natural_
_Language Processing_, 2023.

[48] Aryan Keluskar, Amrita Bhattacharjee, and Huan Liu. Do llms
understand ambiguity in text? a case study in open-world question
answering. In _2024 IEEE International Conference on Big Data_
_(BigData)_, pages 7485–7490. IEEE, 2024.

[49] Kasia Kobalczyk, Nicol´as Astorga, Tennison Liu, and Mihaela van der
Schaar. Active task disambiguation with LLMs. In _The Thirteenth_
_International Conference on Learning Representations_, 2025.

[50] Maximillian Chen, Ruoxi Sun, Tomas Pfister, and Sercan O Arik.
Learning to clarify: Multi-turn conversations with action-based contrastive self-training. In _The Thirteenth International Conference on_
_Learning Representations_, 2025.

[51] Abhigya Verma, Sriram Puttagunta, Seganrasan Subramanian, and
Sravan Ramachandran. Graft: Graph and table reasoning for textual
alignment–a benchmark for structured instruction following and visual
reasoning. _arXiv preprint arXiv:2508.15690_, 2025.

[52] Weixin Xu and Ziliang Wang. Scrnet: Spatial-channel regulation
network for medical ultrasound image segmentation. _arXiv preprint_
_arXiv:2508.13899_, 2025.

[53] Mahyar Abbasian, Iman Azimi, Amir M Rahmani, and Ramesh Jain.
Conversational health agents: A personalized llm-powered agent framework. _arXiv preprint arXiv:2310.02374_, 2023.

[54] Qidong Huang, Xiaoyi Dong, Pan Zhang, Bin Wang, Conghui He,
Jiaqi Wang, Dahua Lin, Weiming Zhang, and Nenghai Yu. Opera:
Alleviating hallucination in multi-modal large language models via
over-trust penalty and retrospection-allocation. In _Proceedings of the_
_Computer Vision and Pattern Recognition Conference_, pages 13418–
13427, 2024.

[55] Jie JW Wu, Manav Chaudhary, Davit Abrahamyan, Arhaan Khaku,
Anjiang Wei, and Fatemeh H Fard. Clarifycoder: Clarificationaware fine-tuning for programmatic problem solving. _arXiv preprint_
_arXiv:2504.16331_, 2025.

[56] Chenyu Shi, Xiao Wang, Qiming Ge, Songyang Gao, Xianjun Yang,
Tao Gui, Qi Zhang, Xuanjing Huang, Xun Zhao, and Dahua Lin.
Navigating the overkill in large language models. _arXiv preprint_
_arXiv:2401.17633_, 2024.

[57] Zirui Zhao, Wee Sun Lee, and David Hsu. Large language models as
commonsense knowledge for large-scale task planning. _Advances in_
_Neural Information Processing Systems_, 36:31967–31987, 2023.

[58] Zheng Chu, Jingchang Chen, Qianglong Chen, Haotian Wang, Kun
Zhu, Xiyuan Du, Weijiang Yu, Ming Liu, and Bing Qin. Beamaggr:
Beam aggregation reasoning over multi-source knowledge for multihop question answering. _arXiv preprint arXiv:2406.19820_, 2024.

[59] Yuqi Zhu, Shuofei Qiao, Yixin Ou, Shumin Deng, Shiwei Lyu, Yue
Shen, Lei Liang, Jinjie Gu, Huajun Chen, and Ningyu Zhang. Knowagent: Knowledge-augmented planning for llm-based agents. _arXiv_
_preprint arXiv:2403.03101_, 2024.

[60] Wenqiang Lai, Tianwei Zhang, Tin Lun Lam, and Yuan Gao. Visionlanguage model-based physical reasoning for robot liquid perception.



In _2024 IEEE/RSJ International Conference on Intelligent Robots and_
_Systems (IROS)_, pages 9652–9659. IEEE, 2024.

[61] Ziyue Wang, Junde Wu, Chang Han Low, and Yueming Jin. Medagentpro: Towards evidence-based multi-modal medical diagnosis via reasoning agentic workflow. 2025.

[62] Alexis R Tudor, Joaqu´ın Arias, and Gopal Gupta. Vecsr: Virtually embodied common sense reasoning system. _arXiv preprint_
_arXiv:2505.02144_, 2025.

[63] Zixiao Zhao, Jing Sun, Zhe Hou, Zhiyuan Wei, Cheng-Hao Cai, Miao
Qiao, and Jin Song Dong. Mactg: Multi-agent collaborative thought
graph for automatic programming, 2025.

[64] Guangya Wan, Yuqi Wu, Jie Chen, and Sheng Li. Cot rerailer:
Enhancing the reliability of large language models in complex reasoning tasks through error detection and correction. _arXiv preprint_
_arXiv:2408.13940_, 2024.

[65] Jiachun Li, Pengfei Cao, Chenhao Wang, Zhuoran Jin, Yubo Chen,
Daojian Zeng, Kang Liu, and Jun Zhao. Focus on your question! interpreting and mitigating toxic cot problems in commonsense reasoning.
_arXiv preprint arXiv:2402.18344_, 2024.

[66] Yaoxiang Wang, Zhiyong Wu, Junfeng Yao, and Jinsong Su. Tdag:
A multi-agent framework based on dynamic task decomposition and
agent generation. _Neural Networks_, page 107200, 2025.

[67] Rennai Qiu, Chen Qian, Ran Li, Yufan Dang, Weize Chen, Cheng
Yang, Yingli Zhang, Ye Tian, Xuantang Xiong, Lei Han, et al. Cosaving: Resource aware multi-agent collaboration for software development. _arXiv preprint arXiv:2505.21898_, 2025.

[68] Maryam Berijanian, Kuldeep Singh, and Amin Sehati. Comparative
analysis of ai agent architectures for entity relationship classification.
_arXiv preprint arXiv:2506.02426_, 2025.

[69] Xiao Yu, Baolin Peng, Ruize Xu, Michel Galley, Hao Cheng, Suman
Nath, Jianfeng Gao, and Zhou Yu. Dyna-think: Synergizing reasoning,
acting, and world model simulation in ai agents. _arXiv preprint_
_arXiv:2506.00320_, 2025.

[70] Weizhi Wang, Hong Wang, and Xifeng Yan. Steps: A benchmark for
order reasoning in sequential tasks. _arXiv preprint arXiv:2306.04441_,
2023.

[71] Kun Chu, Xufeng Zhao, Cornelius Weber, and Stefan Wermter. Llm+
map: Bimanual robot task planning using large language models and
planning domain definition language. _arXiv preprint arXiv:2503.17309_,
2025.

[72] Xiang Huang, Jiayu Shen, Shanshan Huang, Sitao Cheng, Xiaxia Wang,
and Yuzhong Qu. Targa: Targeted synthetic data generation for practical
reasoning over structured data. _arXiv preprint arXiv:2412.19544_, 2024.

[73] Kaiwen Zuo, Yirui Jiang, Fan Mo, and Pietro Lio. Kg4diagnosis:
A hierarchical multi-agent llm framework with knowledge graph enhancement for medical diagnosis. In _AAAI Bridge Program on AI for_
_Medicine and Healthcare_, pages 195–204. PMLR, 2025.

[74] Potsawee Manakul, Adian Liusie, and Mark JF Gales. Selfcheckgpt:
Zero-resource black-box hallucination detection for generative large
language models. _arXiv preprint arXiv:2303.08896_, 2023.

[75] Xiaoou Liu, Tiejin Chen, Longchao Da, Chacha Chen, Zhen Lin, and
Hua Wei. Uncertainty quantification and confidence calibration in large
language models: A survey. _arXiv preprint arXiv:2503.15850_, 2025.

[76] Iman Mirzadeh, Keivan Alizadeh, Hooman Shahrokhi, Oncel Tuzel,
Samy Bengio, and Mehrdad Farajtabar. Gsm-symbolic: Understanding
the limitations of mathematical reasoning in large language models.
_arXiv preprint arXiv:2410.05229_, 2024.

[77] MingShan Liu, Shi Bo, and Jialing Fang. Enhancing mathematical
reasoning in large language models with self-consistency-based hallucination detection. _arXiv preprint arXiv:2504.09440_, 2025.

[78] Jingtian Wu and Claire Cardie. Reasoning court: Combining reasoning, action, and judgment for multi-hop reasoning. _arXiv preprint_
_arXiv:2504.09781_, 2025.

[79] Wei Xu, Gang Luo, Weiyu Meng, Xiaobing Zhai, Keli Zheng, Ji Wu,
Yanrong Li, Abao Xing, Junrong Li, Zhifan Li, et al. Mragent: an llmbased automated agent for causal knowledge discovery in disease via
mendelian randomization. _Briefings in Bioinformatics_, 26(2):bbaf140,
2025.

[80] Nikolas Belle, Dakota Barnes, Alfonso Amayuelas, Ivan Bercovich,
Xin Eric Wang, and William Wang. Agents of change: Self-evolving
llm agents for strategic planning. _arXiv preprint arXiv:2506.04651_,
2025.

[81] Tian Liang, Zhiwei He, Wenxiang Jiao, Xing Wang, Yan Wang, Rui
Wang, Yujiu Yang, Shuming Shi, and Zhaopeng Tu. Encouraging divergent thinking in large language models through multi-agent debate.
_arXiv preprint arXiv:2305.19118_, 2023.


JOURNAL OF L [A] TEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 16




[82] Wenqi Zhang, Yongliang Shen, Linjuan Wu, Qiuying Peng, Jun
Wang, Yueting Zhuang, and Weiming Lu. Self-contrast: Better reflection through inconsistent solving perspectives. _arXiv preprint_
_arXiv:2401.02009_, 2024.

[83] Yanhong Li, Chenghao Yang, and Allyson Ettinger. When hindsight
is not 20/20: Testing limits on reflective thinking in large language
models. _arXiv preprint arXiv:2404.09129_, 2024.

[84] Dongsheng Zhu, Weixian Shi, Zhengliang Shi, Zhaochun Ren,
Shuaiqiang Wang, Lingyong Yan, and Dawei Yin. Divide-thenaggregate: An efficient tool learning method via parallel tool invocation. _arXiv preprint arXiv:2501.12432_, 2025.

[85] Shishir G Patil, Tianjun Zhang, Xin Wang, and Joseph E Gonzalez.
Gorilla: Large language model connected with massive apis. _Advances_
_in Neural Information Processing Systems_, 37:126544–126565, 2024.

[86] Yujia Qin, Shihao Liang, Yining Ye, Kunlun Zhu, Lan Yan, Yaxi
Lu, Yankai Lin, Xin Cong, Xiangru Tang, Bill Qian, et al. Toolllm:
Facilitating large language models to master 16000+ real-world apis.
_arXiv preprint arXiv:2307.16789_, 2023.

[87] Cheng-Yu Hsieh, Si-An Chen, Chun-Liang Li, Yasuhisa Fujii, Alexander Ratner, Chen-Yu Lee, Ranjay Krishna, and Tomas Pfister. Tool
documentation enables zero-shot tool-usage with large language models. _CoRR_, abs/2308.00675, 2023.

[88] Siyu Yuan, Kaitao Song, Jiangjie Chen, Xu Tan, Yongliang Shen, Kan
Ren, Dongsheng Li, and Deqing Yang. EASYTOOL: enhancing llmbased agents with concise tool instruction. _CoRR_, abs/2401.06201,
2024.

[89] Changle Qu, Sunhao Dai, Xiaochi Wei, Hengyi Cai, Shuaiqiang
Wang, Dawei Yin, Jun Xu, and Ji-Rong Wen. From exploration to
mastery: Enabling LLMs to master tools via self-driven interactions. In
_The Thirteenth International Conference on Learning Representations_,
2025.

[90] Ivan Milev, Mislav Balunovic, Maximilian Baader, and Martin T.
Vechev. Toolfuzz   - automated agent tool testing. _CoRR_,
abs/2503.04479, 2025.

[91] Xinyi Ni, Qiuyang Wang, Yukun Zhang, and Pengyu Hong. Toolfactory: Automating tool generation by leveraging LLM to understand
REST API documentations. _CoRR_, abs/2501.16945, 2025.

[92] Zhengliang Shi, Shen Gao, Lingyong Yan, Yue Feng, Xiuyi Chen,
Zhumin Chen, Dawei Yin, Suzan Verberne, and Zhaochun Ren. Tool
learning in the wild: Empowering language models as automatic tool
agents. In _Proceedings of the ACM on Web Conference 2025_, pages
2222–2237. ACM, 2025.

[93] Pan Lu, Bowen Chen, Sheng Liu, Rahul Thapa, Joseph Boen, and
James Zou. Octotools: An agentic framework with extensible tools for
complex reasoning. _CoRR_, abs/2502.11271, 2025.

[94] Shen Gao, Zhengliang Shi, Minghang Zhu, Bowen Fang, Xin Xin,
Pengjie Ren, Zhumin Chen, Jun Ma, and Zhaochun Ren. Confucius:
Iterative tool learning from introspection feedback by easy-to-difficult
curriculum. In _Thirty-Eighth AAAI Conference on Artificial Intelli-_
_gence_, 2024.

[95] Ibrahim Abdelaziz, Kinjal Basu, Mayank Agarwal, Sadhana Kumaravel, Matthew Stallone, Rameswar Panda, Yara Rizk, G. P. Shrivatsa Bhargav, Maxwell Crouse, Chulaka Gunasekara, Shajith Ikbal,
Sachindra Joshi, Hima Karanam, Vineet Kumar, Asim Munawar, Sumit
Neelam, Dinesh Raghu, Udit Sharma, Adriana Meza Soria, Dheeraj
Sreedhar, Praveen Venkateswaran, Merve Unuvar, David Cox, Salim
Roukos, Luis A. Lastras, and Pavan Kapanipathi. Granite-function
calling model: Introducing function calling abilities via multi-task
learning of granular tasks. In _Proceedings of the 2024 Conference_
_on Empirical Methods in Natural Language Processing_, 2024.

[96] Boshi Wang, Hao Fang, Jason Eisner, Benjamin Van Durme, and Yu Su.
Llms in the imaginarium: Tool learning through simulated trial and
error. In _Proceedings of the 62nd Annual Meeting of the Association_
_for Computational Linguistics_, pages 10583–10604, 2024.

[97] Shishir G. Patil, Tianjun Zhang, Xin Wang, and Joseph E. Gonzalez. Gorilla: Large language model connected with massive apis.
In _Advances in Neural Information Processing Systems 38: Annual_
_Conference_, 2024.

[98] Yujia Qin, Shihao Liang, Yining Ye, Kunlun Zhu, Lan Yan, Yaxi Lu,
Yankai Lin, Xin Cong, Xiangru Tang, Bill Qian, Sihan Zhao, Lauren
Hong, Runchu Tian, Ruobing Xie, Jie Zhou, Mark Gerstein, Dahai Li,
Zhiyuan Liu, and Maosong Sun. Toolllm: Facilitating large language
models to master 16000+ real-world apis. In _The Twelfth International_
_Conference on Learning Representations_, 2024.

[99] Guoxin Chen, Zhong Zhang, Xin Cong, Fangda Guo, Yesai Wu, Yankai
Lin, Wenzheng Feng, and Yasheng Wang. Learning evolving tools for
large language models. _CoRR_, abs/2410.06617, 2024.




[100] Elias Lumer, Anmol Gulati, Vamse Kumar Subbiah, Pradeep Honaganahalli Basavaraju, and James A Burke. Scalemcp: Dynamic and
auto-synchronizing model context protocol tools for llm agents. _arXiv_
_preprint arXiv:2505.06416_, 2025.

[101] R Patrick Xian, Qiming Cui, Stefan Bauer, and Reza Abbasi-Asl.
Measuring temporal effects of agent knowledge by date-controlled tool
use. _arXiv preprint arXiv:2503.04188_, 2025.

[102] Hongshen Xu, Su Zhu, Zihan Wang, Hang Zheng, Da Ma, Ruisheng
Cao, Shuai Fan, Lu Chen, and Kai Yu. Reducing tool hallucination
via reliability alignment. _CoRR_, 2024.

[103] Wenxuan Wang, Juluan Shi, Chaozheng Wang, Cheryl Lee, Youliang
Yuan, Jen-tse Huang, and Michael R Lyu. Learning to ask: When llms
meet unclear instruction. _arXiv preprint arXiv:2409.00557_, 2024.

[104] Amit Ranjan Trivedi, Sina Tayebati, Hemant Kumawat, Nastaran
Darabi, Divake Kumar, Adarsh Kumar Kosta, Yeshwanth Venkatesha,
Dinithi Jayasuriya, Nethmi Jayasinghe, Priyadarshini Panda, et al. Intelligent sensing-to-action for robust autonomy at the edge: Opportunities
and challenges. _arXiv preprint arXiv:2502.02692_, 2025.

[105] Qifan Yu, Juncheng Li, Longhui Wei, Liang Pang, Wentao Ye, Bosheng
Qin, Siliang Tang, Qi Tian, and Yueting Zhuang. Hallucidoctor: Mitigating hallucinatory toxicity in visual instruction data. In _Proceedings_
_of the Computer Vision and Pattern Recognition Conference_, pages
12944–12953, 2024.

[106] Jiuhai Chen, Jianwei Yang, Haiping Wu, Dianqi Li, Jianfeng Gao,
Tianyi Zhou, and Bin Xiao. Florence-vl: Enhancing vision-language
models with generative vision encoder and depth-breadth fusion. _arXiv_
_preprint arXiv:2412.04424_, 2024.

[107] Ziyu Liu, Zeyi Sun, Yuhang Zang, Xiaoyi Dong, Yuhang Cao, Haodong
Duan, Dahua Lin, and Jiaqi Wang. Visual-rft: Visual reinforcement
fine-tuning. _arXiv preprint arXiv:2503.01785_, 2025.

[108] Sara Ghazanfari, Alexandre Araujo, Prashanth Krishnamurthy, Siddharth Garg, and Farshad Khorrami. Emma: Efficient visual alignment
in multi-modal llms. _arXiv preprint arXiv:2410.02080_, 2024.

[109] Xin Lai, Zhuotao Tian, Yukang Chen, Yanwei Li, Yuhui Yuan, Shu
Liu, and Jiaya Jia. Lisa: Reasoning segmentation via large language
model. In _Proceedings of the Computer Vision and Pattern Recognition_
_Conference_, pages 9579–9589, 2024.

[110] Bin Wang, Fan Wu, Xiao Han, Jiahui Peng, Huaping Zhong, Pan
Zhang, Xiaoyi Dong, Weijia Li, Wei Li, Jiaqi Wang, et al. Vigc:
Visual instruction generation and correction. In _Proceedings of the_
_AAAI Conference on Artificial Intelligence_, volume 38, pages 5309–
5317, 2024.

[111] Zidi Xiong, Yuping Lin, Wenya Xie, Pengfei He, Jiliang Tang,
Himabindu Lakkaraju, and Zhen Xiang. How memory management
impacts llm agents: An empirical study of experience-following behavior, 2025.

[112] Charles Packer, Vivian Fang, Shishir G Patil, Kevin Lin, Sarah Wooders, and Joseph E Gonzalez. Memgpt: Towards llms as operating
systems. 2023.

[113] Ziheng Huang, Sebastian Gutierrez, Hemanth Kamana, and Stephen
MacNeil. Memory sandbox: Transparent and interactive memory
management for conversational agents. In _Adjunct Proceedings of_
_the 36th Annual ACM Symposium on User Interface Software and_
_Technology_, pages 1–3, 2023.

[114] Johnny Li, Saksham Consul, Eda Zhou, James Wong, Naila Farooqui,
Yuxin Ye, Nithyashree Manohar, Zhuxiaona Wei, Tian Wu, Ben Echols,
et al. Banishing llm hallucinations requires rethinking generalization.
_arXiv preprint arXiv:2406.17642_, 2024.

[115] Aditi Singh, Abul Ehtesham, Saket Kumar, and Tala Talaei Khoei.
Agentic retrieval-augmented generation: A survey on agentic rag. _arXiv_
_preprint arXiv:2501.09136_, 2025.

[116] Karan Singhal, Tao Tu, Juraj Gottweis, Rory Sayres, Ellery Wulczyn,
Mohamed Amin, Le Hou, Kevin Clark, Stephen R Pfohl, Heather ColeLewis, et al. Toward expert-level medical question answering with large
language models. _Nature Medicine_, pages 1–8, 2025.

[117] Yiteng Tu, Weihang Su, Yujia Zhou, Yiqun Liu, and Qingyao Ai. Rbft:
Robust fine-tuning for retrieval-augmented generation against retrieval
defects. _arXiv preprint arXiv:2501.18365_, 2025.

[118] Garima Agrawal, Tharindu Kumarage, Zeyad Alghamdi, and Huanmin
Liu. Mindful-rag: A study of points of failure in retrieval augmented
generation. _2024 2nd International Conference on Foundation and_
_Large Language Models (FLLM)_, pages 607–611, 2024.

[119] Xiaoxin He, Yijun Tian, Yifei Sun, Nitesh Chawla, Thomas Laurent,
Yann LeCun, Xavier Bresson, and Bryan Hooi. G-retriever: Retrievalaugmented generation for textual graph understanding and question
answering. _Advances in Neural Information Processing Systems_,
37:132876–132907, 2024.


JOURNAL OF L [A] TEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 17




[120] Ali Modarressi, Ayyoob Imani, Mohsen Fayyaz, and Hinrich Sch¨utze.
Ret-llm: Towards a general read-write memory for large language
models. _ArXiv_, abs/2305.14322, 2023.

[121] Wan Zhang and Jing Zhang. Hallucination mitigation for retrievalaugmented large language models: a review. _Mathematics_, 13(5):856,
2025.

[122] Chanwoong Yoon, Gangwoo Kim, Byeongguk Jeon, Sungdong Kim,
Yohan Jo, and Jaewoo Kang. Ask optimal questions: Aligning large
language models with retriever’s preference in conversation. In _Find-_
_ings of the Association for Computational Linguistics: NAACL 2025_,
pages 5899–5921, 2025.

[123] Lingrui Mei, Jiayu Yao, Yuyao Ge, Yiwei Wang, Baolong Bi, Yujun
Cai, Jiazhi Liu, Mingyu Li, Zhong-Zhi Li, Duzhen Zhang, et al. A
survey of context engineering for large language models. _arXiv preprint_
_arXiv:2507.13334_, 2025.

[124] Zhuoran Jin, Pengfei Cao, Yubo Chen, Kang Liu, Xiaojian Jiang,
Jiexin Xu, Qiuxia Li, and Jun Zhao. Tug-of-war between knowledge:
Exploring and resolving knowledge conflicts in retrieval-augmented
language models. _arXiv preprint arXiv:2402.14409_, 2024.

[125] Hanxing Ding, Liang Pang, Zihao Wei, Huawei Shen, and Xueqi
Cheng. Retrieve only when it needs: Adaptive retrieval augmentation
for hallucination mitigation in large language models. _arXiv preprint_
_arXiv:2402.10612_, 2024.

[126] Jiatai Wang, Zhiwei Xu, Di Jin, Xuewen Yang, and Tao Li. Accommodate knowledge conflicts in retrieval-augmented llms: Towards reliable
response generation in the wild. _arXiv preprint arXiv:2504.12982_,
2025.

[127] Kevin Wu, Eric Wu, and James Zou. How faithful are rag models?
quantifying the tug-of-war between rag and llms’ internal prior. _arXiv_
_e-prints_, pages arXiv–2404, 2024.

[128] Rongwu Xu, Zehan Qi, Zhijiang Guo, Cunxiang Wang, Hongru Wang,
Yue Zhang, and Wei Xu. Knowledge conflicts for llms: A survey. _arXiv_
_preprint arXiv:2403.08319_, 2024.

[129] Wujiang Xu, Zujie Liang, Kai Mei, Hang Gao, Juntao Tan, and
Yongfeng Zhang. A-mem: Agentic memory for llm agents. _arXiv_
_preprint arXiv:2502.12110_, 2025.

[130] Qitao Qin, Yucong Luo, Yihang Lu, Zhibo Chu, and Xianwei Meng.
Towards adaptive memory-based optimization for enhanced retrievalaugmented generation. _arXiv preprint arXiv:2504.05312_, 2025.

[131] Qingyue Wang, Yanhe Fu, Yanan Cao, Shuai Wang, Zhiliang Tian,
and Liang Ding. Recursively summarizing enables long-term dialogue
memory in large language models. _Neurocomputing_, 639:130193, 2025.

[132] Jinyao Guo, Chengpeng Wang, Xiangzhe Xu, Zian Su, and Xiangyu
Zhang. Repoaudit: An autonomous llm-agent for repository-level code
auditing. _ArXiv_, abs/2501.18160, 2025.

[133] Ye Ye. Task memory engine (tme): Enhancing state awareness for
multi-step llm agent tasks. _arXiv preprint arXiv:2504.08525_, 2025.

[134] Yu Wang and Xi Chen. Mirix: Multi-agent memory system for llmbased agents. _arXiv preprint arXiv:2507.07957_, 2025.
