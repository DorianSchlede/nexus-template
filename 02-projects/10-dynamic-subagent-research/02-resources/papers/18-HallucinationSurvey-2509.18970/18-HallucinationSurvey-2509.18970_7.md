<!-- Source: 18-HallucinationSurvey-2509.18970.pdf | Chunk 7/8 -->

[199] Maohao Shen, Guangtao Zeng, Zhenting Qi, Zhang-Wei Hong, Zhenfang Chen, Wei Lu, Gregory Wornell, Subhro Das, David Cox, and
Chuang Gan. Satori: Reinforcement learning with chain-of-actionthought enhances llm reasoning via autoregressive search. _arXiv_
_preprint arXiv:2502.02508_, 2025.

[200] Qiancheng Xu, Yongqi Li, Heming Xia, and Wenjie Li. Enhancing tool
retrieval with iterative feedback from large language models. In Yaser
Al-Onaizan, Mohit Bansal, and Yun-Nung Chen, editors, _Findings of_
_the Association for Computational Linguistics: EMNLP_, pages 9609–
9619, 2024.

[201] Assaf Ben-Kish, Moran Yanuka, Morris Alper, Raja Giryes, and
Hadar Averbuch-Elor. Mocha: Multi-objective reinforcement mitigating
caption hallucinations. _arXiv preprint arXiv:2312.03631_, 2, 2023.


JOURNAL OF L [A] TEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 19




[202] Tianyu Yu, Yuan Yao, Haoye Zhang, Taiwen He, Yifeng Han, Ganqu
Cui, Jinyi Hu, Zhiyuan Liu, Hai-Tao Zheng, Maosong Sun, et al. Rlhfv: Towards trustworthy mllms via behavior alignment from fine-grained
correctional human feedback. In _Proceedings of the Computer Vision_
_and Pattern Recognition Conference_, pages 13807–13816, 2024.

[203] Jiajie Zhang, Zhongni Hou, Xin Lv, Shulin Cao, Zhenyu Hou, Yilin
Niu, Lei Hou, Yuxiao Dong, Ling Feng, and Juanzi Li. Longreward:
Improving long-context large language models with ai feedback. _arXiv_
_preprint arXiv:2410.21252_, 2024.

[204] Judea Pearl. Causal inference in statistics: An overview. 2009.

[205] Xixun Lin, Qing Yu, Yanan Cao, Lixin Zou, Chuan Zhou, Jia Wu,
Chenliang Li, Peng Zhang, and Shirui Pan. Generative causality-driven
network for graph multi-task learning. _IEEE transactions on pattern_
_analysis and machine intelligence_, 2025.

[206] Shawn Li, Jiashu Qu, Yuxiao Zhou, Yuehan Qin, Tiankai Yang, and Yue
Zhao. Treble counterfactual vlms: A causal approach to hallucination.
_arXiv preprint arXiv:2503.06169_, 2025.

[207] Xinmiao Hu, Chun Wang, Ruihe An, ChenYu Shao, Xiaojun Ye, Sheng
Zhou, and Liangcheng Li. Causal-llava: Causal disentanglement for
mitigating hallucination in multimodal large language models. _arXiv_
_preprint arXiv:2505.19474_, 2025.

[208] Kangsheng Wang, Xiao Zhang, Zizheng Guo, Tianyu Hu, and Huimin
Ma. Csce: Boosting llm reasoning by simultaneous enhancing of casual
significance and consistency. _arXiv preprint arXiv:2409.17174_, 2024.

[209] Ziyi Tang, Ruilin Wang, Weixing Chen, Keze Wang, Yang Liu, Tianshui Chen, and Liang Lin. Towards causalgpt: A multi-agent approach
for faithful knowledge reasoning via promoting causal consistency in
llms. _arXiv preprint arXiv:2308.11914_, 2023.

[210] Feng Xia, Ke Sun, Shuo Yu, Abdul Aziz, Liangtian Wan, Shirui Pan,
and Huan Liu. Graph learning: A survey. _IEEE Transactions on_
_Artificial Intelligence_, 2(2):109–127, 2021.

[211] Xixun Lin, Wenxiao Zhang, Fengzhao Shi, Chuan Zhou, Lixin Zou,
Xiangyu Zhao, Dawei Yin, Shirui Pan, and Yanan Cao. Graph neural
stochastic diffusion for estimating uncertainty in node classification. In
_41st International Conference on Machine Learning (PMLR)_ . MLResearchPress, 2024.

[212] Shichao Zhu, Mufan Li, Guangmou Pan, and Xixun Lin. Ttgl: Largescale multi-scenario universal graph learning at tiktok. In _Proceedings_
_of the 31st ACM SIGKDD Conference on Knowledge Discovery and_
_Data Mining V. 2_, pages 5249–5259, 2025.

[213] Yuanchen Bei, Weizhi Zhang, Siwen Wang, Weizhi Chen, Sheng Zhou,
Hao Chen, Yong Li, Jiajun Bu, Shirui Pan, Yizhou Yu, et al. Graphs
meet ai agents: Taxonomy, progress, and future opportunities. _arXiv_
_preprint arXiv:2506.18019_, 2025.

[214] Yixin Liu, Guibin Zhang, Kun Wang, Shiyuan Li, and Shirui Pan.
Graph-augmented large language model agents: Current progress and
future prospects. _arXiv preprint arXiv:2507.21407_, 2025.

[215] Zhaoyang Liu, Zeqiang Lai, Zhangwei Gao, Erfei Cui, Zhiheng Li,
Xizhou Zhu, Lewei Lu, Qifeng Chen, Yu Qiao, Jifeng Dai, and Wenhai
Wang. Controlllm: Augment language models with tools by searching
on graphs. _arXiv preprint arXiv:2305.10601_, 2023.

[216] Yingxuan Yang, Huacan Chai, Shuai Shao, Yuanyi Song, Siyuan Qi,
Renting Rui, and Weinan Zhang. Agentnet: Decentralized evolutionary coordination for llm-based multi-agent systems. _arXiv preprint_
_arXiv:2504.00587_, 2025.

[217] Weize Chen, Ziming You, Ran Li, Yitong Guan, Chen Qian, Chenyang
Zhao, Cheng Yang, Ruobing Xie, Zhiyuan Liu, and Maosong Sun.
Internet of agents: Weaving a web of heterogeneous agents for collaborative intelligence. _arXiv preprint arXiv:2407.07061_, 2024.

[218] Chenxi Wang, Xiang Chen, Ningyu Zhang, Bozhong Tian, Haoming
Xu, Shumin Deng, and Huajun Chen. Mllm can see? dynamic
correction decoding for hallucination mitigation. _arXiv preprint_
_arXiv:2410.11779_, 2024.

[219] Feilong Tang, Chengzhi Liu, Zhongxing Xu, Ming Hu, Zile Huang,
Haochen Xue, Ziyang Chen, Zelin Peng, Zhiwei Yang, Sijin Zhou,
et al. Seeing far and clearly: Mitigating hallucinations in mllms with
attention causal decoding. In _Proceedings of the Computer Vision and_
_Pattern Recognition Conference_, pages 26147–26159, 2025.

[220] Feilong Tang, Zile Huang, Chengzhi Liu, Qiang Sun, Harry Yang,
and Ser-Nam Lim. Intervening anchor token: Decoding strategy in
alleviating hallucinations for mllms. In _The Thirteenth International_
_Conference on Learning Representations_, 2025.

[221] Sicong Leng, Hang Zhang, Guanzheng Chen, Xin Li, Shijian Lu,
Chunyan Miao, and Lidong Bing. Mitigating object hallucinations in
large vision-language models through visual contrastive decoding. In
_Proceedings of the Computer Vision and Pattern Recognition Confer-_
_ence_, pages 13872–13882, 2024.




[222] Ailin Deng, Zhirui Chen, and Bryan Hooi. Seeing is believing:
Mitigating hallucination in large vision-language models via clipguided decoding. _arXiv preprint arXiv:2402.15300_, 2024.

[223] Jesus Moncada-Ramirez, Jose-Luis Matez-Bandera, Javier GonzalezJimenez, and Jose-Raul Ruiz-Sarmiento. Agentic workflows for improving large language model reasoning in robotic object-centered
planning. _Robotics_, 14(3):24, 2025.

[224] Seongyun Lee, Sue Hyun Park, Yongrae Jo, and Minjoon Seo. Volcano:
Mitigating multimodal hallucination through self-feedback guided revision. In _Proceedings of the 2024 Conference of the North American_
_Chapter of the Association for Computational Linguistics: Human_
_Language Technologies (Volume 1: Long Papers)_, 2024.

[225] Neel P. Bhatt, Yunhao Yang, Rohan Siva, Daniel Milan, ufuk topcu,
and Zhangyang Wang. Know where you’re uncertain when planning
with multimodal foundation models: A formal framework. In _Eighth_
_Conference on Machine Learning and Systems_, 2025.

[226] Zhiyuan Hu, Chumin Liu, Xidong Feng, Yilun Zhao, See-Kiong Ng,
Anh Tuan Luu, Junxian He, Pang Wei Koh, and Bryan Hooi. Uncertainty of thoughts: Uncertainty-aware planning enhances information
seeking in large language models. _arXiv preprint arXiv:2402.03271_,
2024.

[227] Bar Karov, Dor Zohar, and Yam Marcovitz. Attentive reasoning
queries: A systematic method for optimizing instruction-following in
large language models. _arXiv preprint arXiv:2503.03669_, 2025.

[228] Ziwei Ji, Tiezheng Yu, Yan Xu, Nayeon Lee, Etsuko Ishii, and Pascale
Fung. Towards mitigating LLM hallucination via self reflection. In
_Findings of the Association for Computational Linguistics: EMNLP_
_2023_, 2023.

[229] Brian Jalaian, Nathaniel D Bastian, et al. Hydra: An agentic reasoning
approach for enhancing adversarial robustness and mitigating hallucinations in vision-language models. _arXiv preprint arXiv:2504.14395_,
2025.

[230] Fanqi Wan, Xinting Huang, Leyang Cui, Xiaojun Quan, Wei Bi, and
Shuming Shi. Knowledge verification to nip hallucination in the bud. In
_Proceedings of the 2024 Conference on Empirical Methods in Natural_
_Language Processing_, 2024.

[231] Yihao Xue, Kristjan Greenewald, Youssef Mroueh, and Baharan Mirzasoleiman. Verify when uncertain: Beyond self-consistency in black box
hallucination detection. _arXiv preprint arXiv:2502.15845_, 2025.

[232] Masha Belyi, Robert Friel, Shuai Shao, and Atindriyo Sanyal. Luna:
An evaluation foundation model to catch language model hallucinations
with high accuracy and low cost. _arXiv preprint arXiv:2406.00975_,
2024.

[233] Ansong Ni, Srini Iyer, Dragomir Radev, Ves Stoyanov, Wen-tau Yih,
Sida I Wang, and Xi Victoria Lin. Lever: Learning to verify languageto-code generation with execution. In _Proceedings of the 40th Inter-_
_national Conference on Machine Learning (ICML’23)_, 2023.

[234] Jacob Kleiman, Kevin Frank, Joseph Voyles, and Sindy Campagna.
Simulation agent: A framework for integrating simulation and large
language models for enhanced decision-making. _arXiv preprint_
_arXiv:2505.13761_, 2025.

[235] Xinyi Li, Yongfeng Zhang, and Edward C Malthouse. Large language
model agent for fake news detection. _arXiv preprint arXiv:2405.01593_,
2024.

[236] Ziqi Liu, Yangbin Chen, Ziyang Zhou, Yilin Li, Mingxuan Hu, Yushan
Pan, and Zhijie Xu. Sevade: Self-evolving multi-agent analysis with
decoupled evaluation for hallucination-resistant irony detection. _arXiv_
_preprint arXiv:2508.06803_, 2025.

[237] Yuming Chen, Jiangyan Feng, Haodong Zhang, Lijun Gong, Feng Zhu,
Rui Zhao, Qibin Hou, Ming-Ming Cheng, and Yibing Song. Realigning language to visual objects with an agentic workflow. _arXiv_
_preprint arXiv:2503.23508_, 2025.

[238] Tianyi Huang, Jingyuan Yi, Peiyang Yu, and Xiaochuan Xu. Unmasking digital falsehoods: A comparative analysis of llm-based misinformation detection strategies. In _2025 8th International Conference on_
_Advanced Algorithms and Control Engineering (ICAACE)_, pages 2470–
2476. IEEE, 2025.

[239] Xiaoxi Sun, Jinpeng Li, Yan Zhong, Dongyan Zhao, and Rui Yan.
Towards detecting llms hallucination via markov chain-based multiagent debate framework. In _ICASSP 2025-2025 IEEE International_
_Conference on Acoustics, Speech and Signal Processing (ICASSP)_,
pages 1–5. IEEE, 2025.

[240] Diego Gosmar and Deborah A Dahl. Hallucination mitigation using agentic ai natural language-based frameworks. _arXiv preprint_
_arXiv:2501.13946_, 2025.

[241] Xiaoyu Pan, Yang Bai, Ke Zou, Yang Zhou, Jun Zhou, Huazhu Fu, YihChung Tham, and Yong Liu. Eh-benchmark ophthalmic hallucination


JOURNAL OF L [A] TEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 20



benchmark and agent-driven top-down traceable reasoning workflow.
_arXiv preprint arXiv:2507.22929_, 2025.

[242] Xiaoxue Cheng, Junyi Li, Wayne Xin Zhao, Hongzhi Zhang, Fuzheng
Zhang, Di Zhang, Kun Gai, and Ji-Rong Wen. Small agent can also
rock! empowering small language models as hallucination detector.
_arXiv preprint arXiv:2406.11277_, 2024.

[243] Zhixuan Chu, Lei Zhang, Yichen Sun, Siqiao Xue, Zhibo Wang, Zhan
Qin, and Kui Ren. Sora detector: A unified hallucination detection for
large text-to-video models. _arXiv preprint arXiv:2405.04180_, 2024.

[244] Renan Souza, Amal Gueroudji, Stephen DeWitt, Daniel Rosendo,
Tirthankar Ghosal, Robert Ross, Prasanna Balaprakash, and Rafael Ferreira da Silva. Prov-agent: Unified provenance for tracking ai agent
interactions in agentic workflows. _arXiv preprint arXiv:2508.02866_,
2025.

[245] Hayley Ross, Ameya Sunil Mahabaleshwarkar, and Yoshi Suhara.
When2call: When (not) to call tools. _arXiv preprint arXiv:2504.18851_,
2025.

[246] Yue Cui, Liuyi Yao, Shuchang Tao, Weijie Shi, Yaliang Li, Bolin Ding,
and Xiaofang Zhou. Enhancing tool learning in large language models
with hierarchical error checklists. _arXiv preprint arXiv:2506.00042_,
2025.

[247] Jingwen Zhou, Jieshan Chen, Qinghua Lu, Dehai Zhao, and Liming
Zhu. Shielda: Structured handling of exceptions in llm-driven agentic
workflows. _arXiv preprint arXiv:2508.07935_, 2025.

[248] Jiazhan Feng, Shijue Huang, Xingwei Qu, Ge Zhang, Yujia Qin,
Baoquan Zhong, Chengquan Jiang, Jinxin Chi, and Wanjun Zhong.
Retool: Reinforcement learning for strategic tool use in llms. _arXiv_
_preprint arXiv:2504.11536_, 2025.

[249] Gaurang Sriramanan, Siddhant Bharti, Vinu Sankar Sadasivan,
Shoumik Saha, Priyatham Kattakinda, and Soheil Feizi. Llm-check:
Investigating detection of hallucinations in large language models.
_Advances in Neural Information Processing Systems_, 37:34188–34216,
2024.

[250] Xiang Chen, Chenxi Wang, Yida Xue, Ningyu Zhang, Xiaoyan Yang,
Qiang Li, Yue Shen, Lei Liang, Jinjie Gu, and Huajun Chen. Unified
hallucination detection for multimodal large language models. _arXiv_
_preprint arXiv:2402.03190_, 2024.

[251] Yuyan Chen, Qiang Fu, Yichen Yuan, Zhihao Wen, Ge Fan, Dayiheng
Liu, Dongmei Zhang, Zhixu Li, and Yanghua Xiao. Hallucination
detection: Robustly discerning reliable answers in large language
models. In _Proceedings of the 32nd ACM International Conference_
_on Information and Knowledge Management_, pages 245–255, 2023.

[252] Ben Snyder, Marius Moisescu, and Muhammad Bilal Zafar. On
early detection of hallucinations in factual question answering. In
_Proceedings of the 30th ACM SIGKDD Conference on Knowledge_
_Discovery and Data Mining_, pages 2721–2732, 2024.

[253] Hao Fei, Meng Luo, Jundong Xu, Shengqiong Wu, Wei Ji, Mong-Li
Lee, and Wynne Hsu. Fine-grained structural hallucination detection
for unified visual comprehension and generation in multimodal llm.
In _Proceedings of the 1st ACM Multimedia Workshop on Multi-modal_
_Misinformation Governance in the Era of Foundation Models_, pages
13–22, 2024.

[254] Yuiga Wada, Kazuki Matsuda, Komei Sugiura, and Graham Neubig.
Zina: Multimodal fine-grained hallucination detection and editing.
_arXiv preprint arXiv:2506.13130_, 2025.

[255] Mobashir Sadat, Zhengyu Zhou, Lukas Lange, Jun Araki, Arsalan
Gundroo, Bingqing Wang, Rakesh R Menon, Md Rizwan Parvez, and
Zhe Feng. Delucionqa: Detecting hallucinations in domain-specific
question answering. _arXiv preprint arXiv:2312.05200_, 2023.

[256] Anisha Gunjal, Jihan Yin, and Erhan Bas. Detecting and preventing
hallucinations in large vision language models. In _Proceedings of the_
_AAAI Conference on Artificial Intelligence_, volume 38, pages 18135–
18143, 2024.

[257] Borui Yang, Md Afif Al Mamun, Jie M Zhang, and Gias Uddin. Hallucination detection in large language models with metamorphic relations. _Proceedings of the ACM on Software Engineering_, 2(FSE):425–
445, 2025.

[258] Shengqiong Wu, Hao Fei, Liangming Pan, William Yang Wang,
Shuicheng Yan, and Tat-Seng Chua. Combating multimodal llm
hallucination via bottom-up holistic reasoning. In _Proceedings of the_
_AAAI Conference on Artificial Intelligence_, volume 39, pages 8460–
8468, 2025.

[259] Haichuan Hu, Congqing He, Xiaochen Xie, and Quanjun Zhang.
Lrp4rag: Detecting hallucinations in retrieval-augmented generation via
layer-wise relevance propagation. _arXiv preprint arXiv:2408.15533_,
2024.




[260] Jonathan J Ross, Ekaterina Khramtsova, Anton van der Vegt, Bevan
Koopman, and Guido Zuccon. Rarr unraveled: Component-level
insights into hallucination detection and mitigation. In _Proceedings_
_of the 48th International ACM SIGIR Conference on Research and_
_Development in Information Retrieval_, pages 3286–3295, 2025.

[261] ´Ad´am Kov´acs and G´abor Recski. Lettucedetect: A hallucination detection framework for rag applications. _arXiv preprint arXiv:2502.17125_,
2025.

[262] Baolong Bi, Shenghua Liu, Yiwei Wang, Yilong Xu, Junfeng Fang,
Lingrui Mei, and Xueqi Cheng. Parameters vs. context: Fine-grained
control of knowledge reliance in language models. _arXiv preprint_
_arXiv:2503.15888_, 2025.

[263] Rongzhe Wei, Peizhi Niu, Hans Hao-Hsun Hsu, Ruihan Wu, Haoteng
Yin, Mohsen Ghassemi, Yifan Li, Vamsi K Potluru, Eli Chien, Kamalika Chaudhuri, et al. Do llms really forget? evaluating unlearning
with knowledge correlation and confidence awareness. _arXiv preprint_
_arXiv:2506.05735_, 2025.

[264] Zhoubo Li, Ningyu Zhang, Yunzhi Yao, Mengru Wang, Xi Chen, and
Huajun Chen. Unveiling the pitfalls of knowledge editing for large
language models. _arXiv preprint arXiv:2310.02129_, 2023.

[265] Mingda Chen, Yang Li, Karthik Padthe, Rulin Shao, Alicia Sun, Luke
Zettlemoyer, Gargi Ghosh, and Wen-tau Yih. Improving factuality with
explicit working memory. _arXiv preprint arXiv:2412.18069_, 2024.

[266] Zhiyu Li, Shichao Song, Chenyang Xi, Hanyu Wang, Chen Tang, Simin
Niu, Ding Chen, Jiawei Yang, Chunyu Li, Qingchen Yu, et al. Memos:
A memory os for ai system. _arXiv preprint arXiv:2507.03724_, 2025.

[267] Boyu Qiao, Kun Li, Wei Zhou, and Songlin Hu. Dynamic simulation
framework for disinformation dissemination and correction with social
bots. _arXiv preprint arXiv:2507.16848_, 2025.

[268] Hanjun Luo, Shenyu Dai, Chiming Ni, Xinfeng Li, Guibin Zhang,
Kun Wang, Tongliang Liu, and Hanan Salam. Agentauditor: Humanlevel safety and security evaluation for llm agents. _arXiv preprint_
_arXiv:2506.00641_, 2025.

[269] Ohav Barbi, Ori Yoran, and Mor Geva. Preventing rogue agents
improves multi-agent collaboration. _arXiv preprint arXiv:2502.05986_,
2025.

[270] Chris Olah. Mechanistic Interpretability, Variables, and the Importance
of Interpretable Bases, June 2022.

[271] Ziwei Ji, Lei Yu, Yeskendir Koishekenov, Yejin Bang, Anthony
Hartshorn, Alan Schelten, Cheng Zhang, Pascale Fung, and Nicola
Cancedda. Calibrating verbal uncertainty as a linear feature to reduce
hallucinations. _arXiv preprint arXiv:2503.14477_, 2025.

[272] Zhuoran Jin, Pengfei Cao, Hongbang Yuan, Yubo Chen, Jiexin Xu,
Huaijun Li, Xiaojian Jiang, Kang Liu, and Jun Zhao. Cutting
off the head ends the conflict: A mechanism for interpreting and
mitigating knowledge conflicts in language models. _arXiv preprint_
_arXiv:2402.18154_, 2024.

[273] Tianrui Guan, Fuxiao Liu, Xiyang Wu, Ruiqi Xian, Zongxia Li, Xiaoyu
Liu, Xijun Wang, Lichang Chen, Furong Huang, Yaser Yacoob, Dinesh
Manocha, and Tianyi Zhou. Hallusionbench: An advanced diagnostic
suite for entangled language hallucination and visual illusion in large
vision-language models. In _Proceedings of the Computer Vision and_
_Pattern Recognition Conference_, 2024.

[274] Yuanzhe Hu, Yu Wang, and Julian McAuley. Evaluating memory
in llm agents via incremental multi-turn interactions. _arXiv preprint_
_arXiv:2507.05257_, 2025.

[275] Magdalena Biesialska, Katarzyna Biesialska, and Marta R Costa-Jussa.
Continual lifelong learning in natural language processing: A survey.
_arXiv preprint arXiv:2012.09823_, 2020.

[276] Tri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and Christopher
R´e. Flashattention: Fast and memory-efficient exact attention with
io-awareness. _Advances in neural information processing systems_,
35:16344–16359, 2022.

[277] Kewei Cheng, Nesreen K Ahmed, Ryan A Rossi, Theodore Willke, and
Yizhou Sun. Neural-symbolic methods for knowledge graph reasoning:
A survey. _ACM Transactions on Knowledge Discovery from Data_,
18(9):1–44, 2025.

[278] Mitra Baratchi, Can Wang, Steffen Limmer, Jan N Van Rijn, Holger
Hoos, Thomas B¨ack, and Markus Olhofer. Automated machine learning: past, present and future. _Artificial intelligence review_, 57(5):122,
2024.

[279] Zelong Li, Shuyuan Xu, Kai Mei, Wenyue Hua, Balaji Rama, Om Raheja, Hao Wang, He Zhu, and Yongfeng Zhang. Autoflow: Automated
workflow generation for large language model agents. _arXiv preprint_
_arXiv:2407.12821_, 2024.


JOURNAL OF L [A] TEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 21



APPENDIX A
LOOP OF LLM-BASED MULTI-AGENT SYSTEM


Different from the single-agent setting, each LLM-based
agent in the MAS must communicate with other agents to
accomplish goals. To model these processes, a communication
structure _Gt_ is introduced among _N_ agents. Furthermore, _Gt_
would evolve with time to align with the dynamic adjustments
of MAS. Therefore, compared with Section II-B, the loop of
LLM-based MAS includes two additional procedures: **Broad-**
**casting** and **Structure Evolution** .

_•_ **Reasoning** : Each agent _i_ first generates its own plan _p_ _[i]_ _t_
for the next action conditioned on _b_ _[i]_ _t_ [and] _[ g]_ [:]


_p_ _[i]_ _t_ [= Φ] _[i]_ [(] _[b]_ _t_ _[i][, g]_ [)] _[.]_ (15)


_•_ **Execution** : Each agent _i_ then translates _p_ _[i]_ _t_ [into an exe-]
cutable action _a_ _[i]_ _t_ [:]


_a_ _[i]_ _t_ [=] _[ E][i]_ [(] _[b]_ _t_ _[i][, p][i]_ _t_ [)] _[.]_ (16)


_•_ **Broadcasting** : The agent _i_ broadcasts its message to its
neighbors in _Gt_ according to its plan _p_ _[i]_ _t_ [:]

      - _qt_ _[i][→][j]_ �� _j ∈_ _N_ ( _i_ )� = _B_ ( _b_ _[i]_ _t_ _[, p]_ _t_ _[i][, G][t]_ [)] _[.]_ (17)


Here   - _qt_ _[i][→][j]_ �� _j ∈_ _N_ ( _i_ )� denotes the messages sent by
the agent _i_ to its neighbors (i.e., _N_ ( _i_ )) at the time step _t_ .

_•_ **Feedback** : The learning environment would also provide
a reward _rt_ based on _st_ and _{a_ _[i]_ _t_ _[}][N]_ _i_ =1 [:]


_rt_ = _R_             - _st, {a_ _[i]_ _t_ _[}]_ _i_ _[N]_ =1� _._ (18)


_•_ **Environment** **Transition** : The learning environment
transitions to _st_ +1 based on _st_ and _{a_ _[i]_ _t_ _[}][N]_ _i_ =1 [:]


Pr     - _st_ +1�� _st, {ait_ _[}]_ _i_ _[N]_ =1� = _T_     - _st, {a_ _[i]_ _t_ _[}]_ _i_ _[N]_ =1� _._ (19)


_•_ **Perception** : Each agent _i_ perceives _st_ +1 to generate the
observation _o_ _[i]_ _t_ +1 [:]


_o_ _[i]_ _t_ +1 [=] _[ Z]_ _[i]_ [�] _st_ +1 _, b_ _[i]_ _t_ _[, a]_ _t_ _[i][,]_     - _qt_ _[j][→][i]_ �� _j ∈_ _N_ ( _i_ )�� _,_ (20)


where   - _qt_ _[j][→][i]_ �� _j ∈_ _N_ ( _i_ )� denotes the messages received
by the agent _i_ from its neighbors at the time step _t_ .

_•_ **Memorization** : The external memory module of the
agent _i_ is updated as follows,


_m_ _[i]_ _t_ +1 [=] _[ L]_ _M_ _[i]_  - _m_ _[i]_ _t_ _[, a][i]_ _t_ _[,]_  - _qt_ _[j][→][i]_ �� _j ∈_ _N_ ( _i_ )� _, o_ _[i]_ _t_ +1� _._ (21)


_•_ **Belief Update** : Then the agent _i_ refines its belief state as
follows,


_b_ _[i]_ _t_ +1 [=] _[ L]_ _B_ _[i]_   - _b_ _[i]_ _t_ _[, m][i]_ _t_ +1 _[, a][i]_ _t_ _[,]_   - _qt_ _[j][→][i]_ �� _j ∈_ _N_ ( _i_ )� _, rt_ _[i][, o][i]_ _t_ +1 _[, g]_   - _._
(22)

_•_ **Structure Evolution** : Based on _{b_ _[i]_ _t_ +1 _[}][N]_ _i_ =1 [and] _[ G][t]_ [, the]
communication structure can be updated as follows,


_Gt_ +1 = _U_        - _Gt, {b_ _[i]_ _t_ +1 _[}][N]_ _i_ =1� _._ (23)



APPENDIX B
HALLUCINATION EXAMPLE EXPLANATION


As shown in Fig. 5, we present the representative examples
of each type of agent hallucinations as follows,


_•_ **Goal Understanding Hallucinations.** In this example,
the user asks the agent to “recommend a restaurant
suitable for dining with elders”. The underlying goal is
clearly to find a restaurant whose cuisine is light and
easy to digest and whose environment is appropriate
for family gatherings. However, the agent recommends
“Spicy World”, a restaurant specializing in very spicy
crayfish and boiled fish, thereby completely ignoring the
prerequisite “suitable for elders”.
