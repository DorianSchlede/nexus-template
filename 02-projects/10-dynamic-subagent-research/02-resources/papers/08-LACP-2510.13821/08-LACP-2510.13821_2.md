<!-- Source: 08-LACP-2510.13821.pdf | Chunk 2/2 -->


Yingxuan Yang, Huacan Chai, Yuanyi Song, Siyuan Qi, Muning Wen, Ning Li, Junwei Liao, Haoyi Hu, Jianghao
Lin, Gaowei Chang, et al. A survey of ai agent protocols. _arXiv preprint arXiv:2504.16736_, 2025.


Hujun Yin and Siavash Alamouti. Ofdma: A broadband wireless access technology. In _2006 IEEE sarnoff_
_symposium_, pages 1–4. IEEE, 2006.


6


**A** **Experimental Validation of LACP**


To validate the practical feasibility, performance characteristics, and security guarantees of the LLM
Agent Communication Protocol (LACP), we implemented a working prototype and conducted a
series of targeted experiments. This appendix details the methodology and results of these validations.


**A.1** **Experimental Setup**


All experiments were conducted on a local machine. The LACP-compliant server endpoints were
implemented using Python 3.11 with the Flask web framework. Cryptographic operations utilized the
`python-jose` library, and high-performance JSON processing was handled by `orjson` . The clientside test harnesses were custom Python scripts using the `requests` library. For the interoperability
test, we used the `langchain` library (version 0.1.20).


**A.2** **Experiment 1: Performance and Overhead Analysis**


**Objective:** To quantify the latency and payload size overhead of LACP compared to a standard,
non-secured RESTful API communication baseline.


**Methodology:** We developed a benchmark script that sent 10,000 sequential requests to two different
server endpoints:


1. **Baseline Endpoint:** A standard Flask route accepting a JSON payload via HTTP POST.


2. **LACP Endpoint:** A route accepting a JWS-signed LACP message. It performs cryptographic signature verification before processing the payload.


We tested this across three payload sizes to simulate a range of agent interactions, using our optimized
LACP implementation (shortened keys, ECDSA signatures).


    - **Small (51 Bytes):** Simulating a simple ACK or heartbeat message.


    - **Medium (151 Bytes):** Simulating a basic tool call with a few parameters.


    - **Large (1,964 Bytes):** Simulating a complex message, such as a detailed plan for a robotic
arm with multiple steps and coordinates.


**Results:** The performance analysis reveals a clear trend: LACP’s relative overhead is inversely
proportional to the complexity and size of the agent message. The results are summarized in Table 3.


Table 3: Performance and Overhead Analysis of LACP vs. Baseline REST.


**Payload Scenario** **Baseline Size** **LACP Size** **Size Overhead (%)** **Baseline Latency** **LACP Latency** **Latency Overhead (%)**


Small (51B) 51 bytes 306 bytes +500% 0.85 ms 0.88 ms +3.5%
Medium (151B) 151 bytes 442 bytes +191% 0.86 ms 0.89 ms +3.1%
**Large (1,964B)** 1,964 bytes 2,560 bytes **+30%** 0.89 ms 0.92 ms **+2.9%**


**Discussion:** The results strongly indicate LACP’s feasibility for real-world applications. The latency
overhead is minimal across all scenarios, with an absolute increase of only **0.03ms** for large, complex
tasks. The payload size overhead, while significant for trivial messages, shrinks to a modest and
justifiable **+30%** for realistic payloads. This represents the necessary cost for verifiable, end-to-end
cryptographic security and is a reasonable trade-off for the guarantees LACP provides.


**A.3** **Experiment 2: Interoperability Demonstration**


**Objective:** To provide a concrete demonstration of LACP’s ability to enable seamless communication
between an agent built on a major framework and a framework-agnostic tool.


**Methodology:** We constructed a two-part system:


1. **LACP Tool Server:** A standalone Flask server exposing a single endpoint. This server
hosted a simple "calculator" tool and was configured to only accept and respond with valid,
signed LACP messages.


7


2. **LangChain Agent Client:** A standard ReAct agent was implemented using the LangChain
library. We equipped it with a custom tool where the underlying function did not execute
logic locally. Instead, its sole purpose was to construct and send a signed LACP `ACT` message
to the tool server.


The experimental flow was as follows: The LangChain agent was given a prompt requiring calculation.
Its reasoning process triggered the custom tool, which sent the `ACT` message. The server verified,
executed, and returned the result in a signed `OBSERVE` message. The agent’s tool then verified this
response and passed the result to the agent’s `Observation` field to complete the reasoning loop.


**Results:** The experiment was successful. The LangChain agent was able to transparently use the
external, framework-agnostic tool, completing its task correctly. The entire interaction was secured
and structured by LACP without requiring any custom integration code on the server specific to the
LangChain framework.


**Discussion:** This experiment provides a practical blueprint for solving the N² integration problem
that currently fragments the agent ecosystem. It validates that the `PLAN/ACT/OBSERVE` schema is
a practical fit for the operational logic of modern agent frameworks and that LACP can serve as a
universal communication bridge.


**A.4** **Experiment 3: Security Validation**


**Objective:** To empirically demonstrate that LACP’s Transactional Layer provides critical, applicationlayer security guarantees that are not covered by transport-layer security (TLS) alone.


**Methodology:** We implemented an attack simulator script that targeted our LACP server endpoint
with two common application-layer attack vectors.


**A.4.1** **Tampering Attack**


    - **Setup:** The script generated a valid, signed LACP message for a hypothetical financial
transaction to transfer an `amount` of `100` .


    - **Action:** Before sending, the script programmatically altered the payload content _after_ it
had been signed, changing the `amount` to `10000` while keeping the original, now-invalid,
signature.


    - **Observable Outcome:** Upon receiving the tampered message, the server’s cryptographic
verification step immediately failed. The server logged a signature mismatch error and
returned an **HTTP 403 Forbidden** status code, preventing the fraudulent transaction.


**A.4.2** **Replay Attack**


    - **Setup:** The script sent a legitimate, valid LACP message, which the server processed
successfully. The server was configured to track the `transaction_id` of all processed
messages.


    - **Action:** The script then immediately re-sent the exact same valid message.


    - **Observable Outcome:** The server’s signature verification passed, but its Transactional
Layer logic identified the `transaction_id` as a duplicate. The server rejected the request
and returned an **HTTP 409 Conflict** status code, preventing the double-processing of the
operation.


**Discussion:** These experiments provide a definitive, practical answer to the question "Why not just
TLS?". They demonstrate that LACP is not redundant but essential for protecting against attacks that
occur _after_ TLS decryption at an endpoint. The signature verification provides end-to-end message
integrity, while the tracking of transaction IDs ensures idempotency, both of which are crucial for
building trustworthy, high-stakes multi-agent systems.


8


**B** **LACP Protocol Example**


Figure 3 demonstrates LACP’s layer-by-layer message: a semantic `PLAN` payload is wrapped by a
signed, two-phase-commit envelope and then by a binary transport frame, yielding a secure, reliable,
and interoperable message path that scales to complex multi-agent systems.

```
          // (1) Semantic layer

          {
            "type": "plan",
            "intent_id": "move_object_plan_1",
            "role": "planner-agent",
            "natural_language": "Move object from A to B",
            "graph_ops": [
          { "op": "grasp", "args": {"from": "A"} },
          { "op": "move", "args": {"to": "B"} },
          { "op": "release" }
          ]
          }

          // (2) Transactional layer
          { "transaction_id": "tx-28a9ef",
            "sequence_num": 42,
            "timestamp": "2025-05-13T10:15:30Z",
            "source_agent": "planner-agent",
            "target_agent": "robot-controller",
            "payload": /* semantic object above */,
            "signature": "<base64url-ed25519>",
            "timeout_ms": 5000
          }

          // (3) Transport frame header

          [0x00 0x2F ...]
          <transactional message as binary>
          [0xAD 0xDE]    // frame checksum

```

Figure 3: Layer-by-layer encoding of a `PLAN` message in LACP. (1) the bare semantic payload, (2) the
same payload wrapped by the transactional layer with a JSON Web Signature, and (3) the truncated
binary transport frame.


**C** **Protocol Comparison Analysis**


Table 4: Feature support in existing agent-communication proposals.


**Feature** **Functions call** **Agent Protocol** **MCP** **A2A** **Agora** **LACP**


Cross-framework interoperability _◦_ _•_ _•_ _•_ _•_ _•_
Multi-agent coordination _×_ _◦_ _◦_ _•_ _•_ _•_
Layered architecture _×_ _×_ _•_ _◦_ _◦_ _•_
End-to-end message signing _×_ _×_ _◦_ _•_ _◦_ _•_
Transaction guarantees _×_ _◦_ _×_ _◦_ _•_ _•_
Retry/timeout mechanisms _×_ _◦_ _◦_ _◦_ _◦_ _•_
Independence from specific LLM _×_ _•_ _•_ _•_ _•_ _•_
_×_ : Not supported, _◦_ : Partially supported, _•_ : Supported


Contemporary agent communication protocols, while addressing specific use cases, fail to simultaneously deliver the four pillars essential for production-grade multi-agent systems: comprehensive
cross-framework interoperability, explicit multi-agent coordination, mandatory cryptographic security, and robust transaction guarantees. Table 4 demonstrates this critical limitation—no existing
protocol provides comprehensive support across all dimensions, creating systematic vulnerabilities
that compound in safety-critical deployments.


9


**D** **Alternative Views & Rebuttals**


The proposal to standardize LLM agent communication via LACP invites critical assessment. We
proactively address anticipated objections:


**Objection 1: Standardization will stifle innovation.**
_Rebuttal:_ LACP standardizes foundational communication primitives—syntax, transactional integrity,
security—not core agent intelligence or learning algorithms. Analogous to TCP/IP’s role in fostering
internet innovation, LACP aims to provide a stable, interoperable substrate. By abstracting essential
reliability and security concerns to the protocol level, LACP liberates resources for innovation in
higher-order agent functionalities and application-specific logic, consistent with the need for protocols
that enable, rather than restrict, capability evolution.


**Objection 2: The semantic diversity of agent tasks precludes a unified grammar.**
_Rebuttal:_ LACP employs the “narrow waist” architectural principle, analogous to the internet protocol
suite. It standardizes a minimal set of essential message types (e.g., PLAN, ACT, OBSERVE) and
interaction patterns, while deliberately remaining agnostic to payload content. This design allows
diverse domain-specific semantics and complex data structures to be embedded within standardized message envelopes, thereby balancing interoperability with the requisite flexibility for varied
application scenarios.


**Objection 3: Additional protocol overhead will degrade performance, particularly in latency-**
**sensitive applications.**
_Rebuttal:_ LACP’s design inherently considers performance, a critical evaluation dimension for agent
protocols. The use of efficient binary encodings, modern transport protocols (e.g., QUIC, HTTP/2),
and header compression techniques can substantially mitigate transmission overhead. We project
that the overhead attributable to LACP’s transactional and security features will be marginal relative
to the significant improvements in communication reliability, security, and interoperability. This
modest performance cost is particularly justified in safety-critical applications where correctness and
verifiability are paramount.


**Objection 4: Existing agent frameworks and their proprietary protocols offer adequate com-**
**munication solutions.**
_Rebuttal:_ Current frameworks, while valuable, often present ecosystem-specific or incomplete communication mechanisms, as detailed in Table 1 and Section 2. They frequently lack comprehensive
end-to-end security, robust transactional integrity for complex multi-step operations, or true crossframework interoperability without substantial custom integration. LACP is conceptualized to address
these identified critical deficiencies, potentially integrating as a foundational layer to augment, rather
than merely replace, existing systems.


**E** **Extended History of Telecom Protocol Evolution**


Protocols in wireless communication are the invisible contracts that let billions of radios share
spectrum, identify one another, and negotiate Quality of Service (QoS). They differ from isolated
hardware breakthroughs in that, once a rule set is adopted, every additional user or device deepens
the network’s value, creating a positive-feedback loop of interoperability, scale, and economic impact.
From spark-gap transmitters to the emerging International Mobile Telecommunications-2030 (IMT2030) vision, each protocol generation has arrived precisely when the previous rules could no longer
unlock the next order of societal benefit.


From Marconi’s spark-gap experiments in 1895 to the proliferation of Amplitude-Modulation and
Frequency-Modulation (AM/FM) broadcasting in the 1930s, wireless links functioned as largely selfcontained point-to-point systems. Message formats were improvised, spectrum was treated as private
acreage, and interference was mitigated chiefly through geographic separation. A first, tentative
move toward codified coordination came with the 1906 International Radiotelegraph Convention,
which standardised station identifiers and adopted a universal distress call but offered little additional
guidance; nonetheless, it proved that trans-border governance was feasible. For the next half-century,
the ether remained virtually protocol-empty. A genuine system perspective emerged only after largescale propagation campaigns—most notably those by Okumura—were distilled into closed-form
path-loss expressions, and the subsequent 1980 formalisation of these data as the Hata model Hata

[2013] provided engineers with a quantitative slide rule for planning frequency-reusable small cells.


10


This analytical lens opened the way for dedicated cellular control channels and, ultimately, for the
layered protocol stacks that characterise modern mobile networks.


**1G—Channelised Voice.** Quantitative propagation models such as the Hata formulation provided
the first system-level insight that radio spectrum could be reused aggressively Bernhardt [1987],
and this analytic foundation precipitated the first generation (1G) of analogue cellular networks.
Early systems—most notably the Advanced Mobile Phone System (AMPS) and the Nordic Mobile
Telephone (NMT) network defined in Telecommunications Industry Association Standard (TIA) 553
MacDonald [1979] —shared a common control channel implemented as a narrow-band frequencyshift keying (FSK) stream carrying three message types: ORIGINATION, PAGE RESPONSE, and
HANDOFF. This concise signalling grammar satisfied the era’s central requirement of seamless
city-wide mobility within tight frequency-reuse clusters and demonstrated that automated hand-off
could accommodate a rapidly growing subscriber base Lee [1989].


The analogue architecture, however, exposed serious weaknesses. Conversations were transmitted as
unencrypted FM audio, roaming identity was absent, and capacity was constrained by fixed guard
bands that grew ever more expensive as handset density increased; eavesdropping required little more
than a consumer scanner. These limitations drove the transition to digital second-generation (2G)
protocols.


**2G—Digital Identity and Security.**


Digital second-generation (2G) systems were designed explicitly to eliminate the three vulnerabilities
of the analogue era—clear-text audio, weak identity management, and rigid spectrum utilisation—and
to support the rising expectation that mobile phones should work across national borders. The Global
System for Mobile Communications (GSM) replaced frequency-modulated speech with full-rate
and half-rate vocoders surrounded by convolutional coding and cyclic redundancy checks, then
applied stream ciphers derived from a per-session challenge–response procedure Gerstlauer et al.

[2000]. Subscriber credentials moved from handset firmware to a removable Subscriber Identity
Module (SIM), enabling both secure roaming and mass-market prepaid services. A Time-Division
Multiple-Access (TDMA) frame with eight slots compressed eight encrypted calls into the bandwidth
that one analogue conversation had occupied, increasing spectral efficiency by nearly an order of
magnitude. Interim Standard 95 (IS-95) pursued the same goals with direct-sequence Code-Division
Multiple Access (CDMA), spreading each user’s signal across the full carrier and achieving soft
capacity that grew with signal-to-interference ratio. Although conceived only as control-plane text,
GSM’s mobile-originated short-message procedure was quickly commercialised as the 160-character
Short Message Service (SMS), illustrating how richer signalling grammars could spawn unanticipated
revenue streams and setting a precedent for the data-centric evolutions that would define the third
generation.


**3G—State Machines for Soft Handover.**


Although 2G systems such as GSM and IS-95 multiplied spectral efficiency and introduced ciphered
speech, they remained voice-centric, circuit-switched, and locked to kilobit-per-second data rates. The
rapid uptake of laptops and early smartphones exposed those limits and motivated a new air interface
capable of packet-switched megabit throughput. This requirement defined the third generation (3G)
under the International Mobile Telecommunications-2000 (IMT-2000) umbrella.


The Universal Mobile Telecommunications System (UMTS) adopted Wideband Code Division
Multiple Access (WCDMA) on a 5 MHz carrier and inserted a Radio Network Controller (RNC)
between the base station and the core. Variable-rate convolutional and turbo coding, together
with 1,500-Hz-cycle power-control commands, maintained link quality in dense urban multipath,
while soft handover allowed a handset to combine energy from multiple cells Holma and Toskala

[2005]. Because these techniques could not be expressed with the GSM Layer-3 grammar, the Third
Generation Partnership Project specified a four-state Radio Resource Control (RRC) machine in TS
25.331 and introduced new primitives such as `MEASUREMENT_REPORT`, `ACTIVE_SET_UPDATE`, and
high-frequency scheduling commands. Handover logic migrated to the handset, enabling data rates
above 1 Mb/s and permitting applications to adapt radio requirements in real time. These protocol
advances completed the transition from voice-first mobility to data-driven connectivity and set the
stage for the orthogonal-frequency-division-multiple-access Long-Term Evolution (LTE) architecture
that would follow in the fourth generation (4G).


11


**4G—Bearer Abstraction and All-IP Core.**


Even with WCDMA’s megabit-class links, third-generation networks remained spectrum-constrained
because spreading codes were finite and the uplink and downlink shared a coupled bandwidth that
limited scheduling agility. These constraints set the stage for the 4G, formalised as Long-Term
Evolution (LTE).


LTE replaced code-division multiplexing with Orthogonal Frequency-Division Multiple Access
(OFDMA) on the downlink Yin and Alamouti [2006] and Single-Carrier Frequency-Division Multiple
Access (SC-FDMA) on the uplink, while embracing multi-stream Multiple-Input Multiple-Output
(MIMO) antenna processing Paulraj et al. [2004]. Together, these techniques tripled spectral efficiency
without requiring additional spectrum and lifted peak user rates far beyond the reach of 3G. The
architectural breakthrough, however, lay higher in the stack: the Evolved Packet System (EPS) bearer
specified in 3GPP TS 36.300. Because both the control plane (using Diameter for policy) and the
user plane (using GPRS Tunnelling Protocol–User, GTP-U) carried pure IP, application developers
could count on predictable latency and bandwidth. These features, documented succinctly in Raj
Jain’s “Introduction to LTE” notes, enabled voice, video, and data to converge on an all-IP core and
catalysed the mobile-app economy years before massive-MIMO hardware became commonplace.


**5G—Service-Based Architecture.**


While 4G LTE unified voice, video, and data on an all-IP core, it struggled to meet the emerging
demands of ultra-low latency, massive IoT, and immersive applications. To address these limitations,
3GPP introduced the fifth generation (5G) with New Radio (NR), formalised in TR 38.913. It
defined a tri-polar service model—enhanced Mobile Broadband (eMBB), Ultra-Reliable Low-Latency
Communications (URLLC), and massive Machine-Type Communications (mMTC)—alongside
flexible subcarrier numerologies and beam-centric massive MIMO Larsson et al. [2014].


Beyond the air interface, 5G’s core network (TS 23.501/502) adopted a service-based architecture,
replacing rigid LTE signalling with microservices exposed via HTTP/2 and JSON. Sessions now
carry explicit QoS profiles tied to eMBB, URLLC, or mMTC needs, enabling true network slicing.
Recent releases extend this model to non-terrestrial networks, Reconfigurable Intelligent Surfaces
(RIS), and native AI integration, positioning 5G not just as a faster pipe but as a programmable spatial
platform Huang et al. [2019].


**6G—Protocols for Distributed Cognition.**


Building on the service-centric flexibility of 5G, the sixth generation (6G) envisions a leap from
connectivity to distributed cognition. The ITU-R IMT-2030 framework outlines a new protocol
vocabulary to support this shift: symbol-level beam tracking for sub-terahertz channels, joint communication–sensing pilots that unify radar and data functions, and federated learning primitives such
as `MODEL_REGISTER` and `GRADIENT_PUSH` for on-device intelligence coordination. While hardware
prototypes already demonstrate 100 Gb/s links at 140 GHz, such capabilities remain confined to the
lab without standardised signalling to govern when, why, and how each node should act.


As with every generation before, physical breakthroughs alone are not enough. True scale and societal
impact emerge only when those breakthroughs are encoded into shared protocols. From analogue
control tones to JSON APIs and now AI-native message exchanges, wireless systems evolve by
teaching radios to speak a richer language of coordination.


Seen in sequence, each protocol generation has delivered a specific new capability—mobility, digital
security, mobile internet, all-IP convergence, service slicing, intelligent surfaces—that unlocked a
fresh wave of economic and social value while postponing spectrum exhaustion. The lesson is clear:
continued protocol research is not a peripheral activity but the core enabler of every subsequent
hardware advance. Without new rule sets to orchestrate spectrum, topology, and compute, future
breakthroughs in terahertz silicon, satellite constellations, or AI accelerators will remain islands of
potential rather than the next shared infrastructure Letaief et al. [2019], Anthropic [2024], Azari et al.

[2022].


12


