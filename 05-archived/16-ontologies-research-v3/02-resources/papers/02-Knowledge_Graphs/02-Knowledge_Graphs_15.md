<!-- Source: 02-Knowledge_Graphs.pdf | Chunk 15/15 -->

  - We use indexed parentheses â€“ such as (x) _ğ‘–_, (X) _ğ‘–ğ‘—_, or (X) _ğ‘–_ 1 _...ğ‘–ğ‘›_   - to denote elements of vectors,
matrices, and tensors, respectively. If a vector x âˆˆ R _[ğ‘]_ is used in a context that requires a
matrix, the vector is interpreted as an ( _ğ‘,_ 1)-matrix (i.e., a column vector) and can be turned
into a row vector (i.e., a (1 _,ğ‘_ )-matrix) using the transpose operation x _[ğ‘‡]_ . We use x [D] âˆˆ R _[ğ‘,ğ‘]_ to


128


Table 8. Details for selected knowledge graph embeddings, including the plausibility scoring function
_ğœ™_ ( _ğœ€_ ( _ğ‘ _ ) _, ğœŒ_ ( _ğ‘_ ) _,ğœ€_ ( _ğ‘œ_ )) for edge _[ğ‘ ]_ _ğ‘_ _ğ‘œ_, and other conditions applied


**Model** _ğœ™_ ( _ğœ€_ ( _ğ‘ _ ) _, ğœŒ_ ( _ğ‘_ ) _,ğœ€_ ( _ğ‘œ_ )) **Conditions** (for all _ğ‘¥_ âˆˆ _ğ‘‰_, _ğ‘¦_ âˆˆ _ğ¿_ )


TransE [63] âˆ’âˆ¥e _ğ‘ _ + r _ğ‘_ - e _ğ‘œ_ âˆ¥ _ğ‘_ e _ğ‘¥_ âˆˆ R _[ğ‘‘]_, r _ğ‘¦_ âˆˆ R _[ğ‘‘]_, _ğ‘_ âˆˆ{1 _,_ 2}, âˆ¥e _ğ‘¥_ âˆ¥2 = 1


e _ğ‘¥_ âˆˆ R _[ğ‘‘]_, r _ğ‘¦_ âˆˆ R _[ğ‘‘]_, w _ğ‘¦_ âˆˆ R _[ğ‘‘]_,
TransH [553] âˆ’âˆ¥(e _ğ‘ _ âˆ’(e _ğ‘ _ [T] w _ğ‘_ )w _ğ‘_ ) + r _ğ‘_ âˆ’(e _ğ‘œ_ âˆ’(e _ğ‘œ_ [T] w _ğ‘_ )w _ğ‘_ ) âˆ¥ [2] 2 âˆ¥w _ğ‘¦_ âˆ¥2 = 1, wâˆ¥r [T] _ğ‘¦ğ‘¦_ râˆ¥ _ğ‘¦_ 2 [â‰ˆ] [0,][ âˆ¥][e] _[ğ‘¥]_ [âˆ¥][2][ â‰¤] [1]


e _ğ‘¥_ âˆˆ R _[ğ‘‘][ğ‘’]_, r _ğ‘¦_ âˆˆ R _[ğ‘‘][ğ‘Ÿ]_, W _ğ‘¦_ âˆˆ R _[ğ‘‘][ğ‘Ÿ]_ _[,ğ‘‘][ğ‘’]_,
TransR [271] âˆ’âˆ¥W _ğ‘_ e _ğ‘ _ + r _ğ‘_ - W _ğ‘_ e _ğ‘œ_ âˆ¥ [2] 2 âˆ¥e _ğ‘¥_ âˆ¥2 â‰¤ 1, âˆ¥r _ğ‘¦_ âˆ¥2 â‰¤ 1, âˆ¥W _ğ‘¦_ e _ğ‘¥_ âˆ¥2 â‰¤ 1


e _ğ‘¥_ âˆˆ R _[ğ‘‘][ğ‘’]_, r _ğ‘¦_ âˆˆ R _[ğ‘‘][ğ‘Ÿ]_, w _ğ‘¥_ âˆˆ R _[ğ‘‘][ğ‘’]_, w _ğ‘¦_ âˆˆ R _[ğ‘‘][ğ‘Ÿ]_,
TransD [271] âˆ’âˆ¥(w _ğ‘_ âŠ— w _ğ‘ _ + I)e _ğ‘ _ + r _ğ‘_ âˆ’(w _ğ‘_ âŠ— w _ğ‘œ_ + I)e _ğ‘œ_ âˆ¥ [2] 2 âˆ¥e _ğ‘¥_ âˆ¥2 â‰¤ 1, âˆ¥r _ğ‘¦_ âˆ¥2 â‰¤ 1, âˆ¥(w _ğ‘¦_ âŠ— w _ğ‘¥_ + I)e _ğ‘¥_ âˆ¥2 â‰¤ 1


RotatE [511] âˆ’âˆ¥e _ğ‘ _ âŠ™ r _ğ‘_ - e _ğ‘œ_ âˆ¥2 e _ğ‘¥_ âˆˆ C _[ğ‘‘]_, r _ğ‘¦_ âˆˆ C _[ğ‘‘]_, âˆ¥r _ğ‘¦_ âˆ¥2 = 1


RESCAL [386] e _ğ‘ _ [T] R _ğ‘_ e _ğ‘œ_ e _ğ‘¥_ âˆˆ R _[ğ‘‘]_, R _ğ‘¦_ âˆˆ R _[ğ‘‘,ğ‘‘]_, âˆ¥e _ğ‘¥_ âˆ¥2 â‰¤ 1, âˆ¥R _ğ‘¦_ âˆ¥2 _,_ 2 â‰¤ 1


DistMult [568] e _ğ‘ _ [T] r _ğ‘_ [D] e _ğ‘œ_ e _ğ‘¥_ âˆˆ R _[ğ‘‘]_, r _ğ‘¦_ âˆˆ R _[ğ‘‘]_, âˆ¥e _ğ‘¥_ âˆ¥2 = 1, âˆ¥r _ğ‘¦_ âˆ¥2 â‰¤ 1


HolE [385] r _ğ‘_ [T] (e _ğ‘ _ _â˜…_ e _ğ‘œ_ ) e _ğ‘¥_ âˆˆ R _[ğ‘‘]_, r _ğ‘¦_ âˆˆ R _[ğ‘‘]_, âˆ¥e _ğ‘¥_ âˆ¥2 â‰¤ 1, âˆ¥r _ğ‘¦_ âˆ¥2 â‰¤ 1


ComplEx [526] Re(e _ğ‘ _ [T] r _ğ‘_ [D] ~~e~~ _ğ‘œ_ ) e _ğ‘¥_ âˆˆ C _[ğ‘‘]_, r _ğ‘¦_ âˆˆ C _[ğ‘‘]_, âˆ¥e _ğ‘¥_ âˆ¥2 â‰¤ 1, âˆ¥r _ğ‘¦_ âˆ¥2 â‰¤ 1


SimplE [283] e _ğ‘ _ [T] r _ğ‘_ [D] w _ğ‘œ_ +e _ğ‘œ_ [T] w _ğ‘_ [D] w _ğ‘ _ e _ğ‘¥_ âˆˆ R _[ğ‘‘]_, r _ğ‘¦_ âˆˆ R _[ğ‘‘]_, w _ğ‘¥_ âˆˆ R _[ğ‘‘]_, w _ğ‘¦_ âˆˆ R _[ğ‘‘]_,

2 âˆ¥e _ğ‘¥_ âˆ¥2 â‰¤ 1, âˆ¥w _ğ‘¥_ âˆ¥2 â‰¤ 1, âˆ¥r _ğ‘¦_ âˆ¥2 â‰¤ 1 _,_ âˆ¥w _ğ‘¦_ âˆ¥2 â‰¤ 1


TuckER [30] W âŠ—1 e _ğ‘ _ [T] âŠ—2 r _ğ‘_ [T] âŠ—3 e _ğ‘œ_ [T] e _ğ‘¥_ âˆˆ R _[ğ‘‘][ğ‘’]_, r _ğ‘¦_ âˆˆ R _[ğ‘‘][ğ‘Ÿ]_, W âˆˆ R _[ğ‘‘][ğ‘’]_ _[,ğ‘‘][ğ‘Ÿ]_ _[,ğ‘‘][ğ‘’]_


SME Linear [192] (Ve _ğ‘ _ + V [â€²] r _ğ‘_ + v) [T] (We _ğ‘œ_ + W [â€²] r _ğ‘_ + w) e _ğ‘¥_ âˆˆ R _[ğ‘‘]_, r _ğ‘¦_ âˆˆ R _[ğ‘‘]_, v âˆˆ R _[ğ‘¤]_, w âˆˆ R _[ğ‘¤]_, âˆ¥e _ğ‘¥_ âˆ¥2 = 1,
V âˆˆ R _[ğ‘¤,ğ‘‘]_ _,_ V [â€²] âˆˆ R _[ğ‘¤,ğ‘‘]_ _,_ W âˆˆ R _[ğ‘¤,ğ‘‘]_ _,_ W [â€²] âˆˆ R _[ğ‘¤,ğ‘‘]_

SME Bilinear [192] ((V âŠ—3 r _ğ‘_ [T] )e _ğ‘ _ + v) [T] ((W âŠ—3 r _ğ‘_ [T] )e _ğ‘œ_ + w) eV âˆˆ _ğ‘¥_ âˆˆ RR _[ğ‘‘][ğ‘¤,ğ‘‘,ğ‘‘]_, r _ğ‘¦_, W âˆˆâˆˆ R _[ğ‘‘]_, vR âˆˆ _[ğ‘¤,ğ‘‘,ğ‘‘]_ R _[ğ‘¤]_, w âˆˆ R _[ğ‘¤]_, âˆ¥e _ğ‘¥_ âˆ¥2 = 1,




               - ï¿½e _ğ‘ _
NTN [488] r _ğ‘_ [T] _ğœ“_ e _ğ‘ _ [T] We _ğ‘œ_ + W
e _ğ‘œ_




- - e _ğ‘¥_ âˆˆ R _[ğ‘‘]_, r _ğ‘¦_ âˆˆ R _[ğ‘‘]_, w âˆˆ R _[ğ‘¤]_, W âˆˆ R _[ğ‘¤,]_ [2] _[ğ‘‘]_,
+ w W âˆˆ R _[ğ‘‘,ğ‘¤,ğ‘‘]_, âˆ¥e _ğ‘¥_ âˆ¥2 â‰¤ 1, âˆ¥r _ğ‘¦_ âˆ¥2 â‰¤ 1,
âˆ¥w âˆ¥2 â‰¤ 1, âˆ¥W âˆ¥2 _,_ 2 â‰¤ 1, âˆ¥W1 [[Â·] â‰¤ [:] _ğ‘–_ _[ğ‘–]_ [:] â‰¤ [Â·]] _ğ‘¤_ [âˆ¥][2] _[,]_ [2][ â‰¤] [1]


e _ğ‘¥_ âˆˆ R _[ğ‘‘]_, r _ğ‘¦_ âˆˆ R _[ğ‘‘]_, v âˆˆ R _[ğ‘¤]_, w âˆˆ R _[ğ‘¤]_, W âˆˆ R _[ğ‘¤,]_ [3] _[ğ‘‘]_

âˆ¥e _ğ‘¥_ âˆ¥2 â‰¤ 1 âˆ¥r _ğ‘¦_ âˆ¥2 â‰¤ 1



e _ğ‘ _

MLP [131] v [T] _ğœ“_ [ï¿½] W r _ğ‘_

            - ï£®ï£¯ï£¯ï£¯ ï£¹ï£ºï£ºï£º

e _ğ‘œ_

            - ï£¯ï£° ï£ºï£»



+ w [ï¿½]

  
  



 
W âˆ—




e _ğ‘ _ [[] _[ğ‘,ğ‘]_ []]
r _ğ‘_ [[] _[ğ‘,ğ‘]_ []]



ï¿½ï¿½ï¿½ T
e _ğ‘¥_ âˆˆ R _[ğ‘‘]_, r _ğ‘¦_ âˆˆ R _[ğ‘‘]_, _ğ‘‘_ = _ğ‘ğ‘_,
W [ï¿½] e _ğ‘œ_

    - W âˆˆ R _[ğ‘¤]_ [1][ (] _[ğ‘¤]_ [2][+][2] _[ğ‘]_ [âˆ’][1][) (] _[ğ‘¤]_ [3][+] _[ğ‘]_ [âˆ’][1][)] _[,ğ‘‘]_, W âˆˆ R _[ğ‘¤]_ [1] _[,ğ‘¤]_ [2] _[,ğ‘¤]_ [3]

    


ConvE [127] _ğœ“_ [ï¿½] vec

           
           





_ğœ“_




             -              -              - T              - e _ğ‘¥_ âˆˆ R _[ğ‘‘][ğ‘’]_, r _ğ‘¦_ âˆˆ R _[ğ‘‘][ğ‘Ÿ]_, W âˆˆ R _[ğ‘¤]_ [2][ (] _[ğ‘¤]_ [1][+] _[ğ‘‘][ğ‘’]_ [âˆ’][1][)] _[,ğ‘‘][ğ‘’]_,
HypER [28] _ğœ“_ vec r _ğ‘_ [T] W âˆ— e _ğ‘ _ W e _ğ‘œ_ W âˆˆ R _[ğ‘‘][ğ‘Ÿ]_ _[,ğ‘¤]_ [1] _[,ğ‘¤]_ [2]


denote the diagonal matrix with the values of the vector x âˆˆ R _[ğ‘]_ on its diagonal. We denote
the identity matrix by I such that if _ğ‘—_ = _ğ‘˜_, then (I) _ğ‘—ğ‘˜_ = 1; otherwise (I) _ğ‘—ğ‘˜_ = 0.



ï£¹ï£ºï£ºï£ºï£ºï£ºï£»




- We denote by



ï£®ï£¯ï£¯ï£¯ï£¯ï£¯ï£°



X1
_..._
Xn



the vertical stacking of matrices X1 _, . . .,_ X _ğ‘›_ with the same number of



columns. Given a vector x âˆˆ R _[ğ‘ğ‘]_, we denote by x [[] _[ğ‘,ğ‘]_ []] âˆˆ R _[ğ‘,ğ‘]_ the â€œreshapingâ€ of x into an ( _ğ‘,ğ‘_ )matrix such that (x [[] _[ğ‘,ğ‘]_ []] ) _ğ‘–ğ‘—_ = (x) ( _ğ‘–_ + _ğ‘_ ( _ğ‘—_ âˆ’1)) . Conversely, given a matrix X âˆˆ R _[ğ‘,ğ‘]_, we denote by



129


vec(X) âˆˆ R _[ğ‘ğ‘]_ the _vectorisation_ of X such that vec(X) _ğ‘˜_ = (X) _ğ‘–ğ‘—_ where _ğ‘–_ = (( _ğ‘˜_  - 1) mod _ğ‘š_ ) + 1
and _ğ‘—_ = _[ğ‘˜]_ _ğ‘š_ [âˆ’] _[ğ‘–]_ [+][ 1 (observe that vec][(][x][[] _[ğ‘,ğ‘]_ []][)][ =][ x][).]

- Given a tensor X âˆˆ R _[ğ‘,ğ‘,ğ‘]_, we denote by X [[] _[ğ‘–]_ [:][Â·][:][Â·]] âˆˆ R _[ğ‘,ğ‘]_, the _ğ‘–_ [th] _slice_ of tensor X along the first
mode; for example, given X âˆˆ R [5] _[,]_ [2] _[,]_ [3], then X [[][4:][Â·][:][Â·]] returns the (2 _,_ 3)-matrix consisting of the




      (X)411 (X)412 (X)413
elements
(X)421 (X)422 (X)423




. Analogously, we use X [[Â·][:] _[ğ‘–]_ [:][Â·]] âˆˆ R _[ğ‘,ğ‘]_ and X [[Â·][:][Â·][:] _[ğ‘–]_ []] âˆˆ R _[ğ‘,ğ‘]_ to



indicate the _ğ‘–_ [th] slice along the second and third modes of X, respectively.

- We denote by _ğœ“_ (X) the element-wise application of a function _ğœ“_ to the tensor X, such
that ( _ğœ“_ (X)) _ğ‘–ğ‘›_ 1 _...ğ‘–ğ‘›_ = _ğœ“_ (X _ğ‘–_ 1 _...ğ‘–ğ‘›_ ). Common choices for _ğœ“_ include a sigmoid function (e.g., the
logistic function _ğœ“_ ( _ğ‘¥_ ) = 1+ _ğ‘’_ 1 ~~[âˆ’]~~ ~~_[ğ‘¥]_~~ [or the hyperbolic tangent function] _[ ğœ“]_ [(] _[ğ‘¥]_ [)][ =][ tanh] _[ğ‘¥]_ [=] _[ğ‘’]_ _ğ‘’_ _[ğ‘¥]_ ~~_[ğ‘¥]_~~ [âˆ’] + _ğ‘’_ _[ğ‘’]_ ~~[âˆ’]~~ [âˆ’] ~~_[ğ‘¥]_~~ _[ğ‘¥]_ [),]

the rectifier ( _ğœ“_ ( _ğ‘¥_ ) = max(0 _,ğ‘¥_ )), softplus ( _ğœ“_ ( _ğ‘¥_ ) = ln(1 + _ğ‘’_ _[ğ‘¥]_ )), etc.



The first and most elemental operation we consider is that of matrix multiplication.


_Definition B.53 (Matrix multiplication)._ The _multiplication of matrices_ X âˆˆ R _[ğ‘,ğ‘]_ and Y âˆˆ R _[ğ‘,ğ‘]_ is
a matrix XY âˆˆ R _[ğ‘,ğ‘]_ such that (XY) _ğ‘–ğ‘—_ = [ï¿½] _[ğ‘]_ _ğ‘˜_ =1 [(][X][)] _[ğ‘–ğ‘˜]_ [(][Y][)] _[ğ‘˜ğ‘—]_ [. The matrix multiplication of two tensors]
X âˆˆ R _[ğ‘]_ [1] _[,...,ğ‘][ğ‘š][,ğ‘]_ and Y âˆˆ R _[ğ‘,ğ‘]_ [1] _[,...,ğ‘][ğ‘›]_ is a tensor XY âˆˆ R _[ğ‘]_ [1] _[,...,ğ‘][ğ‘š][,ğ‘]_ [1] _[,...,ğ‘][ğ‘›]_ such that (XY) _ğ‘–_ 1 _...ğ‘–ğ‘šğ‘–ğ‘š_ +1 _...ğ‘–ğ‘š_ + _ğ‘›_ =

- _ğ‘_
_ğ‘˜_ =1 [(X)] _[ğ‘–]_ 1 _[...ğ‘–]_ _ğ‘š_ _[ğ‘˜]_ [(Y)] _[ğ‘˜ğ‘–]_ _ğ‘š_ +1 _[ğ‘–]_ _ğ‘š_ + _ğ‘›_ [.]


For convenience, we may implicitly add or remove modes with dimension 1 for the purposes of

matrix multiplication and other operators; for example, given two vectors x âˆˆ R _[ğ‘]_ and y âˆˆ R _[ğ‘]_, we
denote by x [T] y (aka the dot or inner product) the multiplication of matrix x [T] âˆˆ R [1] _[,ğ‘]_ with y âˆˆ R _[ğ‘,]_ [1]

such that x [T] y âˆˆ R [1] _[,]_ [1] (i.e., a scalar in R); conversely, xy [T] âˆˆ R _[ğ‘,ğ‘]_ (the outer product).


Constraints on embeddings are sometimes given in terms of norms, defined next.


_Definition B.54 (ğ¿_ _[ğ‘]_ _-norm, ğ¿_ _[ğ‘,ğ‘]_ _-norm)._ For _ğ‘_ âˆˆ R, the _ğ¿_ _[ğ‘]_ _-norm_ of a vector x âˆˆ R _[ğ‘]_ is the scalar

1
âˆ¥xâˆ¥ _ğ‘_ â‰” (|(x)1| _[ğ‘]_ + _. . ._ + |(x) _ğ‘_ | _[ğ‘]_ ) _ğ‘_, where |(x) _ğ‘–_ | denotes the absolute value of the _ğ‘–_ [th] element of x.




_[ğ‘]_ _ğ‘_ - _ğ‘_ [1]



For _ğ‘,ğ‘_ âˆˆ R, the _ğ¿_ _[ğ‘,ğ‘]_ _-norm_ of a matrix X âˆˆ R _[ğ‘,ğ‘]_ is the scalar âˆ¥Xâˆ¥ _ğ‘,ğ‘_ â‰” ï¿½ï¿½ _ğ‘ğ‘—_ =1 ï¿½ï¿½ _ğ‘–ğ‘_ =1 [|(][X][)] _[ğ‘–ğ‘—]_ [|] _[ğ‘]_ [ï¿½] _[ğ‘]_ _ğ‘_



_ğ‘_
.



The _ğ¿_ [1] norm (i.e., âˆ¥xâˆ¥1) is thus simply the sum of the absolute values of x, while the _ğ¿_ [2] norm
(i.e., âˆ¥xâˆ¥2) is the (Euclidean) length of the vector. The Frobenius norm of the matrix X then equates


_ğ‘_ _ğ‘_ 2
to âˆ¥Xâˆ¥2 _,_ 2 = ï¿½ï¿½ _ğ‘—_ =1 ï¿½ï¿½ _ğ‘–_ =1 [|(][X][)] _[ğ‘–ğ‘—]_ [|][2][ï¿½ï¿½] [1] ; i.e., the square root of the sum of the squares of all elements.


Another type of product used by embedding techniques is the Hadamard product, which multi
plies tensors of the same dimension and computes their product element-wise.


_Definition B.55 (Hadamard product)._ Given two tensors X âˆˆ R _[ğ‘]_ [1] _[,...,ğ‘][ğ‘›]_ and Y âˆˆ R _[ğ‘]_ [1] _[,...,ğ‘][ğ‘›]_, the
_Hadamard product_ X âŠ™Y is defined as a tensor in R _[ğ‘]_ [1] _[,...,ğ‘][ğ‘›]_, with each element computed as
(X âŠ™Y) _ğ‘–_ 1 _...ğ‘–ğ‘›_ â‰” (X) _ğ‘–_ 1 _...ğ‘–ğ‘›_ (Y) _ğ‘–_ 1 _...ğ‘–ğ‘›_ .


Other embedding techniques â€“ namely RotatE [511] and ComplEx [526] â€“ uses _complex space_

based on complex numbers. With a slight abuse of notation, the definitions of vectors, matrices

and tensors can be modified by replacing the set of real numbers R by the set of complex numbers

C, giving rise to complex vectors, complex matrices, and complex tensors. In this case, we denote

by Re(Â·) the real part of a complex number. Given a complex vector x âˆˆ C _[ğ¼]_, we denote by ~~x~~ its

complex conjugate (swapping the sign of the imaginary part of each element). Complex analogues

of the aforementioned operators can then be defined by replacing the multiplication and addition

of real numbers with the analogous operators for complex numbers, where RotateE [511] uses the

complex Hadamard product, and ComplEx [526] uses complex matrix multiplication.


130


One embedding technique â€“ MuRP [29] â€“ uses hyperbolic space, specifically based on the PoincarÃ©

ball. As this is the only embedding we cover that uses this space, and the formalisms are lengthy

(covering the PoincarÃ© ball, MÃ¶bius addition, MÃ¶bius matrixâ€“vector multiplication, logarithmic

maps, exponential maps, etc.), we rather refer the reader to the paper for further details [29].


As discussed in Section 5.2, tensor decompositions are an important concept for many embeddings,

and at the heart of such decompositions is the tensor product.


_Definition B.56 (Tensor product)._ Given two tensors X âˆˆ R _[ğ‘]_ [1] _[,...,ğ‘][ğ‘š]_ and Y âˆˆ R _[ğ‘]_ [1] _[,...,ğ‘][ğ‘›]_, the _tensor_
_product_ X âŠ—Y is defined as a tensor in R _[ğ‘]_ [1] _[,...,ğ‘][ğ‘š][,ğ‘]_ [1] _[,...,ğ‘][ğ‘›]_, with each element computed as (X âŠ—
Y) _ğ‘–_ 1 _...ğ‘–ğ‘š_ _ğ‘—_ 1 _...ğ‘—ğ‘›_ â‰” (X) _ğ‘–_ 1 _...ğ‘–ğ‘š_ (Y) _ğ‘—_ 1 _...ğ‘—ğ‘›_ . [44]


To illustrate the tensor product, assume that X âˆˆ R [2] _[,]_ [3] and Y âˆˆ R [3] _[,]_ [4] _[,]_ [5] . The result of X âŠ—Y will
be a tensor in R [2] _[,]_ [3] _[,]_ [3] _[,]_ [4] _[,]_ [5] . Element (X âŠ—Y)12345 will be computed by multiplying (X)12 and (Y)345.


An _ğ‘›_ -mode product is used by other embeddings to transform elements along a mode of a tensor.


_Definition B.57 (ğ‘›-mode product)._ For a positive integer _ğ‘›_, a tensor X âˆˆ R _[ğ‘]_ [1] _[,...,ğ‘][ğ‘›]_ [âˆ’][1] _[,ğ‘][ğ‘›][,ğ‘][ğ‘›]_ [+][1] _[,...,ğ‘][ğ‘š]_ and
matrix Y âˆˆ R _[ğ‘,ğ‘][ğ‘›]_, the _ğ‘›-mode product_ of X and Y is the tensor X âŠ— _ğ‘›_ Y âˆˆ R _[ğ‘]_ [1] _[,...,ğ‘][ğ‘›]_ [âˆ’][1] _[,ğ‘,ğ‘][ğ‘›]_ [+][1] _[,...,ğ‘][ğ‘š]_ such
that (X âŠ— _ğ‘›_ Y) _ğ‘–_ 1 _...ğ‘–ğ‘›_ âˆ’1 _ğ‘—ğ‘–ğ‘›_ +1 _...ğ‘–ğ‘š_ â‰” [ï¿½] _ğ‘˜_ _[ğ‘][ğ‘›]_ =1 [(X)] _[ğ‘–]_ [1] _[...ğ‘–][ğ‘›]_ [âˆ’][1] _[ğ‘˜ğ‘–][ğ‘›]_ [+][1] _[...ğ‘–][ğ‘š]_ [(][Y][)] _[ğ‘—ğ‘˜]_ [.]


To illustrate, let us assume that X âˆˆ R [2] _[,]_ [3] _[,]_ [4] and Y âˆˆ R [5] _[,]_ [3] . The result of X âŠ—2 Y will be a tensor
in R [2] _[,]_ [5] _[,]_ [4], where, for example, (X âŠ—2 Y)142 will be given as (X)112(Y)41 + (X)122 (Y)42 + (X)132 (Y)43.
Observe that if y âˆˆ R _[ğ‘][ğ‘›]_ - i.e., if y is a (column) vector â€“ then the _ğ‘›_ -mode tensor product X âŠ— _ğ‘›_ y [T]

â€œflattensâ€ the _ğ‘›_ [th] mode of X to one dimension, effectively reducing the order of X by one.


One embedding technique â€“ HolE [385] â€“ uses a circular correlation operator.


_Definition B.58 (Circular correlation)._ The _circular correlation_ of vector x âˆˆ R _[ğ‘]_ with y âˆˆ R _[ğ‘]_ is the
vector x _â˜…_ y âˆˆ R _[ğ‘]_ such that (x _â˜…_ y) _ğ‘˜_ â‰” [ï¿½] _ğ‘–_ _[ğ‘]_ =1 [(][x][)] _[ğ‘–]_ [(][y][)] [(((] _[ğ‘˜]_ [+] _[ğ‘–]_ [âˆ’][2][)][ mod] _[ğ‘]_ [)+][1][)] [.]


Each element of x _â˜…_ y is the sum of _ğ‘_ elements along a diagonal of the outer product x âŠ— y that
â€œwrapsâ€ if not the primary diagonal. Assuming _ğ‘_ = 5, then (x _â˜…_ y)1 = (x)1(y)1 +(x)2(y)2 +(x)3(y)3 +
(x)4(y)4+(x)5(y)5, or a case that wraps: (x _â˜…_ y)4 = (x)1(y)4+(x)2(y)5+(x)3(y)1+(x)4(y)2+(x)5(y)3.


Finally, a couple of neural models that we include â€“ namely ConvE [127] and HypER [28] â€“ are

based on convolutional architectures using the convolution operator.


_Definition B.59 (Convolution)._ Given two matrices X âˆˆ R _[ğ‘,ğ‘]_ and Y âˆˆ R _[ğ‘’,ğ‘“]_, the _convolution_ of X
and Y is the matrix X âˆ— Y âˆˆ R [(] _[ğ‘]_ [+] _[ğ‘’]_ [âˆ’][1][)] _[,]_ [(] _[ğ‘]_ [+] _[ğ‘“]_ [âˆ’][1][)] such that (X âˆ— Y) _ğ‘–ğ‘—_ = [ï¿½] _ğ‘˜_ _[ğ‘]_ =1 - _ğ‘ğ‘™_ =1 [(][X][)] _[ğ‘˜ğ‘™]_ [(][Y][)] [(] _[ğ‘–]_ [+] _[ğ‘˜]_ [âˆ’] _[ğ‘]_ [) (] _[ğ‘—]_ [+] _[ğ‘™]_ [âˆ’] _[ğ‘]_ [)] [.][45]

In cases where ( _ğ‘–_ + _ğ‘˜_ - _ğ‘_ ) _<_ 1, ( _ğ‘—_ + _ğ‘™_ - _ğ‘_ ) _<_ 1, ( _ğ‘–_ + _ğ‘˜_ - _ğ‘_ ) _> ğ‘’_ or ( _ğ‘—_ + _ğ‘™_ - _ğ‘_ ) _> ğ‘“_ (i.e., where
(Y) ( _ğ‘–_ + _ğ‘˜_ - _ğ‘_ ) ( _ğ‘—_ + _ğ‘™_ - _ğ‘_ ) lies outside the bounds of Y), we say that (Y) ( _ğ‘–_ + _ğ‘˜_ - _ğ‘_ ) ( _ğ‘—_ + _ğ‘™_ - _ğ‘_ ) = 0.


Intuitively speaking, the convolution operator overlays X in every possible way over Y such that
at least one pair of elements (X) _ğ‘–ğ‘—,_ (Y) _ğ‘™ğ‘˜_ overlaps, summing the products of pairs of overlapping
elements to generate an element of the result. Elements of X extending beyond Y are ignored
(equivalently we can consider Y to be â€œzero-paddedâ€ outside its borders). To illustrate, given X âˆˆ R [3] _[,]_ [3]

and Y âˆˆ R [4] _[,]_ [5], then X âˆ— Y âˆˆ R [6] _[,]_ [7], where, for example, (X âˆ— Y)11 = (X)33 (Y)11 (with the bottom
right corner of X overlapping the top left corner of Y), while (X âˆ— Y)34 = (X)11 (Y)12 + (X)12 (Y)13 +


44Please note that â€œâŠ—â€ is used here in an unrelated sense to its use in Definition B.34.
45We define the convolution operator per the convention for convolutional neural networks. Strictly speaking, the operator

should be called _cross-correlation_, where traditional convolution requires the matrix X to be initially â€œrotatedâ€ by 180Â°. Since
in our settings the matrix X is learnt, rather than given, the rotation is redundant.


131


(X)13 (Y)14 + (X)21 (Y)22 + (X)22 (Y)23 + (X)23 (Y)24 + (X)31 (Y)32 + (X)32 (Y)33 + (X)33(Y)34 (with
(X)22 â€“ the centre of X â€“ overlapping (Y)23). [46] In a convolution X âˆ— Y, the matrix X is often called

the â€œkernelâ€ (or â€œfilterâ€). Often several kernels are used in order to apply multiple convolutions.

Given a tensor X âˆˆ R _[ğ‘,ğ‘,ğ‘]_ (representing _ğ‘_ ( _ğ‘,ğ‘_ )-kernels) and a matrix Y âˆˆ R _[ğ‘’,ğ‘“]_, we denote by
X âˆ— Y âˆˆ R _[ğ‘,]_ [(] _[ğ‘]_ [+] _[ğ‘’]_ [âˆ’][1][)] _[,]_ [(] _[ğ‘]_ [+] _[ğ‘“]_ [âˆ’][1][)] the result of the convolutions of the _ğ‘_ first-mode slices of X over Y such
that (X âˆ— Y) [[] _[ğ‘–]_ [:][Â·][:][Â·]] = X [[] _[ğ‘–]_ [:][Â·][:][Â·]] âˆ— Y for 1 â‰¤ _ğ‘–_ â‰¤ _ğ‘_, yielding a tensor of results for _ğ‘_ convolutions.


_B.6.3_ _Graph neural networks._ We now provide high-level definitions for graph neural networks

(GNNs) inspired by (for example) the definitions provided by Xu et al. [565]. We assume that the

GNN accepts a directed vector-labelled graph as input (see Definition B.46).


We first abstractly define a recursive graph neural network.


_Definition B.60 (Recursive graph neural network)._ A _recursive graph neural network_ ( _RecGNN_ ) is a

pair of functions â„œâ‰” (Agg _,_ Out), such that (with _ğ‘,ğ‘,ğ‘_ âˆˆ N):


  - Agg : R _[ğ‘]_ Ã— 2 [(][R] _[ğ‘]_ [Ã—][R] _[ğ‘]_ [)â†’][N] â†’ R _[ğ‘]_

  - Out : R _[ğ‘]_ â†’ R _[ğ‘]_


The function Agg computes a new feature vector for a node, given its previous feature vector and

the feature vectors of the nodes and edges forming its neighbourhood; the function Out transforms

the final feature vector computed by Agg for a node to the output vector for that node. We assume

that _ğ‘_ and _ğ‘_ correspond to the dimensions of the input node and edge vectors, respectively, while

_ğ‘_ denotes the dimension of the output vector for each node. Given a RecGNN â„œ= (Agg _,_ Out),
a directed vector-labelled graph _ğº_ = ( _ğ‘‰, ğ¸, ğ¹, ğœ†_ ), and a node _ğ‘¢_ âˆˆ _ğ‘‰_, we define the output vector
assigned to node _ğ‘¢_ in _ğº_ by â„œ (written â„œ( _ğº,ğ‘¢_ )) as follows. First let n _ğ‘¢_ [(][0][)] â‰” _ğœ†_ ( _ğ‘¢_ ). For all _ğ‘–_ â‰¥ 1, let:


                     -                     n _ğ‘¢_ [(] _[ğ‘–]_ [)] â‰” Agg n _ğ‘¢_ [(] _[ğ‘–]_ [âˆ’][1][)] _,_ {{(n _ğ‘£_ [(] _[ğ‘–]_ [âˆ’][1][)] _, ğœ†_ ( _ğ‘£,ğ‘¢_ )) | ( _ğ‘£,ğ‘¢_ ) âˆˆ _ğ¸_ }}


If _ğ‘—_ â‰¥ 1 is an integer such that n _ğ‘¢_ [(] _[ğ‘—]_ [)] = n _ğ‘¢_ [(] _[ğ‘—]_ [âˆ’][1][)] for all _ğ‘¢_ âˆˆ _ğ‘‰_, then â„œ( _ğº,ğ‘¢_ ) â‰” Out(n _ğ‘¢_ [(] _[ğ‘—]_ [)] [)][.]

In a RecGNN, the same aggregation function (Agg) is applied recursively until a fixpoint is

reached, at which point an output function (Out) creates the final output vector for each node.

While in practice RecGNNs will often consider a static feature vector and a dynamic state vec
tor [462], we can more concisely encode this as one vector, where part may remain static throughout

the aggregation process representing input features, and part may be dynamically computed repre
senting the state. In practice, Agg and Out are often based on parametric combinations of vectors,

with the parameters learnt based on a sample of output vectors for labelled nodes.


_Example B.61._ The aggregation function for the GNN of Scarselli et al. [462] is given as:


âˆ‘ï¸
Agg(n _ğ‘¢, ğ‘_ ) â‰” _ğ‘“_ w (n _ğ‘¢,_ n _ğ‘£,_ a _ğ‘£ğ‘¢_ )


(n _ğ‘£,_ a _ğ‘£ğ‘¢_ ) âˆˆ _ğ‘_


where _ğ‘“_ w (Â·) is a contraction function with parameters w. The output function is defined as:


Out (n _ğ‘¢_ ) â‰” _ğ‘”_ **w** [â€²] (n _ğ‘¢_ )


where again _ğ‘”_ w [â€²] (Â·) is a function with parameters w [â€²] . Given a set of nodes labelled with their
expected output vectors, the parameters w and w [â€²] are learnt.


46Models applying convolutions may differ regarding how edge cases are handled, or on the â€œstrideâ€ of the convolution

applied, where, for example, a stride of 3 for (X âˆ— Y) would see the kernel X centred only on elements (Y) _ğ‘–ğ‘—_ such that
_ğ‘–_ mod 3 = 0 and _ğ‘—_ mod 3 = 0, reducing the number of output elements by a factor of 9. We do not consider such details here.


132


There are notable similarities between graph parallel frameworks (GPFs; see Definition B.48)

and RecGNNs. While we defined GPFs using separate Msg and Agg functions, this is not essential:

conceptually they could be defined in a similar way to RecGNN, with a single Agg function that

â€œpullsâ€ information from its neighbours (we maintain Msg to more closely reflect how GPFs are

defined/implemented in practice). The key difference between GPFs and GNNs is that in the former,

the functions are defined by the user, while in the latter, the functions are generally learnt from

labelled examples. Another difference arises from the termination condition present in GPFs, though

often the GPFâ€™s termination condition will â€“ like in RecGNNs â€“ reflect convergence to a fixpoint.


Next we abstractly define a non-recursive graph neural network.


_Definition B.62 (Non-recursive graph neural network)._ A _non-recursive graph neural network_
(NRecGNN) with _ğ‘™_ layers is an _ğ‘™_ -tuple of functions ğ”‘â‰” (Agg [(][1][)] _, . . .,_ Agg [(] _[ğ‘™]_ [)] ), such that, for 1 â‰¤ _ğ‘˜_ â‰¤ _ğ‘™_
(with _ğ‘_ 0 _, . . . ğ‘ğ‘™,ğ‘_ âˆˆ N), Agg [(] _[ğ‘˜]_ [)] : R _[ğ‘][ğ‘˜]_ [âˆ’][1] Ã— 2 [(][R] _[ğ‘ğ‘˜]_ [âˆ’][1] [Ã—][R] _[ğ‘]_ [)â†’][N] â†’ R _[ğ‘][ğ‘˜]_ .


Each function Agg [(] _[ğ‘˜]_ [)] (as before) computes a new feature vector for a node, given its previous

feature vector and the feature vectors of the nodes and edges forming its neighbourhood. We

assume that _ğ‘_ 0 and _ğ‘_ correspond to the dimensions of the input node and edge vectors, respectively,
where each function Agg [(] _[ğ‘˜]_ [)] for 2 â‰¤ _ğ‘˜_ â‰¤ _ğ‘™_ accepts as input node vectors of the same dimension
as the output of the function Agg [(] _[ğ‘˜]_ [âˆ’][1][)] . Given an NRecGNN ğ”‘= (Agg [(][1][)] _, . . .,_ Agg [(] _[ğ‘™]_ [)] ), a directed
vector-labelled graph _ğº_ = ( _ğ‘‰, ğ¸, ğ¹, ğœ†_ ), and a node _ğ‘¢_ âˆˆ _ğ‘‰_, we define the output vector assigned to
node _ğ‘¢_ in _ğº_ by ğ”‘ (written ğ”‘( _ğº,ğ‘¢_ )) as follows. First let n _ğ‘¢_ [(][0][)] â‰” _ğœ†_ ( _ğ‘¢_ ). For all _ğ‘–_ â‰¥ 1, let:


                                           n _ğ‘¢_ [(] _[ğ‘–]_ [)] â‰” Agg [(] _[ğ‘–]_ [)][ ï¿½] n _ğ‘¢_ [(] _[ğ‘–]_ [âˆ’][1][)] _,_ {{(n _ğ‘£_ [(] _[ğ‘–]_ [âˆ’][1][)] _, ğœ†_ ( _ğ‘£,ğ‘¢_ )) | ( _ğ‘£,ğ‘¢_ ) âˆˆ _ğ¸_ }}


Then ğ”‘( _ğº,ğ‘¢_ ) â‰” n _ğ‘¢_ [(] _[ğ‘™]_ [)] [.]

In an _ğ‘™_ -layer NRecGNN, a different aggregation function can be applied at each step (i.e., in each

layer), up to a fixed number of steps _ğ‘™_ . We do not consider a separate Out function as it can be
combined with the final aggregation function Agg [(] _[ğ‘™]_ [)] . When the aggregation functions are based

on a convolutional operator, we call the result a _convolutional graph neural network_ ( _ConvGNN_ ).

We refer to the survey by Wu et al. [559] for discussion of ConvGNNs proposed in the literature.


We have considered GNNs that define the neighbourhood of a node based on its incoming

edges. However, these definitions can be adapted to also consider outgoing neighbours by either

adding inverse edges to the directed vector-labelled graph in pre-processing, or by adding outgoing

neighbours as arguments to the Agg(Â·) function. More generally, GNNs (and indeed GPFs) relying

solely on the neighbourhood of each node have limited expressivity in terms of their ability to

distinguish nodes and graphs [565]; for example, BarcelÃ³ et al. [32] show that such NRecGNNs

have a similar expressiveness for classifying nodes as the ALCQ Description Logic discussed

in Section B.5.3. More expressive GNN variants have been proposed that allow the aggregation

functions to access and update a globally shared vector [32]. We refer to the papers by Xu et al. [565]

and BarcelÃ³ et al. [32] for further discussion on the expressivity of GNNs.


_B.6.4_ _Symbolic learning._ We provide some abstract formal definitions for the tasks of _rule mining_

and _axiom mining_ over graphs, which we generically call _hypothesis mining_ . First we introduce

_hypothesis induction_ : a task that captures a more abstract (ideal) case for hypothesis mining.


_Definition B.63 (Hypothesis induction)._ The task of _hypothesis induction_ assumes a particular
graph entailment relation |=Î¦ (see Definition B.39; hereafter simply |=). Given _background knowledge_

in the form of a knowledge graph _ğº_ (a directed edge-labelled graph, possibly extended with rules or
ontologies), a set of _positive edges ğ¸_ [+] such that _ğº_ does not entail any edge in _ğ¸_ [+] (i.e., for all _ğ‘’_ [+] âˆˆ _ğ¸_ [+],


133


_ğº_ Ì¸|= _ğ‘’_ [+] ) and _ğ¸_ [+] does not contradict _ğº_ (i.e., there is a model of _ğº_ âˆª _ğ¸_ [+] ), and a set of _negative edges_
_ğ¸_ [âˆ’] such that _ğº_ does not entail any edge in _ğ¸_ [âˆ’] (i.e., for all _ğ‘’_ [âˆ’] âˆˆ _ğ¸_ [âˆ’], _ğº_ Ì¸|= _ğ‘’_ [âˆ’] ), the task is to find a set
of _hypotheses_ (i.e., a set of directed edge-labelled graphs) Î¨ such that:

  - _ğº_ Ì¸|= _ğœ“_ for all _ğœ“_ âˆˆ Î¨ (the background knowledge does not entail any hypothesis)

  - _ğº_ âˆª Î¨ [âˆ—] |= _ğ¸_ [+] (the background knowledge and hypotheses entail all positive edges);

  - for all _ğ‘’_ [âˆ’] âˆˆ _ğ¸_ [âˆ’], _ğº_ âˆª Î¨ [âˆ—] Ì¸|= _ğ‘’_ [âˆ’] (the background knowledge and hypotheses do not entail any

negative edge);

  - _ğº_ âˆª Î¨ [âˆ—] âˆª _ğ¸_ [+] has a model (the background knowledge, hypotheses and positive edges taken

together do not contain a contradiction);

  - for all _ğ‘’_ [+] âˆˆ _ğ¸_ [+], Î¨ [âˆ—] Ì¸|= _ğ‘’_ [+] (the hypotheses alone do not entail a positive edge).
where by Î¨ [âˆ—] â‰” âˆª _ğœ“_ âˆˆÎ¨ _ğœ“_ we denote the union of all graphs in Î¨.


_Example B.64._ Let us assume ontological entailment |= with semantic conditions Î¦ as defined in

Tables 3â€“5. Given the graph of Figure 30 as the background knowledge _ğº_, along with

  - a set of positive edges _ğ¸_ [+] = { [SCL] flight ARI _,_ [SCL] domestic flight ARI }, and

then a set of hypotheses Î¨ = { [flight] type Symmetric _,_ [domestic flight] type Symmetric } would entail
all positive edges in _ğ¸_ [+] and no negative edges in _ğ¸_ [âˆ’] when combined with _ğº_ .


This task represents a somewhat idealised case. Often there is no set of positive edges distinct

from the background knowledge itself. Furthermore, hypotheses not entailing a few positive edges,

or entailing a few negative edges, may still be useful. The task of _hypothesis mining_ rather accepts
as input the background knowledge _ğº_ and a set of negative edges _ğ¸_ [âˆ’] (such that for all _ğ‘’_ [âˆ’] âˆˆ _ğ¸_ [âˆ’],
_ğº_ Ì¸|= _ğ‘’_ [âˆ’] ), and attempts to _score_ individual hypotheses _ğœ“_ (such that _ğº_ Ì¸|= _ğœ“_ ) in terms of their ability
to â€œexplainâ€ _ğº_ while minimising the number of elements of _ğ¸_ [âˆ’] entailed by _ğº_ and _ğœ“_ .

We can now abstractly define the task of hypothesis mining.


_Definition B.65 (Hypothesis mining)._ Given a knowledge graph _ğº_, a set of negative edges _ğ¸_ [âˆ’],

a scoring function _ğœ_, and a threshold min _ğœ_, the goal of _hypothesis mining_ is to identify a set of
hypotheses { _ğœ“_ | _ğº_ Ì¸|= _ğœ“_ and _ğœ_ ( _ğœ“,ğº, ğ¸_ [âˆ’] ) â‰¥ min _ğœ_ }.


There are two main scoring functions used for _ğœ_ in the literature: _support_ and _confidence_ .


_Definition B.66 (Hypothesis support and confidence)._ Given a knowledge graph _ğº_ = ( _ğ‘‰, ğ¸, ğ¿_ ) and

a hypothesis _ğœ“_, the _positive support_ of _ğœ“_ is defined as follows:


_ğœ_ [+] ( _ğœ“,ğº_ ) â‰” |{ _ğ‘’_ âˆˆ _ğ¸_ | _ğº_ [â€²] Ì¸|= _ğ‘’_ and _ğº_ [â€²] âˆª _ğœ“_ |= _ğ‘’_ }|


where _ğº_ [â€²] denotes _ğº_ with the edge _ğ‘’_ removed. Further given a set of negative edges _ğ¸_ [âˆ’], the _negative_

_support_ of _ğœ“_ is defined as follows:


_ğœ_ [âˆ’] ( _ğœ“,ğº, ğ¸_ [âˆ’] ) â‰” |{ _ğ‘’_ [âˆ’] âˆˆ _ğ¸_ [âˆ’] | _ğº_ âˆª _ğœ“_ |= _ğ‘’_ [âˆ’] }|


_ğœ_ [+] ( _ğœ“,ğº_ )
Finally, the _confidence_ of _ğœ“_ is defined as _ğœ_ [Â±] ( _ğœ“,ğº, ğ¸_ [âˆ’] ) â‰” _ğœ_ ~~[+]~~ ( _ğœ“,ğº_ )+ _ğœ_ ~~[âˆ’]~~ ( _ğœ“,ğº,ğ¸_ ~~[âˆ’]~~ ) [.]


We have yet to define how the set of negative edges are defined, which, in the context of a

knowledge graph _ğº_, depends on which assumption is applied:


  - _Closed world assumption (CWA)_ : For any (positive) edge _ğ‘’_, _ğº_ Ì¸|= _ğ‘’_ if and only if _ğº_ |= Â¬ _ğ‘’_ . Under

CWA, any edge _ğ‘’_ not entailed by _ğº_ can be considered a negative edge.

  - _Open world assumption_ : For a (positive) edge _ğ‘’_, _ğº_ Ì¸|= _ğ‘’_ does not necessarily imply _ğº_ |= Â¬ _ğ‘’_ .

Under OWA, the negation of an edge must be entailed by _ğº_ for it to be considered negative.


134


  - _Partial completeness assumption (PCA)_ : If there exists ( _ğ‘ , ğ‘,ğ‘œ_ ) such that _ğº_ |= ( _ğ‘ , ğ‘,ğ‘œ_ ), then
for all _ğ‘œ_ [â€²] such that _ğº_ Ì¸|= ( _ğ‘ , ğ‘,ğ‘œ_ [â€²] ), it holds that _ğº_ |= Â¬( _ğ‘ , ğ‘,ğ‘œ_ [â€²] ). Under PCA, if _ğº_ entails some

outgoing edge(s) labelled _ğ‘_ from a node _ğ‘ _, then such edges are assumed to be complete, and

any edge ( _ğ‘ , ğ‘,ğ‘œ_ ) not entailed by _ğº_ can be considered a negative edge.

Knowledge graphs are generally incomplete â€“ in fact, one of the main applications of hypothesis

mining is to try to improve the completeness of the knowledge graph â€“ and thus it would appear

unwise to assume that any edge that is not currently entailed is false/negative. We can thus rule

out CWA. Conversely, under OWA, potentially few (or no) negative edges might be entailed by

the given ontologies/rules, and thus hypotheses may end up having low negative support despite

entailing many edges that do not make sense in practice. Hence the PCA can be adopted as a

heuristic to increase the number of negative edges and apply more sensible scoring of hypotheses.

Different implementations of hypothesis mining may consider different logical languages. Rule

mining, for example, mines hypotheses expressed either as monotonic rules (with positive edges)

or non-monotonic edges (possibly with negated edges). On the other hand, axiom mining considers

hypotheses expressed in a logical language such as Description Logics. Particular implementations

may, for practical reasons, impose further syntactic restrictions on the hypotheses generated, such

as to impose thresholds on their length, on the symbols they use, or on other structural properties

(such as â€œclosed rulesâ€ in the case of the AMIE rule mining system [170]; see Section 5.4). Systems

may further implement different search strategies for hypotheses. Systems such as AMIE [170],

RuLES [241], CARL [406], DL-Learner [73], etc., propose _discrete mining_ that recursively generates

candidate formulae through refinement/genetic operators that are then scored and checked for

threshold criteria, thus navigating a branching search space. On the other hand, systems such as

NeuralLP [569] and DRUM [455] apply _differentiable mining_ that allows for learning (path-like)

rules and their scores in a more continuous fashion (e.g., using gradient descent). We refer to

Section 5.4 for further discussion and examples of such techniques for mining hypotheses.


135


