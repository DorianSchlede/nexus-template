---
batch_id: "methodology_6"
field: methodology
extracted_at: "2025-12-31T00:00:00Z"
chunks_read: 4
patterns_found: 18
---

patterns:
  # Paper 12: Foundations of Process Event Data
  - name: "Bottom-up Event Log Construction"
    chunk_ref: "12-Foundations_of_Process_Event_Data (Chunk 1:14-21)"
    quote: "Process event data is a fundamental building block for process mining as event logs portray the execution trails of business processes from which knowledge and insights can be extracted."
    description: "Process mining methodology starts from empirical data (event logs) captured from real business process executions. This is a quintessentially bottom-up approach where theoretical constructs emerge from observed behavioral patterns rather than being imposed top-down."

  - name: "Three Essential Requirements Method"
    chunk_ref: "12-Foundations_of_Process_Event_Data (Chunk 1:62-89)"
    quote: "the three essential data requirements for event logs to be analysis-ready for process mining technique application. First, each event should be linked to a case or process instance"
    description: "The methodology defines three mandatory data requirements: (1) Case ID linking events to process instances, (2) Activity labels mapping events to business process steps, and (3) Timestamps for temporal ordering. This represents a hybrid approach combining empirical constraints with methodological rigor."

  - name: "Extraction-Correlation-Abstraction Pipeline"
    chunk_ref: "12-Foundations_of_Process_Event_Data (Chunk 1:380-388)"
    quote: "event log preparation often includes three types of techniques: extraction, correlation and abstraction. Figure 5 illustrates the relationship between these techniques and fundamental process mining concepts."
    description: "A structured three-stage methodology for event log preparation: extraction (obtaining data from source systems), correlation (mapping events to cases), and abstraction (elevating granularity to business-meaningful activities). This is a hybrid methodology combining empirical data sourcing with conceptual transformation."

  - name: "Knowledge Discovery in Databases (KDD) Process"
    chunk_ref: "12-Foundations_of_Process_Event_Data (Chunk 1:230-244)"
    quote: "Data preprocessing is a fundamental part of any data science project... One model illustrating the typical data analytics process is depicted in Fig. 4. This model, originally introduced in [25] as the Knowledge Discovery in Databases (KDD) process"
    description: "The paper situates process mining within the broader KDD methodology, which includes stages of data selection, cleaning, and transformation. This adapts the standard bottom-up data science methodology to the specific requirements of process-oriented analysis."

  - name: "PM2 Process Mining Methodology"
    chunk_ref: "12-Foundations_of_Process_Event_Data (Chunk 1:370-377)"
    quote: "When making an assessment of one of the most recently introduced process mining methodologies, i.e. PM2 [56], four event data preprocessing tasks are defined: (1) creating views, (2) filtering logs, (3) enriching logs, and (4) aggregating events."
    description: "PM2 is a specialized process mining methodology that defines four preprocessing tasks tailored to event log analysis. This represents a domain-specific hybrid approach adapted from general data analytics methodologies like CRISP-DM."

  - name: "Ontology-Based Data Access (OBDA)"
    chunk_ref: "12-Foundations_of_Process_Event_Data (Chunk 1:426-433)"
    quote: "One noteworthy scientific initiative in this context is ontology-based data access (ODBA) for event log extraction. The approach is based on an ontological view of the domain of interest and linking it as such to a database schema"
    description: "A top-down methodology that uses ontological models to guide extraction of event data from relational databases. This represents a bridge between foundational ontology work and empirical data extraction, implemented in tools like Onprom."

  - name: "Pattern-Based Abstraction"
    chunk_ref: "12-Foundations_of_Process_Event_Data (Chunk 1:509-514)"
    quote: "Another frequently used paradigm to perform abstraction is pattern matching. The work by Bose and van der Aalst can be considered as origination of pattern-based abstraction."
    description: "Bottom-up methodology for abstracting low-level events into higher-level activities using pattern recognition techniques such as maximal repeats and tandem arrays. This empirical approach discovers structure from observed event sequences."

  - name: "Quality-Informed Process Mining"
    chunk_ref: "12-Foundations_of_Process_Event_Data (Chunk 1:596-608)"
    quote: "most of the existing process mining algorithms do not explicitly take the potential presence of data quality issues. A notable exception is the removal of infrequent behaviors or noises from discovered process models."
    description: "An emerging hybrid methodology that incorporates data quality annotations at event, trace, and log levels to enable quality-aware analysis. This adds a reflexive layer to empirical process mining methods."

  # Paper 15: SciAgents Multi-Agent Graph Reasoning
  - name: "Multi-Agent System for Scientific Discovery"
    chunk_ref: "15-SciAgents_Multi-Agent_Graph_Reasoning (Chunk 1:30-44)"
    quote: "we present SciAgents, an approach that leverages three core concepts: (1) the use of large-scale ontological knowledge graphs to organize and interconnect diverse scientific concepts, (2) a suite of large language models (LLMs) and data retrieval tools, and (3) multi-agent systems with in-situ learning capabilities."
    description: "A hybrid methodology combining top-down ontological knowledge organization with bottom-up multi-agent collaboration. The approach integrates structured ontological knowledge graphs with emergent multi-agent reasoning for automated scientific hypothesis generation."

  - name: "Hierarchical Expansion Strategy"
    chunk_ref: "15-SciAgents_Multi-Agent_Graph_Reasoning (Chunk 1:200-211)"
    quote: "We employ a hierarchical expansion strategy where answers are successively refined and improved, enriched with retrieved data, critiqued and amended by identification or critical modeling, simulation and experimental tasks and adversarial prompting."
    description: "Top-down methodology that decomposes research hypothesis generation into structured phases: initial keyword identification, path sampling to create subgraphs, structured JSON output generation, iterative expansion via individual prompting, and critical review. Each phase builds on previous outputs in a systematic refinement process."

  - name: "Knowledge Graph Path Sampling"
    chunk_ref: "15-SciAgents_Multi-Agent_Graph_Reasoning (Chunk 1:239-248)"
    quote: "Unlike in earlier work where the shortest path was utilized, our study employs a random path approach. As illustrated in Figure 4, the random approach infuses the path with a richer array of concepts and relationships"
    description: "Bottom-up empirical methodology that uses random path sampling rather than shortest path to extract subgraphs from knowledge graphs. This introduces stochastic exploration to increase novelty in hypothesis generation, balancing structured graph representation with emergent discovery."

  - name: "Ontologist-Scientist-Critic Agent Pipeline"
    chunk_ref: "15-SciAgents_Multi-Agent_Graph_Reasoning (Chunk 1:225-231)"
    quote: "Each agent plays a specialized role: The Ontologist defines key concepts and relationships, Scientist 1 crafts a detailed research proposal, Scientist 2 expands and refines the proposal, and the Critic agent conducts a thorough review and suggests improvements."
    description: "A structured multi-agent methodology where specialized agents collaborate in a defined sequence: Ontologist provides semantic grounding, Scientist agents generate and expand hypotheses, and Critic agents provide adversarial review. This represents a top-down orchestration of agent roles with emergent inter-agent negotiation."

  - name: "Pre-programmed vs Autonomous Agent Interactions"
    chunk_ref: "15-SciAgents_Multi-Agent_Graph_Reasoning (Chunk 1:186-197)"
    quote: "The key difference between these approaches lies in the nature of the interaction between the agents. In the first approach, the interactions between agents are pre-programmed and follow a predefined sequence of tasks... In contrast, the second approach features fully automated agent interactions"
    description: "Two distinct methodological variants: (1) top-down pre-programmed agent sequences for consistency, and (2) bottom-up autonomous agent self-organization for adaptability. The second approach includes human-in-the-loop intervention capabilities, creating a hybrid human-AI methodology."

  - name: "In-Context Learning with Graph Context"
    chunk_ref: "15-SciAgents_Multi-Agent_Graph_Reasoning (Chunk 1:95-102)"
    quote: "in-context learning emerges as a compelling strategy to enhance the performance of LLMs without the need for costly and time-intensive fine-tuning. This approach exploits the model's inherent ability to adapt its responses based on the context embedded within the prompt"
    description: "Methodology that uses graph-derived context (concepts and relationships) to condition LLM reasoning without model fine-tuning. This combines top-down ontological structure with the emergent capabilities of foundation models."

  - name: "Novelty and Feasibility Assessment via Literature Search"
    chunk_ref: "15-SciAgents_Multi-Agent_Graph_Reasoning (Chunk 2:125-128)"
    quote: "the assistant agent executes the tool to assess the novelty and feasibility of the proposed research idea against the literature. It then returns a detailed analysis suggesting that the proposed research hypothesis has a high degree of novelty and a reasonable level of feasibility."
    description: "Bottom-up validation methodology that uses Semantic Scholar API to compare generated hypotheses against existing literature. This empirical grounding ensures generated ideas are novel yet feasible based on the current state of knowledge."

  - name: "Heuristic Pathfinding with Random Waypoints"
    chunk_ref: "15-SciAgents_Multi-Agent_Graph_Reasoning (Chunk 2:245-260)"
    quote: "The algorithm presented in this work combines heuristic-based pathfinding with node embeddings and randomized waypoints to discover diverse paths in a graph... By relying on these embeddings, the algorithm adapts to the topological structure of the graph"
    description: "Hybrid methodology combining heuristic graph traversal with stochastic exploration. Uses embedding-based distance estimation with controlled randomization (factor 0.2) to balance deterministic pathfinding with exploratory search for novel concept connections."

  - name: "Structured JSON Output Schema for Hypothesis"
    chunk_ref: "15-SciAgents_Multi-Agent_Graph_Reasoning (Chunk 2:305-327)"
    quote: "the algorithm generates a structured scientific hypothesis that leverages each of the nodes and relationships in the graph. The output, in JSON format, provides key fields such as 'mechanisms', 'unexpected_properties', and 'comparison'"
    description: "Top-down methodology that structures hypothesis generation output into predefined JSON schema with seven key aspects: hypothesis, outcome, mechanisms, design principles, unexpected properties, comparison, and novelty. This structured approach ensures comprehensive coverage of research proposal elements."

  - name: "Iterative Prompt-Driven Expansion"
    chunk_ref: "15-SciAgents_Multi-Agent_Graph_Reasoning (Chunk 2:343-377)"
    quote: "The next phase involves systematically expanding specific aspects of the hypothesis using a series of targeted prompts. For each aspect of the research, a detailed prompt is constructed to critically assess and improve the scientific content"
    description: "Top-down iterative methodology where each JSON field is expanded via targeted prompts requesting quantitative details, chemical formulas, sequences, and rationale. This systematic expansion transforms initial hypotheses into detailed research proposals through structured prompt engineering."
