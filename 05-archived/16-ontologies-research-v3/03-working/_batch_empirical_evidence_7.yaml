---
batch_id: "empirical_evidence_7"
field: empirical_evidence
extracted_at: "2025-12-31T00:00:00Z"
chunks_read: 9
patterns_found: 32
---

patterns:
  # Paper 18 - Multi-Agent Architecture Taxonomy LLM (Chunk 4)

  - name: "Seven LLM Multi-Agent Systems Taxonomic Classification"
    chunk_ref: "18-Multi-Agent_Architecture_Taxonomy_LLM (Chunk 4:3-9)"
    quote: "Based on our taxonomic classification and the resulting system profiles as illustrated in Fig. 9, we can categorize the selected 7 systems under analysis into three distinct system groups"
    description: "Empirical validation of the proposed taxonomy through classification of 7 real LLM-powered multi-agent systems (AUTO-GPT, BABYAGI, SUPERAGI, AGENTGPT, HUGGINGGPT, METAGPT, CAMEL) into three categories: general-purpose, central-controller, and role-agent systems. This demonstrates the taxonomy's practical applicability."

  - name: "General-Purpose Systems Empirical Analysis"
    chunk_ref: "18-Multi-Agent_Architecture_Taxonomy_LLM (Chunk 4:12-37)"
    quote: "General-Purpose Systems - representing multi-agent systems designed for and adaptable to a broad spectrum of tasks and applications... AUTO-GPT, BABYAGI, SUPERAGI, and AGENTGPT"
    description: "Empirical analysis of four general-purpose LLM multi-agent systems revealing common patterns: goals decomposed autonomously as prioritized task lists (L2 Decom), multi-cycle process frameworks, low autonomy levels (L0) for communication protocol and network management, high autonomy (L2) for task execution."

  - name: "Central LLM Controller Pattern Validation"
    chunk_ref: "18-Multi-Agent_Architecture_Taxonomy_LLM (Chunk 4:39-49)"
    quote: "HUGGINGGPT serves as an archetype of such systems, utilizing resources especially in terms of existing ML models integrated via HUGGING FACE... highest levels of autonomy granted to this central agent (mostly L2)"
    description: "Empirical validation of central LLM controller architecture pattern through analysis of HUGGINGGPT system, demonstrating highest autonomy levels for central agent with monologue-based reflection and planning, using language as generic interface for coordinating multiple foundation models."

  - name: "Role-Agent Systems Collaboration Analysis"
    chunk_ref: "18-Multi-Agent_Architecture_Taxonomy_LLM (Chunk 4:51-67)"
    quote: "Role-Agent Systems - employ an interplay or simulation between multiple dedicated roles agents... METAGPT and CAMEL represent such systems"
    description: "Empirical study of role-playing multi-agent systems (METAGPT, CAMEL) showing dedicated role collaboration patterns, instructor-executor relationships, waterfall development processes (METAGPT), and dialogue cycles between AI-user and AI-assistant roles (CAMEL)."

  - name: "Bounded Autonomy Pattern Discovery"
    chunk_ref: "18-Multi-Agent_Architecture_Taxonomy_LLM (Chunk 4:70-90)"
    quote: "high-autonomy aspects are mostly combined with low alignment levels, resulting in bounded autonomy aspects... Autonomous decomposition directly depends on the user-prompted goal"
    description: "Empirical finding that high-autonomy aspects in LLM multi-agent systems are systematically paired with low alignment levels, creating bounded autonomy through predefined mechanisms that guide and control autonomous operations."

  - name: "Operational Issues Observation"
    chunk_ref: "18-Multi-Agent_Architecture_Taxonomy_LLM (Chunk 4:175-185)"
    quote: "our engagement with the analyzed multi-agent systems has revealed additional operational issues. Occasionally, we witness non-terminating activities, where the system falls into infinite loops"
    description: "Empirical observation of operational failures in multi-agent systems including infinite loops (solutions continually fine-tuned) and dead ends (tasks requiring unavailable competencies), revealing insufficiency of current control mechanisms for handling exceptions."

  - name: "User-Centric Alignment Gap Finding"
    chunk_ref: "18-Multi-Agent_Architecture_Taxonomy_LLM (Chunk 4:142-156)"
    quote: "Within the scope of analyzed systems, user-centric alignment options are very rare. Alignment mechanisms are predominantly integrated into the system architecture"
    description: "Empirical finding across all analyzed systems showing limited user-guided alignment options, with internal agent composition and collaboration largely opaque to users, identifying potential for runtime documentation improvements and user modifications."

  # Paper 19 - Graph of Thoughts LLM Reasoning (Chunks 1-7)

  - name: "GoT Sorting Quality Improvement"
    chunk_ref: "19-Graph_of_Thoughts_LLM_Reasoning (Chunk 1:23-25)"
    quote: "GoT offers advantages over state of the art on different tasks, for example increasing the quality of sorting by 62% over ToT, while simultaneously reducing costs by >31%"
    description: "Empirical benchmark results showing Graph of Thoughts (GoT) framework achieves 62% quality improvement in sorting tasks compared to Tree of Thoughts (ToT), while also reducing inference costs by more than 31%."

  - name: "100 Input Samples Evaluation Methodology"
    chunk_ref: "19-Graph_of_Thoughts_LLM_Reasoning (Chunk 1:779-783)"
    quote: "We use 100 input samples for each task and comparison baseline. We set the temperature to 1.0 and use a 4k context size unless stated otherwise"
    description: "Empirical evaluation methodology using 100 input samples per task and baseline, standardized temperature setting of 1.0, 4k context size, with extensive parameter experimentation on branching factor k and number of levels L."

  - name: "GPT-3.5 Primary Evaluation Model"
    chunk_ref: "19-Graph_of_Thoughts_LLM_Reasoning (Chunk 2:74-76)"
    quote: "Due to budget restrictions, we focus on GPT-3.5. We also experimented with Llama-2, but it was usually worse than GPT-3.5 and also much slower to run"
    description: "Empirical model comparison showing GPT-3.5 used as primary evaluation model due to budget constraints, with Llama-2 tested but found to perform worse and significantly slower, making large-scale sampling infeasible."

  - name: "GoT vs ToT Performance Comparison"
    chunk_ref: "19-Graph_of_Thoughts_LLM_Reasoning (Chunk 2:87-97)"
    quote: "GoT improves upon ToT and ToT2 by a large margin over all the considered problem instances... it reduces median error by approximately 62%, thereby achieving a higher quality of sorting"
    description: "Comprehensive empirical comparison showing GoT consistently outperforms ToT across all problem instances, with 62% median error reduction for P=128 sorting while ensuring >31% cost reduction through task decomposition into solvable subtasks."

  - name: "GoT vs IO/CoT Quality Comparison"
    chunk_ref: "19-Graph_of_Thoughts_LLM_Reasoning (Chunk 2:98-101)"
    quote: "GoT consistently delivers much higher quality of outcomes than IO/CoT. For example, for sorting (P=64), GoT's median error is approximately 65% and approximately 83% lower than, respectively, CoT and IO"
    description: "Empirical results demonstrating GoT's superior quality over Input-Output and Chain-of-Thought prompting, with 65% lower median error than CoT and 83% lower than IO for 64-element sorting tasks."

  - name: "Increasing Problem Complexity Advantages"
    chunk_ref: "19-Graph_of_Thoughts_LLM_Reasoning (Chunk 2:109-122)"
    quote: "the advantages of GoT in the quality increase for all the baselines with the growing size of the problem P... for P=32 GoT only negligibly improves upon ToT2, its median error count becomes lower by approximately 61% for P=64 and approximately 69% for P=128"
    description: "Empirical finding that GoT's advantages scale with problem complexity: negligible improvement at P=32, 61% improvement at P=64, and 69% improvement at P=128, demonstrating GoT is well-suited for elaborate problem cases."

  - name: "Set Intersection Evaluation Results"
    chunk_ref: "19-Graph_of_Thoughts_LLM_Reasoning (Chunk 1:676-681)"
    quote: "Set intersection of two sets is implemented similarly as the sorting. The second input set is split into subsets... For the evaluation we use different set sizes of 32, 64 and 128 elements"
    description: "Empirical evaluation of set intersection tasks across three problem sizes (32, 64, 128 elements) with 25-75% overlap variation, using error scope metric counting missing or incorrectly included elements plus duplicates."

  - name: "Keyword Counting Task Evaluation"
    chunk_ref: "19-Graph_of_Thoughts_LLM_Reasoning (Chunk 1:701-709)"
    quote: "Keyword counting finds the frequency of keywords in a given category (countries in our example implementation) within the input text... to score a thought, we first - for each keyword - derive the absolute difference between the computed count and the correct one"
    description: "Empirical evaluation of keyword counting task using country names as keywords, with scoring based on sum of absolute differences between computed and correct keyword frequencies across multiple text passages."

  - name: "Document Merging LLM Scoring"
    chunk_ref: "19-Graph_of_Thoughts_LLM_Reasoning (Chunk 1:715-725)"
    quote: "To score a solution, we query the LLM for two values (3 times for each value, and take the average). The first value corresponds to the solution redundancy (10 indicates no redundancy)... the second value stands for information retention"
    description: "Novel empirical evaluation methodology for document merging using LLM-based scoring with redundancy (0-10) and information retention (0-10) metrics, queried 3 times per value and averaged, computing harmonic mean for final score."

  - name: "Sorting 32-Element Task Execution Trace"
    chunk_ref: "19-Graph_of_Thoughts_LLM_Reasoning (Chunk 3:804-817)"
    quote: "1. Split the input list into two sub-lists of equal size (split_prompt) 2. For each sub-list: Sort the sub-list (sort prompt) five times; score each sort attempt; keep the best"
    description: "Detailed empirical execution trace for 32-element sorting showing GoO (Graph of Operations): split into sublists, sort each 5 times with scoring, merge sorted lists 10 times with best selection, improve 10 times with final selection."

  - name: "Sorting Response Accuracy Analysis"
    chunk_ref: "19-Graph_of_Thoughts_LLM_Reasoning (Chunk 3:887-899)"
    quote: "1. [0, 0, 1, 1, 1, 1, 1, 1, 2, 3, 3, 4, 5, 7, 8, 9] (Fully Correct) 2. [0, 0, 1, 1, 1, 1, 1, 2, 3, 3, 4, 5, 7, 8, 9] (1 Error - Missing one 1)"
    description: "Empirical response analysis showing LLM sorting accuracy varies: 1 out of 5 responses fully correct, 4 responses with single element errors (missing one 1), demonstrating value of multiple sampling and best selection."

  - name: "Set Intersection Accuracy Patterns"
    chunk_ref: "19-Graph_of_Thoughts_LLM_Reasoning (Chunk 4:444-454)"
    quote: "1. [11, 14, 46, 14, 19] (1 Error - Duplicated 14) 2. Output: [11, 14, 46, 19] (Fully Correct) 3. [11, 14, 46, 14, 19] (1 Error - Duplicated 14)"
    description: "Empirical analysis of set intersection responses showing common error pattern of element duplication, with 2 of 5 responses fully correct and 3 with duplicate errors, validating need for multiple sampling strategy."

  - name: "Keyword Counting Accuracy Analysis"
    chunk_ref: "19-Graph_of_Thoughts_LLM_Reasoning (Chunk 5:140-149)"
    quote: "1. {{ 'Peru': 1, 'Argentina': 1, 'Brazil': 1 }} (3 Errors - Missing two 'Argentina' and one 'Brazil') 2. {{ 'Peru': 1, 'Argentina': 2, 'Brazil': 2 }} (1 Error - Missing one 'Argentina')"
    description: "Empirical keyword counting results showing significant accuracy variation: best response has 1 error while worst has 3 errors, with common pattern of under-counting repeated country mentions in text."

  - name: "Merge Operation Accuracy in GoT"
    chunk_ref: "19-Graph_of_Thoughts_LLM_Reasoning (Chunk 4:70-111)"
    quote: "Step 3 - 10 Responses: 1. [0, 0, 0, 1, 1, 1, 1, 1, 1, 2, 2, 2...] (2 Errors - Missing one 1 and one 5)... 8. [0, 0, 0, 1, 1, 1, 1, 1, 1, 2, 2, 2...] (1 Error - Missing one 1)"
    description: "Empirical merge operation analysis showing 10 responses with varying error counts (1-3 errors), best result having only 1 error, demonstrating that even merge operations benefit from multiple sampling and selection."

  - name: "Improvement Operation Effectiveness"
    chunk_ref: "19-Graph_of_Thoughts_LLM_Reasoning (Chunk 4:154-192)"
    quote: "Step 4 - 10 Responses... 10. Reason: The incorrectly sorted list is missing three 1s... Output: [0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2...] (Fully Correct)"
    description: "Empirical validation of improvement operation: only 1 of 10 improvement attempts achieves fully correct result, with others ranging from 1-10 errors, demonstrating both the value and difficulty of self-correction in LLMs."

  - name: "Document Merging Score Distribution"
    chunk_ref: "19-Graph_of_Thoughts_LLM_Reasoning (Chunk 6:233-313)"
    quote: "Response (1/5)... Score: 6.60... Response (2/5)... Score: 6.87... Response (3/5)... Score: 6.60... Response (4/5)... Score: 5.78... Response (5/5)... Score: 6.50"
    description: "Empirical score distribution for NDA document merging showing 5 attempts with scores ranging from 5.78 to 6.87, demonstrating moderate variance in merge quality and justifying best-selection approach."

  - name: "Final Document Merging Score Improvement"
    chunk_ref: "19-Graph_of_Thoughts_LLM_Reasoning (Chunk 7:580-583)"
    quote: "Final Overall Score (Harmonic Mean of Averages): 7.78"
    description: "Empirical result showing iterative refinement through aggregation and improvement operations increases document merging score from initial best of 6.87 to final 7.78, validating the graph-based improvement approach."

  # Paper 20 - Agentic RAG Survey (Chunk 2)

  - name: "Multi-Agent RAG Parallel Processing"
    chunk_ref: "20-Agentic_RAG_Survey (Chunk 2:19)"
    quote: "The retrieval process is executed in parallel, allowing for efficient processing of diverse query types"
    description: "Empirical architectural pattern in multi-agent RAG systems where specialized agents execute retrieval in parallel across vector search, Text-to-SQL, web search, and APIs, enabling efficient handling of diverse query types."

  - name: "Renewable Energy Multi-Agent Use Case"
    chunk_ref: "20-Agentic_RAG_Survey (Chunk 2:78-98)"
    quote: "Prompt: What are the economic and environmental impacts of renewable energy adoption in Europe?... Integrated Response: 'Adopting renewable energy in Europe has led to a 20% reduction in greenhouse gas emissions'"
    description: "Empirical use case demonstrating multi-agent RAG workflow: Agent 1 retrieves economic data via SQL, Agent 2 searches academic papers, Agent 3 performs web search for news, Agent 4 consults recommendations, producing integrated response with specific statistics."

  - name: "Hierarchical RAG Financial Analysis"
    chunk_ref: "20-Agentic_RAG_Survey (Chunk 2:166-187)"
    quote: "Prompt: What are the best investment options given the current market trends in renewable energy?... renewable energy stocks have shown a 15% growth over the past quarter"
    description: "Empirical validation of hierarchical agentic RAG for financial analysis: top-tier agent prioritizes reliable databases, mid-level retrieves market data, lower-level searches web and recommendations, producing integrated investment guidance."

  - name: "Corrective RAG Academic Research"
    chunk_ref: "20-Agentic_RAG_Survey (Chunk 2:271-311)"
    quote: "Prompt: What are the latest findings in generative AI research?... Integrated Response: 'Recent findings in generative AI highlight advancements in diffusion models, reinforcement learning for text-to-video tasks'"
    description: "Empirical use case for Corrective RAG in academic research: context retrieval, relevance evaluation with classification (relevant/ambiguous/irrelevant), query refinement, external knowledge retrieval, and response synthesis."

  - name: "Adaptive RAG Customer Support"
    chunk_ref: "20-Agentic_RAG_Survey (Chunk 2:406-441)"
    quote: "Prompt: Why is my package delayed, and what alternatives do I have?... classifier analyzes the query and determines it to be complex, requiring multi-step reasoning"
    description: "Empirical demonstration of Adaptive RAG classifying query complexity to select appropriate retrieval strategy: multi-step retrieval activated for complex customer support query, retrieving tracking, shipping API, and web search data."

  - name: "Agent-G Healthcare Diagnostics"
    chunk_ref: "20-Agentic_RAG_Survey (Chunk 2:539-580)"
    quote: "Prompt: What are the common symptoms of Type 2 Diabetes, and how are they related to heart disease?... Studies show a 50% correlation between diabetes and heart disease"
    description: "Empirical validation of Agent-G graph-based RAG for healthcare: graph retriever extracts disease relationships, document retriever adds symptom context, critic module validates quality, producing integrated medical response with correlation statistics."

  - name: "GeAR Multi-Hop Question Answering"
    chunk_ref: "20-Agentic_RAG_Survey (Chunk 2:666-698)"
    quote: "Prompt: Which author influenced the mentor of J.K. Rowling?... GeAR advances RAG performance through two primary innovations: Graph Expansion and Agent Framework"
    description: "Empirical use case for GeAR framework demonstrating multi-hop retrieval: top-tier agent evaluates complexity, graph expansion traces literary influences through mentor relationships, agent-based retrieval integrates structured and unstructured data."

  - name: "Agentic Document Workflow Invoice Processing"
    chunk_ref: "20-Agentic_RAG_Survey (Chunk 2:763-783)"
    quote: "Prompt: Generate a payment recommendation report... Invoice INV-2025-045 for $15,000.00 has been processed. An early payment discount of 2% is available"
    description: "Empirical validation of Agentic Document Workflows for invoice processing: document parsing extracts invoice details, contract retrieval verifies terms, generates recommendation with specific amounts ($15,000 reduced to $14,700 via 2% discount)."

  - name: "Twitch Ad Sales RAG Enhancement"
    chunk_ref: "20-Agentic_RAG_Survey (Chunk 2:905-908)"
    quote: "Twitch leveraged an agentic workflow with RAG on Amazon Bedrock to streamline ad sales. The system dynamically retrieved advertiser data, historical campaign performance, and audience demographics"
    description: "Real-world empirical case study: Twitch deployed agentic RAG on Amazon Bedrock for ad sales, dynamically retrieving advertiser data, campaign history, and demographics to generate detailed proposals, demonstrating production-scale effectiveness."

  - name: "Comparative Analysis RAG Frameworks"
    chunk_ref: "20-Agentic_RAG_Survey (Chunk 2:810-878)"
    quote: "Table 2 provides a comprehensive comparative analysis of the three architectural frameworks: Traditional RAG, Agentic RAG, and Agentic Document Workflows (ADW)"
    description: "Empirical comparative analysis table contrasting Traditional RAG, Agentic RAG, and ADW across dimensions: focus, context maintenance, adaptability, orchestration, tool integration, scalability, reasoning complexity, applications, strengths, and challenges."
