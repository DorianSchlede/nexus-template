---
batch_id: "methodology_3"
field: methodology
extracted_at: "2025-12-31T00:00:00Z"
chunks_read: 6
patterns_found: 28
---

patterns:
  - name: "Translational Embedding Learning Approach"
    chunk_ref: "02-Knowledge_Graphs (Chunk 5:7-24)"
    quote: "Translational models interpret edge labels as transformations from subject nodes to object nodes; for example, in the edge [San Pedro] bus Moon Valley, the edge label bus is seen as transforming [San Pedro] to [Moon Valley]"
    description: "TransE and related translational models use a geometric approach where relation embeddings act as translation vectors in embedding space. The methodology learns vectors es, rp, and eo aiming to make es + rp approximate eo for positive edges, while keeping es + rp away from eo for negative examples. This bottom-up, data-driven approach learns representations directly from observed graph structure."

  - name: "Tensor Decomposition for Graph Embeddings"
    chunk_ref: "02-Knowledge_Graphs (Chunk 5:63-79)"
    quote: "Tensor decomposition involves decomposing a tensor into more elemental tensors from which the original tensor can be recomposed... These elemental tensors can be viewed as capturing latent factors underlying the information contained in the original tensor"
    description: "A mathematical methodology for deriving graph embeddings through tensor rank decomposition. The approach encodes graphs as one-hot 3-order tensors and applies Canonical Polyadic (CP) decomposition to extract latent factors. This represents a bottom-up empirical methodology that learns structure from data using mathematical decomposition techniques."

  - name: "Neural Network Plausibility Scoring"
    chunk_ref: "02-Knowledge_Graphs (Chunk 5:240-258)"
    quote: "A limitation of the previously discussed approaches is that they assume either linear or bilinear operations over embeddings to compute plausibility scores. A number of approaches rather use neural networks to learn embeddings with non-linear scoring functions for plausibility"
    description: "Neural models like SME, NTN, and MLP use neural networks with learnable parameters to compute plausibility scores for edges. This methodology moves beyond linear/bilinear assumptions to capture more complex patterns in data through non-linear functions, representing an empirical bottom-up approach."

  - name: "Convolutional Kernel-Based Feature Extraction"
    chunk_ref: "02-Knowledge_Graphs (Chunk 5:260-291)"
    quote: "ConvE proposes to generate a matrix from es and rp by wrapping each vector over several rows and concatenating both matrices. The concatenated matrix serves as the input for a set of (2D) convolutional layers, which returns a feature map tensor"
    description: "Convolutional approaches apply 2D convolutional kernels over entity and relation embeddings to extract features. This methodology adapts image processing techniques for graph learning, using parameter sharing through small kernels applied across different matrix regions."

  - name: "Language Model Transfer for Graph Embeddings"
    chunk_ref: "02-Knowledge_Graphs (Chunk 5:294-339)"
    quote: "Embeddings generated by both approaches are widely used in natural language processing tasks. Another approach for graph embeddings is thus to leverage proven approaches for language embeddings"
    description: "RDF2Vec and KGloVe adapt word embedding techniques (word2vec, GloVe) for graphs. RDF2Vec performs random walks on graphs and records paths as sentences for word2vec input. KGloVe uses personalized PageRank to determine node relatedness. This represents a hybrid methodology combining NLP techniques with graph structure."

  - name: "Entailment-Aware Embedding with Fuzzy Logics"
    chunk_ref: "02-Knowledge_Graphs (Chunk 5:341-390)"
    quote: "KALE computes entity and relation embeddings using a translational model that is adapted to further consider rules using t-norm fuzzy logics"
    description: "Hybrid methodology combining deductive knowledge (rules/ontologies) with inductive embeddings. KALE uses t-norm fuzzy logics to compute plausibility updates when applying rules to predicted edges. This represents a methodological bridge between symbolic rule-based approaches and numeric embedding approaches."

  - name: "Recursive Graph Neural Network Architecture"
    chunk_ref: "02-Knowledge_Graphs (Chunk 5:452-494)"
    quote: "Recursive graph neural networks are the seminal approach to graph neural networks. The approach is conceptually similar to the systolic abstraction... where messages are passed between neighbours towards recursively computing some result"
    description: "RecGNNs use recursive message passing where nodes update their state vectors based on neighbors' states through a parametric transition function. The methodology applies functions recursively up to a fixpoint, using supervised nodes to learn parameters. This represents a bottom-up learning approach that builds on graph topology."

  - name: "GNN Transition and Output Function Learning"
    chunk_ref: "02-Knowledge_Graphs (Chunk 5:464-489)"
    quote: "Scarselli et al. proposed what they generically call a graph neural network, which takes as input a directed graph where nodes and edges are associated with feature vectors... Each node is associated with a state vector, which is recursively updated based on information from the nodes neighbours"
    description: "The GNN methodology uses parametric transition functions fw(.) to update node states based on neighbor information, and output functions gw'(.) to compute final outputs. Both functions are implemented as neural networks with learnable parameters trained on supervised examples."

  - name: "Convolutional Graph Neural Network Approach"
    chunk_ref: "02-Knowledge_Graphs (Chunk 5:593-643)"
    quote: "The core idea in the image setting is to apply small kernels over localised regions of an image using a convolution operator to extract features from that local region. When applied to all local regions, the convolution outputs a feature map"
    description: "ConvGNNs adapt image processing convolution concepts to graphs by applying operators over local graph regions (node neighborhoods). Approaches use spectral or spatial representations to induce regular structure, or attention mechanisms to learn which neighbor features are most important."

  - name: "Symbolic Learning via Rule Mining"
    chunk_ref: "02-Knowledge_Graphs (Chunk 5:646-702)"
    quote: "An alternative approach is to adopt symbolic learning in order to learn hypotheses in a symbolic (logical) language that explain a given set of positive and negative edges. These edges are typically generated from the knowledge graph in an automatic manner"
    description: "Rule mining methodology learns interpretable logical rules from data rather than numeric embeddings. Rules serve as models that can be used for deductive reasoning and offer explainability. This represents a hybrid approach that bridges inductive learning with symbolic/deductive representations."

  - name: "Top-Down Rule Refinement in AMIE"
    chunk_ref: "02-Knowledge_Graphs (Chunk 5:772-827)"
    quote: "AMIE adopts the PCA measure of confidence, and builds rules in a top-down fashion starting with rule heads... For each rule head of this form, three types of refinements are considered, each of which adds a new edge to the body of the rule"
    description: "AMIE uses a top-down methodology starting with rule heads and systematically applying refinements to build rule bodies. Three refinement types add edges with fresh variables, constants, or existing variables. This search-space exploration approach prunes based on support thresholds."

  - name: "Differentiable Rule Mining via Matrix Operations"
    chunk_ref: "02-Knowledge_Graphs (Chunk 5:867-909)"
    quote: "The core idea is that the joins in rule bodies can be represented as matrix multiplication... Adjacency matrices representing relations can be multiplied to compute entailed edges"
    description: "NeuralLP and DRUM use differentiable approaches where rule learning is formulated as matrix operations. Joins become matrix multiplications over adjacency matrices. This methodology enables end-to-end learning of rules using attention mechanisms and recurrent neural networks."

  - name: "Axiom Mining for Disjointness Discovery"
    chunk_ref: "02-Knowledge_Graphs (Chunk 6:13-57)"
    quote: "Aside from rules, more general forms of axioms expressed in logical languages such as DLs can be mined from a knowledge graph. We can divide these approaches into those mining specific axioms and more general axioms"
    description: "Axiom mining methodology extracts ontological axioms like disjointness constraints from data. Approaches use negative association rule mining, TF-IDF cosine similarity, or terminological cluster trees to identify disjoint class descriptions. This represents bottom-up extraction of schema-level knowledge."

  - name: "DL-Learner Class Learning Methodology"
    chunk_ref: "02-Knowledge_Graphs (Chunk 6:59-76)"
    quote: "DL-Learner is based on algorithms for class learning whereby given a set of positive nodes and negative nodes, the goal is to find a logical class description that divides the positive and negative sets"
    description: "Concept learning methodology that discovers DL class descriptions to separate positive and negative examples. Uses refinement operators to move between general and specific classes, with confidence scoring functions and search strategies analogous to rule mining systems like AMIE."

  - name: "Agile Knowledge Graph Creation Methodology"
    chunk_ref: "02-Knowledge_Graphs (Chunk 6:110-133)"
    quote: "The appropriate methodology to follow when creating a knowledge graph depends on the actors involved, the domain, the envisaged applications... generally speaking the flexibility of knowledge graphs lends itself to starting with an initial core that can be incrementally enriched"
    description: "Agile or pay-as-you-go methodology for knowledge graph creation. The approach starts with an initial core and incrementally enriches from additional sources. This represents a pragmatic, iterative methodology that adapts to available data sources and application requirements."

  - name: "Text Extraction Pipeline Methodology"
    chunk_ref: "02-Knowledge_Graphs (Chunk 6:166-181)"
    quote: "Techniques from Natural Language Processing and Information Extraction can be applied. Though processes vary considerably across text extraction frameworks... four core tasks for text extraction"
    description: "Bottom-up text extraction methodology involving pre-processing, Named Entity Recognition (NER), Entity Linking (EL), and Relation Extraction (RE). This pipeline approach transforms unstructured text into structured graph data through sequential processing stages."

  - name: "Distant Supervision for Entity and Relation Extraction"
    chunk_ref: "02-Knowledge_Graphs (Chunk 6:220-290)"
    quote: "Distant supervision uses known entities in a knowledge graph as seed examples through which similar entities can be detected... finds sentences in a large corpus mentioning pairs of entities with a known relation, which are used to learn patterns"
    description: "Distant supervision methodology leverages existing knowledge graph content to bootstrap extraction from text. Known entities and relations serve as seed examples to learn patterns for detecting similar entities and relations. This hybrid approach combines existing structured knowledge with text mining."

  - name: "Wrapper-Based Markup Extraction"
    chunk_ref: "02-Knowledge_Graphs (Chunk 6:457-510)"
    quote: "Many general approaches are based on wrappers that locate and extract useful information directly from the markup document... modern approaches apply distant supervision whereby EL is used to identify and link entities in the webpage to nodes in the knowledge graph"
    description: "Wrapper induction methodology extracts knowledge from HTML/markup documents. Modern approaches use distant supervision to automatically induce wrappers by finding paths in markup that connect known entity pairs. High-confidence paths are then used to extract novel edges."

  - name: "Direct Mapping from Structured Sources"
    chunk_ref: "02-Knowledge_Graphs (Chunk 6:586-629)"
    quote: "A direct mapping automatically generates a graph from a table. A standard direct mapping creates an edge [x] y z for each non-header, non-empty, non-null cell of the table"
    description: "Standardized methodology for transforming tabular data into graphs. Direct mapping creates edges where x represents the row, y the column name, and z the cell value. Primary/foreign key relationships enable entity linking across tables. This represents a deterministic, reversible transformation approach."

  - name: "Custom R2RML Mapping Methodology"
    chunk_ref: "02-Knowledge_Graphs (Chunk 6:649-696)"
    quote: "Declarative mapping languages allow for manually defining custom mappings from tabular sources to graphs. R2RML allows for mapping from individual rows to one or more custom edges, with nodes and edges defined as constants, cell values, or templates"
    description: "Custom mapping methodology using declarative languages like R2RML to transform structured data to graphs. Supports templates combining multiple values, SQL queries for complex transformations, and alignment with existing knowledge graph vocabularies. Enables both ETL materialization and query rewriting virtualization."

  - name: "Ontology Engineering Methodologies"
    chunk_ref: "02-Knowledge_Graphs (Chunk 6:765-801)"
    quote: "Ontology engineering refers to the development and application of methodologies for building ontologies... Early methodologies were often based on a waterfall-like process... more iterative and agile ways of building and maintaining ontologies have been proposed"
    description: "Top-down methodologies for schema/ontology development. Evolution from waterfall approaches to agile methodologies like DILIGENT, eXtreme Design (XD), and Modular Ontology Modelling (MOM). Modern approaches incorporate ontology requirements via Competency Questions and Ontology Design Patterns."

  - name: "Ontology Learning from Text"
    chunk_ref: "02-Knowledge_Graphs (Chunk 6:849-886)"
    quote: "Ontology learning can be used to semi-automatically extract information from text that is useful for the ontology engineering process. Early methods focussed on extracting terminology... Axioms may also be extracted from text"
    description: "Semi-automated bottom-up methodology extracting ontological structure from text corpora. Uses measures of unithood and termhood for terminology extraction, Hearst patterns for subclass axioms, and textual definitions for taxonomy induction. Results inform manual ontology engineering processes."

  - name: "Quality Assessment Dimension Framework"
    chunk_ref: "02-Knowledge_Graphs (Chunk 7:4-14)"
    quote: "Quality dimensions capture aspects of multifaceted data quality which evolves from the traditional domain of databases to the domain of knowledge graphs... quality metrics provide ways to measure quantitative aspects of these dimensions"
    description: "Systematic methodology for knowledge graph quality assessment. Organizes quality into dimensions (accuracy, coverage, coherency, succinctness) with corresponding quantitative metrics. This evaluation framework adapts database quality concepts for graph-structured knowledge."

  - name: "Semantic Accuracy Validation Methodology"
    chunk_ref: "02-Knowledge_Graphs (Chunk 7:50-65)"
    quote: "Semantic accuracy is the degree to which data values correctly represent real world phenomena, which may be affected by imprecise extraction results, imprecise entailments, vandalism... options include manual verification or checking against several sources"
    description: "Methodology for assessing whether graph content accurately represents real-world phenomena. Approaches include manual verification, multi-source cross-checking, and validation of extraction process quality using precision metrics with human experts or gold standards."

  - name: "Link Prediction for Knowledge Graph Completion"
    chunk_ref: "02-Knowledge_Graphs (Chunk 7:295-334)"
    quote: "Knowledge graph completion aims at filling in the missing edges of a knowledge graph... This task is often addressed with link prediction techniques proposed in Statistical Relational Learning, which predict the existence or probability of correctness of missing edges"
    description: "Bottom-up methodology for completing knowledge graphs using statistical/inductive techniques. Leverages knowledge graph embeddings and rule mining to predict missing links. Addresses general links, type links, and identity links with techniques adapted for each setting."

  - name: "Fact Validation with Reference Sources"
    chunk_ref: "02-Knowledge_Graphs (Chunk 7:426-499)"
    quote: "Fact validation involves assigning plausibility or veracity scores to facts/edges, typically between 0 and 1... works on fact validation are characterised by their consideration of external reference sources, which may be unstructured or structured"
    description: "Methodology for knowledge graph correction through fact checking against external sources. Approaches use verbalization functions to convert edges to natural language, then apply fact finder algorithms over bipartite graphs connecting sources and facts. Computes trustworthiness and plausibility scores iteratively."

  - name: "Inconsistency Repair via Minimal Hitting Sets"
    chunk_ref: "02-Knowledge_Graphs (Chunk 7:526-568)"
    quote: "Techniques are required to repair inconsistencies, which is not trivial... Bonatti et al. propose an automated method to repair inconsistencies based on minimal hitting sets, where each set is a minimal explanation for an inconsistency"
    description: "Automated methodology for repairing knowledge graph inconsistencies detected through ontological axioms like disjointness. Uses minimal hitting sets to identify edges to remove, selecting based on source trustworthiness scores. Revises knowledge graph to prevent re-entailment of removed edges."

  - name: "FAIR Principles for Data Publication"
    chunk_ref: "02-Knowledge_Graphs (Chunk 7:621-717)"
    quote: "FAIR itself is an acronym for four foundational principles... Findability refers to ease of locating the dataset; Accessibility refers to ease of access; Interoperability refers to ease of exploitation with standard tools; Reusability refers to ease of re-use"
    description: "Methodological principles for publishing knowledge graphs. FAIR (Findable, Accessible, Interoperable, Reusable) provides guidelines with specific sub-goals including persistent identifiers, rich metadata, standard protocols, shared formalisms, and clear licensing. Knowledge graph technologies directly support many FAIR requirements."
