field: tools_standards
aggregated_at: '2026-01-01T16:22:31.926583'
batches_merged: 11
patterns_input: 517
patterns_output: 480
patterns:
- name: OntoUML Modeling Language
  sources:
  - chunk_ref: 01-UFO_Unified_Foundational_Ontology (Chunk 1:209-211)
    quote: the use of UFO in the design of an ontology-driven conceptual modeling
      language, which later came to be known as OntoUML
  description: OntoUML is a conceptual modeling language designed based on UFO foundations.
    It reflects the ontological micro-theories comprising UFO and provides a graphical
    notation for representing domain models grounded in foundational ontology distinctions.
- name: OntoUML Model Repository
  sources:
  - chunk_ref: 01-UFO_Unified_Foundational_Ontology (Chunk 1:248)
    quote: Several dozens of these models are available at http://purl.org/krdb-core/model-repository/
  description: A publicly accessible repository containing OntoUML models developed
    by multiple research groups across various domains, serving as empirical validation
    and reference implementations.
- name: UFO TPTP Formalization
  sources:
  - chunk_ref: 01-UFO_Unified_Foundational_Ontology (Chunk 1:441-445)
    quote: this section is based on a first-order logic formalization of UFO specified
      in the TPTP syntax (Sutcliffe, 2017), and submitted to multiple automated provers
      for consistency and satisfability checks
  description: UFO has been formally specified in TPTP (Thousands of Problems for
    Theorem Provers) syntax, enabling automated verification using theorem provers.
    The complete formalization is available at https://purl.org/ufo-formalization.
- name: First-Order Modal Logic QS5 Formalization
  sources:
  - chunk_ref: 01-UFO_Unified_Foundational_Ontology (Chunk 1:431-435)
    quote: For our purposes, the first-order modal logic QS5 plus the Barcan formula
      and its converse suffices
  description: UFO is formalized using first-order modal logic QS5 with Barcan formula,
    enabling representation of necessity, possibility, and cross-world identity for
    ontological analysis.
- name: BPMN Reengineering
  sources:
  - chunk_ref: 01-UFO_Unified_Foundational_Ontology (Chunk 1:203-204)
    quote: It has been employed as a basis for analyzing, reengineering, and integrating
      many modeling languages and standards in different domains (e.g., UML, BPMN,
      ArchiMate)
  description: UFO has been used to analyze and reengineer BPMN (Business Process
    Model and Notation), providing ontological foundations for business process modeling
    language semantics.
- name: UML Ontological Analysis
  sources:
  - chunk_ref: 01-UFO_Unified_Foundational_Ontology (Chunk 1:203-204)
    quote: It has been employed as a basis for analyzing, reengineering, and integrating
      many modeling languages and standards in different domains (e.g., UML, BPMN,
      ArchiMate)
  description: UFO provides ontological foundations for UML (Unified Modeling Language)
    analysis and reengineering, addressing conceptual modeling constructs like entity
    types and relationship types.
- name: ArchiMate Integration
  sources:
  - chunk_ref: 01-UFO_Unified_Foundational_Ontology (Chunk 1:203-204)
    quote: It has been employed as a basis for analyzing, reengineering, and integrating
      many modeling languages and standards in different domains (e.g., UML, BPMN,
      ArchiMate)
  description: UFO has been applied to ArchiMate enterprise architecture modeling
    language to provide ontological grounding for enterprise architecture concepts.
- name: OWL Ontological Evaluation
  sources:
  - chunk_ref: 01-UFO_Unified_Foundational_Ontology (Chunk 1:154-155)
    quote: including NIAM (Weber and Zhang, 1991), ER (Wand et al., 1999), UML (Evermann
      and Wand, 2001), and OWL (Bera and Wand, 2004)
  - chunk_ref: 01-UFO_Unified_Foundational_Ontology (Chunk 3:687)
    quote: UML (Costal et al., 2011; Guizzardi, 2005)
  description: Merged from 2 sources. UFO provides ontological foundations for UML
    (Unified Modeling Language), clarifying the formal semantics of UML constructs
    through ontological analysis.
- name: NIAM Ontological Evaluation
  sources:
  - chunk_ref: 01-UFO_Unified_Foundational_Ontology (Chunk 1:154)
    quote: including NIAM (Weber and Zhang, 1991), ER (Wand et al., 1999)
  description: NIAM (Natural-language Information Analysis Method) was evaluated using
    ontological foundations, demonstrating the application of foundational ontology
    to conceptual modeling languages.
- name: ER Model Ontological Evaluation
  sources:
  - chunk_ref: 01-UFO_Unified_Foundational_Ontology (Chunk 1:154)
    quote: including NIAM (Weber and Zhang, 1991), ER (Wand et al., 1999)
  description: The Entity-Relationship model was evaluated using Bunge-Wand-Weber
    ontological foundations, informing UFO's development of theories for entity types
    and relationship types.
- name: OntoClean Methodology Integration
  sources:
  - chunk_ref: 01-UFO_Unified_Foundational_Ontology (Chunk 1:182)
    quote: we needed something in the spirit of the ontology of universals underlying
      the OntoClean methodology (Guarino and Welty, 2009)
  description: OntoClean methodology provides meta-property analysis (rigidity, identity,
    unity, dependence) that influenced UFO's theory of universals and type hierarchies.
- name: DOLCE Foundational Ontology
  sources:
  - chunk_ref: 01-UFO_Unified_Foundational_Ontology (Chunk 1:173-175)
    quote: the Descriptive Ontology for Linguistic and Cognitive Engineering (DOLCE;
      Masolo et al., 2003). In this setting, their first attempt was to unify DOLCE
      and GFO
  - chunk_ref: 23-UFO_Story_Ontological_Foundations (Chunk 1:139-140)
    quote: developing the Descriptive Ontology for Linguistic and Cognitive Engineering
      (DOLCE)
  description: Merged from 2 sources. DOLCE (Descriptive Ontology for Linguistic and
    Cognitive Engineering) was one of the foundational ontologies unified to create
    UFO. DOLCE is based on the Aristotelian Square but designed as an ontology of
    particulars.
- name: GFO/GOL Foundational Ontology
  sources:
  - chunk_ref: 01-UFO_Unified_Foundational_Ontology (Chunk 1:171-172)
    quote: Guizzardi and Wagner attempted to employ the General Formal Ontology (GFO)
      and the General Ontology Language (GOL) being developed in Leipzig
  description: GFO (General Formal Ontology) and GOL (General Ontology Language) were
    reference theories used in early UFO development, providing philosophical grounding
    and the Aristotelian Square structure.
- name: OntoUML Stereotypes Notation
  sources:
  - chunk_ref: 01-UFO_Unified_Foundational_Ontology (Chunk 2:562-563)
    quote: In OntoUML, the stereotypes <<kind>> and <<quantity>> stand for ObjectKind
      and QuantityKind, respectively
  description: OntoUML uses UML-style stereotypes to denote ontological categories
    from UFO, such as <<kind>>, <<quantity>>, <<role>>, <<relator>>, enabling visual
    representation of ontological distinctions in conceptual models.
- name: OntoUML Automatic Model Generation
  sources:
  - chunk_ref: 01-UFO_Unified_Foundational_Ontology (Chunk 2:701-705)
    quote: From an OntoUML model and using its supporting tools, we can automatically
      generate models that satisfy the logical theory corresponding to the OntoUML
      model
  description: OntoUML has supporting tools that can automatically generate instances
    (visual models) that satisfy the logical constraints implied by an OntoUML conceptual
    model, enabling validation and simulation.
- name: OntoUML Anti-Pattern Detection Tool
  sources:
  - chunk_ref: 01-UFO_Unified_Foundational_Ontology (Chunk 2:865-866)
    quote: This constraint is automatically detected and included in the specification
      by the anti-pattern detection and rectification support of the OntoUML tool
  description: OntoUML includes tooling support for automatic detection of modeling
    anti-patterns and rectification suggestions, referenced as (Guerson et al., 2015),
    improving model quality through automated analysis.
- name: UFO-S Service Pattern
  sources:
  - chunk_ref: 01-UFO_Unified_Foundational_Ontology (Chunk 2:820-821)
    quote: A Course Offering in this model is a particular instantiation of the UFO-S
      Service Offering pattern (Falbo et al., 2016)
  description: UFO-S provides reusable ontology patterns for service modeling, including
    Service Offering and Service Agreement patterns that can be instantiated in domain-specific
    models like course enrollments.
- name: Quality Structure Datatypes
  sources:
  - chunk_ref: 01-UFO_Unified_Foundational_Ontology (Chunk 2:906-908)
    quote: In OntoUML, the simplest way to represent quality structures is as datatypes
      (Guizzardi, 2005). Classes stereotyped as <<enumeration>> are particular types
      of datatypes
  description: OntoUML provides mechanisms to represent quality structures (value
    spaces) as datatypes, with enumerations as a special case for discrete value spaces.
    More sophisticated representations are described in Albuquerque and Guizzardi
    (2013).
- name: GoT DSL Configuration Syntax
  sources:
  - chunk_ref: 19-Graph_of_Thoughts_LLM_Reasoning (Chunk 7:636-651)
    quote: 'Generate(k=1) # Split second set into two halves of 16 elements... Score(k=1)
      # Score locally the intersected subsets... KeepBestN(1) # Keep the best intersected
      subset'
  description: Graph of Thoughts uses a domain-specific configuration language with
    primitives like Generate(k), Score(k), KeepBestN(n), Aggregate(n), and GroundTruth()
    to define reasoning workflows. This DSL enables declarative specification of LLM
    reasoning operations.
- name: GoT Operations Primitive Set
  sources:
  - chunk_ref: 19-Graph_of_Thoughts_LLM_Reasoning (Chunk 7:640-650)
    quote: 'Generate(k=5) # Determine intersected subset... Score(k=1) # Score locally...
      KeepBestN(1) # Keep the best... Aggregate(10) # Merge both intersected subsets'
  description: 'The GoT framework defines four core operations: Generate (create candidate
    solutions), Score (evaluate quality), KeepBestN (selection), and Aggregate (merge
    results). These form a complete toolkit for structured LLM reasoning.'
- name: GoT Foreach Parallel Iteration
  sources:
  - chunk_ref: 19-Graph_of_Thoughts_LLM_Reasoning (Chunk 7:742-746)
    quote: 'Generate(k=1) # Split list into two halves of 16 elements... foreach list
      part: Generate(k=5) # Sort list part'
  description: GoT supports foreach iteration constructs for parallel processing of
    subproblems, enabling divide-and-conquer strategies in LLM reasoning workflows.
- name: GoT Iterative Refinement Pattern
  sources:
  - chunk_ref: 19-Graph_of_Thoughts_LLM_Reasoning (Chunk 7:750-753)
    quote: 'Generate(k=10) # Try to improve solution... Score(k=1) # Score locally
      the sorted result lists... KeepBestN(1) # Keep the best result... GroundTruth()'
  description: GoT implements iterative refinement through repeated Generate-Score-KeepBest
    cycles, allowing progressive improvement of solutions before final ground truth
    validation.
- name: GitHub Repository for Agentic RAG
  sources:
  - chunk_ref: 20-Agentic_RAG_Survey (Chunk 1:66-67)
    quote: 'The GitHub link for this survey is available at: https://github.com/asinghcsu/AgenticRAG-Survey'
  description: The Agentic RAG survey provides an open-source repository with implementation
    resources and frameworks for building agentic retrieval-augmented generation systems.
- name: Dense Passage Retrieval (DPR)
  sources:
  - chunk_ref: 20-Agentic_RAG_Survey (Chunk 1:232-233)
    quote: Advanced RAG systems build upon the limitations of Naive RAG by incorporating
      semantic understanding... These systems leverage dense retrieval models, such
      as Dense Passage Retrieval (DPR)
  description: Dense Passage Retrieval is a key technical standard for advanced RAG
    systems, enabling semantic understanding and neural ranking for improved retrieval
    precision.
- name: TF-IDF and BM25 Retrieval
  sources:
  - chunk_ref: 20-Agentic_RAG_Survey (Chunk 1:200-201)
    quote: These systems rely on simple keyword-based retrieval techniques, such as
      TF-IDF and BM25, to fetch documents from static datasets
  description: TF-IDF and BM25 are foundational sparse retrieval standards used in
    Naive RAG implementations for keyword-based document retrieval.
- name: Hybrid Retrieval Strategies
  sources:
  - chunk_ref: 20-Agentic_RAG_Survey (Chunk 1:274-276)
    quote: 'Hybrid Retrieval Strategies: Combining sparse retrieval methods (e.g.,
      a sparse encoder-BM25) with dense retrieval techniques (e.g., DPR - Dense Passage
      Retrieval) to maximize accuracy'
  description: Modular RAG implements hybrid retrieval combining sparse (BM25) and
    dense (DPR) methods as a technical standard for optimizing retrieval across diverse
    query types.
- name: External API and Tool Integration
  sources:
  - chunk_ref: 20-Agentic_RAG_Survey (Chunk 1:279-280)
    quote: 'Tool Integration: Incorporating external APIs, databases, or computational
      tools to handle specialized tasks, such as real-time data analysis or domain-specific
      computations'
  description: Modular RAG architectures standardize API integration patterns for
    external tools, enabling real-time data access and domain-specific computation
    capabilities.
- name: Vector Database Search
  sources:
  - chunk_ref: 20-Agentic_RAG_Survey (Chunk 1:161-162)
    quote: 'Retrieveal: Responsible for querying external data sources such as knowledge
      bases, APIs, or vector databases. Advanced retrievers leverage dense vector
      search'
  description: Vector databases are a core technical infrastructure for RAG systems,
    enabling dense vector search and semantic retrieval across knowledge bases.
- name: Text-to-SQL Integration
  sources:
  - chunk_ref: 20-Agentic_RAG_Survey (Chunk 1:775-776)
    quote: 'Structured Databases: For queries requiring tabular data access, the system
      may use a Text-to-SQL engine that interacts with databases like PostgreSQL or
      MySQL'
  description: Text-to-SQL engines are integrated into Agentic RAG for structured
    data retrieval, enabling natural language queries against relational databases.
- name: GPT-4 Function Calling
  sources:
  - chunk_ref: 20-Agentic_RAG_Survey (Chunk 1:561-563)
    quote: The implementation of this pattern has evolved significantly with advancements
      like GPT-4's function calling capabilities and systems capable of managing access
      to numerous tools
  description: GPT-4's function calling capability is a key technical standard enabling
    agents to dynamically select and execute tools within agentic workflows.
- name: Vector Search Tool
  sources:
  - chunk_ref: 20-Agentic_RAG_Survey (Chunk 2:11)
    quote: 'Vector Search: For semantic relevance'
  description: Vector search is a standard tool type in multi-agent RAG systems for
    retrieving semantically relevant information from vector databases.
- name: Web Search Tool
  sources:
  - chunk_ref: 20-Agentic_RAG_Survey (Chunk 2:15)
    quote: 'Web Search: For real-time public information'
  description: Web search integration is a standard tool in Agentic RAG for accessing
    real-time public information beyond static knowledge bases.
- name: APIs for External Services
  sources:
  - chunk_ref: 20-Agentic_RAG_Survey (Chunk 2:17)
    quote: 'APIs: For accessing external services or proprietary systems'
  description: API integration is a standardized approach in multi-agent RAG for connecting
    to external services and proprietary enterprise systems.
- name: LlamaIndex Agentic Document Workflows
  sources:
  - chunk_ref: 20-Agentic_RAG_Survey (Chunk 2:704-708)
    quote: Agentic Document Workflows (ADW) extend traditional Retrieval-Augmented
      Generation (RAG) paradigms by enabling end-to-end knowledge work automation
  description: LlamaIndex's ADW framework provides a technical standard for document-centric
    agentic workflows, integrating parsing, retrieval, reasoning, and structured outputs.
- name: LlamaParse Document Processing
  sources:
  - chunk_ref: 20-Agentic_RAG_Survey (Chunk 2:717)
    quote: Documents are parsed using enterprise-grade tools (e.g., LlamaParse) to
      extract relevant data fields
  description: LlamaParse is an enterprise-grade document parsing tool used within
    Agentic Document Workflows for structured data extraction from documents.
- name: LlamaCloud Knowledge Retrieval
  sources:
  - chunk_ref: 20-Agentic_RAG_Survey (Chunk 2:733)
    quote: Relevant references are retrieved from external knowledge bases (e.g.,
      LlamaCloud) or vector indexes
  description: LlamaCloud provides cloud-based knowledge retrieval infrastructure
    for Agentic Document Workflows, supporting vector indexes and external knowledge
    bases.
- name: LangChain Framework
  sources:
  - chunk_ref: 20-Agentic_RAG_Survey (Chunk 3:164)
    quote: 'LangChain and LangGraph: LangChain provides modular components for building
      RAG pipelines, seamlessly integrating retrievers, generators, and external tools'
  - chunk_ref: 03-PROV-AGENT (Chunk 1:141)
    quote: LangChain [10], [11], AutoGen [12], LangGraph [13], Academy [3], and CrewAI
      [14] support multi-agent systems
  description: Merged from 2 sources. LangChain is a major framework for building
    RAG pipelines, providing modular components for retriever-generator integration
    and external tool connectivity.
- name: LangGraph Workflow Framework
  sources:
  - chunk_ref: 20-Agentic_RAG_Survey (Chunk 3:165-167)
    quote: LangGraph complements this by introducing graph-based workflows that support
      loops, state persistence, and human-in-the-loop interactions
  description: LangGraph extends LangChain with graph-based workflow capabilities
    including loops, state persistence, and human-in-the-loop interactions for sophisticated
    agentic orchestration.
- name: LlamaIndex Framework
  sources:
  - chunk_ref: 20-Agentic_RAG_Survey (Chunk 3:169-172)
    quote: LlamaIndex's Agentic Document Workflows (ADW) enable end-to-end automation
      of document processing, retrieval, and structured reasoning
  description: LlamaIndex is a framework for document-centric RAG with meta-agent
    architecture supporting sub-agent coordination for compliance analysis and contextual
    understanding.
- name: Hugging Face Transformers
  sources:
  - chunk_ref: 20-Agentic_RAG_Survey (Chunk 3:174)
    quote: 'Hugging Face Transformers and Qdrant: Hugging Face offers pre-trained
      models for embedding and generation tasks'
  description: Hugging Face Transformers provides pre-trained models for embedding
    and generation tasks, serving as a foundation for RAG system components.
- name: Qdrant Vector Database
  sources:
  - chunk_ref: 20-Agentic_RAG_Survey (Chunk 3:175-176)
    quote: Qdrant enhances retrieval workflows with adaptive vector search capabilities,
      allowing agents to optimize performance by dynamically switching between sparse
      and dense vector methods
  description: Qdrant is a vector database that provides adaptive vector search capabilities
    with dynamic switching between sparse and dense retrieval methods.
- name: CrewAI Multi-Agent Framework
  sources:
  - chunk_ref: 20-Agentic_RAG_Survey (Chunk 3:182-183)
    quote: 'CrewAI and AutoGen: These frameworks emphasize multi-agent architectures.
      CrewAI supports hierarchical and sequential processes, robust memory systems,
      and tool integrations'
  description: CrewAI is a multi-agent framework supporting hierarchical processes,
    sequential workflows, memory systems, and tool integrations for collaborative
    AI agents.
- name: AutoGen/AG2 Framework
  sources:
  - chunk_ref: 20-Agentic_RAG_Survey (Chunk 3:183-185)
    quote: AG2 (formerly knows as AutoGen) excels in multi-agent collaboration with
      advanced support for code generation, tool execution, and decision-making
  description: AutoGen (now AG2) is a multi-agent framework from Microsoft excelling
    in code generation, tool execution, and collaborative decision-making capabilities.
- name: OpenAI Swarm Framework
  sources:
  - chunk_ref: 20-Agentic_RAG_Survey (Chunk 3:188-189)
    quote: 'OpenAI Swarm Framework: An educational framework designed for ergonomic,
      lightweight multi-agent orchestration, emphasizing agent autonomy and structured
      collaboration'
  description: OpenAI Swarm is a lightweight educational framework for multi-agent
    orchestration emphasizing agent autonomy and structured collaboration patterns.
- name: Google Vertex AI for Agentic RAG
  sources:
  - chunk_ref: 20-Agentic_RAG_Survey (Chunk 3:192-195)
    quote: 'Agentic RAG with Vertex AI: Developed by Google, Vertex AI integrates
      seamlessly with Agentic RAG, providing a platform to build, deploy, and scale
      machine learning models'
  description: Google Vertex AI provides enterprise infrastructure for building, deploying,
    and scaling Agentic RAG systems with advanced retrieval and decision-making workflows.
- name: Microsoft Semantic Kernel
  sources:
  - chunk_ref: 20-Agentic_RAG_Survey (Chunk 3:198-202)
    quote: Semantic Kernel is an open-source SDK by Microsoft that integrates large
      language models (LLMs) into applications. It supports agentic patterns
  description: Microsoft Semantic Kernel is an open-source SDK for integrating LLMs
    into applications, supporting agentic patterns for autonomous AI agents with task
    automation.
- name: Amazon Bedrock for Agentic RAG
  sources:
  - chunk_ref: 20-Agentic_RAG_Survey (Chunk 3:205-206)
    quote: 'Amazon Bedrock for Agentic RAG: Amazon Bedrock provides a robust platform
      for implementing Agentic RAG workflows'
  description: Amazon Bedrock provides AWS cloud infrastructure for implementing Agentic
    RAG workflows with foundation model access and agentic capabilities.
- name: IBM watsonx.ai
  sources:
  - chunk_ref: 20-Agentic_RAG_Survey (Chunk 3:209-211)
    quote: 'IBM Watson and Agentic RAG: IBM''s watsonx.ai supports building Agentic
      RAG systems, exemplified by using the Granite-3-8B-Instruct model'
  description: IBM watsonx.ai supports Agentic RAG systems using Granite models for
    complex query answering with external information integration.
- name: Neo4j Graph Database
  sources:
  - chunk_ref: 20-Agentic_RAG_Survey (Chunk 3:214-217)
    quote: 'Neo4j and Vector Databases: Neo4j, a prominent open-source graph database,
      excels in handling complex relationships and semantic queries'
  description: Neo4j is an open-source graph database used in Agentic RAG for handling
    complex relationships and semantic queries in knowledge graph applications.
- name: Vector Database Ecosystem
  sources:
  - chunk_ref: 20-Agentic_RAG_Survey (Chunk 3:215-217)
    quote: Alongside Neo4j, vector databases like Weaviate, Pinecone, Milvus, and
      Qdrant provide efficient similarity search and retrieval capabilities
  description: Weaviate, Pinecone, Milvus, and Qdrant form the vector database ecosystem
    providing similarity search and retrieval infrastructure for Agentic RAG workflows.
- name: BEIR Benchmark
  sources:
  - chunk_ref: 20-Agentic_RAG_Survey (Chunk 3:234-236)
    quote: 'BEIR (Benchmarking Information Retrieval): A versatile benchmark designed
      for evaluating embedding models on a variety of information retrieval tasks,
      encompassing 17 datasets'
  description: BEIR is a standard benchmark for evaluating embedding models across
    17 diverse information retrieval datasets spanning bioinformatics, finance, and
    QA domains.
- name: MS MARCO Benchmark
  sources:
  - chunk_ref: 20-Agentic_RAG_Survey (Chunk 3:239-240)
    quote: 'MS MARCO (Microsoft Machine Reading Comprehension): Focused on passage
      ranking and question answering, this benchmark is widely used for dense retrieval
      tasks'
  description: MS MARCO is a standard benchmark for passage ranking and question answering,
    widely used for evaluating dense retrieval in RAG systems.
- name: FlashRAG Toolkit
  sources:
  - chunk_ref: 20-Agentic_RAG_Survey (Chunk 3:276-277)
    quote: 'FlashRAG Toolkit: Implements 12 RAG methods and includes 32 benchmark
      datasets to support efficient and standardized RAG evaluation'
  description: FlashRAG is a toolkit implementing 12 RAG methods with 32 benchmark
    datasets for standardized RAG system evaluation and comparison.
- name: BPMN 2.0 Choreographies
  sources:
  - chunk_ref: 21-LLM_Smart_Contracts_from_BPMN (Chunk 1:338-339)
    quote: In the current instantiation of our framework, we support BPMN 2.0 Choreographies.
      This is a purely practical implementation choice
  description: BPMN 2.0 Choreographies is a standard notation for modeling business
    processes, used as input for smart contract generation in blockchain-based process
    execution.
- name: Ethereum Virtual Machine (EVM)
  sources:
  - chunk_ref: 21-LLM_Smart_Contracts_from_BPMN (Chunk 1:341-342)
    quote: We instantiate our framework for an Ethereum virtual machine (EVM) blockchain
      environment, the most widely employed environment
  description: The Ethereum Virtual Machine (EVM) is the primary blockchain execution
    environment for smart contracts, serving as the target platform for BPMN-to-contract
    transformation.
- name: Solidity Smart Contract Language
  sources:
  - chunk_ref: 21-LLM_Smart_Contracts_from_BPMN (Chunk 1:135)
    quote: training ingests very large textual corpora, comprising not only natural
      language, but also programming code (such as Solidity for blockchain smart contracts)
  description: Solidity is the primary programming language for Ethereum smart contracts,
    used as the target output for LLM-based code generation from process models.
- name: Hardhat Development Framework
  sources:
  - chunk_ref: 21-LLM_Smart_Contracts_from_BPMN (Chunk 1:363-365)
    quote: To provide the replayer with a blockchain environment, we use hardhat,
      a popular Ethereum development framework that allows testing, deployment, and
      debugging of smart contracts
  description: Hardhat is an Ethereum development framework for testing, deploying,
    and debugging smart contracts in simulated EVM environments.
- name: OpenRouter LLM API Platform
  sources:
  - chunk_ref: 21-LLM_Smart_Contracts_from_BPMN (Chunk 1:364-366)
    quote: As LLM provider, we use OpenRouter, a platform that provides a unified
      API across multiple language model providers
  description: OpenRouter is an API platform providing unified access to multiple
    LLM providers, used for benchmarking smart contract generation across different
    models.
- name: Chorpiler BPMN Transformation Tool
  sources:
  - chunk_ref: 21-LLM_Smart_Contracts_from_BPMN (Chunk 1:346-348)
    quote: We extend the open source tool Chorpiler, first introduced in [40] with
      simulation capabilities. Chorpiler transforms BPMN Choreographies to smart contracts
  description: Chorpiler is an open-source tool for transforming BPMN Choreographies
    into smart contracts, extended with simulation capabilities for trace generation.
- name: pm4py Process Mining Library
  sources:
  - chunk_ref: 21-LLM_Smart_Contracts_from_BPMN (Chunk 1:351-354)
    quote: we adopt the implementation of pm4py, a popular Python library for process
      mining, which includes a playout functionality to generate event log traces
      from Petri nets
  description: pm4py is a Python process mining library used for generating conforming
    traces from Petri net representations of business processes.
- name: SAP-SAM Process Model Dataset
  sources:
  - chunk_ref: 21-LLM_Smart_Contracts_from_BPMN (Chunk 1:386-388)
    quote: Our process model data is based on the SAP Signavio Academic Models Dataset
      (SAP-SAM), which was initially introduced in [37]
  description: SAP-SAM is a large academic dataset of 4,096 BPMN 2.0 choreography
    models, used for benchmarking LLM-based smart contract generation.
- name: GitHub Copilot Integration
  sources:
  - chunk_ref: 21-LLM_Smart_Contracts_from_BPMN (Chunk 1:43-44)
    quote: testified by the integration of commercial tools like Github's Copilot...
      into popular development environments like Visual Studio Code
  description: GitHub Copilot represents the commercial integration of LLM-based code
    generation into development environments like Visual Studio Code.
- name: RPA Technology Platform
  sources:
  - chunk_ref: 22-RPA_Framework_BPM_Activities (Chunk 1:107-109)
    quote: In this work, we define RPA as an automation technology which performs
      work on the presentation layer, can be set up by a business user, and is managed
      on a centralized platform
  description: RPA (Robotic Process Automation) is defined as a centralized automation
    platform operating on the presentation layer, enabling business users to automate
    UI-based tasks.
- name: Low-Code RPA Development
  sources:
  - chunk_ref: 22-RPA_Framework_BPM_Activities (Chunk 1:104-105)
    quote: little to no programming knowledge is required to implement and manage
      the orchestration and execution of the robots often referred to as low-code
      development
  description: Low-code development is a key characteristic of RPA platforms, enabling
    business users without programming expertise to configure automation robots.
- name: Process Mining Software (Celonis)
  sources:
  - chunk_ref: 22-RPA_Framework_BPM_Activities (Chunk 1:425-426)
    quote: We determine process characteristics with Process Mining Software. Hence,
      we test the framework for its applicability in a practical environment
  description: Celonis is a process mining platform used to analyze event logs and
    evaluate RPA automation candidates based on process characteristics.
- name: SAP ECC Enterprise System
  sources:
  - chunk_ref: 22-RPA_Framework_BPM_Activities (Chunk 1:535)
    quote: Since the event log originates from an SAP ECC system, which is a roofing
      system, we cannot determine whether there are more systems involved
  description: SAP ECC is an enterprise system generating event logs used for process
    mining analysis to identify RPA automation opportunities.
- name: BPI Challenge 2019 Dataset
  sources:
  - chunk_ref: 22-RPA_Framework_BPM_Activities (Chunk 1:654-656)
    quote: 'van Dongen, B.: Bpi challenge 2019... https://doi.org/10.4121/uuid:d06aff4b-79f0-45e6-8ec8-e19730c248f1,
      4TU.Centre for Research Data. Dataset'
  description: BPI Challenge 2019 dataset is a publicly available process mining dataset
    used to validate the RPA process characteristics evaluation framework.
- name: OntoUML as UML Profile
  sources:
  - chunk_ref: 23-UFO_Story_Ontological_Foundations (Chunk 1:243-249)
    quote: The decision to build the language as a version of UML (technically, as
      a UML profile) was mainly motivated by the fact that UML...has a standardized
      and explicitly defined metamodel
  description: OntoUML was implemented as a UML 2.0 profile, leveraging UML's standardized
    metamodel and metamodeling tools. This approach enabled the use of existing UML
    tools like Enterprise Architect to create ontology-driven conceptual models.
- name: OWL DL Codification Target
  sources:
  - chunk_ref: 23-UFO_Story_Ontological_Foundations (Chunk 1:306)
    quote: we can have different mappings to different codification languages (e.g.,
      OWL DL, RDFS, F-Logic, Haskell, Relational Database languages, CASL, among many
      others)
  description: OntoUML models can be transformed into multiple implementation languages
    including OWL DL, RDFS, F-Logic, Haskell, and relational database schemas. Six
    different automatic mappings from OntoUML to OWL have been implemented.
- name: OCL for Domain Constraints
  sources:
  - chunk_ref: 23-UFO_Story_Ontological_Foundations (Chunk 1:288-290)
    quote: the current editor supports the specification of OCL (Guerson et al., 2014)
      and temporal OCL formal constraints (Guerson & Almeida, 2015)
  description: Object Constraint Language (OCL) and temporal OCL are supported for
    specifying domain-specific formal constraints that cannot be represented using
    diagrammatic notation. The editor provides syntax highlighting, code-completion
    and syntax verification.
- name: SBVR Model Verbalization
  sources:
  - chunk_ref: 23-UFO_Story_Ontological_Foundations (Chunk 1:275-276)
    quote: The editor incorporates a functionality for automatically generating model
      verbalization in structured English following a slightly modified version of
      the SBVR (Semantics for Business Vocabularies and Rules) OMG proposal
  description: SBVR (Semantics for Business Vocabularies and Rules) is used for automatic
    model verbalization, generating controlled natural language renderings of OntoUML
    models to allow domain experts to access conceptual model content.
- name: Alloy Formal Verification
  sources:
  - chunk_ref: 23-UFO_Story_Ontological_Foundations (Chunk 1:442-443)
    quote: 'Transforming OntoUML into alloy: Towards conceptual model validation using
      a lightweight formal method'
  description: Alloy is used as a lightweight formal method for OntoUML model validation,
    enabling automatic generation of visual instances for validation via simulation.
- name: XML Schema Generation
  sources:
  - chunk_ref: 23-UFO_Story_Ontological_Foundations (Chunk 1:316)
    quote: other authors have implemented alternative mappings from OntoUML to languages
      such as XML (Baumann, 2009)
  description: OntoUML models can be mapped to XML schemas for semantic data representation,
    enabling integration with XML-based systems.
- name: Smalltalk Code Generation
  sources:
  - chunk_ref: 23-UFO_Story_Ontological_Foundations (Chunk 1:316-317)
    quote: other authors have implemented alternative mappings from OntoUML to languages
      such as XML (Baumann, 2009), Smalltalk (Pergl et al., 2013)
  description: OntoUML can be transformed into Smalltalk code for implementation purposes,
    demonstrating the language-independent nature of the ontological foundations.
- name: Modal Prolog Implementation
  sources:
  - chunk_ref: 23-UFO_Story_Ontological_Foundations (Chunk 1:317)
    quote: a version of a Modal Prolog (Araujo, 2015)
  description: OntoUML can be mapped to Modal Prolog, enabling logical reasoning over
    ontological models with modal properties.
- name: Enterprise Architect Plugin
  sources:
  - chunk_ref: 23-UFO_Story_Ontological_Foundations (Chunk 1:246-247)
    quote: an OntoUML plug-in has been implemented for the professional tool Enterprise
      Architect
  description: A plugin for Enterprise Architect enables professional use of OntoUML
    in industrial settings, supporting pattern-based model construction, formal verification,
    and anti-pattern detection.
- name: Protege Tool Mention
  sources:
  - chunk_ref: 23-UFO_Story_Ontological_Foundations (Chunk 1:245-246)
    quote: a significant set of metamodeling tools that can be used to manipulate
      and extend this metamodel
  description: UML metamodeling tools are leveraged for OntoUML development, with
    Protege being commonly used for OWL implementations in the ontology engineering
    community.
- name: Sortal Quantified Modal Logics
  sources:
  - chunk_ref: 23-UFO_Story_Ontological_Foundations (Chunk 1:176)
    quote: a theory of object identifiers (including a formal semantics in a Sortal
      Quantified Modal Logics (Guizzardi, 2015))
  description: UFO-A includes formal semantics expressed in Sortal Quantified Modal
    Logics for identity theory, providing rigorous mathematical foundations for conceptual
    modeling.
- name: GFO General Formalized Ontology
  sources:
  - chunk_ref: 23-UFO_Story_Ontological_Foundations (Chunk 1:137-138)
    quote: we attempted to employ the GFO (General Formalized Ontology)/GOL (General
      Ontology Language) being developed in Leipzig, Germany
  description: GFO (General Formalized Ontology) and GOL (General Ontology Language)
    were considered as reference theories for UFO development, representing alternative
    foundational ontology approaches.
- name: Standards Integration - TOGAF ArchiMate
  sources:
  - chunk_ref: 23-UFO_Story_Ontological_Foundations (Chunk 1:197)
    quote: UFO has been employed as a basis for analyzing, reengineering and integrating
      many modeling languages and standards in different domains (e.g., UML, TOGAF,
      ArchiMate, RM-ODP, TROPOS/i*, AORML, ARIS, BPMN)
  description: UFO has been applied to analyze and integrate multiple enterprise architecture
    standards including TOGAF, ArchiMate, RM-ODP, and ARIS, demonstrating its role
    as a foundational framework for standards alignment.
- name: BPMN Process Modeling
  sources:
  - chunk_ref: 23-UFO_Story_Ontological_Foundations (Chunk 1:197-201)
    quote: Business Processes (Guizzardi & Wagner, 2011b; Santos Junior et al., 2010),
      Discrete Event Simulation (Guizzardi & Wagner, 2010; 2011a; 2012; 2013)
  description: UFO has been applied to provide ontological foundations for BPMN business
    process modeling and discrete event simulation, grounding process-related standards.
- name: OntoClean Methodology
  sources:
  - chunk_ref: 23-UFO_Story_Ontological_Foundations (Chunk 1:153)
    quote: we needed something in the spirit of the ontology of universals underlying
      the OntoClean methodology (Guarino & Welty, 2009)
  - chunk_ref: 05-DOLCE (Chunk 1:73-75)
    quote: The analysis underlying the formalization of DOLCE leverages the techniques
      of ontological engineering and the study of classes' meta-properties of the
      OntoClean methodology
  description: Merged from 2 sources. Methodology for evaluating ontological decisions
    through meta-properties like rigidity, identity, unity, dependence. Used to validate
    class hierarchies and taxonomic choices.
- name: UML 2.0 Class Diagrams
  sources:
  - chunk_ref: 23-UFO_Story_Ontological_Foundations (Chunk 1:213)
    quote: OntoUML (Guizzardi, 2005) was conceived as an ontologically well-founded
      version of the UML 2.0 fragment of class diagrams
  description: UML 2.0 class diagrams serve as the structural foundation for OntoUML,
    with the language extending UML with ontological distinctions from UFO-A.
- name: OWL 2 DL Implementation
  sources:
  - chunk_ref: 31-BBO_BPMN_Ontology (Chunk 1:359)
    quote: We have formalized and implemented the conceptual model of BBO in OWL 2
      DL using Protege
  description: BBO ontology is implemented in OWL 2 DL using Protege, enabling formal
    reasoning, consistency checking, and SPARQL querying over business process knowledge
    bases.
- name: BPMN 2.0 Meta-model Reuse
  sources:
  - chunk_ref: 31-BBO_BPMN_Ontology (Chunk 1:45-47)
    quote: we developed the BBO (BPMN 2.0 Based Ontology) ontology for business process
      representation, by reusing existing ontologies and meta-models like BPMN 2.0,
      the state-of-the-art meta-model for business process representation
  description: BBO is built by reusing and extending the BPMN 2.0 meta-model, extracting
    the process-execution specification fragment and transforming it into OWL representation.
- name: METHONTOLOGY Development Process
  sources:
  - chunk_ref: 31-BBO_BPMN_Ontology (Chunk 1:95-96)
    quote: a presentation of METHONTOLOGY (Fernandez et al., 1997), the methodology
      followed to develop BBO in five classical stages
  description: 'METHONTOLOGY was used as the ontology engineering methodology for
    BBO development, following five stages: Specification, Conceptualization, Formalization,
    Implementation, and Maintenance.'
- name: Protege Reasoners - Hermit Fact Pellet
  sources:
  - chunk_ref: 31-BBO_BPMN_Ontology (Chunk 1:426-428)
    quote: According to the various reasoners in Protege (i.e., Hermit, Fact and Pellet),
      the ontology is consistent and remains consistent, even after its population
      with assertions describing several BP models
  description: Multiple OWL reasoners (Hermit, Fact, Pellet) are used in Protege for
    consistency checking and automatic classification of BBO instances, validating
    ontology design.
- name: SPARQL Querying
  sources:
  - chunk_ref: 31-BBO_BPMN_Ontology (Chunk 1:485)
    quote: we design SPARQL queries corresponding to the competency questions and
      check their results
  description: SPARQL is used to query BBO knowledge bases populated with business
    process data, enabling answers to competency questions about process structure,
    resources, and execution.
- name: UML to OWL Transformation Rules
  sources:
  - chunk_ref: 31-BBO_BPMN_Ontology (Chunk 1:368-386)
    quote: 'Generating an OWL representation from the UML diagrams results in the
      following algorithm: For each UML class, create an owl class; For each relation
      between UML classes create an OWL ObjectProperty'
  description: Systematic transformation rules convert UML class diagrams to OWL,
    including class-to-class mapping, relation-to-ObjectProperty mapping, and cardinality-to-restriction
    mapping.
- name: UO Unit of Measure Ontology
  sources:
  - chunk_ref: 31-BBO_BPMN_Ontology (Chunk 1:289)
    quote: The UnitOfMeasure class is specified using the two concepts Unit and Prefix
      of the unit measures ontology UO (UO-onto, 2019)
  description: BBO reuses the UO (Units of Measurement Ontology) for specifying measurement
    units, demonstrating ontology reuse and interoperability.
- name: Camunda BPMN Editor
  sources:
  - chunk_ref: 31-BBO_BPMN_Ontology (Chunk 1:482)
    quote: we represent this BP with BPMN graphical elements using an open source
      software, Camunda (https://camunda.com/)
  description: Camunda is used as an open-source BPMN modeling tool for creating graphical
    process representations that are then populated into BBO ontology instances.
- name: XML Schema for BPMN
  sources:
  - chunk_ref: 31-BBO_BPMN_Ontology (Chunk 1:398-399)
    quote: BPMN 2.0 specification provides a meta-model for BPMN elements as a UML
      class diagram and in the form of an XML schema
  description: BPMN 2.0 is specified both as UML diagrams and XML Schema, though these
    formal representations do not capture all natural language specifications from
    the standard.
- name: rdfs:comment for Documentation
  sources:
  - chunk_ref: 31-BBO_BPMN_Ontology (Chunk 1:424-425)
    quote: for each BPMN element, we added its description as mentioned in BPMN 2.0
      specification (OMG., 2011) using the rdfs:comment property
  description: RDFS comment property is used to document BBO classes with descriptions
    from the BPMN 2.0 specification, enhancing ontology usability.
- name: OMG Standards Body
  sources:
  - chunk_ref: 31-BBO_BPMN_Ontology (Chunk 1:82-83)
    quote: BPMN is the most adopted meta-model for representing BPs (OMG., 2011).
      Indeed, it is a standard for BPM maintained by the Object Management Group (OMG)
  description: Object Management Group (OMG) maintains BPMN as an industry standard
    for business process modeling, providing the foundation for BBO's process representation
    capabilities.
- name: OntoUML Conceptual Modeling Language
  sources:
  - chunk_ref: 01-UFO_Unified_Foundational_Ontology (Chunk 3:5-8)
    quote: In Figure 8, we present an OntoUML model [17] representing this situation.
      In this model, a Rose is modeled as a subkind of Flower.
  description: OntoUML is a modeling language used for conceptual modeling grounded
    in UFO. It provides visual representation of ontological structures including
    classes, qualities, and relationships with stereotypes like subkind and characterization.
- name: OntoUML Quality Structures as Datatypes
  sources:
  - chunk_ref: 01-UFO_Unified_Foundational_Ontology (Chunk 3:17-19)
    quote: In OntoUML, the simplest way to represent quality structures is as datatypes
      (Guizzardi, 2005). Classes stereotyped as enumeration are particular types of
      datatypes.
  description: OntoUML uses datatypes to represent quality structures, with enumeration
    stereotypes for specific datatype categories. This provides a formal way to model
    qualities and their value spaces.
- name: OntoUML Automated Visual Model Generation
  sources:
  - chunk_ref: 01-UFO_Unified_Foundational_Ontology (Chunk 3:85-86)
    quote: In the sequel, we show examples (Figure 9) of the visual models automatically
      generated for the OntoUML diagram of Figure 8.
  description: OntoUML tools can automatically generate visual world structures from
    conceptual models, enabling validation and simulation of ontological specifications.
- name: OntoUML Event Type Modeling
  sources:
  - chunk_ref: 01-UFO_Unified_Foundational_Ontology (Chunk 3:154-157)
    quote: EventType(JoggingProcess) EventType(JoggingEvent)
  description: OntoUML provides EventType stereotypes for modeling perdurant entities
    (events and processes), enabling formal representation of temporal phenomena.
- name: Allen Temporal Relations in UFO
  sources:
  - chunk_ref: 01-UFO_Unified_Foundational_Ontology (Chunk 3:176-178)
    quote: if we have two Jogging Processes that are manifestations of the same Jog
      and that immediately follow each other, meeting in the Allen sense
  description: UFO/OntoUML incorporates Allen temporal relations (meet, before, overlaps,
    etc.) for reasoning about temporal relationships between events and processes.
- name: OntoUML Anti-Pattern Detection and Rectification
  sources:
  - chunk_ref: 01-UFO_Unified_Foundational_Ontology (Chunk 3:366-369)
    quote: This constraint is automatically generated by the OntoUML tool after an
      automated process of anti-pattern detection and rectification (Sales and Guizzardi,
      2015).
  description: OntoUML tools include automated anti-pattern detection capabilities
    that identify modeling errors and generate corrective constraints, improving model
    quality.
- name: ArchiMate Enterprise Architecture Standard
  sources:
  - chunk_ref: 01-UFO_Unified_Foundational_Ontology (Chunk 3:668-669)
    quote: ArchiMate (Almeida et al., 2009; Amaral et al., 2020a; Azevedo et al.,
      2011, 2015; Griffo et al., 2017; Sales et al., 2018a, 2019)
  description: UFO has been used to analyze and reengineer the ArchiMate enterprise
    architecture standard, demonstrating its applicability to industry modeling frameworks.
- name: ARIS Process Modeling Integration
  sources:
  - chunk_ref: 01-UFO_Unified_Foundational_Ontology (Chunk 3:671)
    quote: ARIS (Santos Junior et al., 2010, 2013)
  description: UFO has been applied to provide ontological analysis and semantic foundations
    for the ARIS (Architecture of Integrated Information Systems) business process
    modeling method.
- name: BPMN Ontological Analysis
  sources:
  - chunk_ref: 01-UFO_Unified_Foundational_Ontology (Chunk 3:679)
    quote: BPMN (Guizzardi and Wagner, 2011a)
  description: UFO provides ontological foundations for BPMN (Business Process Model
    and Notation), enabling formal analysis of business process modeling constructs.
- name: TOGAF Framework Analysis
  sources:
  - chunk_ref: 01-UFO_Unified_Foundational_Ontology (Chunk 3:683)
    quote: TOGAF (Almeida et al., 2009)
  description: UFO has been applied to analyze the TOGAF (The Open Group Architecture
    Framework), providing ontological grounding for enterprise architecture concepts.
- name: OntoUML Adoption as Modeling Language
  sources:
  - chunk_ref: 01-UFO_Unified_Foundational_Ontology (Chunk 3:691-696)
    quote: OntoUML is among the most used languages in ontology-driven conceptual
      modeling (together with UML, (E)ER, OWL, and BPMN). Moreover, empirical evidence
      shows that OntoUML significantly contributes to improving the quality of conceptual
      models
  description: OntoUML is recognized as one of the top five languages in ontology-driven
    conceptual modeling, with empirical validation showing improved model quality
    compared to classical languages like EER.
- name: OntoUML as a Service (OaaS) Infrastructure
  sources:
  - chunk_ref: 01-UFO_Unified_Foundational_Ontology (Chunk 3:697-700)
    quote: the OntoUML as a Service infrastructure (Fonseca et al., 2021b), or OaaS,
      is designed to decouple model services developed by OntoUML researchers (e.g.,
      transformations, verifications, simulations, verbalizations)
  description: OaaS is a microservice-based infrastructure for OntoUML that provides
    model intelligence services including transformations, verifications, simulations,
    and verbalizations through a decoupled architecture.
- name: HTTP and JSON for Model Serialization
  sources:
  - chunk_ref: 01-UFO_Unified_Foundational_Ontology (Chunk 3:705-706)
    quote: This is enabled by OaaS's low requirements on services and tool developers
      that consists of support to HTTP and JSON used for service request and model
      serialization respectively.
  description: The OntoUML service infrastructure uses HTTP for API communication
    and JSON for model serialization, enabling interoperability with various tools
    and programming languages.
- name: OntoUML JSON Schema Specification
  sources:
  - chunk_ref: 01-UFO_Unified_Foundational_Ontology (Chunk 3:706-707)
    quote: The current implementation of OaaS consists of a JSON Schema specification
      to govern the JSON serialization of OntoUML models
  description: A formal JSON Schema specification governs the serialization of OntoUML
    models, ensuring consistent representation across tools and services.
- name: OntoUML TypeScript Library (ontouml-js)
  sources:
  - chunk_ref: 01-UFO_Unified_Foundational_Ontology (Chunk 3:707-708)
    quote: a TypeScript library to support the manipulation and serialization of OntoUML
      models
  description: The ontouml-js TypeScript library provides programmatic support for
    manipulating and serializing OntoUML models, enabling integration with JavaScript/TypeScript
    applications.
- name: Visual Paradigm UML CASE Tool Integration
  sources:
  - chunk_ref: 01-UFO_Unified_Foundational_Ontology (Chunk 3:708-709)
    quote: a plugin for the UML CASE Visual Paradigm that extends it with OntoUML
      modeling capabilities
  description: A plugin for Visual Paradigm extends this commercial UML CASE tool
    with OntoUML modeling capabilities, enabling enterprise adoption of ontology-driven
    conceptual modeling.
- name: ISO/IEC 24744 Software Engineering Standard
  sources:
  - chunk_ref: 01-UFO_Unified_Foundational_Ontology (Chunk 4:93-94)
    quote: 'Henderson-Sellers, B., Gonzalez-Perez, C., McBride, T., and Low, G. (2014).
      An ontology for ISO software engineering standards: 1) creating the infrastructure.
      Computer Standards & Interfaces'
  description: UFO has been applied to create ontological infrastructure for ISO/IEC
    24744 and other ISO software engineering standards, enabling formal analysis of
    software development methodologies.
- name: ITU-T G.805 Telecommunications Standard
  sources:
  - chunk_ref: 01-UFO_Unified_Foundational_Ontology (Chunk 3:676-677)
    quote: ITU-T G.805 (Barcelos et al., 2011)
  description: UFO has been used for ontological evaluation of ITU-T G.805, a telecommunications
    networking standard, demonstrating applicability to technical infrastructure domains.
- name: RDF (Resource Description Framework)
  sources:
  - chunk_ref: 02-Knowledge_Graphs (Chunk 1:435-438)
    quote: A standardised data model based on directed edge-labelled graphs is the
      Resource Description Framework (RDF) [111], which has been recommended by the
      W3C.
  - chunk_ref: 04-PROV-O_to_BFO (Chunk 1:42)
    quote: Resource Description Framework (RDF) [4]
  description: Merged from 2 sources. RDF is the W3C-recommended standardized data
    model for directed edge-labelled graphs, forming the foundation for knowledge
    graph representation with support for IRIs, literals, and blank nodes.
- name: Internationalized Resource Identifiers (IRIs)
  sources:
  - chunk_ref: 02-Knowledge_Graphs (Chunk 1:439-441)
    quote: The RDF model defines different types of nodes, including Internationalized
      Resource Identifiers (IRIs) [134] which allow for global identification of entities
      on the Web
  description: IRIs provide global Web-based identification for entities in RDF knowledge
    graphs, enabling unique naming across distributed systems.
- name: SPARQL Query Language for RDF
  sources:
  - chunk_ref: 02-Knowledge_Graphs (Chunk 1:664-665)
    quote: A number of practical languages have been proposed for querying graphs
      [16], including the SPARQL query language for RDF graphs [217]
  - chunk_ref: 02-Knowledge_Graphs (Chunk 8:128-129)
    quote: public services offering such a protocol (most often supporting SPARQL
      queries [217]) have been found to often exhibit downtimes, timeouts
  - chunk_ref: 04-PROV-O_to_BFO (Chunk 1:180)
    quote: SPARQL [24] queries can also encode mappings that are equivalent to some
      SWRL rules
  - chunk_ref: 17-KG_Reasoning (Chunk 1:361)
    quote: Conventional query answering is conducted based on structure query languages
      such as SPARQL to retrieve and manipulate knowledge in a KG
  description: Merged from 4 sources. SPARQL is identified as the standard query language
    for retrieving and manipulating knowledge in knowledge graphs, enabling structured
    access to graph data.
- name: Cypher Query Language for Property Graphs
  sources:
  - chunk_ref: 02-Knowledge_Graphs (Chunk 1:665)
    quote: and Cypher [165], Gremlin [445], and G-CORE [15] for querying property
      graphs
  description: Cypher is a declarative query language for property graphs, used prominently
    in Neo4j, supporting pattern matching with isomorphism-based semantics on edges.
- name: Gremlin Graph Traversal Language
  sources:
  - chunk_ref: 02-Knowledge_Graphs (Chunk 1:665)
    quote: Cypher [165], Gremlin [445], and G-CORE [15] for querying property graphs
  description: Gremlin is a graph traversal language for property graphs that provides
    a functional, data-flow-based approach to graph queries.
- name: G-CORE Query Language
  sources:
  - chunk_ref: 02-Knowledge_Graphs (Chunk 1:665)
    quote: Cypher [165], Gremlin [445], and G-CORE [15] for querying property graphs
  description: G-CORE is a composable graph query language that extends property graph
    querying with path return values, graph projections, and cost function support.
- name: Neo4j Property Graph Database
  sources:
  - chunk_ref: 02-Knowledge_Graphs (Chunk 1:562)
    quote: Property graphs are most prominently used in popular graph databases, such
      as Neo4j [16, 354].
  description: Neo4j is the most prominent property graph database system, supporting
    native graph storage, property graph data model, and Cypher query language.
- name: Triple Table Storage for Graphs
  sources:
  - chunk_ref: 02-Knowledge_Graphs (Chunk 1:646-648)
    quote: Directed-edge labelled graphs can be stored in relational databases either
      as a single relation of arity three (triple table), as a binary relation for
      each property (vertical partitioning)
  description: Triple table is a storage strategy for RDF graphs using a single three-column
    relational table, enabling simple storage but potentially limiting query performance.
- name: Vertical Partitioning Storage
  sources:
  - chunk_ref: 02-Knowledge_Graphs (Chunk 1:647-648)
    quote: as a binary relation for each property (vertical partitioning)
  description: Vertical partitioning stores RDF graphs by creating separate two-column
    tables for each property, improving certain query patterns at the cost of increased
    table count.
- name: Property Tables Storage
  sources:
  - chunk_ref: 02-Knowledge_Graphs (Chunk 1:648-650)
    quote: or as n-ary relations for entities of a given type (property tables)
  description: Property tables storage strategy groups entities by type into n-ary
    relations, combining the benefits of relational normalization with type-specific
    query optimization.
- name: RDFS (RDF Schema) Semantic Schema
  sources:
  - chunk_ref: 02-Knowledge_Graphs (Chunk 2:176-180)
    quote: A prominent standard for defining a semantic schema for (RDF) graphs is
      the RDF Schema (RDFS) standard [70], which allows for defining subclasses, subproperties,
      domains, and ranges
  description: RDFS is the W3C standard for defining semantic schemas for RDF graphs,
    supporting class hierarchies, property hierarchies, and domain/range constraints.
- name: OWL (Web Ontology Language)
  sources:
  - chunk_ref: 02-Knowledge_Graphs (Chunk 2:188-190)
    quote: the semantics of terms used in a graph can be defined in much more depth
      than seen here, as is supported by the Web Ontology Language (OWL) standard
      [239] for RDF graphs
  - chunk_ref: 04-PROV-O_to_BFO (Chunk 1:41-42)
    quote: A popular way to construct ontologies, and the way relevant to this paper,
      is by leveraging the W3C standards Web Ontology Language (OWL) [3] and Resource
      Description Framework (RDF) [4]
  - chunk_ref: 07-Classifying_Processes_Barry_Smith (Chunk 1:157-161)
    quote: and the (OWL) Web Ontology Language. Common Logic is an ISO Standard family
      of languages with an expressivity equivalent to that of first-order logic. OWL-DL
      is a fragment of the language of first order logic belonging to the family of
      what are called Description Logics.
  - chunk_ref: 17-KG_Reasoning (Chunk 1:44-48)
    quote: HermiT [Glimm et al., 2014] is a classic description logic reasoner for
      OWL ontologies; RDFox [Nenov et al., 2015] is a famous KG storage supporting
      Datalog rule reasoning
  description: Merged from 4 sources. OWL (Web Ontology Language) and specifically
    OWL-DL is a Description Logic-based standard for ontology formulation with restricted
    expressivity that enables automatic consistency checking through dedicated software
    reasoners.
- name: ShEx (Shape Expressions) Validation Language
  sources:
  - chunk_ref: 02-Knowledge_Graphs (Chunk 2:381-383)
    quote: 'Two shapes languages have recently emerged for RDF graphs: Shape Expressions
      (ShEx), published as a W3C Community Group Report [423]'
  description: ShEx is a W3C Community Group specification for validating RDF graphs
    using shape-based constraints, supporting recursive validation and constraint
    specification.
- name: SHACL (Shapes Constraint Language)
  sources:
  - chunk_ref: 02-Knowledge_Graphs (Chunk 2:383-385)
    quote: and SHACL (Shapes Constraint Language), published as a W3C Recommendation
      [296]
  description: SHACL is a W3C Recommendation for validating RDF graphs using shapes,
    supporting constraint specification, SPARQL-based validation, and integration
    with OWL semantics.
- name: UML-Style Shapes Graph Notation
  sources:
  - chunk_ref: 02-Knowledge_Graphs (Chunk 2:262-264)
    quote: Shapes graphs can be depicted as UML-like class diagrams, where Figure
      13 illustrates an example of a shapes graph based on Figure 1
  description: Shapes graphs for RDF validation can be depicted using UML-like class
    diagram notation, making constraint specifications more accessible to practitioners
    familiar with UML.
- name: Linked Data Web Integration
  sources:
  - chunk_ref: 02-Knowledge_Graphs (Chunk 2:613-615)
    quote: A prominent use-case for graph datasets is to manage and query Linked Data
      composed of interlinked documents of RDF graphs spanning the Web
  description: Linked Data is a method for publishing structured data on the Web using
    RDF, enabling interlinking between distributed knowledge graphs through shared
    URIs.
- name: XML Schema Datatypes (XSD)
  sources:
  - chunk_ref: 02-Knowledge_Graphs (Chunk 2:714-715)
    quote: RDF utilises XML Schema Datatypes (XSD) [411], amongst others, where a
      datatype node is given as a pair (l,d)
  description: XSD provides the datatype vocabulary for RDF literals, including string,
    integer, decimal, boolean, dateTime, and other standard datatypes with defined
    lexical representations.
- name: DOI (Digital Object Identifiers)
  sources:
  - chunk_ref: 02-Knowledge_Graphs (Chunk 2:565-566)
    quote: Prominent examples of PID schemes include Digital Object Identifiers (DOIs)
      for papers, ORCID iDs for authors
  description: DOI is a persistent identifier scheme used for scholarly papers and
    other digital objects, enabling stable references in knowledge graphs.
- name: PURL (Persistent URL) Services
  sources:
  - chunk_ref: 02-Knowledge_Graphs (Chunk 2:642-644)
    quote: Persistent URL (PURL) services offer redirects from a central server to
      a particular location, where the PURL can be redirected to a new location if
      necessary
  description: PURL services provide redirect mechanisms for HTTP IRIs, enabling persistent
    identification even when resource locations change.
- name: RDF Lists for Ordered Data
  sources:
  - chunk_ref: 02-Knowledge_Graphs (Chunk 2:800-801)
    quote: Existential nodes are supported in RDF as blank nodes [111], which are
      also commonly used to support modelling complex elements in graphs, such as
      RDF lists [111, 247]
  description: RDF lists provide linked-list structures using blank nodes to encode
    ordered sequences, essential for representing property chains, enumerations, and
    other ordered data.
- name: Time Ontology for Temporal Context
  sources:
  - chunk_ref: 02-Knowledge_Graphs (Chunk 3:110-112)
    quote: One example is the Time Ontology [107], which specifies how temporal entities,
      intervals, time instants, etc. and relations between them such as before, overlaps,
      etc. can be described in RDF
  description: The W3C Time Ontology provides a standard vocabulary for describing
    temporal entities and relations in RDF graphs, enabling interoperable temporal
    reasoning.
- name: PROV Data Model for Provenance
  sources:
  - chunk_ref: 02-Knowledge_Graphs (Chunk 3:114-118)
    quote: Another example is the PROV Data Model [188], which specifies how provenance
      can be described in RDF graphs, where entities are derived from other entities,
      are generated and/or used by activities
  - chunk_ref: 02-Knowledge_Graphs (Chunk 7:715-716)
    quote: the PROV Data Model [188] discussed in Section 3 allows for capturing detailed
      provenance
  description: Merged from 2 sources. PROV-DM is the W3C standard data model for provenance,
    describing how entities relate to activities and agents, enabling tracking of
    data lineage in knowledge graphs.
- name: RDF Reification Pattern
  sources:
  - chunk_ref: 02-Knowledge_Graphs (Chunk 3:37-38)
    quote: (a) RDF Reification
  description: RDF reification is a standard pattern for making statements about statements,
    using subject/predicate/object properties to describe edges, enabling metadata
    and context annotation.
- name: N-ary Relations Pattern
  sources:
  - chunk_ref: 02-Knowledge_Graphs (Chunk 3:27)
    quote: (b) n-ary Relations
  description: N-ary relations pattern connects source nodes directly to edge nodes
    with the original edge label, providing an alternative to RDF reification for
    representing complex relations.
- name: Singleton Properties Pattern
  sources:
  - chunk_ref: 02-Knowledge_Graphs (Chunk 3:41)
    quote: (c) Singleton properties
  description: Singleton properties use edge labels as individual nodes connected
    to their type via singleton relation, enabling edge-specific property assertions.
- name: RDF* Extension for Edge Metadata
  sources:
  - chunk_ref: 02-Knowledge_Graphs (Chunk 3:17-18)
    quote: 'Third, we can use RDF* [218] (Figure 19c): an extension of RDF that allows
      edges to be defined as nodes'
  description: RDF* is an RDF extension that allows edges to be treated as nodes directly,
    simplifying edge annotation without traditional reification overhead.
- name: Temporal RDF for Time Annotations
  sources:
  - chunk_ref: 02-Knowledge_Graphs (Chunk 3:111-113)
    quote: Temporal RDF [210] allows for annotating edges with time intervals, such
      as Chile [2006,2010] M. Bachelet
  description: Temporal RDF extends RDF with time interval annotations on edges, enabling
    temporal validity tracking and time-based reasoning.
- name: Fuzzy RDF for Uncertainty
  sources:
  - chunk_ref: 02-Knowledge_Graphs (Chunk 3:115-118)
    quote: Fuzzy RDF [502] allows for annotating edges with a degree of truth such
      as Santiago 0.8 Semi-Arid, indicating that it is more-or-less true with a degree
      of 0.8
  description: Fuzzy RDF extends RDF with fuzzy truth values on edges, enabling representation
    and reasoning with uncertain or vague knowledge.
- name: Annotated RDF with Semi-rings
  sources:
  - chunk_ref: 02-Knowledge_Graphs (Chunk 3:120-123)
    quote: 'Annotated RDF [130, 530, 583] allows for representing various forms of
      context modelled as semi-rings: algebraic structures consisting of domain values
      and two main operators to combine domain values: meet and join'
  description: Annotated RDF provides domain-independent context annotation using
    semi-ring algebraic structures, enabling formal reasoning about temporal, fuzzy,
    and other contextual annotations through meet and join operators.
- name: OWL 2 RL/RDF Rules
  sources:
  - chunk_ref: 02-Knowledge_Graphs (Chunk 3:944-945)
    quote: A more comprehensive set of rules for the OWL features of Tables 3-5 have
      been defined as OWL 2 RL/RDF [363]
  description: OWL 2 RL/RDF provides rule-based semantics for a subset of OWL features,
    enabling scalable reasoning through forward-chaining rules while remaining incomplete
    for some constructs.
- name: Datalog for Graph Rules
  sources:
  - chunk_ref: 02-Knowledge_Graphs (Chunk 3:934)
    quote: Rules of this form correspond to (positive) Datalog [85] in databases,
      Horn clauses [323] in logic programming
  description: Datalog provides a logical foundation for expressing inference rules
    over knowledge graphs, with extensions like Datalog+/- supporting existentials
    for more expressive ontological reasoning.
- name: Rete Networks for Materialization
  sources:
  - chunk_ref: 02-Knowledge_Graphs (Chunk 3:961)
    quote: the efficiency and scalability of materialisation can be enhanced through
      optimisations like Rete networks [164]
  description: Rete networks are pattern-matching algorithms that optimize rule execution
    during materialization, reducing redundant computation in knowledge graph reasoning.
- name: MapReduce for Distributed Reasoning
  sources:
  - chunk_ref: 02-Knowledge_Graphs (Chunk 3:961)
    quote: or using distributed frameworks like MapReduce [531]
  description: MapReduce provides a distributed computing framework for scaling knowledge
    graph reasoning and materialization across large clusters.
- name: OBOF (Open Biomedical Ontologies Format)
  sources:
  - chunk_ref: 02-Knowledge_Graphs (Chunk 3:353-356)
    quote: Amongst the most popular ontology languages used in practice are the Web
      Ontology Language (OWL) [239] [12], recommended by the W3C and compatible with
      RDF graphs; and the Open Biomedical Ontologies Format (OBOF) [366]
  description: OBOF is an ontology format widely used in the biomedical domain, providing
    many features similar to OWL for representing biomedical knowledge structures.
- name: OWL 2 RL/RDF Rules for Ontological Entailment
  sources:
  - chunk_ref: 02-Knowledge_Graphs (Chunk 4:45-49)
    quote: A more comprehensive set of rules for the OWL features of Tables 3-5 have
      been defined as OWL 2 RL/RDF; these rules are likewise incomplete
  description: OWL 2 RL/RDF is a standardized rule-based reasoning profile for OWL
    that enables practical reasoning over knowledge graphs. It provides rules for
    sub-class, sub-property, domain, and range features but cannot fully capture negation,
    existentials, universals, or counting features.
- name: Datalog for Rule-Based Reasoning
  sources:
  - chunk_ref: 02-Knowledge_Graphs (Chunk 4:35-36)
    quote: Rules of this form correspond to (positive) Datalog [85] in databases,
      Horn clauses [323] in logic programming, etc.
  description: Datalog is a foundational rule language used in databases that supports
    if-then-style inference rules with body and head patterns. Positive Datalog rules
    encode deductive knowledge and enable automated entailment checking.
- name: Datalog Extensions (Datalog+-)
  sources:
  - chunk_ref: 02-Knowledge_Graphs (Chunk 4:51-52)
    quote: Other rule languages have, however, been proposed to support additional
      such features, including existentials (see, e.g., Datalog [+-] [36]), disjunction
      (see, e.g., Disjunctive Datalog [449])
  description: Datalog+- and Disjunctive Datalog extend basic Datalog to support existential
    quantification and disjunction in rule heads, enabling more expressive reasoning
    capabilities beyond standard Datalog limitations.
- name: Horn Clauses in Logic Programming
  sources:
  - chunk_ref: 02-Knowledge_Graphs (Chunk 4:35)
    quote: Rules of this form correspond to (positive) Datalog [85] in databases,
      Horn clauses [323] in logic programming, etc.
  description: Horn clauses from logic programming provide a formal foundation for
    inference rules in knowledge graphs, enabling if-then-style deductive reasoning
    with graph patterns.
- name: First Order Logic Theorem Provers
  sources:
  - chunk_ref: 02-Knowledge_Graphs (Chunk 4:10-11)
    quote: Though option (3) has been explored using, e.g., theorem provers for First
      Order Logic [466], options (1) and (2) are more commonly pursued
  description: First Order Logic theorem provers can provide complete reasoning but
    may not halt on certain inputs. They offer an alternative to rules and Description
    Logics for ontological reasoning.
- name: Materialisation for Rule Reasoning
  sources:
  - chunk_ref: 02-Knowledge_Graphs (Chunk 4:54-58)
    quote: Materialisation refers to the idea of applying rules recursively to a graph,
      adding the conclusions generated back to the graph until a fixpoint is reached
  description: Materialisation is a reasoning strategy that recursively applies inference
    rules to a graph, adding derived conclusions until no new facts can be inferred.
    It is useful for pre-computing entailments for faster query answering.
- name: Rete Networks for Materialisation Optimization
  sources:
  - chunk_ref: 02-Knowledge_Graphs (Chunk 4:61-62)
    quote: the efficiency and scalability of materialisation can be enhanced through
      optimisations like Rete networks [164], or using distributed frameworks like
      MapReduce [531]
  description: Rete networks provide an optimization technique for materialisation-based
    reasoning, improving efficiency and scalability of rule application over knowledge
    graphs.
- name: Query Rewriting for Ontological Reasoning
  sources:
  - chunk_ref: 02-Knowledge_Graphs (Chunk 4:92-95)
    quote: Another strategy is to use rules for query rewriting, which given a query,
      will automatically extend the query in order to find solutions entailed by a
      set of rules
  description: Query rewriting is an alternative to materialisation that extends user
    queries at runtime to capture ontological entailments, avoiding the need to pre-compute
    and store all derived facts.
- name: OWL 2 QL Profile for Query Rewriting
  sources:
  - chunk_ref: 02-Knowledge_Graphs (Chunk 4:107-109)
    quote: The OWL 2 QL profile [363] is a subset of OWL designed specifically for
      query rewriting of this form [21].
  description: OWL 2 QL is a standardized OWL profile optimized for query rewriting,
    enabling efficient ontological reasoning over large datasets without materialisation.
- name: Tableau Methods for DL Satisfiability
  sources:
  - chunk_ref: 02-Knowledge_Graphs (Chunk 4:195-199)
    quote: Thereafter methods such as tableau can be used to check satisfiability,
      cautiously constructing models by completing them along similar lines to the
      materialisation strategy
  description: Tableau methods are decision procedures for Description Logic satisfiability
    checking, constructing models through completion rules and branching for disjunction.
- name: Notation3 (N3) Rule Language
  sources:
  - chunk_ref: 02-Knowledge_Graphs (Chunk 4:122)
    quote: 'Various languages allow for expressing rules over graphs... including:
      Notation3 (N3) [42], Rule Interchange Format (RIF) [288]'
  description: Notation3 (N3) is a language for expressing rules over graphs, enabling
    declarative specification of if-then inference rules independently or alongside
    ontology languages.
- name: Rule Interchange Format (RIF)
  sources:
  - chunk_ref: 02-Knowledge_Graphs (Chunk 4:122)
    quote: 'Various languages allow for expressing rules over graphs... including:
      Notation3 (N3) [42], Rule Interchange Format (RIF) [288]'
  - chunk_ref: 02-Knowledge_Graphs (Chunk 14:313-317)
    quote: rules can be directly specified in a rule language such as Notation3 (N3),
      Rule Interchange Format (RIF), Semantic Web Rule Language (SWRL)
  description: Merged from 2 sources. W3C standard for exchanging rules between rule
    systems. Part of the semantic web stack enabling interoperability of rule-based
    reasoning across different systems.
- name: Semantic Web Rule Language (SWRL)
  sources:
  - chunk_ref: 02-Knowledge_Graphs (Chunk 4:123-124)
    quote: Semantic Web Rule Language (SWRL) [254], and SPARQL Inferencing Notation
      (SPIN) [295].
  description: SWRL combines OWL with RuleML, providing a rule language that extends
    OWL's expressivity for knowledge graph reasoning.
- name: SPARQL Inferencing Notation (SPIN)
  sources:
  - chunk_ref: 02-Knowledge_Graphs (Chunk 4:124)
    quote: and SPARQL Inferencing Notation (SPIN) [295].
  - chunk_ref: 02-Knowledge_Graphs (Chunk 14:315-319)
    quote: SPARQL Inferencing Notation (SPIN) [295]. Languages such as SPIN represent
      rules as graphs, allowing the rules of a knowledge graph to be embedded in the
      data graph
  description: Merged from 2 sources. Standard for representing SPARQL-based rules
    within RDF graphs, enabling rules to be embedded as data and queried/manipulated
    like other graph content.
- name: Description Logics (DLs)
  sources:
  - chunk_ref: 02-Knowledge_Graphs (Chunk 4:127-134)
    quote: Description Logics (DLs) were initially introduced as a way to formalise
      the meaning of frames [355] and semantic networks [426]... DLs thus hold an
      important place in the logical formalisation of knowledge graphs
  description: Description Logics form a family of logics that provide the formal
    foundation for OWL, enabling decidable reasoning over knowledge graphs with expressivity/complexity
    trade-offs.
- name: OWL 2 DL Language Fragment
  sources:
  - chunk_ref: 02-Knowledge_Graphs (Chunk 4:179-181)
    quote: the OWL 2 DL language is a fragment of OWL restricted so that entailment
      becomes decidable
  description: OWL 2 DL is a restricted OWL fragment ensuring decidable reasoning
    while maintaining high expressivity, forming the basis for practical knowledge
    graph ontologies.
- name: Apache Spark GraphX
  sources:
  - chunk_ref: 02-Knowledge_Graphs (Chunk 4:498-499)
    quote: Various frameworks have been proposed for large-scale graph analytics,
      often in a distributed (cluster) setting. Amongst these we can mention Apache
      Spark (GraphX) [119, 563]
  description: Apache Spark GraphX is a distributed graph processing framework supporting
    large-scale analytics on knowledge graphs using a systolic abstraction for parallel
    computation.
- name: GraphLab
  sources:
  - chunk_ref: 02-Knowledge_Graphs (Chunk 4:499-500)
    quote: GraphLab [326], Pregel [335], Signal-Collect [503]
  description: GraphLab is a distributed machine learning framework for graph analytics
    supporting iterative vertex-centric computation on large-scale graphs.
- name: Pregel Graph Processing Framework
  sources:
  - chunk_ref: 02-Knowledge_Graphs (Chunk 4:500)
    quote: Pregel [335], Signal-Collect [503]
  description: Pregel is Google's graph processing framework implementing a Bulk Synchronous
    Parallel model for distributed graph analytics with message passing between vertices.
- name: Signal-Collect Framework
  sources:
  - chunk_ref: 02-Knowledge_Graphs (Chunk 4:500)
    quote: Signal-Collect [503]
  description: Signal-Collect is a distributed graph processing framework based on
    signal-collect abstraction where vertices signal neighboring vertices and collect
    incoming signals.
- name: Shark Distributed Analytics
  sources:
  - chunk_ref: 02-Knowledge_Graphs (Chunk 4:500)
    quote: Shark [564], etc.
  description: Shark is a distributed analytics system enabling SQL-like queries over
    large-scale graph data.
- name: MapReduce for Graph Materialisation
  sources:
  - chunk_ref: 02-Knowledge_Graphs (Chunk 4:62)
    quote: or using distributed frameworks like MapReduce [531]
  description: MapReduce can be used to scale materialisation-based reasoning, distributing
    rule application across a cluster for large knowledge graphs.
- name: Systolic Abstraction for Graph Computing
  sources:
  - chunk_ref: 02-Knowledge_Graphs (Chunk 4:501-504)
    quote: These graph parallel frameworks apply a systolic abstraction [304] based
      on a directed graph, where nodes are processors that can send messages to other
      nodes along edges
  description: The systolic abstraction provides the computational model underlying
    graph parallel frameworks, with nodes as processors and edges as communication
    channels for iterative computation.
- name: PageRank Algorithm
  sources:
  - chunk_ref: 02-Knowledge_Graphs (Chunk 4:519-522)
    quote: A good way to measure this is using centrality, where we choose PageRank
      [391], which computes the probability of a tourist randomly following the routes
      shown in the graph being at a particular place
  description: PageRank is a graph centrality algorithm computing node importance
    based on random walk probabilities, applicable for ranking entities in knowledge
    graphs.
- name: TransE Translational Embedding Model
  sources:
  - chunk_ref: 02-Knowledge_Graphs (Chunk 5:12-14)
    quote: The most elementary approach in this family is TransE [63]. Over all positive
      edges [s] p -, TransE learns vectors es, rp, and eo aiming to make es + rp as
      close as possible to eo
  description: TransE is a foundational translational embedding model that interprets
    edge labels as transformations translating subject entity embeddings to object
    entity embeddings in vector space.
- name: TransH Hyperplane Translation
  sources:
  - chunk_ref: 02-Knowledge_Graphs (Chunk 5:38-42)
    quote: TransH [553] represents different relations using distinct hyperplanes,
      where for the edge [s] p -, [s] is first projected onto the hyperplane of p
      before the translation to [o] is learnt
  description: TransH extends TransE by projecting entities onto relation-specific
    hyperplanes before translation, handling complex relation patterns better.
- name: TransR Relation-Specific Projection
  sources:
  - chunk_ref: 02-Knowledge_Graphs (Chunk 5:42-44)
    quote: TransR [318] generalises this approach by projecting [s] and [o] into a
      vector space specific to p, which involves multiplying the entity embeddings
      for [s] and [o] by a projection matrix specific to p
  description: TransR projects entities into relation-specific vector spaces using
    learned projection matrices, enabling distinct representations for different relation
    types.
- name: TransD Simplified Projection
  sources:
  - chunk_ref: 02-Knowledge_Graphs (Chunk 5:44-47)
    quote: TransD [271] simplifies TransR by associating entities and relations with
      a second vector, where these secondary vectors are used to project the entity
      into a relation-specific vector space
  description: TransD simplifies TransR by using secondary vectors for projection
    instead of full matrices, reducing parameters while maintaining expressivity.
- name: RotatE Complex Space Embeddings
  sources:
  - chunk_ref: 02-Knowledge_Graphs (Chunk 5:48-52)
    quote: RotatE [511] proposes translational embeddings in complex space, which
      allows to capture more characteristics of relations, such as direction, symmetry,
      inversion, antisymmetry, and composition
  description: RotatE models relations as rotations in complex vector space, capturing
    symmetric, antisymmetric, inverse, and compositional relation patterns.
- name: MuRP Hyperbolic Space Embeddings
  sources:
  - chunk_ref: 02-Knowledge_Graphs (Chunk 5:54-58)
    quote: MuRP [29] uses relation embeddings that transform entity embeddings in
      the hyperbolic space of the Poincare ball model, whose curvature provides more
      'space' to separate entities
  description: MuRP embeds knowledge graphs in hyperbolic space (Poincare ball model),
    providing better separation for hierarchical structures than Euclidean embeddings.
- name: DistMult Tensor Decomposition
  sources:
  - chunk_ref: 02-Knowledge_Graphs (Chunk 5:194-202)
    quote: DistMult [568] is a seminal method for computing knowledge graph embeddings
      based on rank decompositions, where each entity and relation is associated with
      a vector of dimension d
  description: DistMult applies tensor decomposition to knowledge graphs, learning
    entity and relation vectors that maximize plausibility scores through element-wise
    vector multiplication.
- name: RESCAL Matrix Relation Embeddings
  sources:
  - chunk_ref: 02-Knowledge_Graphs (Chunk 5:207-211)
    quote: RESCAL [386] uses a matrix, which allows for combining values from es and
      eo across all dimensions, and thus can capture (e.g.) edge direction
  description: RESCAL uses matrices rather than vectors for relation embeddings, enabling
    capture of edge direction and more complex relation patterns.
- name: HolE Circular Correlation
  sources:
  - chunk_ref: 02-Knowledge_Graphs (Chunk 5:211-216)
    quote: HolE [385] uses vectors for relation and entity embeddings, but proposes
      to use the circular correlation operator - which takes sums along the diagonals
      of the outer product of two vectors
  description: HolE uses circular correlation to combine entity embeddings, capturing
    asymmetric relations while maintaining vector-based efficiency.
- name: ComplEx Complex Vector Embeddings
  sources:
  - chunk_ref: 02-Knowledge_Graphs (Chunk 5:216-221)
    quote: ComplEx [526], on the other hand, uses a complex vector (i.e., a vector
      containing complex numbers) as a relational embedding, which similarly allows
      for breaking the aforementioned symmetry
  description: ComplEx uses complex-valued vectors for embeddings, breaking symmetry
    limitations of real-valued approaches while keeping parameters low.
- name: SimplE CP Decomposition
  sources:
  - chunk_ref: 02-Knowledge_Graphs (Chunk 5:222-225)
    quote: SimplE [283] rather proposes to compute a standard CP decomposition computing
      two initial vectors for entities from X and Z and then averaging terms
  description: SimplE applies standard Canonical Polyadic (CP) decomposition to knowledge
    graphs, computing averaged entity vectors from decomposed tensors.
- name: TuckER Tucker Decomposition
  sources:
  - chunk_ref: 02-Knowledge_Graphs (Chunk 5:225-237)
    quote: TuckER [30] employs a different type of decomposition - called a Tucker
      Decomposition [528], which computes a smaller 'core' tensor T and a sequence
      of three matrices A, B and C
  description: TuckER applies Tucker decomposition to knowledge graphs, achieving
    state-of-the-art results on standard benchmarks through a core tensor approach.
- name: Canonical Polyadic (CP) Decomposition
  sources:
  - chunk_ref: 02-Knowledge_Graphs (Chunk 5:103)
    quote: rank decomposition of tensors; this method is called Canonical Polyadic
      (CP) decomposition [236].
  description: CP decomposition decomposes a tensor into sum of rank-1 tensors, providing
    a foundational technique for tensor-based knowledge graph embeddings.
- name: Semantic Matching Energy (SME)
  sources:
  - chunk_ref: 02-Knowledge_Graphs (Chunk 5:248-251)
    quote: 'One of the earliest proposals of a neural model was Semantic Matching
      Energy (SME) [192], which learns parameters (aka weights: w, w'') for two functions'
  description: SME is an early neural model for knowledge graph embeddings using parameterized
    functions to compute plausibility scores via dot product.
- name: Neural Tensor Networks (NTN)
  sources:
  - chunk_ref: 02-Knowledge_Graphs (Chunk 5:251-257)
    quote: Neural Tensor Networks (NTN) [488], which rather proposes to maintain a
      tensor W of internal weights, such that the plausibility score is computed by
      a complex function
  description: NTN uses a weight tensor to compute plausibility scores through outer
    products of entity embeddings combined with neural layers, though limited in scalability.
- name: Multi Layer Perceptron (MLP) for KG
  sources:
  - chunk_ref: 02-Knowledge_Graphs (Chunk 5:257-258)
    quote: Multi Layer Perceptron (MLP) [131] is a simpler model, where es, rp and
      eo are concatenated and fed into a hidden layer to compute the plausibility
      score.
  description: MLP-based embedding models concatenate entity and relation vectors
    and pass through hidden layers for plausibility scoring, offering simpler architecture
    than NTN.
- name: ConvE Convolutional Embeddings
  sources:
  - chunk_ref: 02-Knowledge_Graphs (Chunk 5:260-268)
    quote: ConvE [127] proposes to generate a matrix from es and rp by 'wrapping'
      each vector over several rows and concatenating both matrices. The concatenated
      matrix serves as the input for a set of (2D) convolutional layers
  - chunk_ref: 02-Knowledge_Graphs (Chunk 5:272-280)
    quote: HypER [28] is a similar model using convolutions, but avoids the need to
      wrap vectors into matrices. Instead, a fully connected layer (called the 'hypernetwork')
      is applied to rp
  description: Merged from 2 sources. HypER uses a hypernetwork to generate relation-specific
    convolutional filters applied directly to entity embeddings, outperforming ConvE
    on benchmarks.
- name: Graph Neural Networks (GNNs)
  sources:
  - chunk_ref: 02-Knowledge_Graphs (Chunk 5:427-429)
    quote: A graph neural network (GNN) [462] builds a neural network based on the
      topology of the data graph; i.e., nodes are connected to their neighbours per
      the data graph
  description: GNNs build neural network architectures reflecting knowledge graph
    topology, enabling end-to-end supervised learning for node classification and
    graph-level tasks.
- name: Recursive Graph Neural Networks (RecGNNs)
  sources:
  - chunk_ref: 02-Knowledge_Graphs (Chunk 5:452-455)
    quote: Recursive graph neural networks (RecGNNs) are the seminal approach to graph
      neural networks [462, 493]. The approach is conceptually similar to the systolic
      abstraction
  description: RecGNNs recursively pass messages between neighboring nodes until fixpoint,
    learning transition and output functions from supervised node labels.
- name: Convolutional Graph Neural Networks (ConvGNNs)
  sources:
  - chunk_ref: 02-Knowledge_Graphs (Chunk 5:613-615)
    quote: a number of convolutional graph neural networks (ConvGNNs) [71, 289, 559]
      have been proposed, where the transition function is implemented by means of
      convolutions
  - chunk_ref: 02-Knowledge_Graphs (Chunk 15:502-506)
    quote: When the aggregation functions are based on a convolutional operator, we
      call the result a convolutional graph neural network (ConvGNN)
  description: Merged from 2 sources. Graph neural network architecture applying convolutional
    operators over graph neighborhoods. Supports multi-layer aggregation with learnable
    parameters for node classification and link prediction.
- name: Attention Mechanism for GNNs
  sources:
  - chunk_ref: 02-Knowledge_Graphs (Chunk 5:631-633)
    quote: An alternative is to use an attention mechanism [535] to learn the nodes
      whose features are most important to the current node.
  description: Attention mechanisms enable GNNs to learn which neighboring nodes are
    most relevant, addressing varying neighborhood sizes and structures.
- name: word2vec Language Embeddings
  sources:
  - chunk_ref: 02-Knowledge_Graphs (Chunk 5:296-301)
    quote: word2vec [352] and GloVe [408] being two seminal approaches. Both approaches
      compute embeddings for words based on large corpora of text such that words
      used in similar contexts have similar vectors
  description: word2vec computes word embeddings using neural networks (continuous
    bag of words or skip-gram), foundational for language-based graph embedding approaches.
- name: GloVe Word Embeddings
  sources:
  - chunk_ref: 02-Knowledge_Graphs (Chunk 5:306-308)
    quote: GloVe rather applies a regression model over a matrix of co-occurrence
      probabilities of word pairs.
  description: GloVe computes word embeddings via regression on word co-occurrence
    matrices, providing an alternative to neural approaches for embedding generation.
- name: RDF2Vec Graph-to-Language Embeddings
  sources:
  - chunk_ref: 02-Knowledge_Graphs (Chunk 5:316-328)
    quote: RDF2Vec [441] performs (biased [95]) random walks on the graph and records
      the paths (the sequence of nodes and edge labels traversed) as 'sentences',
      which are then fed as input into the word2vec model
  description: RDF2Vec generates sentences from random walks on knowledge graphs and
    applies word2vec, enabling transfer of language embedding techniques to graphs.
- name: KGloVe PageRank-Based Embeddings
  sources:
  - chunk_ref: 02-Knowledge_Graphs (Chunk 5:332-338)
    quote: KGloVe [96] is based on the GloVe model. Much like how the original GloVe
      model considers words that co-occur frequently in windows of text to be more
      related, KGloVe uses personalised PageRank
  description: KGloVe adapts GloVe for knowledge graphs using personalized PageRank
    to determine node relatedness instead of text co-occurrence.
- name: AMIE Rule Mining System
  sources:
  - chunk_ref: 02-Knowledge_Graphs (Chunk 5:773-775)
    quote: An influential rule-mining system for graphs is AMIE [169, 170], which
      adopts the PCA measure of confidence, and builds rules in a top-down fashion
  - chunk_ref: 17-KG_Reasoning (Chunk 1:444)
    quote: Conventional methods like AMIE [Galarraga et al., 2015] and AnyBURL [Meilicke
      et al., 2019] are symbolic-based
  description: Merged from 2 sources. AMIE is an influential rule mining system that
    discovers high-confidence rules from knowledge graphs using Partial Completeness
    Assumption and top-down rule refinement.
- name: AMIE+ Optimized Rule Mining
  sources:
  - chunk_ref: 02-Knowledge_Graphs (Chunk 5:827)
    quote: For further discussion of possible optimisations based on pruning and indexing,
      we refer to the paper on AMIE+ [169].
  description: AMIE+ provides optimizations over AMIE for more efficient rule mining
    through improved pruning and indexing strategies.
- name: RuLES Non-Monotonic Rule Learning
  sources:
  - chunk_ref: 02-Knowledge_Graphs (Chunk 5:845-848)
    quote: The RuLES system [241] - which is also capable of learning non-monotonic
      rules - proposes to mitigate the limitations of the PCA heuristic by extending
      the confidence measure
  description: RuLES learns non-monotonic rules and extends confidence measures using
    knowledge graph embedding plausibility scores for edges not in the graph.
- name: CARL Cardinality-Aware Rule Learning
  sources:
  - chunk_ref: 02-Knowledge_Graphs (Chunk 5:855-858)
    quote: CARL [406] exploits additional knowledge about the cardinalities of relations
      to refine the set of negative examples and the confidence measure for candidate
      rules.
  description: CARL enhances rule mining by incorporating relation cardinality constraints
    to better identify negative examples and compute rule confidence.
- name: NeuralLP Differentiable Rule Mining
  sources:
  - chunk_ref: 02-Knowledge_Graphs (Chunk 5:896-899)
    quote: NeuralLP [569] uses an attention mechanism to select a variable-length
      sequence of edge labels for path-like rules of the form [?x] p1 ?y1 p2 ... pn
      ?yn pn+1 ?z => [?x] p ?z
  description: NeuralLP applies differentiable rule mining using attention mechanisms
    to learn path-like rules with variable-length edge label sequences and confidence
    scores.
- name: DRUM Recurrent Neural Rule Mining
  sources:
  - chunk_ref: 02-Knowledge_Graphs (Chunk 5:901-908)
    quote: DRUM [455] also learns path-like rules, where... the system uses bidirectional
      recurrent neural networks (a popular technique for learning over sequential
      data) to learn sequences of relations for rules
  description: DRUM uses bidirectional RNNs for differentiable rule mining, learning
    relation sequences that form path-like rules.
- name: DL-Learner Axiom Mining
  sources:
  - chunk_ref: 02-Knowledge_Graphs (Chunk 6:59-61)
    quote: A prominent such system is DL-Learner [73], which is based on algorithms
      for class learning (aka concept learning), whereby given a set of positive nodes
      and negative nodes
  description: DL-Learner mines Description Logic axioms through class learning, finding
    logical class descriptions that separate positive from negative node examples.
- name: T-Norm Fuzzy Logics for Embeddings
  sources:
  - chunk_ref: 02-Knowledge_Graphs (Chunk 5:357-366)
    quote: KALE [207] computes entity and relation embeddings using a translational
      model (specifically TransE) that is adapted to further consider rules using
      t-norm fuzzy logics
  description: T-norm fuzzy logics provide a mechanism to integrate rules with embeddings,
    computing updated plausibility scores based on rule application.
- name: Named Entity Recognition (NER)
  sources:
  - chunk_ref: 02-Knowledge_Graphs (Chunk 6:205-209)
    quote: The NER task identifies mentions of named entities in a text [369, 434],
      typically targetting mentions of people, organisations, locations, and potentially
      other types
  description: NER identifies named entities in text for knowledge graph construction,
    using lexical features, gazetteers, and supervised/bootstrapping approaches.
- name: Entity Linking (EL)
  sources:
  - chunk_ref: 02-Knowledge_Graphs (Chunk 6:233-238)
    quote: The EL task associates mentions of entities in a text with the existing
      nodes of a target knowledge graph, which may be the nucleus of a knowledge graph
      under creation
  description: Entity Linking connects text mentions to knowledge graph nodes, handling
    mention ambiguity and entity aliases through disambiguation.
- name: Relation Extraction (RE)
  sources:
  - chunk_ref: 02-Knowledge_Graphs (Chunk 6:274-277)
    quote: The RE task extracts relations between entities in the text [24, 582].
      The simplest case is that of extracting binary relations in a closed setting
      wherein a fixed set of relation types are considered
  description: Relation Extraction identifies relations between entities in text for
    knowledge graph enrichment, supporting binary, n-ary, and open information extraction.
- name: Open Information Extraction (OIE)
  sources:
  - chunk_ref: 02-Knowledge_Graphs (Chunk 6:292-296)
    quote: Binary RE can also be applied using unsupervised methods in an open setting
      - often referred to as Open Information Extraction (OIE) [31, 149, 150, 341,
      342, 357]
  description: OIE extracts relations without predefined relation types, deriving
    relations from dependency parse trees and requiring subsequent alignment with
    knowledge graph vocabularies.
- name: Frame Semantics for N-ary RE
  sources:
  - chunk_ref: 02-Knowledge_Graphs (Chunk 6:306-314)
    quote: Various methods for n-ary RE are based on frame semantics [159], which,
      for a given verb... captures the entities involved and how they may be interrelated
  description: Frame semantics (FrameNet) provides semantic frames for verbs enabling
    n-ary relation extraction with contextual elements like time, place, and purpose.
- name: FrameNet Semantic Frames
  sources:
  - chunk_ref: 02-Knowledge_Graphs (Chunk 6:309-314)
    quote: Resources such as FrameNet [26] then define frames for words, such as to
      identify that the semantic frame for 'named' includes a speaker (the person
      naming something), an entity (the thing named) and a name
  description: FrameNet defines semantic frames associating verbs with participant
    roles and optional context elements for n-ary relation extraction.
- name: Discourse Representation Theory (DRT)
  sources:
  - chunk_ref: 02-Knowledge_Graphs (Chunk 6:316-324)
    quote: Other RE methods are rather based on Discourse Representation Theory (DRT)
      [278], which considers a logical representation of text based on existential
      events
  description: DRT provides logical text representation based on existential events,
    supporting neo-Davidsonian formulas analogous to reification for n-ary relations.
- name: WordNet Lexical Database
  sources:
  - chunk_ref: 02-Knowledge_Graphs (Chunk 6:197-198)
    quote: linking words with a lexicon of senses (e.g., WordNet [353] or BabelNet
      [373])
  - chunk_ref: 05-DOLCE (Chunk 1:459-460)
    quote: DOLCE, used to reorganize the WordNet top level and causing Princeton WordNet
      developers to include the individual/class distinction in their lexicon
  description: Merged from 2 sources. Large lexical database improved using DOLCE
    ontological analysis. Individual/class distinction added based on foundational
    ontology insights.
- name: BabelNet Multilingual Knowledge Graph
  sources:
  - chunk_ref: 02-Knowledge_Graphs (Chunk 6:198)
    quote: (e.g., WordNet [353] or BabelNet [373])
  description: BabelNet combines WordNet and Wikipedia for multilingual lexical knowledge,
    supporting word sense disambiguation and entity linking.
- name: R2RML RDB-to-RDF Mapping Language
  sources:
  - chunk_ref: 02-Knowledge_Graphs (Chunk 6:649-654)
    quote: A standard language along these lines is the RDB2RDF Mapping Language (R2RML)
      [118], which allows for mapping from individual rows of a table to one or more
      custom edges
  description: R2RML is a W3C standard for declarative mapping from relational databases
    to RDF graphs, supporting custom mappings with templates and SQL queries.
- name: Direct Mapping for Tables to RDF
  sources:
  - chunk_ref: 02-Knowledge_Graphs (Chunk 6:597-599)
    quote: A direct mapping automatically generates a graph from a table. We present
      in Figure 34 the result of a standard direct mapping [20]
  description: W3C Direct Mapping automatically converts relational tables to RDF
    graphs, creating edges from rows with table-encoded node identifiers.
- name: CSV/Tabular Data Mapping to RDF
  sources:
  - chunk_ref: 02-Knowledge_Graphs (Chunk 6:625-629)
    quote: Another direct mapping has been defined for CSV and other tabular data
      [516] that further allows for specifying column names, primary/foreign keys,
      and datatypes
  description: W3C standards extend direct mapping for CSV and tabular data, allowing
    specification of metadata like keys and datatypes as part of the mapping.
- name: GRDDL XML-to-RDF Mapping
  sources:
  - chunk_ref: 02-Knowledge_Graphs (Chunk 6:710-711)
    quote: the GRDDL standard [99] allows for mapping from XML to (RDF) graphs
  description: GRDDL is a W3C standard for extracting RDF graphs from XML documents
    using transformations.
- name: JSON-LD JSON-to-RDF Mapping
  sources:
  - chunk_ref: 02-Knowledge_Graphs (Chunk 6:711-712)
    quote: the JSON-LD standard [494] allows for mapping from JSON to (RDF) graphs
  description: JSON-LD is a W3C standard for embedding RDF in JSON, enabling JSON
    data to be interpreted as linked data graphs.
- name: XSPARQL Hybrid Query Language
  sources:
  - chunk_ref: 02-Knowledge_Graphs (Chunk 6:713-716)
    quote: hybrid query languages such as XSPARQL [47] allow for querying XML and
      RDF in an integrated fashion, thus supporting both materialisation and virtualisation
  description: XSPARQL integrates XQuery and SPARQL for querying XML and RDF together,
    supporting both materialisation and virtualisation approaches.
- name: Ontology-Based Data Access (OBDA)
  sources:
  - chunk_ref: 02-Knowledge_Graphs (Chunk 6:689-695)
    quote: The area of Ontology-Based Data Access (OBDA) [561] is then concerned with
      QR approaches that support ontological entailments
  description: OBDA provides query rewriting approaches that translate graph queries
    to SQL while supporting ontological entailments, including recursive entailments.
- name: Extract-Transform-Load (ETL)
  sources:
  - chunk_ref: 02-Knowledge_Graphs (Chunk 6:673-677)
    quote: Once the mappings have been defined, one option is to use them to materialise
      graph data following an Extract-Transform-Load (ETL) approach
  description: ETL processes materialise knowledge graphs by transforming and loading
    source data according to defined mappings.
- name: Shape Expressions for Validation
  sources:
  - chunk_ref: 02-Knowledge_Graphs (Chunk 7:179-181)
    quote: Validity means that the knowledge graph is free of constraint violations,
      such as captured by shape expressions [521] (see Section 3.1.2)
  description: Shape expressions provide constraint languages for validating knowledge
    graphs, defining expected patterns and cardinalities for node types.
- name: FAIR Principles for Data Publication
  sources:
  - chunk_ref: 02-Knowledge_Graphs (Chunk 7:621-629)
    quote: The FAIR Principles were originally proposed in the context of publishing
      scientific data [556]... FAIR itself is an acronym for four foundational principles
  description: FAIR (Findable, Accessible, Interoperable, Reusable) provides principles
    for publishing knowledge graphs to maximize re-use and machine-readability.
- name: Linked Data Principles
  sources:
  - chunk_ref: 02-Knowledge_Graphs (Chunk 7:731-738)
    quote: the Linked Data Principles, proposed by Berners-Lee [41], which provide
      a technical basis for one possible way in which these FAIR Principles can be
      achieved
  description: Linked Data Principles (use IRIs, HTTP lookups, standard formats, include
    links) provide technical implementation for FAIR publishing of knowledge graphs.
- name: Vocabulary of Interlinked Datasets (VoID)
  sources:
  - chunk_ref: 02-Knowledge_Graphs (Chunk 7:703-705)
    quote: resources such as the Vocabulary of Interlinked Datasets (VoID) [11] allow
      for representing meta-data about graphs
  description: VoID provides vocabulary for describing dataset metadata, supporting
    Findability principle of FAIR through structured dataset descriptions.
- name: Cypher Query Language
  sources:
  - chunk_ref: 02-Knowledge_Graphs (Chunk 4:686-688)
    quote: Query languages such as SPARQL [217], Cypher [165], and G-CORE [15] allow
      for outputting graphs, where such queries can be used to select sub-graphs for
      analysis
  description: Cypher is the declarative query language for property graphs (Neo4j),
    supporting graph pattern matching and shortest path finding.
- name: G-CORE Graph Query Language
  sources:
  - chunk_ref: 02-Knowledge_Graphs (Chunk 4:687)
    quote: Query languages such as SPARQL [217], Cypher [165], and G-CORE [15]
  description: G-CORE is a property graph query language supporting graph construction,
    composable queries, and paths as first-class citizens.
- name: Gremlin Traversal Language
  sources:
  - chunk_ref: 02-Knowledge_Graphs (Chunk 4:718)
    quote: While one could solve this task using an imperative language such as Gremlin
      [445], GraphX [563], or R [518]
  description: Gremlin is an imperative graph traversal language enabling programmatic
    navigation and manipulation of property graphs.
- name: Triple Pattern Fragments
  sources:
  - chunk_ref: 02-Knowledge_Graphs (Chunk 8:65-70)
    quote: Edge patterns - also known as triple patterns in the case of directed,
      edge-labelled graphs - are singleton graph patterns, i.e., graph patterns with
      a single edge
  description: Triple Pattern Fragments provide a lightweight access protocol returning
    solutions for single edge patterns, enabling client-side query processing.
- name: HTTP Dereferencing for Linked Data
  sources:
  - chunk_ref: 02-Knowledge_Graphs (Chunk 8:30-34)
    quote: Such a protocol is the basis for the Linked Data principles outlined previously,
      where node lookups are implemented through HTTP dereferencing
  description: HTTP dereferencing enables node lookups in Linked Data by returning
    RDF descriptions when HTTP IRIs are accessed.
- name: ODRL Rights Language
  sources:
  - chunk_ref: 02-Knowledge_Graphs (Chunk 8:188-192)
    quote: the W3C Open Digital Rights Language (ODRL) [261] provides an information
      model and related vocabularies that can be used to specify permissions, duties,
      and prohibitions
  description: ODRL is a W3C standard for expressing digital rights as graphs, supporting
    machine-readable licenses with permissions, duties, and prohibitions.
- name: WebAccessControl (WAC)
  sources:
  - chunk_ref: 02-Knowledge_Graphs (Chunk 8:217-218)
    quote: WebAccessControl (WAC) [31] is an access control framework for graphs that
      uses WebID for authentication
  description: WAC provides access control for graphs using WebID authentication and
    vocabulary for specifying access policies.
- name: DALICC License Clearance
  sources:
  - chunk_ref: 02-Knowledge_Graphs (Chunk 8:210-214)
    quote: the Data Licenses Clearance Center (DALICC), which includes a library of
      standard machine readable licenses, and tools that enable users both to compose
      arbitrary custom licenses and also to verify the compatibility
  description: DALICC provides tools for license composition and compatibility verification,
    supporting Apache, Creative Commons, and BSD license families.
- name: CryptOntology for Encryption Metadata
  sources:
  - chunk_ref: 02-Knowledge_Graphs (Chunk 8:282-286)
    quote: The CryptOntology [182] can further be used to embed details about the
      encryption mechanism used within the knowledge graph
  description: CryptOntology provides vocabulary for embedding encryption metadata
    (algorithm, key-length) within knowledge graphs for fine-grained access control.
- name: K-Anonymity for Graphs
  sources:
  - chunk_ref: 02-Knowledge_Graphs (Chunk 8:324-328)
    quote: Approaches to apply k-anonymity on graphs identify and suppress 'quasi-identifiers'
      that would allow a given individual to be distinguished from fewer than k-1
      other individuals
  description: K-anonymity suppresses quasi-identifiers in graphs to ensure individuals
    cannot be distinguished from k-1 others, protecting privacy.
- name: K-Degree Anonymity
  sources:
  - chunk_ref: 02-Knowledge_Graphs (Chunk 8:354-358)
    quote: k-degree anonymity [321], which ensures that individuals cannot be deanonymised
      by attackers with knowledge of the degree of particular individuals
  description: K-degree anonymity modifies graphs so each node has at least k-1 other
    nodes with the same degree, preventing degree-based deanonymisation.
- name: K-Isomorphic Neighbour Anonymity
  sources:
  - chunk_ref: 02-Knowledge_Graphs (Chunk 8:360-367)
    quote: k-isomorphic neighbour anonymity [580], avoids neighbourhood attacks where
      an attacker knows how an individual is connected to nodes in their neighbourhood
  description: K-isomorphic neighbour anonymity ensures each node has k-1 other nodes
    with isomorphic neighborhoods, preventing neighborhood-based attacks.
- name: K-Automorphism for Graph Anonymity
  sources:
  - chunk_ref: 02-Knowledge_Graphs (Chunk 8:370-374)
    quote: k-automorphism [584], which ensures that for every node, it is structurally
      indistinguishable from k-1 other nodes, thus avoiding any attack based on structural
      information
  description: K-automorphism provides strongest structural anonymity guarantee, making
    every node structurally indistinguishable from k-1 others.
- name: Differential Privacy for Graph Queries
  sources:
  - chunk_ref: 02-Knowledge_Graphs (Chunk 8:395-400)
    quote: One approach is to apply epsilon-differential privacy [137] for querying
      graphs [483]. Such mechanisms are typically used for aggregate (e.g., count)
      queries, where noise is added
  description: Epsilon-differential privacy adds noise to query results to prevent
    individual identification, controlling privacy-utility tradeoff via epsilon parameter.
- name: DBpedia Knowledge Graph
  sources:
  - chunk_ref: 02-Knowledge_Graphs (Chunk 8:458-476)
    quote: The DBpedia project was developed to extract a graph-structured representation
      of the semi-structured data embedded in Wikipedia articles [22]
  - chunk_ref: 05-DOLCE (Chunk 1:457-458)
    quote: identifying and fixing millions of inconsistencies in DBpedia, on-the-go
      discovering modelling anti-patterns that were completely opaque to the axioms
      of the DBpedia ontology
  description: Merged from 2 sources. Structured knowledge graph extracted from Wikipedia.
    DOLCE+DUL used to detect and fix ontological inconsistencies and discover modeling
    anti-patterns.
- name: YAGO Knowledge Graph
  sources:
  - chunk_ref: 02-Knowledge_Graphs (Chunk 8:505-529)
    quote: YAGO likewise extracts graph-structured data from Wikipedia, which are
      then unified with the hierarchical structure of WordNet
  description: YAGO combines Wikipedia extraction with WordNet hierarchies, supporting
    reification, n-ary relations, and data types in its RDFS-based model.
- name: Freebase Knowledge Graph
  sources:
  - chunk_ref: 02-Knowledge_Graphs (Chunk 8:544-570)
    quote: Freebase was a general collection of human knowledge that aimed to address
      some of the large scale information integration problems
  - chunk_ref: 02-Knowledge_Graphs (Chunk 8:579-621)
    quote: The Wikimedia Foundation thus uses Wikidata as a centralised, collaboratively
      edited knowledge graph to supply Wikipedia
  - chunk_ref: 16-KG-Agent (Chunk 1:235)
    quote: reasoning over KG (e.g., Freebase or Wikidata) typically requires three
      fundamental operations
  - chunk_ref: 16-KG-Agent (Chunk 1:235)
    quote: reasoning over KG (e.g., Freebase or Wikidata) typically requires three
      fundamental operations
  description: Merged from 4 sources. Freebase is referenced as one of the major knowledge
    graph implementations used for KG reasoning benchmarks and research. It serves
    as a standard dataset for evaluating KGQA methods.
- name: SKOS Knowledge Organization
  sources:
  - chunk_ref: 02-Knowledge_Graphs (Chunk 8:493-495)
    quote: These schemata include a Simple Knowledge Organization System (SKOS) representation
      of Wikipedia categories
  description: SKOS provides vocabulary for representing knowledge organization systems
    like taxonomies and thesauri within knowledge graphs.
- name: UMBEL Ontology Categorisation
  sources:
  - chunk_ref: 02-Knowledge_Graphs (Chunk 8:496-497)
    quote: an Upper Mapping and Binding Exchange Layer (UMBEL) ontology categorisation
      schema
  description: UMBEL provides upper-level ontology categories for classifying entities
    in knowledge graphs like DBpedia.
- name: SHACL Shapes Validation
  sources:
  - chunk_ref: 02-Knowledge_Graphs (Chunk 14:27-39)
    quote: The semantics we present here assumes that each node in the graph either
      satisfies or does not satisfy each shape labelled by the schema
  description: SHACL (Shapes Constraint Language) for validating knowledge graphs
    against shapes schemas. Supports partial shapes maps with three-valued logic,
    path counting constraints, and SAT solver-based validation for recursive cases.
- name: Kleene Three-Valued Logic for Shapes
  sources:
  - chunk_ref: 02-Knowledge_Graphs (Chunk 14:29-31)
    quote: More complex semantics - for example, based on Kleene's three-valued logic
      [104, 305] - have been proposed that support partial shapes maps
  description: Extension of shapes validation semantics using three-valued logic to
    support partial shapes maps where satisfaction of some nodes for some shapes can
    be left undefined.
- name: Quotient Graph Emergent Schema
  sources:
  - chunk_ref: 02-Knowledge_Graphs (Chunk 14:42-74)
    quote: Emergent schemata are often based on the notion of a quotient graph...
      a quotient graph can merge multiple nodes into one node
  description: Emergent schema discovery through quotient graphs that partition nodes
    based on equivalence relations (e.g., same types) while preserving edge topology
    through simulation and bisimulation relations.
- name: Annotation Domain Semi-Ring
  sources:
  - chunk_ref: 02-Knowledge_Graphs (Chunk 14:113-158)
    quote: An annotation domain is defined as an idempotent, commutative semi-ring
      D = <A, plus, times, bottom, top>
  description: Formal algebraic structure for graph annotations supporting context
    metadata. Enables reasoning and querying over annotated graphs with well-defined
    operations for temporal, trust, or other domain-specific contexts.
- name: Graph Interpretation Semantics
  sources:
  - chunk_ref: 02-Knowledge_Graphs (Chunk 14:186-198)
    quote: A (graph) interpretation I is defined as a pair I := (Gamma, dot_I) where
      Gamma = (V_Gamma, E_Gamma, L_Gamma) is a graph called the domain graph
  description: Formal model-theoretic semantics for knowledge graphs supporting both
    Unique Name Assumption (UNA) and no-UNA interpretations, enabling formal reasoning
    about graph entailment.
- name: SWRL Semantic Web Rule Language
  sources:
  - chunk_ref: 02-Knowledge_Graphs (Chunk 14:315)
    quote: Semantic Web Rule Language (SWRL) [254]
  description: Rule language combining OWL with RuleML to express Horn-like rules
    for web ontologies. Implemented by semantic reasoners like HermiT for automated
    inference.
- name: Description Logic Knowledge Base
  sources:
  - chunk_ref: 02-Knowledge_Graphs (Chunk 14:335-344)
    quote: 'A DL knowledge base K is defined as a tuple (A, T, R), where A is the
      A-Box: a set of assertional axioms; T is the T-Box: a set of class axioms; and
      R is the R-Box: a set of relation axioms'
  description: 'Standard structure for Description Logic ontologies with three components:
    A-Box (instance data), T-Box (terminological/class definitions), R-Box (role/relation
    definitions). Forms the basis for OWL reasoning.'
- name: OWL 2 DL SROIQ
  sources:
  - chunk_ref: 02-Knowledge_Graphs (Chunk 14:467-469)
    quote: DLs have been very influential in the definition of OWL, where the OWL
      2 DL fragment (roughly) corresponds to the DL SROIQ
  description: OWL 2 DL profile corresponds to the SROIQ description logic, providing
    decidable reasoning with features including transitive closure, complex role inclusions,
    nominals, inverse relations, and qualified number restrictions.
- name: ALC Description Logic Base
  sources:
  - chunk_ref: 02-Knowledge_Graphs (Chunk 14:426-430)
    quote: ALC (Attributive Language with Complement), supports atomic classes, the
      top and bottom classes, class intersection, class union, class negation, universal
      restrictions and existential restrictions
  description: Foundation description logic providing core expressivity for ontology
    languages. Extended by S (transitivity), H (role inclusion), R (complex roles),
    O (nominals), I (inverse), and N/Q (number restrictions).
- name: Knowledge Graph Embeddings
  sources:
  - chunk_ref: 02-Knowledge_Graphs (Chunk 15:265-282)
    quote: Other embedding techniques - namely RotatE [511] and ComplEx [526] - uses
      complex space based on complex numbers
  description: Neural representation methods for knowledge graphs including TransE,
    RotatE, ComplEx, DistMult, SimplE, TuckER. Support plausibility scoring for link
    prediction with varying trade-offs between parameters and expressiveness.
- name: Graph Parallel Framework (GPF)
  sources:
  - chunk_ref: 02-Knowledge_Graphs (Chunk 15:458-472)
    quote: The key difference between GPFs and GNNs is that in the former, the functions
      are defined by the user, while in the latter, the functions are generally learnt
      from labelled examples
  description: Computational framework for parallel graph processing with user-defined
    message passing, aggregation, and termination functions. Foundation for algorithms
    like PageRank on distributed systems.
- name: Symbolic Learning Rule Mining
  sources:
  - chunk_ref: 02-Knowledge_Graphs (Chunk 15:656-682)
    quote: Systems such as AMIE [170], RuLES [241], CARL [406], DL-Learner [73], etc.,
      propose discrete mining that recursively generates candidate formulae
  description: Rule and axiom mining systems for knowledge graphs. Includes discrete
    mining (AMIE, RuLES, CARL) using refinement operators and differentiable mining
    (NeuralLP, DRUM) using gradient descent for path-like rules.
- name: SSSOM Mapping Standard
  sources:
  - chunk_ref: 02-Knowledge_Graphs (Chunk 15:not applicable - referenced in 04 chunk)
    quote: N/A
  description: Simple Standard for Sharing Ontology Mappings provides vocabulary and
    format for encoding ontology alignments with provenance metadata.
- name: W3C PROV Standard
  sources:
  - chunk_ref: 03-PROV-AGENT (Chunk 1:117-121)
    quote: PROV-AGENT, a provenance model that extends the W3C PROV [7] standard and
      incorporates concepts from the Model Context Protocol (MCP)
  description: 'W3C recommended standard for provenance representation with three
    core classes: Entity, Activity, Agent. Extended by PROV-AGENT for AI agent workflow
    provenance tracking.'
- name: Model Context Protocol (MCP)
  sources:
  - chunk_ref: 03-PROV-AGENT (Chunk 1:144-150)
    quote: MCP defines core agentic AI development concepts, including tools, prompts,
      resources, context management, and agent-client architecture that can communicate
      with external sources
  description: Emerging standard for AI agent development defining concepts for tools,
    prompts, resources, context management, and RAG integration. Supported by frameworks
    like LangChain, AutoGen, LangGraph, CrewAI.
- name: Flowcept Provenance Framework
  sources:
  - chunk_ref: 03-PROV-AGENT (Chunk 1:321-335)
    quote: we extend Flowcept [9], an open-source distributed provenance framework
      designed for complex, heterogeneous workflows spanning experimental facilities
      at the edge, cloud platforms, and HPC environments
  description: Open-source distributed provenance framework using federated broker-based
    model for streaming provenance data from heterogeneous sources (Dask, MLflow,
    Redis, Kafka) with W3C PROV-extended model.
- name: Python Decorator Instrumentation
  sources:
  - chunk_ref: 03-PROV-AGENT (Chunk 1:343-351)
    quote: applying the @flowcept_task decorator ensures that, upon execution, the
      function's inputs, outputs, and any generated telemetry or scheduling data are
      automatically captured
  description: Decorator-based instrumentation pattern for capturing agent tool execution
    provenance. @flowcept_agent_tool creates AgentTool activities linked to executing
    agent and PROV relationships.
- name: FlowceptLLM Wrapper
  sources:
  - chunk_ref: 03-PROV-AGENT (Chunk 1:358-364)
    quote: a generic wrapper for abstract LLM objects, compatible with models from
      popular LLM interfaces, including CrewAI, LangChain, and OpenAI
  description: Generic wrapper for capturing LLM invocation provenance including prompts,
    responses, model metadata (provider, name, temperature), and telemetry. Compatible
    with major LLM frameworks.
- name: AutoGen Multi-Agent Framework
  sources:
  - chunk_ref: 03-PROV-AGENT (Chunk 1:141)
    quote: AutoGen [12]
  - chunk_ref: 15-SciAgents (Chunk 2:483-484)
    quote: We design AI agents using the general-purpose LLM GPT-4 family models.
      The automated multi-agent collaboration is implemented in the AutoGen framework
  description: Merged from 2 sources. AutoGen is used as the core framework for implementing
    automated multi-agent collaboration. It provides an open-source ecosystem for
    agent-based AI modeling, enabling the construction of complex multi-agent systems
    with specialized roles.
- name: RAG Retrieval-Augmented Generation
  sources:
  - chunk_ref: 03-PROV-AGENT (Chunk 1:149-150)
    quote: Retrieval-Augmented Generation (RAG) [15] to dynamically augment prompts
  description: Pattern for enhancing LLM prompts with retrieved contextual knowledge
    from external sources like knowledge bases or web pages.
- name: ProvONE Workflow Extension
  sources:
  - chunk_ref: 03-PROV-AGENT (Chunk 1:214)
    quote: ProvONE [21] adds workflow-specific metadata and aims at supporting existing
      workflow management systems
  description: PROV extension adding workflow-specific metadata for provenance capture
    in scientific workflow management systems.
- name: PROV-ML Machine Learning Extension
  sources:
  - chunk_ref: 03-PROV-AGENT (Chunk 1:239-241)
    quote: PROV-ML [22] combines general workflow concepts with ML-specific artifacts,
      especially for model training and evaluation
  description: PROV extension for machine learning workflows capturing model training
    and evaluation provenance.
- name: FAIR4ML Metadata Schema
  sources:
  - chunk_ref: 03-PROV-AGENT (Chunk 1:241-242)
    quote: FAIR4ML [23] adopts a model-centric approach to support the Findability,
      Accessibility, Interoperability, and Reproducibility (FAIR) principles
  description: Metadata schema for ML models supporting FAIR principles for model
    findability, accessibility, interoperability, and reproducibility.
- name: BFO Basic Formal Ontology ISO Standard
  sources:
  - chunk_ref: 04-PROV-O_to_BFO (Chunk 1:78)
    quote: Basic Formal Ontology (BFO) is a top-level ontology ISO standard [10] used
      to provide foundational classes to structure different domain ontologies
  description: ISO 21838-2:2020 standard top-level ontology providing foundational
    classes (continuant, occurrent) for domain ontology development and semantic interoperability.
- name: OBO Foundry Relations Ontology (RO)
  sources:
  - chunk_ref: 04-PROV-O_to_BFO (Chunk 1:80-81)
    quote: The OBO Relations Ontology (RO) is an extension of BFO developed by Open
      Biomedical Ontologies (OBO) Foundry [13, 14, 15] to standardize relations between
      domain ontologies
  description: Extension of BFO standardizing relations for OBO Foundry ontologies.
    Provides common relation vocabulary for cross-domain interoperability.
- name: Common Core Ontologies (CCO)
  sources:
  - chunk_ref: 04-PROV-O_to_BFO (Chunk 1:82-84)
    quote: The Common Core Ontologies (CCO) are a suite of mid-level ontologies, used
      to span across different domain ontologies and intended to bridge the gap between
      domain ontologies and BFO
  description: Suite of mid-level ontologies bridging BFO and domain ontologies. Provides
    Information Content Entity, Agent, Plan, and other commonly needed classes.
- name: OWL equivalentClass Mapping
  sources:
  - chunk_ref: 04-PROV-O_to_BFO (Chunk 1:146-155)
    quote: Equivalence relations represented by OWL equivalentClass and OWL equivalentProperty
      give necessary and sufficient conditions for something to be an instance of
      a certain BFO class
  description: OWL construct for bidirectional ontology mapping providing necessary
    and sufficient conditions for class membership. Enables automated reasoning across
    aligned ontologies.
- name: RDFS subClassOf Subsumption
  sources:
  - chunk_ref: 04-PROV-O_to_BFO (Chunk 1:158-164)
    quote: Subsumption relations represented by RDFS subClassOf or RDFS subPropertyOf
      give sufficient conditions for an instance of one class or relation to be an
      instance of another
  description: RDFS construct for one-way ontology mapping establishing sufficient
    conditions. Allows non-injective alignments where multiple terms map to a single
    term.
- name: SWRL Rules for Complex Mappings
  sources:
  - chunk_ref: 04-PROV-O_to_BFO (Chunk 1:168-183)
    quote: SWRL rules [21] are especially useful for restricting the domain or range
      of an OWL object property in order to use it in a valid mapping. An advantage
      of SWRL is that it is implemented by semantic reasoners such as HermiT
  description: Semantic Web Rule Language for expressing complex ontology mappings
    with domain/range restrictions. Implemented by HermiT reasoner for automated inference.
- name: SKOS Vocabulary for Mappings
  sources:
  - chunk_ref: 04-PROV-O_to_BFO (Chunk 1:186-195)
    quote: Mapping predicates from the SKOS vocabulary [25] represent informal relations
      between terms which may be interpreted by users to be intuitively similar
  description: Simple Knowledge Organization System vocabulary for informal ontology
    mappings. SKOS relatedMatch provides symmetric, non-transitive associations without
    strong inferential semantics.
- name: SSSOM Simple Standard for Sharing Ontology Mappings
  sources:
  - chunk_ref: 04-PROV-O_to_BFO (Chunk 1:188-189)
    quote: Simple Standard for Sharing Ontology Mappings (SSSOM) vocabulary [7]
  description: Standard for encoding ontology mappings with provenance metadata. Supports
    annotations like subject_label, object_label, mapping_justification from SEMAPV
    vocabulary.
- name: HermiT OWL Reasoner
  sources:
  - chunk_ref: 04-PROV-O_to_BFO (Chunk 1:179-180)
    quote: An advantage of SWRL is that it is implemented by semantic reasoners such
      as HermiT [22, 23]
  description: OWL 2 DL reasoner using hypertableau calculus for satisfiability and
    consistency checking. Used for validating ontology alignments and detecting inconsistencies.
- name: RDF Turtle Serialization
  sources:
  - chunk_ref: 04-PROV-O_to_BFO (Chunk 1:426)
    quote: every canonical example instance from the W3C documentation for PROV-O
      and its extensions was copied into RDF files serialized in the Terse Triple
      Language (TTL or 'RDF Turtle')
  description: Terse RDF Triple Language (Turtle) for human-readable RDF serialization.
    Used for ontology files and alignment encoding with reified OWL axioms.
- name: ROBOT Ontology Tool
  sources:
  - chunk_ref: 04-PROV-O_to_BFO (Chunk 1:419)
    quote: HermiT reasoner [22, 23] and ROBOT command-line tool [34] before running
      the query
  description: Command-line tool for ontology engineering workflows. Supports SPARQL
    queries, reasoning, diff operations, and integration with GNU Make for automated
    pipelines.
- name: Protege Ontology Editor
  sources:
  - chunk_ref: 04-PROV-O_to_BFO (Chunk 1:571)
    quote: a separate RDF Turtle file imports all three of the alignment files, along
      with the BFO, RO, and CCO ontologies, for viewing the alignments in context
      in Protege
  description: Open-source ontology editor for OWL. Used for viewing, editing, and
    testing ontology alignments with reasoner integration.
- name: GitHub Actions CI/CD Pipeline
  sources:
  - chunk_ref: 04-PROV-O_to_BFO (Chunk 1:473-474)
    quote: These Makefile tasks are run within a continuous development pipeline using
      GitHub Actions. The result is that changes to the alignments can be automatically,
      rigorously tested when committed
  description: Automated testing pipeline for ontology alignments using GitHub Actions
    with ROBOT, HermiT, and SPARQL queries for continuous validation of coherence,
    consistency, and conservativity.
- name: GNU Make for Ontology Workflows
  sources:
  - chunk_ref: 04-PROV-O_to_BFO (Chunk 1:470-471)
    quote: an ontology engineering pipeline using ROBOT and GNU Make
  description: Build automation tool for composing ROBOT commands and SPARQL queries
    into reproducible ontology engineering workflows.
- name: OWL Property Chain Axioms
  sources:
  - chunk_ref: 04-PROV-O_to_BFO (Chunk 1:182-183)
    quote: If a set of relations in one ontology should imply a relation in another
      ontology, OWL property chain axioms can be used to axiomatize this complex subsumption
      relation between object properties
  description: OWL construct for expressing complex role compositions (e.g., hasPart
    o locatedIn implies locatedIn). Used for mapping relations that require composition.
- name: SOSA/SSN Sensor Ontology
  sources:
  - chunk_ref: 04-PROV-O_to_BFO (Chunk 1:440-444)
    quote: The alignments were further tested for consistency with an alignment between
      PROV-O and the SOSA (Sensor, Observation, Sample, and Actuator) ontology [36],
      which is a core subset of the Semantic Sensor Network Ontology (SSN) [37]
  description: W3C recommendation ontology for sensors, observations, samples, and
    actuators. SOSA is lightweight core; SSN provides fuller axiomatization aligned
    with PROV-O.
- name: OWL 2 RL Profile
  sources:
  - chunk_ref: 04-PROV-O_to_BFO (Chunk 2:200-206)
    quote: PROV-O and its extensions conform to OWL 2 RL, corresponding to the description
      logic DLP, with the exception of some axioms that conform to OWL 2 DL
  description: OWL 2 profile for rule-based reasoning corresponding to description
    logic DLP. Enables efficient implementation in rule engines while supporting key
    ontology features.
- name: Zenodo Data Archive
  sources:
  - chunk_ref: 04-PROV-O_to_BFO (Chunk 2:344-346)
    quote: The alignment files [18] for each ontology and all related project resources
      are archived at https://doi.org/10.5281/zenodo.14692262
  description: Open-access repository for research data and software archiving with
    persistent DOI identifiers. Used for FAIR-compliant publication of ontology alignments.
- name: First-Order Modal Logic QS5
  sources:
  - chunk_ref: 05-DOLCE (Chunk 1:227-229)
    quote: The formal theory of DOLCE is written in the first-order quantified modal
      logic QS5, including the Barcan and the converse Barcan formula
  description: Expressive logic for foundational ontology axiomatization supporting
    possibilistic view of entities. Trade-off between expressiveness and computability
    addressed through OWL approximations.
- name: DOLCE-Lite OWL Versions
  sources:
  - chunk_ref: 05-DOLCE (Chunk 1:69-70)
    quote: several application-oriented, 'lite' versions were later published, including
      DOLCE-lite, DOLCE-ultralite, and DOLCE-zero
  description: Simplified OWL versions of DOLCE for semantic web applications. DOLCE-ultralite
    (DUL) widely adopted for knowledge graphs, multimedia annotation, robotics, process
    mining.
- name: Common Logic CLIF Syntax
  sources:
  - chunk_ref: 05-DOLCE (Chunk 1:110-111)
    quote: Today DOLCE is becoming part of the ISO 21838 standard, under development,
      and is available also in CLIF, a syntax of Common Logic ISO 24707
  description: ISO 24707 Common Logic Interchange Format for expressing DOLCE axioms.
    Enables formal verification with tools like Mace4 for consistency proofs.
- name: General Extensional Mereology (GEM)
  sources:
  - chunk_ref: 05-DOLCE (Chunk 1:264)
    quote: The first follows the principles of the General Extensional Mereology (GEM)
  description: Formal theory of parthood relations for atemporal mereology in DOLCE.
    Temporary parthood drops antisymmetry for time-indexed part relations.
- name: Conceptual Spaces (Gardenfors)
  sources:
  - chunk_ref: 05-DOLCE (Chunk 1:197-198)
    quote: Quality spaces in DOLCE are based on Gardenfors' conceptual spaces (Gardenfors,
      2000)
  description: Geometric framework for representing qualities and properties. Used
    in DOLCE for quality spaces where qualia represent positions enabling property
    comparison.
- name: CIDOC CRM Cultural Heritage
  sources:
  - chunk_ref: 05-DOLCE (Chunk 1:463)
    quote: CIDOC CRM (CIDOC Conceptual Reference Model)
  description: ISO 21127 standard ontology for cultural heritage documentation. Compatible
    with DOLCE/DUL for semantic integration of museum and heritage data.
- name: SSN Semantic Sensor Network
  sources:
  - chunk_ref: 05-DOLCE (Chunk 1:463)
    quote: SSN (Semantic Sensor Network Ontology)
  description: W3C recommendation for sensor data and observations. Based on or compatible
    with DOLCE+DUL patterns.
- name: SAREF Smart Appliances Ontology
  sources:
  - chunk_ref: 05-DOLCE (Chunk 1:465)
    quote: SAREF (Smart REFerence Ontology)
  description: ETSI standard ontology for smart appliances and IoT. Compatible with
    DOLCE/DUL for semantic interoperability in smart environments.
- name: Framester Knowledge Graph
  sources:
  - chunk_ref: 05-DOLCE (Chunk 1:461-462)
    quote: the recent massive Framester knowledge graph (Gangemi et al., 2016), which
      unifies many different linguistic databases under a frame semantics, and maps
      them to widely used ontologies under a common DUL hat
  description: Large-scale knowledge graph unifying linguistic databases (FrameNet,
    VerbNet, PropBank, WordNet) under frame semantics with DUL alignment.
- name: OWL2 Punning
  sources:
  - chunk_ref: 05-DOLCE (Chunk 2:429-431)
    quote: later much facilitated by punning in OWL2 W3C OWL Working Group (2012)
      (i.e. the ability to use a constant as the name for a class, an individual,
      or a binary relation)
  description: OWL 2 feature allowing the same IRI to denote both a class and an individual.
    Enables the D&S pattern for reifying concepts as first-class entities in DOLCE
    extensions.
- name: Description and Situations (D&S) Pattern
  sources:
  - chunk_ref: 05-DOLCE (Chunk 2:428-429)
    quote: They also address the need for some extensions of DOLCE categories, by
      reusing the D&S (Description and Situations) ontology pattern framework
  description: Ontology design pattern for overcoming OWL expressivity limits. Enables
    modeling of contexts, plans, norms, and social aspects as first-class entities.
- name: Mace4 Model Finder
  sources:
  - chunk_ref: 05-DOLCE (Chunk 1:256-258)
    quote: A CLIF version of DOLCE plus the theory of concepts and roles from (Masolo
      et al., 2004) is formalized and proved consistent by means of Mace4
  description: Automated model finder for first-order logic. Used to prove consistency
    of DOLCE axiomatization in CLIF format.
- name: OBO Foundry Ontology Framework
  sources:
  - chunk_ref: 06-BFO_Function_Role_Disposition (Chunk 1:42-46)
    quote: The ontologies which together form the Open Biomedical Ontologies (OBO)
      Foundryincluding the Gene Ontology, the Foundational Model of Anatomy, the
      Protein Ontology, and the Ontology for Biomedical Investigationsutilize Basic
      Formal Ontology (BFO)
  description: BFO serves as the upper-level ontology for the OBO Foundry, providing
    a standard framework that enables interoperability among biomedical domain ontologies
    including Gene Ontology, Foundational Model of Anatomy, Protein Ontology, and
    Ontology for Biomedical Investigations.
- name: BioPAX Ontology Standard
  sources:
  - chunk_ref: 06-BFO_Function_Role_Disposition (Chunk 1:45)
    quote: Individuals and groups in organizations such as BioPAX, Science Commons,
      Ontology Works, the National Cancer Institute, and Computer Task Group, also
      utilize BFO in their work.
  description: BioPAX (Biological Pathway Exchange) uses BFO as its upper-level ontology
    framework, demonstrating BFO's adoption as a standard across multiple biomedical
    organizations and initiatives.
- name: Gene Ontology Molecular Function Ontology
  sources:
  - chunk_ref: 06-BFO_Function_Role_Disposition (Chunk 1:136-139)
    quote: One of the three constituent ontologies of the Gene Ontology (GO) is devoted
      to the representation of molecular functions associated with genes and gene
      products.
  description: The Gene Ontology includes a dedicated molecular function ontology
    for representing functions of genes and gene products, demonstrating how BFO's
    function concepts are implemented in domain-specific standards.
- name: WHO ICF Classification Standard
  sources:
  - chunk_ref: 06-BFO_Function_Role_Disposition (Chunk 1:138-139)
    quote: An example is the World Health Organization's International Classification
      of Functions, Disabilities and Health (ICF).
  description: The WHO International Classification of Functions, Disabilities and
    Health (ICF) is a standard classification system that uses function concepts aligned
    with ontological foundations for health and disability classification.
- name: BFO Upper-Level Ontology Architecture
  sources:
  - chunk_ref: 06-BFO_Function_Role_Disposition (Chunk 1:48-52)
    quote: BFO is an upper-level ontology developed to support integration of data
      obtained through scientific research. It is deliberately designed to be very
      small, so that it may represent in a consistent fashion those upper-level categories
      common to domain ontologies developed by scientists in different fields.
  description: BFO provides a minimal upper-level ontology architecture designed for
    modularity and division of expertise, enabling consistent representation of categories
    across domain ontologies without including domain-specific terms.
- name: Continuant-Occurrent Dual Hierarchy
  sources:
  - chunk_ref: 06-BFO_Function_Role_Disposition (Chunk 1:54-57)
    quote: BFO adopts a view of reality as comprising (1) continuants, entities that
      continue or persist through time, such as objects, qualities, and functions,
      and (2) occurrents, the events or happenings in which continuants participate.
  description: BFO implements a dual hierarchy standard separating continuants (persistent
    entities) from occurrents (events/processes), providing a foundational organizational
    structure for domain ontologies.
- name: BFO 1.1 Continuant Taxonomy
  sources:
  - chunk_ref: 06-BFO_Function_Role_Disposition (Chunk 1:66-88)
    quote: continuant, independent continuant, material entity, object, fiat object
      part, object aggregate, object boundary, site, dependent continuant, generically
      dependent continuant, specifically dependent continuant, quality, realizable
      entity, role, disposition, capability, function, spatial region...
  description: BFO 1.1 provides a standardized taxonomy of continuant types including
    independent continuants (objects), dependent continuants (qualities, realizable
    entities), and spatial regions, enabling consistent ontological modeling across
    domains.
- name: BFO 1.1 Occurrent Taxonomy
  sources:
  - chunk_ref: 06-BFO_Function_Role_Disposition (Chunk 1:92-108)
    quote: occurrent, processual entity, process, process boundary, process aggregate,
      fiat process part, processual context, spatiotemporal region, scattered spatiotemporal
      region, connected spatiotemporal region, spatiotemporal instant, spatiotemporal
      interval, temporal region...
  description: BFO 1.1 provides a standardized taxonomy of occurrent types including
    processes, process boundaries, spatiotemporal regions, and temporal regions for
    representing events and happenings.
- name: NIH NCBO Funding Standard
  sources:
  - chunk_ref: 06-BFO_Function_Role_Disposition (Chunk 1:514-515)
    quote: This work is funded by the United States National Institutes of Health
      (NIH) through the NIH Roadmap for Medical Research, National Center for Biomedical
      Ontologies (NCBO), Grant 1 U54 HG004028.
  description: The National Center for Biomedical Ontology (NCBO) is the NIH-funded
    center that supports BFO and related ontology standards, providing infrastructure
    and resources for biomedical ontology development.
- name: Common Logic Interchange Format (CLIF)
  sources:
  - chunk_ref: 07-Classifying_Processes_Barry_Smith (Chunk 1:154-159)
    quote: Ontological axioms such as (1) and (2), together with accompanying definitions
      of terms and relations, are formulated using logical languages  typically fragments
      of first-order logic  developed to facilitate the representation and interchange
      of information and data among disparate computer systems. Prominent examples
      are the (CLIF) Common Logic Interchange Format
  description: Common Logic Interchange Format (CLIF) is an ISO Standard family of
    languages with expressivity equivalent to first-order logic, used for formulating
    ontological axioms and definitions for data interchange between computer systems.
- name: ISO Common Logic Standard
  sources:
  - chunk_ref: 07-Classifying_Processes_Barry_Smith (Chunk 1:166-168)
    quote: Common Logic  A Framework for a Family of Logic-Based Languages, ed. Harry
      Delugach. ISO/IEC JTC 1/SC 32N1377, International Standards Organization Final
      Committee Draft, 2005-12-13
  description: Common Logic is an ISO/IEC standard (JTC 1/SC 32N1377) providing a
    framework for logic-based languages used in ontology development and data interchange.
- name: OWL 2 W3C Standard
  sources:
  - chunk_ref: 07-Classifying_Processes_Barry_Smith (Chunk 1:169)
    quote: '''OWL 2 Web Ontology Language'', http://www.w3.org/TR/owl2-overview'
  description: OWL 2 is a W3C standard for web ontology language, providing the technical
    specification for ontology formulation on the semantic web.
- name: Reasoner Software Tools
  sources:
  - chunk_ref: 07-Classifying_Processes_Barry_Smith (Chunk 1:182-184)
    quote: the consistency of such mergers can be checked automatically using dedicated
      software applications called 'reasoners'.
  description: Reasoners are dedicated software tools that automatically check the
    consistency of merged ontologies formulated in languages like OWL, enabling validation
    of ontological axioms and definitions.
- name: Gene Ontology Biomedical Standard
  sources:
  - chunk_ref: 07-Classifying_Processes_Barry_Smith (Chunk 1:190-207)
    quote: The GO consists of three sub-ontologies, together comprehending some 30,000
      terms representing types and subtypes of biological processes, molecular functions,
      and cellular components. The GO is used by researchers in biology and biomedicine
      as a controlled vocabulary for describing in species-neutral fashion the attributes
      of genes and gene products
  description: The Gene Ontology (GO) is a standardized controlled vocabulary of approximately
    30,000 terms organized into three sub-ontologies (biological processes, molecular
    functions, cellular components) for annotating genes and gene products.
- name: XES Event Log Standard
  sources:
  - chunk_ref: 07-Classifying_Processes_Barry_Smith (Chunk 1:71)
    quote: In event log formats such as XES, this leads to replicating the same event.
  description: XES (eXtensible Event Stream) is a standard format for storing event
    logs used in process mining, though it has limitations with convergence problems
    requiring event replication.
- name: Ontology for Biomedical Investigations (OBI)
  sources:
  - chunk_ref: 07-Classifying_Processes_Barry_Smith (Chunk 1:239-252)
    quote: This aspect of the unification of science is addressed by the Ontology
      for Biomedical Investigations (OBI), which comprehends a set of terms which
      can be used to describe the attributes of experiments in biological and related
      domains.
  description: OBI is an ontology standard providing preferred terms and logical definitions
    for describing experimental methods, protocols, statistical algorithms, sample
    processing techniques, software, and equipment used in biomedical investigations.
- name: BFO 34-Term Ontology Standard
  sources:
  - chunk_ref: 07-Classifying_Processes_Barry_Smith (Chunk 1:285-287)
    quote: BFO is, by the standards predominant in contemporary ontology, very small,
      consisting of just 34 terms, including both familiar terms such as 'process',
      'object', 'function', 'role' and 'disposition', and less familiar terms such
      as 'generically dependent continuant' and 'continuant fiat boundary'.
  description: BFO is a minimal standard with exactly 34 terms providing core ontological
    categories including process, object, function, role, disposition, and boundary
    concepts used as a framework for domain ontology development.
- name: International System of Units Analogy
  sources:
  - chunk_ref: 07-Classifying_Processes_Barry_Smith (Chunk 1:74-77)
    quote: It thereby extends into the terminology of scientific theories some of
      the advantages brought by the International System of Units to the consistent
      representation of experimental data expressed in quantitative terms.
  description: Ontologies function analogously to the International System of Units
    (SI) by providing standardized terminology for scientific data representation,
    enabling consistent interpretation across different research contexts.
- name: Directed Acyclic Graph Structure
  sources:
  - chunk_ref: 07-Classifying_Processes_Barry_Smith (Chunk 1:78-80)
    quote: Each ontology can be conceived as a set of terms (nouns and noun phrases)
      which form the nodes of a directed acyclical graph
  - chunk_ref: 19-Graph_of_Thoughts (Chunk 1:69)
    quote: Executing algorithms also expose networked patterns, often represented
      by Directed Acyclic Graphs
  description: Merged from 2 sources. Ontologies are technically structured as directed
    acyclic graphs (DAGs) where terms form nodes connected by relations like is_a
    and part_of, enabling hierarchical classification and reasoning.
- name: Information Artifact Ontology
  sources:
  - chunk_ref: 07-Classifying_Processes_Barry_Smith (Chunk 1:629)
    quote: 'the BFO:generically dependent continuant expression: ''1.7 m tall''. Each
      item on this list is unproblematically identifiable as instantiating a BFO category.
      (4) is an information artifact.'
  description: The Information Artifact Ontology (IAO) extends BFO to handle generically
    dependent continuants like measurement expressions and records that can be transferred
    between bearers.
- name: OCEL 2.0 Standard Specification
  sources:
  - chunk_ref: 09-OCEL_20_Specification (Chunk 1:1-3)
    quote: OCEL (Object-Centric Event Log) 2.0 Specification
  description: OCEL 2.0 is the new standard specification for recording and exchanging
    object-centric event logs, superseding OCEL 1.0 with enhanced expressiveness for
    object-centric process mining.
- name: OCEL 2.0 Triple Exchange Formats
  sources:
  - chunk_ref: 09-OCEL_20_Specification (Chunk 1:38-39)
    quote: 'OCEL 2.0 offers three exchange formats: a relational database (SQLite),
      XML, and JSON format.'
  description: 'OCEL 2.0 provides three standardized exchange formats: SQLite relational
    database, XML, and JSON, enabling interoperability across different process mining
    tools and systems.'
- name: OCEL XML Schema Validation
  sources:
  - chunk_ref: 09-OCEL_20_Specification (Chunk 1:23)
    quote: 'XML: https://www.ocel-standard.org/2.0/ocel20-schema-xml.xsd'
  description: OCEL 2.0 provides an official XML Schema Definition (XSD) file for
    validating XML-formatted object-centric event logs against the standard specification.
- name: OCEL JSON Schema Validation
  sources:
  - chunk_ref: 09-OCEL_20_Specification (Chunk 1:25)
    quote: 'JSON: https://www.ocel-standard.org/2.0/ocel20-schema-json.json'
  description: OCEL 2.0 provides an official JSON Schema file for validating JSON-formatted
    object-centric event logs against the standard specification.
- name: OCEL Relational Schema Specification
  sources:
  - chunk_ref: 09-OCEL_20_Specification (Chunk 1:27)
    quote: 'Relational: https://www.ocel-standard.org/2.0/ocel20-schema-relational.pdf'
  description: OCEL 2.0 provides a relational schema specification document for implementing
    object-centric event logs in SQLite or other relational databases.
- name: IEEE XES Standard Evolution
  sources:
  - chunk_ref: 09-OCEL_20_Specification (Chunk 1:237-240)
    quote: The first comprehensive standard for storing event data was the IEEE Standard
      for eXtensible Event Stream (XES). XES became an official IEEE standard in 2016.
      The revised standard (IEEE 1849-2023) was published on 8 September 2023 and
      will be valid for another ten years.
  description: IEEE XES (IEEE 1849) is the established IEEE standard for event log
    storage, with the 2023 revision extending its validity. OCEL 2.0 addresses XES
    limitations for object-centric scenarios.
- name: OCEL Object-to-Object Relationships
  sources:
  - chunk_ref: 09-OCEL_20_Specification (Chunk 1:286-290)
    quote: 'Object-to-Object (O2O) Relationships: OCEL 2.0 allows a deeper understanding
      of how objects interact within a business process. It shows that objects are
      part of a complex network of relationships and actions.'
  description: OCEL 2.0 introduces Object-to-Object (O2O) relationships as a standard
    feature, enabling representation of complex object networks and interactions beyond
    simple event-object associations.
- name: OCEL Dynamic Object Attributes
  sources:
  - chunk_ref: 09-OCEL_20_Specification (Chunk 1:293-297)
    quote: 'Dynamic Object Attribute Values: OCEL 2.0 adopts a dynamic approach where
      attribute values can change over time. Instead of having a single, fixed value,
      an object attribute may have a value that changes during the process.'
  description: OCEL 2.0 standardizes dynamic object attribute values that change over
    time during process execution, providing more realistic representation of evolving
    business objects.
- name: OCEL Relationship Qualifiers
  sources:
  - chunk_ref: 09-OCEL_20_Specification (Chunk 1:306-308)
    quote: 'Relationship Qualifiers: OCEL 2.0 offers capabilities to express qualifiers
      for relationships, both for Object-to-Object (O2O) and Event-to-Object (E2O)
      relationships.'
  description: OCEL 2.0 introduces relationship qualifiers as a standard feature to
    characterize the role or nature of relationships between objects and events in
    process logs.
- name: OCEL Dense Tables Relational Design
  sources:
  - chunk_ref: 09-OCEL_20_Specification (Chunk 1:316-321)
    quote: 'Relational Specification based on Dense Tables: One major feature of OCEL
      2.0 is its data structure using dense tables. Each table corresponds to a unique
      event or object type, storing only relevant attributes.'
  description: OCEL 2.0 uses a dense table design pattern where each event/object
    type has its own table storing only relevant attributes, improving storage efficiency
    and scalability.
- name: ISO 8601 Timestamp Format
  sources:
  - chunk_ref: 09-OCEL_20_Specification (Chunk 1:387-388)
    quote: In the formalization, time is mapped on the non-negative reals, but concrete
      implementations will use, for example, the ISO 8601 time format.
  description: OCEL 2.0 implementations use ISO 8601 standard format for timestamps,
    ensuring consistent temporal representation across different systems and platforms.
- name: OCEL SQLite Reference Implementation
  sources:
  - chunk_ref: 09-OCEL_20_Specification (Chunk 2:182-188)
    quote: In accordance with the definition of OCEL 2.0, rows possessing the smallest
      feasible timestamp, specifically 1970-01-01 00:00 UTC  which equates to 0 in
      Definition 2  correspond to the initial values of all the attributes for a
      given object.
  description: The OCEL 2.0 SQLite implementation uses Unix epoch (1970-01-01 00:00
    UTC) as the reference timestamp for initial object attribute values, following
    the formal definition of time as 0.
- name: OCEL Foreign Key Constraints
  sources:
  - chunk_ref: 09-OCEL_20_Specification (Chunk 2:354-396)
    quote: The uniqueness of the event/object types in the tables event_map_type and
      object_map_type is ensured by setting the type as the primary key... There is
      a foreign key between the specific event type tables and the generic event table.
  description: OCEL 2.0 relational implementation enforces referential integrity through
    foreign key constraints between event tables, object tables, and relationship
    tables, ensuring data consistency.
- name: OCEL JSON Schema Draft-07
  sources:
  - chunk_ref: 09-OCEL_20_Specification (Chunk 4:206)
    quote: '"$schema": "http://json-schema.org/draft-07/schema#",'
  description: The OCEL 2.0 JSON validation schema is based on JSON Schema draft-07
    specification, providing a standard mechanism for validating JSON-formatted event
    logs.
- name: Object-Centric Petri Nets Discovery
  sources:
  - chunk_ref: 09-OCEL_20_Specification (Chunk 1:258-259)
    quote: Several OCEL 1.0 data sets were provided and the availability of the standard
      fueled the development of a range of OCPM techniques, e.g., discovering object-centric
      Petri nets
  - chunk_ref: 11-Process_Mining_Event_Knowledge_Graphs (Chunk 3:17-20)
    quote: An alternative formalization of this concept are object-centric Petri nets.
      Object-centric Petri nets also first discover one Petri net per entity type,
      then annotate the places and arcs with entity identifiers
  description: Merged from 2 sources. Object-centric Petri nets are formal process
    models that discover separate Petri nets per entity type, then compose them using
    entity identifiers on places and arcs, resulting in coloured Petri net models.
- name: OC-PM Web-Based Tool
  sources:
  - chunk_ref: 10-OC-PM_Object-Centric_Process_Mining (Chunk 1:139-141)
    quote: Also, the paper describes the OC-PM tool(s) for object-centric process
      mining, providing the proposed techniques as a web-based interface and as a
      plugin of the ProM framework. These are available at the address https://ocpm.info.
  description: OC-PM is a comprehensive tool for object-centric process mining available
    as both a web-based interface and a ProM plugin, implementing OCEL-based analysis
    techniques.
- name: ProM Process Mining Framework
  sources:
  - chunk_ref: 10-OC-PM_Object-Centric_Process_Mining (Chunk 2:291-293)
    quote: We present another implementation of the process discovery techniques proposed
      in this paper, on top of the popular process mining framework ProM 6.x. The
      implementation is proposed in the package OCELStandard
  description: ProM 6.x is a popular open-source process mining framework that supports
    OCEL through the OCELStandard package, enabling object-centric process mining
    analysis.
- name: JSON-OCEL and XML-OCEL Formats
  sources:
  - chunk_ref: 10-OC-PM_Object-Centric_Process_Mining (Chunk 1:529-533)
    quote: Recently, the OCEL format has been proposed for object-centric event logs.
      Two implementations of the format exist (JSON-OCEL, supported by JSON; XML-OCEL,
      supported by XML; MongoDB)
  description: JSON-OCEL and XML-OCEL are the two primary serialization formats for
    OCEL event logs, with additional support for MongoDB storage for scalable implementations.
- name: OCEL Multi-Language Library Support
  sources:
  - chunk_ref: 10-OC-PM_Object-Centric_Process_Mining (Chunk 1:530-531)
    quote: with tool support available for some popular languages (Java/ProM framework,
      Javascript, Python)
  description: OCEL standard has library implementations in multiple programming languages
    including Java (ProM), JavaScript, and Python, enabling wide adoption across different
    development environments.
- name: Object-Centric Directly-Follows Multigraph (OC-DFG)
  sources:
  - chunk_ref: 10-OC-PM_Object-Centric_Process_Mining (Chunk 1:746-751)
    quote: In this section, we formalize one object-centric process model, the object-centric
      directly-follows multigraph (OC-DFG), and how to discover an object-centric
      directly-follows multigraph starting from an object-centric event log.
  description: Object-Centric Directly-Follows Multigraph (OC-DFG) is a process model
    formalism for representing object-centric processes with typed edges between activities
    for each object type.
- name: Neo4J Graph Database for Process Mining
  sources:
  - chunk_ref: 10-OC-PM_Object-Centric_Process_Mining (Chunk 2:413-416)
    quote: In [20], the execution time of process mining tasks in a popular graph
      database (Neo4J) is shown to be disappointing.
  description: Neo4J is a graph database that has been explored for object-centric
    event data storage and querying, though scalability challenges have been identified
    for process mining tasks.
- name: Colored Petri Nets Standard
  sources:
  - chunk_ref: 10-OC-PM_Object-Centric_Process_Mining (Chunk 2:379-382)
    quote: Colored Petri nets have been proposed in the '80 and have a wide range
      of applications. Colored Petri nets allow the storage of a data value for each
      token. The data value is called the token color.
  description: Colored Petri nets are an extension of Petri nets from the 1980s that
    support data values (colors) on tokens and type-based places (color sets), providing
    rich semantics for process modeling.
- name: Neo4j Graph Database for Event Knowledge Graphs
  sources:
  - chunk_ref: 11-Process_Mining_Event_Knowledge_Graphs (Chunk 1:182-184)
    quote: All concepts for constructing and analyzing event knowledge graphs presented
      in this chapter are implemented as Cypher queries on the graph database system
      Neo4j
  description: Neo4j is identified as the primary graph database system for implementing
    event knowledge graphs. The paper references a GitHub repository with tutorial
    implementations using Cypher queries.
- name: Cypher Query Language for Graph Operations
  sources:
  - chunk_ref: 11-Process_Mining_Event_Knowledge_Graphs (Chunk 1:182-183)
    quote: All concepts for constructing and analyzing event knowledge graphs presented
      in this chapter are implemented as Cypher queries
  description: Cypher is the query language used for constructing, querying, and analyzing
    event knowledge graphs in Neo4j. Used for all graph operations including entity
    inference and directly-follows relationship construction.
- name: Labeled Property Graphs (LPG) Data Model
  sources:
  - chunk_ref: 11-Process_Mining_Event_Knowledge_Graphs (Chunk 1:550-552)
    quote: A typed graph data model such as labeled property graphs allows to distinguish
      different types of nodes (events, entities) and relationships (directly-follows,
      correlated-to)
  description: Labeled property graphs serve as the foundational data model for event
    knowledge graphs, allowing typed nodes with labels (Event, Entity) and typed relationships
    (df, corr) with properties attached to both.
- name: LPG Formal Definition Standard
  sources:
  - chunk_ref: 11-Process_Mining_Event_Knowledge_Graphs (Chunk 1:574-583)
    quote: 'A labeled property graph (LPG) G = (N, R, lambda, #) is a graph with nodes
      N, and relationships R with... Each node n carries a label... Each relationship
      r carries a label'
  description: Formal mathematical definition of labeled property graphs with nodes
    carrying labels from set Lambda_N, relationships carrying labels from Lambda_R,
    and both supporting attribute-value pairs as properties.
- name: RDF Comparison and Limitations
  sources:
  - chunk_ref: 11-Process_Mining_Event_Knowledge_Graphs (Chunk 1:681-682)
    quote: While the nodes and relationships of Definition 8 can also be encoded in
      RDF, the df-paths rely on attributes of relationships which are not supported
      by RDF but by LPGs
  description: RDF is identified as an alternative encoding for event knowledge graphs,
    but LPGs are preferred because RDF does not support attributes on relationships,
    which are essential for df-paths with entity type properties.
- name: OCEL Object-Centric Event Logs Standard
  sources:
  - chunk_ref: 11-Process_Mining_Event_Knowledge_Graphs (Chunk 1:236-239)
    quote: Note that Definition 2 formalizes the object-centric event logs (OCEL)
      described in Sect. 3.4 of [1]; we here use the more general term 'entity' instead
      of 'object'
  description: OCEL (Object-Centric Event Logs) is a standard format for multi-entity
    event data. The paper's event table with entity types formalizes and extends the
    OCEL concept using 'entity' as a more general term.
- name: XOC Event Logs for Dynamic Relations
  sources:
  - chunk_ref: 11-Process_Mining_Event_Knowledge_Graphs (Chunk 1:341-342)
    quote: Modeling such dynamics requires additional concepts as defined in XOC event
      logs
  description: XOC event logs are an extension that captures dynamically changing
    relationships between entities over time, addressing limitations of static relation
    views in standard event logs.
- name: GitHub Repository Implementation
  sources:
  - chunk_ref: 11-Process_Mining_Event_Knowledge_Graphs (Chunk 1:183-184)
    quote: at https://github.com/multi-dimensional-process-mining/eventgraph_tutorial
  description: Reference implementation of event knowledge graph concepts available
    as open-source tutorial with Cypher queries for Neo4j graph database.
- name: Cypher Queries for Graph Construction
  sources:
  - chunk_ref: 11-Process_Mining_Event_Knowledge_Graphs (Chunk 2:60-62)
    quote: All steps of the method can be implemented as a series of Cypher queries
      to construct event knowledge graphs in a graph database
  description: Cypher queries enable implementation of the complete event knowledge
    graph construction method, from entity inference to directly-follows relationship
    creation, in production graph databases.
- name: Causal Event Graph Extraction from Relational Databases
  sources:
  - chunk_ref: 11-Process_Mining_Event_Knowledge_Graphs (Chunk 2:63-65)
    quote: A variant of event knowledge graphs, called causal event graph that only
      models events but not the entities, can be extracted automatically from relational
      databases
  description: Causal event graphs are a simplified variant of event knowledge graphs
    that model only events without explicit entity nodes, supporting automatic extraction
    from relational database sources.
- name: Real-Life Dataset References with DOIs
  sources:
  - chunk_ref: 11-Process_Mining_Event_Knowledge_Graphs (Chunk 2:62-63)
    quote: several event knowledge graphs of real-life processes are available [19-24]
  description: Multiple real-life event knowledge graph datasets are available with
    DOI references (BPI Challenges 2014-2019), providing empirical validation resources
    for process mining research.
- name: Inductive Miner Process Discovery
  sources:
  - chunk_ref: 11-Process_Mining_Event_Knowledge_Graphs (Chunk 1:458-461)
    quote: the directly-follows graph (DFG) of the log in Table 2 and the corresponding
      process model discovered with the Inductive Miner (IM) annotated with the mean
      waiting times
  description: Inductive Miner is a process discovery algorithm that produces process
    models from event logs, used for comparison to demonstrate limitations of traditional
    single-case-identifier approaches.
- name: Multi-Entity Directly-Follows Graph (DFG)
  sources:
  - chunk_ref: 11-Process_Mining_Event_Knowledge_Graphs (Chunk 2:366-367)
    quote: The resulting graph is a multi-entity directly-follows graph, also called
      multi-viewpoint DFG or artifact-centric model
  description: Multi-entity DFG is a process model representation that respects local
    directly-follows relations per entity type, enabling accurate multi-dimensional
    process discovery.
- name: Synchronous Proclets Multi-Entity Model
  sources:
  - chunk_ref: 11-Process_Mining_Event_Knowledge_Graphs (Chunk 2:383-391)
    quote: An alternative representation of the multi-entity DFG is the proclet model...
      constructed by creating a Class node per unique pair of activity name and entity
      type
  description: Synchronous proclets are a multi-entity extension of Petri nets where
    each proclet describes behavior of one entity type, with synchronization edges
    indicating which transitions occur together.
- name: Event and DF Aggregation Queries
  sources:
  - chunk_ref: 11-Process_Mining_Event_Knowledge_Graphs (Chunk 2:380-382)
    quote: Event and df-aggregation can be implemented as simple, scalable queries
      over standard graph databases, enabling efficient in-database process discovery
  description: Aggregation operations for discovering multi-entity process models
    can be implemented as efficient, scalable queries directly within graph databases.
- name: Performance Spectrum Visual Analytics
  sources:
  - chunk_ref: 11-Process_Mining_Event_Knowledge_Graphs (Chunk 2:521-522)
    quote: Setting the x-coordinate of each event by its time property and the y-coordinate
      by its Activity entity results in the graph in Fig. 17, which is called the
      Performance Spectrum
  description: The Performance Spectrum is a visual analytics technique for analyzing
    process performance over time, implemented as a specialized graph layout with
    time on x-axis and activity on y-axis.
- name: Performance Spectrum Mining Tool
  sources:
  - chunk_ref: 11-Process_Mining_Event_Knowledge_Graphs (Chunk 2:590-591)
    quote: It is also implemented as a visual analytics tool over event data and in
      combination with process models
  description: Performance Spectrum has been implemented as a dedicated visual analytics
    tool for fine-grained performance analysis, revealing patterns like batching,
    FIFO violations, and bottlenecks.
- name: Coloured Petri Nets for Multi-Entity Analysis
  sources:
  - chunk_ref: 11-Process_Mining_Event_Knowledge_Graphs (Chunk 3:19-21)
    quote: then compose all entity nets along transitions for the same activity, resulting
      in a coloured Petri net model that is accessible for analysis and measuring
      model quality
  description: Coloured Petri nets result from composing object-centric Petri nets,
    enabling formal analysis and quality measurement of multi-entity process models.
- name: Modular DCR Graphs
  sources:
  - chunk_ref: 11-Process_Mining_Event_Knowledge_Graphs (Chunk 3:29-32)
    quote: Extensions of declarative models (see [10]) such as modular DCR graphs,
      that apply similar principles as synchronous proclets, could be more suitable
  description: Modular DCR (Dynamic Condition Response) graphs are declarative process
    models that apply modular composition principles, potentially better suited for
    complex entity interaction patterns than procedural models.
- name: Scenario-Based Models for Partial Orders
  sources:
  - chunk_ref: 11-Process_Mining_Event_Knowledge_Graphs (Chunk 3:33-34)
    quote: Alternatively, scenario-based models that specify conditional partial orders
      of events over multiple entities could be applied
  description: Scenario-based models specify conditional partial orders of events
    across multiple entities, providing an alternative formalism for describing complex
    multi-entity interactions.
- name: BPI Challenge Datasets with DOIs
  sources:
  - chunk_ref: 11-Process_Mining_Event_Knowledge_Graphs (Chunk 3:112-123)
    quote: Event Graph of BPI Challenge 2014. Dataset. https://doi.org/10.4121/14169494...
      Event Graph of BPI Challenge 2019. Dataset. https://doi.org/10.4121/14169614
  description: BPI Challenge datasets (2014-2019) are available as event knowledge
    graphs with persistent DOI identifiers, providing standardized benchmark datasets
    for process mining research.
- name: Zenodo Dataset Repository
  sources:
  - chunk_ref: 11-Process_Mining_Event_Knowledge_Graphs (Chunk 3:122-123)
    quote: Event Data and Queries for Multi-Dimensional Event Data in the Neo4j Graph
      Database, April 2021. https://doi.org/10.5281/zenodo.4708117
  description: Zenodo is used as a repository for sharing multi-dimensional event
    data and query implementations, supporting reproducible process mining research.
- name: XES Standard (IEEE 1849-2016)
  sources:
  - chunk_ref: 12-Foundations_of_Process_Event_Data (Chunk 1:163-168)
    quote: This observation drove the development of the eXtensible Event Stream (XES)
      standard, an IEEE Standards Association-approved language to transport, store,
      and exchange event data. Its metadata structure is represented in Fig. 2. XES
      uses the W3C XML Schema definition language
  description: XES (eXtensible Event Stream) is the IEEE 1849-2016 standard for event
    log interchange, using W3C XML Schema for defining event log structure with logs,
    traces, events, and extensible attributes.
- name: XES Extensions Mechanism
  sources:
  - chunk_ref: 12-Foundations_of_Process_Event_Data (Chunk 1:172-175)
    quote: it does allow for extensions. An extension can be used to define a set
      of attributes for events, traces and/or logs. For instance, a common set of
      attributes can be defined for event logs within a particular application domain
  description: XES supports domain-specific extensions that define standardized attribute
    sets for events, traces, or logs, enabling interoperability within specific application
    domains.
- name: XES Lifecycle Extension
  sources:
  - chunk_ref: 12-Foundations_of_Process_Event_Data (Chunk 1:185-186)
    quote: Also in IEEE XES, a lifecycle extension has been approved, which specifies
      a default activity lifecycle
  description: The XES lifecycle extension provides a standardized activity lifecycle
    state machine for representing event types like start, complete, suspend, resume
    within process execution.
- name: BPMN 2.0 Activity Lifecycle
  sources:
  - chunk_ref: 12-Foundations_of_Process_Event_Data (Chunk 1:183-184)
    quote: One example of such a transactional lifecycle model is shown in Fig. 3a.
      This is the transition lifecycle model of the BPMN 2.0 standard
  description: BPMN 2.0 defines a transactional lifecycle model for activities with
    standardized state transitions, providing a reference model for event type semantics
    in process-aware systems.
- name: OCEL Standard for Object-Centric Event Data
  sources:
  - chunk_ref: 12-Foundations_of_Process_Event_Data (Chunk 1:431-433)
    quote: Finally, the recently introduced OCEL standard is another relevant piece
      of work, putting forward a general standard to interchange object-centric event
      data with multiple case notions
  description: OCEL (Object-Centric Event Log) standard provides a format for exchanging
    event data with multiple case notions, addressing limitations of single-case-identifier
    formats.
- name: BPMS Event Logging
  sources:
  - chunk_ref: 12-Foundations_of_Process_Event_Data (Chunk 1:217-218)
    quote: When retrieving data from process-aware information system, especially
      from Business Process Management Systems (BPMS), a large collection of event
      types might be readily available
  description: Business Process Management Systems (BPMS) provide native event logging
    with rich event type information, representing the most process-aware source for
    event data extraction.
- name: Ontology-Based Data Access (OBDA) for Event Extraction
  sources:
  - chunk_ref: 12-Foundations_of_Process_Event_Data (Chunk 1:428-431)
    quote: One noteworthy scientific initiative in this context is ontology-based
      data access (ODBA) for event log extraction. The approach is based on an ontological
      view of the domain of interest and linking it as such to a database schema and
      has been implemented in the Onprom tool
  description: OBDA (Ontology-Based Data Access) uses ontological domain views linked
    to database schemas for event log extraction, implemented in the Onprom tool for
    semantic event extraction.
- name: ProM Import Framework
  sources:
  - chunk_ref: 12-Foundations_of_Process_Event_Data (Chunk 1:416-419)
    quote: One of the first tools stemming from scientific research was the ProM Import
      Framework. Already in these early days, the idea of an extensible plug-in architecture
      allowing to develop adapters to hook into a large variety of systems was proposed
  description: ProM Import Framework pioneered extensible plug-in architecture for
    connecting to diverse source systems, establishing the pattern for process mining
    tool integration.
- name: XESame Event Log Extraction Tool
  sources:
  - chunk_ref: 12-Foundations_of_Process_Event_Data (Chunk 1:419-420)
    quote: With the uptake of XES, XESame was developed as a more flexible successor
      to the ProM Import Framework
  description: XESame is a flexible event log extraction tool developed as successor
    to ProM Import Framework, aligned with XES standard for improved interoperability.
- name: EVS Model Builder and XTract for ERP Extraction
  sources:
  - chunk_ref: 12-Foundations_of_Process_Event_Data (Chunk 1:421-422)
    quote: Other researchers have focused on extraction from ERP systems, e.g. the
      EVS Model Builder and XTract, or other operational systems, e.g. Eventifier
  description: EVS Model Builder and XTract are specialized tools for extracting event
    logs from ERP systems, while Eventifier handles other operational systems.
- name: ETL Processing for Event Data Integration
  sources:
  - chunk_ref: 12-Foundations_of_Process_Event_Data (Chunk 1:464-466)
    quote: Hereto, especially when an organizational data warehousing architecture
      is present, Extract-Transform-Load (ETL) processing would be a default technology
      to resort to. ETL tools are perfectly equipped to derive and deploy matching
      schemes to integrate data
  description: ETL (Extract-Transform-Load) tools are standard technology for integrating
    event data from non-integrated sources, particularly within data warehousing architectures.
- name: Data Federation for Event Log Integration
  sources:
  - chunk_ref: 12-Foundations_of_Process_Event_Data (Chunk 1:467-470)
    quote: Increasingly, companies start to focus on the introduction of data virtualization
      layers in order to realize a more federation-oriented data integration. Data
      federation can prevent the creation of yet another duplicated database
  description: Data federation and virtualization layers provide an alternative to
    ETL consolidation, enabling flexible querying across multiple source systems without
    data duplication.
- name: JSON for Web Event Data
  sources:
  - chunk_ref: 12-Foundations_of_Process_Event_Data (Chunk 1:313-315)
    quote: in many cases, including for instance learning environments such as MOOCs,
      a default standard for web-based platforms to store data is JSON (JavaScript
      Object Notation)
  description: JSON is the default data format for event data from web-based platforms
    like MOOCs, requiring transformation for process mining compatibility.
- name: SAP ECC/S4 HANA and Salesforce as Top Sources
  sources:
  - chunk_ref: 12-Foundations_of_Process_Event_Data (Chunk 1:327-329)
    quote: In an online survey with 289 participants spanning the roles of practitioners,
      researchers, software vendors, and end-users, SAP ECC (R/3), SAP S/4 HANA, and
      Salesforce are selected as the top three most analyzed source systems
  description: SAP ECC, SAP S/4 HANA, and Salesforce are empirically validated as
    the three most common source systems for process mining, based on a 289-participant
    industry survey.
- name: PM2 Process Mining Methodology
  sources:
  - chunk_ref: 12-Foundations_of_Process_Event_Data (Chunk 1:370-373)
    quote: 'When making an assessment of one of the most recently introduced process
      mining methodologies, i.e. PM2, four event data preprocessing tasks are defined:
      (1) creating views, (2) filtering logs, (3) enriching logs, and (4) aggregating
      events'
  description: PM2 is a process mining methodology that defines four specialized preprocessing
    tasks for event data, distinct from traditional data analytics pipelines like
    CRISP-DM.
- name: CRISP-DM Adaptation for Process Mining
  sources:
  - chunk_ref: 12-Foundations_of_Process_Event_Data (Chunk 1:374-375)
    quote: Several process mining case studies such as the one presented in [6] adapted
      CRISP-DM to work with healthcare datasets
  description: CRISP-DM (Cross-Industry Standard Process for Data Mining) has been
    adapted for process mining projects, particularly in healthcare domains, bridging
    traditional data science and process analytics.
- name: GPT-4 LLM API for Multi-Agent Systems
  sources:
  - chunk_ref: 15-SciAgents_Multi-Agent_Graph_Reasoning (Chunk 1:708-709)
    quote: The automated multi-agent system consists of a team of AI agents, each
      powered by a state-of-the-art general purpose large language model from the
      GPT-4 family, accessed via the OpenAI API
  description: GPT-4 family models accessed via OpenAI API serve as the foundation
    for multi-agent scientific discovery systems, powering specialized agents with
    distinct roles.
- name: Semantic Scholar API for Novelty Assessment
  sources:
  - chunk_ref: 15-SciAgents_Multi-Agent_Graph_Reasoning (Chunk 1:195-197)
    quote: For instance, we have empowered our automated multi-agent model with the
      Semantic Scholar API as a tool that provides it with an ability to check the
      novelty of the generated hypothesis against the existing literature
  description: Semantic Scholar API integration enables automated novelty assessment
    of generated research hypotheses against existing scientific literature.
- name: Ontological Knowledge Graphs from Scientific Papers
  sources:
  - chunk_ref: 15-SciAgents_Multi-Agent_Graph_Reasoning (Chunk 1:31-34)
    quote: the use of large-scale ontological knowledge graphs to organize and interconnect
      diverse scientific concepts... Applied to biologically inspired materials
  description: Large-scale ontological knowledge graphs constructed from scientific
    papers (around 1,000 papers) provide structured interconnection of concepts as
    nodes and relationships as edges.
- name: GROMACS and AMBER Molecular Dynamics Software
  sources:
  - chunk_ref: 15-SciAgents_Multi-Agent_Graph_Reasoning (Chunk 1:396-398)
    quote: The model suggests using Molecular Dynamics (MD) Simulations to explore
      interactions at the molecular level. Specifically, it proposes employing software
      like GROMACS or AMBER
  description: GROMACS and AMBER are recommended molecular dynamics simulation tools
    for modeling molecular interactions in bio-inspired materials research.
- name: CHARMM and AMBER Force Fields
  sources:
  - chunk_ref: 15-SciAgents_Multi-Agent_Graph_Reasoning (Chunk 1:623-625)
    quote: Appropriate force fields, such as CHARMM or AMBER, are selected, with parameters
      defined using tools like CGenFF
  description: CHARMM and AMBER force fields with CGenFF parameterization are used
    for molecular dynamics simulations of silk fibroin and pigment interactions.
- name: VMD and PyMOL Molecular Visualization
  sources:
  - chunk_ref: 15-SciAgents_Multi-Agent_Graph_Reasoning (Chunk 1:625-626)
    quote: using VMD or GROMACS for setup... using tools like PyMOL, Chimera, and
      GROMACS
  description: VMD, PyMOL, and Chimera are molecular visualization tools used for
    system setup and post-simulation structure analysis in scientific discovery workflows.
- name: Finite Element Analysis (FEA) for Material Modeling
  sources:
  - chunk_ref: 15-SciAgents_Multi-Agent_Graph_Reasoning (Chunk 1:596-598)
    quote: Use FEA to simulate the mechanical behavior of the composite under different
      loading conditions. Use dynamic mechanical analysis (DMA) to study the viscoelastic
      properties
  description: Finite Element Analysis (FEA) and Dynamic Mechanical Analysis (DMA)
    are used for simulating mechanical behavior and viscoelastic properties of composite
    materials.
- name: Life Cycle Assessment (LCA) for Sustainability
  sources:
  - chunk_ref: 15-SciAgents_Multi-Agent_Graph_Reasoning (Chunk 1:602-604)
    quote: Use life cycle assessment (LCA) to evaluate the environmental impact and
      energy efficiency of the production process
  description: Life Cycle Assessment (LCA) methodology is recommended for evaluating
    environmental sustainability and energy efficiency of bio-inspired material production.
- name: AutoGen UserProxyAgent Class
  sources:
  - chunk_ref: 15-SciAgents (Chunk 2:487-488)
    quote: In our multi-agent system, the human agent is constructed using UserProxyAgent
      class from Autogen, and Assistant, Planner, Ontologist...
  - chunk_ref: 15-SciAgents (Chunk 2:488)
    quote: Assistant, Planner, Ontologist, Scientist 1, Scientist 2, and Critic agents
      are created via AssistantAgent class from Autogen
  description: Merged from 2 sources. UserProxyAgent is used to construct human agent
    proxies in the multi-agent system, enabling human-in-the-loop interactions within
    the AutoGen framework.
- name: AutoGen GroupChatManager
  sources:
  - chunk_ref: 15-SciAgents (Chunk 2:489)
    quote: and the group chat manager is created using GroupChatManager class
  description: GroupChatManager class enables coordination and management of group
    conversations between multiple agents in the AutoGen framework.
- name: Python Function-Based Tool Design
  sources:
  - chunk_ref: 15-SciAgents (Chunk 2:498-499)
    quote: All the tools implemented in this work are defined as python functions.
      Each function is characterized by a name, a description, and input properties
  description: Tools are implemented as Python functions with well-defined names,
    descriptions, and typed input properties, enabling structured function calling
    by LLM agents.
- name: Semantic Scholar API Integration
  sources:
  - chunk_ref: 15-SciAgents (Chunk 2:505-510)
    quote: We use the Semantic Scholar API, an AI-powered search engine for academic
      resources, to search for related publications using a set of keywords
  description: Semantic Scholar API is integrated as an external tool for novelty
    assessment, enabling agents to search academic literature and compare research
    hypotheses against existing publications.
- name: BAAI/bge-large-en-v1.5 Embedding Model
  sources:
  - chunk_ref: 15-SciAgents (Chunk 2:239)
    quote: We use the `BAAI/bge-large-en-v1.5` embedding model
  description: The BAAI/bge-large-en-v1.5 embedding model is used for generating node
    embeddings in the knowledge graph, enabling semantic similarity calculations for
    pathfinding algorithms.
- name: GPT-4 Family Models
  sources:
  - chunk_ref: 15-SciAgents (Chunk 2:483)
    quote: We design AI agents using the general-purpose LLM GPT-4 family models
  description: GPT-4 family of large language models serves as the foundation for
    agent intelligence, providing natural language understanding and generation capabilities.
- name: GitHub Code Repository
  sources:
  - chunk_ref: 15-SciAgents (Chunk 2:529-530)
    quote: All data and codes are available on GitHub at `https://github.com/lamm-mit/SciAgentsDiscovery`
      and `https://github.com/lamm-mit/GraphReasoning/`
  description: Open-source code repositories on GitHub provide implementation reference
    for the SciAgents framework and graph reasoning components.
- name: Ontological Knowledge Graph
  sources:
  - chunk_ref: 15-SciAgents (Chunk 2:134)
    quote: leveraging LLMs and a comprehensive ontological knowledge graph
  description: Large-scale ontological knowledge graphs (33,159 nodes, 48,753 edges,
    92 communities) serve as the substrate for multi-agent reasoning and hypothesis
    generation.
- name: JSON Structured Output Format
  sources:
  - chunk_ref: 15-SciAgents (Chunk 2:305-306)
    quote: The output, in JSON format, provides key fields such as `mechanisms`, `unexpected_properties`,
      and `comparison`
  description: JSON format is used for structured output from agents, enabling standardized
    representation of research hypotheses with fields like hypothesis, outcome, mechanisms,
    design_principles.
- name: Dijkstra Algorithm Variant
  sources:
  - chunk_ref: 15-SciAgents (Chunk 2:250-251)
    quote: the algorithm uses a modified version of Dijkstra's algorithm that introduces
      a randomness factor to the priority queue
  description: Modified Dijkstra's algorithm with randomness factor (0.2) is used
    for heuristic pathfinding in knowledge graphs, enabling diverse path exploration.
- name: PDF and CSV Output Generation
  sources:
  - chunk_ref: 15-SciAgents (Chunk 2:458-459)
    quote: The entire research concept, expanded and reviewed, is then compiled into
      a final document which is saved as both a PDF and CSV file
  description: Final research documents are exported in both PDF and CSV formats for
    further analysis and dissemination.
- name: Molecular Dynamics Simulation Tools
  sources:
  - chunk_ref: 15-SciAgents (Chunk 3:104-105)
    quote: 'Molecular Dynamics (MD) Simulations: Use MD simulations to model the interactions
      between silk fibroin and dandelion pigments... Software such as GROMACS or AMBER'
  description: GROMACS and AMBER molecular dynamics simulation software are recommended
    for modeling molecular interactions in materials science applications.
- name: Finite Element Analysis Software
  sources:
  - chunk_ref: 15-SciAgents (Chunk 3:106-107)
    quote: 'Finite Element Analysis (FEA): Apply FEA to simulate the mechanical behavior...
      Software like ANSYS or COMSOL Multiphysics'
  - chunk_ref: 15-SciAgents (Chunk 6:482)
    quote: Design the hierarchical structure using computational modeling techniques,
      such as finite element analysis (FEA), to optimize the geometry for crashworthiness
      and stiffness memory.
  description: Merged from 2 sources. FEA is a computational modeling technique used
    to optimize hierarchical structure geometry for mechanical properties. Creates
    3D models and predicts stress, strain, and deformation under loading.
- name: FTIR Spectroscopy
  sources:
  - chunk_ref: 15-SciAgents (Chunk 3:113)
    quote: 'Fourier Transform Infrared Spectroscopy (FTIR): To confirm the formation
      of beta-sheets in silk fibroin'
  description: FTIR spectroscopy is used as an analytical technique to characterize
    molecular structures and confirm chemical compositions in biomaterials.
- name: X-ray Diffraction Analysis
  sources:
  - chunk_ref: 15-SciAgents (Chunk 3:114)
    quote: 'X-ray Diffraction (XRD): To analyze the crystalline structure of the composite
      material'
  description: XRD is used for crystalline structure analysis of composite materials,
    providing insights into molecular organization.
- name: Thermogravimetric Analysis
  sources:
  - chunk_ref: 15-SciAgents (Chunk 3:115)
    quote: 'Thermogravimetric Analysis (TGA): To assess the thermal stability of the
      composite material'
  - chunk_ref: 15-SciAgents (Chunk 10:78)
    quote: The thermal stability of the composites will be assessed using thermogravimetric
      analysis (TGA) and differential scanning calorimetry (DSC).
  description: Merged from 2 sources. TGA is used to assess thermal stability of composite
    materials. Expected structural integrity maintained up to 300-400 degrees Celsius.
- name: UV-Vis Spectroscopy
  sources:
  - chunk_ref: 15-SciAgents (Chunk 3:294)
    quote: 'UV-Vis Spectroscopy: To monitor the distribution of pigments within the
      silk matrix'
  - chunk_ref: 15-SciAgents (Chunk 10:146-147)
    quote: Raman spectroscopy will be employed to analyze the chemical interactions
      and binding mechanisms between graphene and amyloid fibrils.
  description: Merged from 2 sources. Raman spectroscopy is used to analyze chemical
    interactions and binding mechanisms. Monitors characteristic peaks (G and 2D bands)
    to assess molecular interactions.
- name: Atomic Force Microscopy
  sources:
  - chunk_ref: 15-SciAgents (Chunk 3:295)
    quote: 'Atomic Force Microscopy (AFM) and Scanning Electron Microscopy (SEM):
      To observe the microstructure and distribution'
  - chunk_ref: 15-SciAgents (Chunk 6:111)
    quote: Characterize the mechanical properties and microstructure of the synthesized
      materials using SEM, AFM, and tensile testing.
  description: Merged from 2 sources. AFM is used for characterizing mechanical properties
    at the nanoscale and for imaging hierarchical structures. Enables measurement
    of surface roughness and nanoscale features.
- name: Scanning Electron Microscopy
  sources:
  - chunk_ref: 15-SciAgents (Chunk 3:295)
    quote: 'Scanning Electron Microscopy (SEM): To observe the microstructure and
      distribution of pigments'
  - chunk_ref: 15-SciAgents (Chunk 6:111)
    quote: Characterize the mechanical properties and microstructure of the synthesized
      materials using SEM, AFM, and tensile testing.
  description: Merged from 2 sources. SEM is a characterization technique for analyzing
    material microstructure and surface morphology. Used to confirm hierarchical porous
    architecture in biomimetic materials.
- name: Transmission Electron Microscopy
  sources:
  - chunk_ref: 15-SciAgents (Chunk 3:296)
    quote: 'Transmission Electron Microscopy (TEM) and Small-angle X-ray Scattering
      (SAXS): To analyze the nanoscale structures'
  - chunk_ref: 15-SciAgents (Chunk 10:145)
    quote: 'Transmission Electron Microscopy (TEM): TEM will provide detailed images
      of the internal structure and arrangement of the composites at atomic resolution
      (0.1-1 nm).'
  description: Merged from 2 sources. TEM provides atomic resolution imaging (0.1-1
    nm) of internal structure and arrangement of composite materials.
- name: Small-Angle X-ray Scattering
  sources:
  - chunk_ref: 15-SciAgents (Chunk 3:296)
    quote: 'Small-angle X-ray Scattering (SAXS): To analyze the nanoscale structures
      formed by self-assembly'
  description: SAXS is used to analyze nanoscale periodic structures and self-assembled
    organizations in materials.
- name: Synchrotron Micro-CT
  sources:
  - chunk_ref: 15-SciAgents (Chunk 3:243)
    quote: synchrotron radiation-based micro-computed tomography (SR-microCT)
  description: Synchrotron-based micro-computed tomography is used for high-resolution
    3D imaging of hierarchical material structures.
- name: FDTD Optical Simulations
  sources:
  - chunk_ref: 15-SciAgents (Chunk 3:343)
    quote: Finite-difference time-domain (FDTD) simulations to model light interaction
      with the composite material
  description: FDTD simulations are used for modeling optical properties and predicting
    structural coloration in materials.
- name: Dynamic Mechanical Analysis
  sources:
  - chunk_ref: 15-SciAgents (Chunk 3:355)
    quote: Use dynamic mechanical analysis (DMA) to study the viscoelastic properties
  - chunk_ref: 15-SciAgents (Chunk 7:6-7)
    quote: Measure mechanical properties before and after thermal cycling using tensile
      tests and dynamic mechanical analysis (DMA).
  description: Merged from 2 sources. DMA is used for evaluating viscoelastic properties,
    storage modulus, and loss modulus. Measures mechanical properties before and after
    environmental exposure.
- name: Life Cycle Assessment
  sources:
  - chunk_ref: 15-SciAgents (Chunk 3:364)
    quote: Use life cycle assessment (LCA) to evaluate the environmental impact and
      energy efficiency
  description: LCA methodology is used for evaluating environmental impact and sustainability
    of production processes.
- name: Response Surface Methodology
  sources:
  - chunk_ref: 15-SciAgents (Chunk 3:365-366)
    quote: Apply process optimization algorithms such as response surface methodology
      (RSM) to identify the optimal conditions
  description: RSM is used as a process optimization technique to identify optimal
    processing conditions for material fabrication.
- name: MEEP Electromagnetic Simulations
  sources:
  - chunk_ref: 15-SciAgents (Chunk 3:753-754)
    quote: using finite-difference time-domain (FDTD) simulations. Software like MEEP
      (MIT Electromagnetic Equation Propagation)
  description: MEEP software is used for electromagnetic equation propagation and
    predicting reflectance spectra of photonic structures.
- name: VMD Molecular Visualization
  sources:
  - chunk_ref: 15-SciAgents (Chunk 3:718)
    quote: Use tools like VMD (Visual Molecular Dynamics) or GROMACS for this step
  description: VMD is used for molecular visualization and system preparation in molecular
    dynamics simulations.
- name: MDAnalysis and Cluster Analysis
  sources:
  - chunk_ref: 15-SciAgents (Chunk 3:742-743)
    quote: Perform cluster analysis to identify and categorize different self-assembled
      structures... Tools like GROMACS or MDAnalysis
  description: MDAnalysis and GROMACS tools are used for cluster analysis and identifying
    self-assembled molecular structures.
- name: PyMOL and Chimera Visualization
  sources:
  - chunk_ref: 15-SciAgents (Chunk 3:736)
    quote: Use visualization tools like PyMOL or Chimera to analyze the binding interfaces
  description: PyMOL and Chimera are used for molecular visualization and analyzing
    binding interfaces between molecules.
- name: LAMMPS Molecular Dynamics
  sources:
  - chunk_ref: 15-SciAgents (Chunk 3:751)
    quote: Use software like LAMMPS or custom scripts to simulate light interaction
      with the nanostructures
  - chunk_ref: 15-SciAgents (Chunk 6:94)
    quote: Develop a molecular model of the lamellar structure using software such
      as LAMMPS or GROMACS.
  description: Merged from 2 sources. LAMMPS (Large-scale Atomic/Molecular Massively
    Parallel Simulator) is identified as a tool for molecular modeling of lamellar
    structures and heat transfer analysis. Used for developing molecular models with
    defined thermal properties.
- name: Computational Fluid Dynamics
  sources:
  - chunk_ref: 15-SciAgents (Chunk 5:699-700)
    quote: 'Computational Fluid Dynamics (CFD): Use CFD to simulate fluid flow within
      the microfluidic channels'
  description: CFD simulations are used to model fluid flow, pressure drop, and heat
    transfer efficiency in microfluidic systems.
- name: Particle Image Velocimetry
  sources:
  - chunk_ref: 15-SciAgents (Chunk 5:756-757)
    quote: Conduct fluid dynamics experiments using particle image velocimetry (PIV)
      to visualize and quantify the flow patterns
  description: PIV is used as an experimental technique to visualize and quantify
    flow patterns within microfluidic channels.
- name: GROMACS Molecular Dynamics Software
  sources:
  - chunk_ref: 15-SciAgents (Chunk 6:94)
    quote: Develop a molecular model of the lamellar structure using software such
      as LAMMPS or GROMACS.
  description: GROMACS is referenced as molecular dynamics simulation software for
    analyzing material properties at the molecular level, particularly for heat transfer
    and structural analysis.
- name: VMD Visualization Tool
  sources:
  - chunk_ref: 15-SciAgents (Chunk 6:97)
    quote: Use tools like VMD or OVITO to visualize and analyze the heat transfer
      pathways and efficiency within the lamellar structure.
  description: VMD (Visual Molecular Dynamics) is a molecular visualization tool used
    to visualize and analyze heat transfer pathways and molecular structures.
- name: OVITO Visualization Tool
  sources:
  - chunk_ref: 15-SciAgents (Chunk 6:97)
    quote: Use tools like VMD or OVITO to visualize and analyze the heat transfer
      pathways and efficiency within the lamellar structure.
  description: OVITO is an open visualization tool for atomistic simulations, used
    for analyzing heat transfer pathways and structural efficiency in materials.
- name: Soft Lithography Fabrication
  sources:
  - chunk_ref: 15-SciAgents (Chunk 6:68)
    quote: The use of soft lithography to create intricate lamellar structures may
      present challenges in reproducibility and scalability.
  description: Soft lithography is a fabrication technique for creating intricate
    lamellar and microfluidic structures. Identified as having challenges in reproducibility
    and scalability for manufacturing.
- name: Electrospinning Manufacturing
  sources:
  - chunk_ref: 15-SciAgents (Chunk 6:110)
    quote: Synthesize biomimetic materials with a lamellar structure using techniques
      such as electrospinning or layer-by-layer assembly.
  description: Electrospinning is a manufacturing technique for synthesizing biomimetic
    materials with hierarchical structures. Used for creating collagen micro/nanofibers
    with controlled deposition times.
- name: Layer-by-Layer Assembly
  sources:
  - chunk_ref: 15-SciAgents (Chunk 6:110)
    quote: Synthesize biomimetic materials with a lamellar structure using techniques
      such as electrospinning or layer-by-layer assembly.
  description: Layer-by-layer assembly is a fabrication technique for creating hierarchical
    structures in biomimetic materials. Enables precise control over material layer
    composition.
- name: 3D Printing for Scaffold Fabrication
  sources:
  - chunk_ref: 15-SciAgents (Chunk 6:484)
    quote: Fabricate the collagen-based material using advanced manufacturing techniques,
      such as 3D printing or electrospinning, to achieve the desired hierarchical,
      porous architecture.
  description: 3D printing is identified as an advanced manufacturing technique for
    fabricating collagen-based materials with hierarchical porous architecture and
    controlled microstructure.
- name: X-ray Computed Tomography (XCT)
  sources:
  - chunk_ref: 15-SciAgents (Chunk 6:736)
    quote: 'X-ray Computed Tomography (XCT): To obtain 3D images of the material''s
      internal structure and quantify pore distribution.'
  description: XCT is used for obtaining 3D images of material internal structure
    and quantifying pore distribution in hierarchical materials.
- name: Fourier Transform Infrared Spectroscopy (FTIR)
  sources:
  - chunk_ref: 15-SciAgents (Chunk 8:20-24)
    quote: 'Fourier Transform Infrared Spectroscopy (FTIR): Purpose: To confirm the
      incorporation of nanocomposites and assess their distribution within the scaffolds.'
  description: FTIR is used for identifying chemical interactions, confirming nanocomposite
    incorporation, and assessing distribution within scaffold materials. Analyzes
    characteristic peaks in infrared spectra.
- name: Molecular Dynamics (MD) Simulations
  sources:
  - chunk_ref: 15-SciAgents (Chunk 6:706-710)
    quote: MD simulations will be used to study the self-assembly and deformation
      mechanisms of collagen at the molecular level.
  description: MD simulations are used to study molecular-level self-assembly, deformation
    mechanisms, and interactions between materials. Provides insights into reversible
    deformation and mechanical properties.
- name: Monte Carlo Simulations
  sources:
  - chunk_ref: 15-SciAgents (Chunk 10:519-520)
    quote: 'Monte Carlo Simulations: To explore the configurational space and optimize
      the structure.'
  description: Monte Carlo simulations are used for exploring configurational space
    and optimizing material structures. Identifies stable and energetically favorable
    configurations.
- name: Density Functional Theory (DFT)
  sources:
  - chunk_ref: 15-SciAgents (Chunk 10:518)
    quote: 'Density Functional Theory (DFT): To investigate the electronic properties
      and binding energies.'
  description: DFT is a computational technique for investigating electronic properties,
    binding energies, and charge distribution in composite materials.
- name: Tensile Testing
  sources:
  - chunk_ref: 15-SciAgents (Chunk 8:6-10)
    quote: 'Mechanical Testing: Purpose: To evaluate the tensile strength, compression
      strength, and elasticity of the scaffolds. Method: Perform tensile testing,
      compression testing, and dynamic mechanical analysis.'
  description: Tensile testing is used to measure tensile strength, Young's modulus,
    and elongation at break. Evaluates mechanical properties of scaffolds and composite
    materials.
- name: Nanoindentation Testing
  sources:
  - chunk_ref: 15-SciAgents (Chunk 9:123-124)
    quote: Nanoindentation tests will be performed to measure the elastic modulus
      and hardness of the material.
  description: Nanoindentation is used to measure elastic modulus and hardness at
    the nanoscale. Expected values include elastic modulus around 70 GPa and hardness
    around 3 GPa.
- name: Three-Point Bending Tests
  sources:
  - chunk_ref: 15-SciAgents (Chunk 9:121-122)
    quote: Mechanical testing will be conducted using methods such as three-point
      bending tests to measure the fracture toughness.
  description: Three-point bending tests and single-edge notched bending (SENB) tests
    are used to measure fracture toughness. Target values of at least 10 MPa.m^0.5.
- name: Impact Testing
  sources:
  - chunk_ref: 15-SciAgents (Chunk 7:141-142)
    quote: Perform drop-weight impact tests or dynamic crash tests to measure the
      energy absorption capacity of the material.
  description: Drop-weight impact tests and dynamic crash tests are used to measure
    energy absorption capacity and crashworthiness of hierarchical materials.
- name: MTT Cell Viability Assay
  sources:
  - chunk_ref: 15-SciAgents (Chunk 8:124)
    quote: Assess cell proliferation rates using assays like MTT or Alamar Blue. Aim
      for a 25-35% increase in cell proliferation over a 7-day period.
  description: MTT assay is a standard method for assessing cell viability and proliferation
    rates in biocompatibility studies. Used with targets of 25-35% improvement.
- name: Live/Dead Cell Staining
  sources:
  - chunk_ref: 15-SciAgents (Chunk 8:125)
    quote: 'Cell Viability: Evaluate cell viability using live/dead staining. Aim
      for over 90% cell viability in nanocomposite-enhanced scaffolds.'
  description: Live/dead staining is used to evaluate cell viability with target of
    over 90% viability in enhanced scaffolds for biocompatibility assessment.
- name: Fluorescence Microscopy
  sources:
  - chunk_ref: 15-SciAgents (Chunk 6:500)
    quote: Use techniques such as fluorescence microscopy and biochemical assays to
      monitor changes in material properties.
  description: Fluorescence microscopy is used to monitor changes in material properties
    during biological interactions and cell culture experiments.
- name: Differential Scanning Calorimetry (DSC)
  sources:
  - chunk_ref: 15-SciAgents (Chunk 10:78)
    quote: The thermal stability of the composites will be assessed using thermogravimetric
      analysis (TGA) and differential scanning calorimetry (DSC).
  description: DSC is used alongside TGA for thermal stability assessment of composite
    materials and phase transition analysis.
- name: Four-Point Probe Measurement
  sources:
  - chunk_ref: 15-SciAgents (Chunk 10:150-152)
    quote: The electrical conductivity of the composites will be measured using four-point
      probe and Hall effect measurements.
  description: Four-point probe method measures voltage drop while passing current
    through probes to determine electrical conductivity. Expected conductivities of
    10^5 to 10^6 S/m.
- name: Hall Effect Measurement
  sources:
  - chunk_ref: 15-SciAgents (Chunk 10:150-152)
    quote: The electrical conductivity of the composites will be measured using four-point
      probe and Hall effect measurements.
  description: Hall effect measurements provide information on carrier concentration
    and mobility in conductive materials.
- name: Surface Plasmon Resonance (SPR)
  sources:
  - chunk_ref: 15-SciAgents (Chunk 10:18-19)
    quote: We hypothesize that the binding affinity between graphene and amyloid fibrils
      can be quantified using techniques such as isothermal titration calorimetry
      (ITC) or surface plasmon resonance (SPR).
  description: SPR is used to quantify binding affinity between materials with expected
    dissociation constants (Kd) in nanomolar to micromolar range.
- name: Isothermal Titration Calorimetry (ITC)
  sources:
  - chunk_ref: 15-SciAgents (Chunk 10:18-19)
    quote: We hypothesize that the binding affinity between graphene and amyloid fibrils
      can be quantified using techniques such as isothermal titration calorimetry
      (ITC) or surface plasmon resonance (SPR).
  description: ITC is used for quantifying binding affinity and thermodynamic parameters
    of molecular interactions between composite components.
- name: Chemical Vapor Deposition (CVD)
  sources:
  - chunk_ref: 15-SciAgents (Chunk 10:132-133)
    quote: 'Chemical Vapor Deposition (CVD): High-quality graphene sheets will be
      synthesized using CVD. The process involves the decomposition of a carbon-containing
      gas at high temperatures (~1000C).'
  description: CVD is the standard technique for producing high-quality graphene sheets.
    Involves methane decomposition at approximately 1000 degrees Celsius on copper
    substrate.
- name: CRISPR/Cas9 Gene Editing
  sources:
  - chunk_ref: 15-SciAgents (Chunk 7:345)
    quote: Use CRISPR/Cas9 or other gene-editing techniques to engineer cells with
      the desired signaling pathways.
  description: CRISPR/Cas9 is used for engineering cells with specific signaling pathways
    to control material properties through synthetic biology approaches.
- name: Quantitative PCR (qPCR)
  sources:
  - chunk_ref: 15-SciAgents (Chunk 10:47)
    quote: We expect that the gene circuits will allow for fine-tuning of protein
      expression with high precision, achieving desired protein concentrations within
      a range of 1-100 uM. This control can be validated through quantitative PCR
      (qPCR).
  description: qPCR is used for validating protein expression levels in synthetic
    biology applications. Enables precise measurement of gene expression for material
    production control.
- name: Goniometer Contact Angle Measurement
  sources:
  - chunk_ref: 15-SciAgents (Chunk 9:114)
    quote: The water contact angle will be measured using a goniometer. A contact
      angle greater than 150 degrees will confirm superhydrophobicity.
  description: Goniometry is the standard method for measuring water contact angle
    to assess superhydrophobicity. Target contact angles greater than 150 degrees
    indicate superhydrophobic surfaces.
- name: SPARQL Structured Query Language for KG
  sources:
  - chunk_ref: 16-KG-Agent (Chunk 1:66-68)
    quote: synergy-augmented methods can benefit from the structured search on KG
      (e.g., SPARQL) and the language understanding capacity of LLMs
  description: SPARQL is identified as a key standard for structured search on knowledge
    graphs. The paper positions SPARQL as enabling structured queries that complement
    LLM language understanding in synergy-augmented reasoning methods.
- name: LLaMA2-7B as Backbone LLM
  sources:
  - chunk_ref: 16-KG-Agent (Chunk 1:271)
    quote: we construct a high-quality instruction dataset for fine-tuning a small
      LLM (i.e., LLaMA2-7B)
  description: LLaMA2-7B is used as the standard backbone language model for instruction
    tuning in the KG-Agent framework, demonstrating that smaller open-source LLMs
    can be effective for autonomous KG reasoning.
- name: SQL Query Format for KGQA
  sources:
  - chunk_ref: 16-KG-Agent (Chunk 1:284-288)
    quote: These KGQA datasets contain the annotated SQL queries that can be executed
      to directly extract the answer entities for each question
  description: SQL is used as the query format standard in KGQA datasets, providing
    structured representations that can be grounded on knowledge graphs for reasoning
    program synthesis.
- name: Python Program Compiler for KG Execution
  sources:
  - chunk_ref: 16-KG-Agent (Chunk 1:568-569)
    quote: the KG-based executor will execute it using a program compiler. It can
      cache or operate the intermediate variables
  description: A program compiler (Python-based) serves as the execution environment
    for KG-Agent, enabling code-format function calls to be executed against the knowledge
    graph.
- name: WebQSP Benchmark Dataset
  sources:
  - chunk_ref: 16-KG-Agent (Chunk 1:676-677)
    quote: i.e., WebQSP, CWQ, and GrailQA, which are based on Freebase
  - chunk_ref: 17-KG_Reasoning (Chunk 1:534)
    quote: Commonly used benchmarks for KGEs' evaluation, such as WN18RR, FB15k-237
      and NELL
  description: Merged from 2 sources. WebQSP (Web Questions Semantic Parsing) is a
    standard benchmark dataset for KGQA based on Freebase, used for evaluating in-domain
    KG reasoning performance.
- name: CWQ Complex WebQuestions Dataset
  sources:
  - chunk_ref: 16-KG-Agent (Chunk 1:676-677)
    quote: i.e., WebQSP, CWQ, and GrailQA, which are based on Freebase
  description: Complex WebQuestions (CWQ) is a challenging benchmark dataset extending
    WebQSP with multi-hop and constraint-based questions for evaluating complex KGQA.
- name: GrailQA Benchmark Dataset
  sources:
  - chunk_ref: 16-KG-Agent (Chunk 1:677)
    quote: i.e., WebQSP, CWQ, and GrailQA, which are based on Freebase
  description: 'GrailQA is a comprehensive benchmark dataset for evaluating generalization
    in KGQA across three levels: i.i.d., compositional, and zero-shot settings.'
- name: KQA Pro Wikidata-based Dataset
  sources:
  - chunk_ref: 16-KG-Agent (Chunk 1:678)
    quote: and KQA Pro, which is based on Wikidata
  description: KQA Pro is a benchmark dataset based on Wikidata requiring multiple
    reasoning capabilities including compositional reasoning, multi-hop reasoning,
    and quantitative comparison.
- name: OpenAI API for LLM Access
  sources:
  - chunk_ref: 16-KG-Agent (Chunk 2:633-635)
    quote: When evaluating the performance of Davinci-003, ChatGPT, and GPT4, we use
      the latest February version of APIs from OpenAI
  description: OpenAI APIs (gpt-3.5-turbo-instruct, gpt-3.5-turbo, gpt-4) serve as
    standard interfaces for accessing commercial LLMs for KGQA evaluation and baseline
    comparison.
- name: MetaQA Movie Domain KG
  sources:
  - chunk_ref: 16-KG-Agent (Chunk 2:469-470)
    quote: we further select the MetaQA (Zhang et al., 2018), which is based on a
      domain-specific movie KG
  description: MetaQA is a domain-specific benchmark based on a movie knowledge graph,
    used to evaluate generalizability of KG reasoning methods to specialized domains.
- name: HermiT Description Logic Reasoner
  sources:
  - chunk_ref: 17-KG_Reasoning (Chunk 1:44)
    quote: HermiT [Glimm et al., 2014] is a classic description logic reasoner for
      OWL ontologies
  description: HermiT is referenced as a classic implementation of description logic
    reasoning for OWL ontologies, representing the standard tool for symbolic ontology
    reasoning.
- name: RDFox KG Storage with Datalog
  sources:
  - chunk_ref: 17-KG_Reasoning (Chunk 1:45)
    quote: RDFox [Nenov et al., 2015] is a famous KG storage supporting Datalog rule
      reasoning
  description: RDFox is identified as a prominent knowledge graph storage system supporting
    Datalog rule reasoning, combining efficient storage with logical inference capabilities.
- name: SROIQ Description Logic
  sources:
  - chunk_ref: 17-KG_Reasoning (Chunk 1:87-88)
    quote: It is based on the SROIQ DL [Horrocks et al., ]. OWL 2 provides rich expressive
      power
  description: SROIQ is the description logic foundation for OWL 2, providing the
    formal logical basis for the web ontology language standard.
- name: OWL 2 Schema Language
  sources:
  - chunk_ref: 17-KG_Reasoning (Chunk 1:86-92)
    quote: the Web Ontology Language OWL 2, which is based on Description Logics (DLs),
      is a key standard schema language of KGs
  description: OWL 2 is identified as the key standard schema language for knowledge
    graphs, supporting rich expressive power including datatypes and rules for defining
    class hierarchies and complex relations.
- name: TransE Embedding Method
  sources:
  - chunk_ref: 17-KG_Reasoning (Chunk 1:102)
    quote: Many successful KGE methods, such as TransE [Bordes et al., 2013], ComplEx
      [Trouillon et al., 2016] and RotatE [Sun et al., 2019]
  - chunk_ref: 17-KG_Reasoning (Chunk 1:102)
    quote: Many successful KGE methods, such as TransE [Bordes et al., 2013], ComplEx
      [Trouillon et al., 2016] and RotatE [Sun et al., 2019]
  description: Merged from 2 sources. TransE is one of the foundational KG embedding
    methods, using translation-based scoring in Euclidean space where h+r should approximate
    t for valid triples.
- name: ComplEx Embedding Method
  sources:
  - chunk_ref: 17-KG_Reasoning (Chunk 1:102)
    quote: Many successful KGE methods, such as TransE [Bordes et al., 2013], ComplEx
      [Trouillon et al., 2016] and RotatE [Sun et al., 2019]
  description: ComplEx is a KG embedding method using complex vector space to model
    asymmetric relations, extending beyond real-valued embeddings for richer relational
    modeling.
- name: FB15k-237 Benchmark Dataset
  sources:
  - chunk_ref: 17-KG_Reasoning (Chunk 1:535)
    quote: Commonly used benchmarks for KGEs' evaluation, such as WN18RR, FB15k-237
      and NELL
  description: FB15k-237 is a standard benchmark dataset for KGE evaluation derived
    from Freebase, addressing test leakage issues in the original FB15k.
- name: NELL Knowledge Base
  sources:
  - chunk_ref: 17-KG_Reasoning (Chunk 1:535)
    quote: Commonly used benchmarks for KGEs' evaluation, such as WN18RR, FB15k-237
      and NELL
  description: NELL (Never-Ending Language Learning) is referenced as a benchmark
    knowledge base for evaluating knowledge graph embedding methods.
- name: Prolog Logic Programming Language
  sources:
  - chunk_ref: 17-KG_Reasoning (Chunk 1:419)
    quote: Conventional theorem proving methods are based on different logic languages,
      such as Prolog, Datalog, and OWL
  description: Prolog is identified as a foundational logic programming language for
    theorem proving, serving as the basis for differentiable theorem proving approaches
    like NTP.
- name: Datalog Rule Language
  sources:
  - chunk_ref: 17-KG_Reasoning (Chunk 1:419)
    quote: Conventional theorem proving methods are based on different logic languages,
      such as Prolog, Datalog, and OWL
  description: Datalog is referenced as a standard rule language for logic-based reasoning
    and theorem proving in knowledge graph systems.
- name: TensorLog Differentiable Probabilistic Logic
  sources:
  - chunk_ref: 17-KG_Reasoning (Chunk 1:470-472)
    quote: They are inspired by TensorLog [Cohen et al., 2020], a differentiable probabilistic
      logic. Tensorlog establishes a connection between inference using first-order
      rules and sparse matrix multiplication
  description: TensorLog is a differentiable probabilistic logic system enabling first-order
    logical inference through sparse matrix operations, foundational for differentiable
    rule mining.
- name: AnyBURL Rule Mining System
  sources:
  - chunk_ref: 17-KG_Reasoning (Chunk 1:444)
    quote: Conventional methods like AMIE [Galarraga et al., 2015] and AnyBURL [Meilicke
      et al., 2019] are symbolic-based
  description: AnyBURL is a bottom-up rule learning system for knowledge graph completion,
    representing anytime symbolic rule mining approaches.
- name: UML Class Diagram for Domain Ontology
  sources:
  - chunk_ref: 18-Multi-Agent (Chunk 1:816-818)
    quote: Our domain ontology is represented as a conceptual model in terms of a
      class diagram of the Unified Modeling Language (UML2)
  description: UML2 class diagrams are used as the standard notation for representing
    domain ontology models, organizing concepts as classes with generalizations and
    associations.
- name: LangChain Python Framework
  sources:
  - chunk_ref: 18-Multi-Agent (Chunk 1:348-352)
    quote: Some of these recent multi-agent systems as well as further related projects
      such as GORILLA or VOYAGER are built upon the LANGCHAIN Python framework
  description: LangChain is identified as a foundational Python framework for building
    multi-agent systems, providing predefined components for agent types, prompt templates,
    and tool/data integration.
- name: Vector Databases for Agent Memory
  sources:
  - chunk_ref: 18-Multi-Agent (Chunk 2:135-136)
    quote: For optimal processing by LLMs, unstructured text is typically stored in
      vector databases like PINECONE or CHROMA
  description: Vector databases (Pinecone, Chroma) are standard tools for storing
    unstructured text data in multi-agent systems, enabling semantic search through
    vector embeddings.
- name: Hugging Face Model Platform
  sources:
  - chunk_ref: 18-Multi-Agent (Chunk 2:170-171)
    quote: Platforms like HUGGING FACE even offer access to numerous models provided
      by the global machine learning community
  description: Hugging Face is identified as the standard platform for accessing foundation
    models, providing the global ML community's models for multi-agent system integration.
- name: Wolfram Alpha Reasoning Tool
  sources:
  - chunk_ref: 18-Multi-Agent (Chunk 2:116-117)
    quote: For instance, platforms like WOLFRAM ALPHA empower agents with advanced
      computational skills
  description: Wolfram Alpha is referenced as an example reasoning tool that enhances
    agent computational intelligence capabilities in multi-agent systems.
- name: AutoGPT Multi-Agent System
  sources:
  - chunk_ref: 18-Multi-Agent (Chunk 1:336)
    quote: Exemplary but representative autonomous multi-agent systems are AUTOGPT,
      BABYAGI, SUPERAGI, HUGGINGGPT, CAMEL, AGENTGPT and METAGPT
  - chunk_ref: 18-Multi-Agent (Chunk 1:338)
    quote: Exemplary but representative autonomous multi-agent systems are AUTOGPT,
      BABYAGI, SUPERAGI, HUGGINGGPT, CAMEL, AGENTGPT and METAGPT
  description: Merged from 2 sources. AutoGPT is identified as a representative autonomous
    multi-agent system providing general-purpose task management with generic agent
    types and collaboration mechanics.
- name: GPT-3 Foundation Model
  sources:
  - chunk_ref: 18-Multi-Agent (Chunk 1:317)
    quote: The advent and widespread use of large language models (LLMs) like GPT-3
      have opened up new opportunities for creating increasingly sophisticated and
      human-like AI systems
  description: GPT-3 is referenced as a foundational large language model that enabled
    the emergence of autonomous multi-agent systems with advanced reasoning capabilities.
- name: API-based Resource Access
  sources:
  - chunk_ref: 18-Multi-Agent (Chunk 2:174-176)
    quote: Access to LLMs, as well as associated resources such as tools, foundation
      models, and external data resources, is typically facilitated through Application
      Programming Interfaces (APIs)
  description: APIs are the standard mechanism for accessing LLMs and contextual resources
    in multi-agent systems, with access details integrated into the interaction layer.
- name: GPT Transformer Architecture
  sources:
  - chunk_ref: 19-Graph_of_Thoughts (Chunk 1:37-38)
    quote: Recent years saw a rapid development of models primarily based on the decoder-only
      Transformer variant, such as GPT, PaLM, or LLaMA
  description: The decoder-only Transformer architecture is identified as the foundation
    for major LLMs (GPT, PaLM, LLaMA) that enable modern prompting and reasoning frameworks.
- name: GPT-4 Language Model
  sources:
  - chunk_ref: 19-Graph_of_Thoughts (Chunk 1:95-96)
    quote: This enables rapid prototyping of novel prompting ideas using GoT, while
      experimenting with different models such as GPT-3.5, GPT-4, or Llama-2
  description: GPT-4 is supported as one of the LLM backends for the Graph of Thoughts
    framework, enabling experimentation with different model capabilities.
- name: GPT-3.5 Language Model
  sources:
  - chunk_ref: 19-Graph_of_Thoughts (Chunk 1:95)
    quote: experimenting with different models such as GPT-3.5, GPT-4, or Llama-2
  description: GPT-3.5 (ChatGPT) is used as the primary LLM for evaluation in the
    Graph of Thoughts framework due to cost considerations while maintaining quality.
- name: Llama-2 Open Source LLM
  sources:
  - chunk_ref: 19-Graph_of_Thoughts (Chunk 1:96)
    quote: experimenting with different models such as GPT-3.5, GPT-4, or Llama-2
  description: Llama-2 is supported as an open-source LLM option in the GoT framework,
    though noted to be slower and typically performing worse than GPT-3.5 in experiments.
- name: GitHub Repository for GoT
  sources:
  - chunk_ref: 19-Graph_of_Thoughts (Chunk 1:31)
    quote: 'Website & code: https://github.com/spcl/graph-of-thoughts'
  description: The GoT framework is released as open-source on GitHub, providing extensible
    APIs for implementing different prompting schemes and thought transformations.
