field: limitations
aggregated_at: '2026-01-01T16:22:28.234938'
batches_merged: 7
patterns_input: 152
patterns_output: 149
patterns:
- name: Mismatch Between Bunge's Ontology and Conceptual Modeling Requirements
  sources:
  - chunk_ref: 01-UFO (Chunk 1:155-161)
    quote: it soon became clear that there was a mismatch between the purposes for
      which Bunge's ontology was developed and the requirements of ontological foundations
      for conceptual modeling
  description: UFO was developed because existing ontological frameworks like Bunge-Wand-Weber
    had fundamental limitations when applied to conceptual modeling. BWW predictions
    conflicted with modeler intuitions and practical knowledge, particularly regarding
    reified relationships which Jackendoff's semantic structures and practitioners
    accepted but BWW rejected.
- name: GFO Theory of Relations Subject to Bradley's Regress
  sources:
  - chunk_ref: 01-UFO (Chunk 1:186-190)
    quote: the GFO theory of relations is subject to the so-called Bradley's Regress
      and, hence, it can only be instantiated by infinite (logical) models. This feature
      renders it ill-suited for most conceptual modeling applications
  description: 'The General Formal Ontology (GFO) has a fundamental theoretical limitation:
    its theory of relations leads to Bradley''s Regress, requiring infinite logical
    models for instantiation. This makes it practically unusable for conceptual modeling
    applications which require finite, tractable models.'
- name: DOLCE Lacks Universal Category for Types
  sources:
  - chunk_ref: 01-UFO (Chunk 1:185-186)
    quote: DOLCE does not include universal as a category (DOLCE was designed as an
      ontology of particulars)
  - chunk_ref: 23-UFO_Story_Ontological_Foundations (Chunk 1:156-158)
    quote: In that respect, GFO's theory of universals still does not recognize these
      notions and DOLCE does not include universal as a category (DOLCE was designed
      as an ontology of particulars)
  description: Merged from 2 sources. DOLCE foundational ontology is limited in scope
    because it was designed only for particulars (individual entities) and does not
    include a category for universals/types. This limits its applicability for conceptual
    modeling which requires both particulars and types.
- name: GFO Lacks Theory of Universal Types
  sources:
  - chunk_ref: 01-UFO (Chunk 1:184-185)
    quote: GFO's theory of universals still does not recognize these notions
  description: GFO ontology lacks recognition of important conceptual modeling notions
    like types, roles, phases/states, and mixins that are pervasive in the conceptual
    modeling literature but have no precise definitions or consensus in GFO.
- name: DOLCE Lacks Theory of Relational Properties
  sources:
  - chunk_ref: 01-UFO (Chunk 1:186-188)
    quote: DOLCE still does not include a theory of particularized relational properties
      (relational qualities)
  description: DOLCE foundational ontology is limited because it lacks a theory of
    particularized relational properties (relational qualities), which are essential
    for modeling relationships and their properties in conceptual modeling applications.
- name: Incomplete Constitution Theory Due to Missing Grounding Theory
  sources:
  - chunk_ref: 01-UFO (Chunk 1:987-989)
    quote: we advocate that a complete theory of constitution requires a proper theory
      of grounding. For this reason, we postpone proposing a complete theory for the
      former relation in UFO until we can fully advance a complete theory of the latter
      relation
  description: UFO acknowledges an incomplete theory of constitution relationships
    because it requires a proper theory of grounding which has not yet been fully
    developed. Constitution relies on grounding for explaining asymmetric dependence
    and derivation of properties from constituents.
- name: OWL Entailment is Undecidable
  sources:
  - chunk_ref: 02-KG (Chunk 3:895-901)
    quote: 'given two graphs, deciding if the first entails the second - per the notion
      of entailment we have defined and for all of the ontological features listed
      - is undecidable: no (finite) algorithm for such entailment can exist that halts
      on all inputs'
  description: 'A fundamental computational limitation of OWL ontology reasoning:
    full entailment checking is undecidable. No finite algorithm can correctly determine
    entailment for all possible inputs with all OWL features enabled. This forces
    practical systems to use incomplete reasoning or restricted feature sets.'
- name: Rules Cannot Fully Capture OWL Semantics
  sources:
  - chunk_ref: 02-KG (Chunk 4:45-49)
    quote: these rules are likewise incomplete as such rules cannot fully capture
      negation (e.g., Complement), existentials (e.g., Some Values), universals (e.g.,
      All Values), or counting (e.g., Cardinality and Qualified Cardinality)
  description: Rule-based reasoning systems (like OWL 2 RL/RDF) have inherent expressiveness
    limitations. They cannot fully capture important ontological features including
    negation, existential quantification, universal quantification, and cardinality
    constraints.
- name: DL Reasoning Prohibitive Computational Complexity
  sources:
  - chunk_ref: 02-KG (Chunk 4:203-209)
    quote: Due to their prohibitive computational complexity - where for example,
      disjunction may lead to an exponential number of branching possibilities - such
      reasoning strategies are not typically applied in the case of large-scale data
  description: Description Logic tableau-based reasoning, while complete for expressive
    DLs, has prohibitive computational complexity. Disjunctions lead to exponential
    branching possibilities, making it impractical for large-scale knowledge graph
    data. This forces a tradeoff between expressiveness and scalability.
- name: TransE Cannot Handle Multiple Target Relations
  sources:
  - chunk_ref: 02-KG (Chunk 5:25-29)
    quote: TransE can be too simplistic; for example, in Figure 24, bus not only transforms
      San Pedro to Moon Valley, but also to Arica, Calama, and so forth. TransE will,
      in this case, aim to give similar vectors to all such target locations, which
      may not be feasible
  description: 'The TransE embedding model has a fundamental limitation: it cannot
    properly handle one-to-many or many-to-many relations. When an entity has multiple
    objects for the same relation, TransE tries to assign similar vectors to all targets,
    which may be infeasible and conflicts with other edges.'
- name: TransE Assigns Zero Vector to Cyclical Relations
  sources:
  - chunk_ref: 02-KG (Chunk 5:34-36)
    quote: TransE will also tend to assign cyclical relations a zero vector, as the
      directional components will tend to cancel each other out
  description: TransE has a mathematical limitation with symmetric or cyclical relations.
    For relations where the source and target can be the same or bidirectional, the
    directional translation components cancel out, resulting in a zero vector that
    fails to capture the relation's meaning.
- name: DistMult Cannot Capture Edge Direction
  sources:
  - chunk_ref: 02-KG (Chunk 5:204-205)
    quote: the plausibility of s p o will always be equal to that of o p s; in other
      words, DistMult does not consider edge direction
  description: 'The DistMult embedding model has a fundamental symmetry limitation:
    its scoring function treats edges as undirected. The plausibility of an edge is
    identical whether read forward or backward, making it unable to model directed
    or asymmetric relations.'
- name: Materialisation Can Become Unfeasibly Large
  sources:
  - chunk_ref: 02-KG (Chunk 4:60-64)
    quote: depending on the rules and the data, the materialised graph may become
      unfeasibly large to manage
  description: Rule-based materialisation (forward chaining) for reasoning has a scalability
    limitation. Depending on the rules and data, the materialised graph containing
    all inferred facts may grow to sizes that are impractical to store and manage.
- name: Graph Parallel Frameworks Limited Expressiveness
  sources:
  - chunk_ref: 02-KG (Chunk 4:557-561)
    quote: 'such frameworks - based on message passing between neighbours - have limitations:
      not all types of analytics can be expressed in such frameworks'
  description: Graph parallel frameworks using systolic abstraction (like Pregel,
    GraphX) have expressiveness limitations. Not all graph analytics can be expressed
    through neighbor-based message passing. They are proven to be only as powerful
    as the Weisfeiler-Lehman graph isomorphism test for distinguishing structures.
- name: Shapes Languages Semantic Problems with Recursion and Negation
  sources:
  - chunk_ref: 02-KG (Chunk 2:336-357)
    quote: shapes languages that freely combine recursion and negation may lead to
      semantic problems, depending on how their semantics are defined
  description: Validating schema languages (like SHACL, ShEx) face semantic paradoxes
    when combining recursion with negation (the barber paradox). A shape can reference
    itself with negation, creating undecidable conformance. Various workarounds exist
    (stratification, partial assignments, stable models) but limit expressiveness.
- name: Neural Embedding Models Lack Interpretability
  sources:
  - chunk_ref: 02-KG (Chunk 5:653-659)
    quote: such models are often difficult to explain or understand. For example,
      knowledge graph embeddings might predict the edge SCL flight ARI as being highly
      plausible, but they will not provide an interpretable model to help understand
      why
  description: Knowledge graph embeddings and neural models suffer from a fundamental
    interpretability limitation. They can predict plausibility but cannot explain
    why - the reasoning is encoded in opaque matrices of learned parameters that do
    not provide human-understandable justification.
- name: Out-of-Vocabulary Problem for Embeddings
  sources:
  - chunk_ref: 02-KG (Chunk 5:660-665)
    quote: Such approaches also suffer from the out-of-vocabulary problem, where they
      are unable to provide results for edges involving previously unseen nodes or
      edges
  description: Embedding-based approaches cannot handle novel entities not seen during
    training. If a new node is added to the graph, the model lacks an entity embedding
    for it and must be retrained before making predictions about edges involving that
    node.
- name: Differentiable Rule Mining Limited to Path-Like Rules
  sources:
  - chunk_ref: 02-KG (Chunk 5:907-909)
    quote: These differentiable rule mining techniques are, however, currently limited
      to learning path-like rules
  - chunk_ref: 02-Knowledge_Graphs (Chunk 6:6-10)
    quote: These differentiable rule mining techniques are, however, currently limited
      to learning path-like rules.
  description: Merged from 2 sources. Neural approaches to rule mining using bidirectional
    recurrent neural networks can learn sequences of relations and their confidences,
    but are constrained to path-like rule structures. This limits their ability to
    capture more complex relational patterns that may exist in knowledge graphs.
- name: RDF* Limited Context Flexibility
  sources:
  - chunk_ref: 02-KG (Chunk 3:82-92)
    quote: The least flexible option is RDF*, which, in the absence of an edge id,
      does not permit different groups of contextual values to be assigned to an edge
  description: RDF* is the least flexible option among higher-arity representations
    for modeling context. Without an edge id, it cannot associate different groups
    of contextual values to the same edge. For example, representing that a president
    served from 2006-2010 AND 2014-2018 requires creating separate nodes rather than
    annotating a single edge with multiple temporal contexts.
- name: Human Collaboration Incurs High Costs
  sources:
  - chunk_ref: 02-Knowledge_Graphs (Chunk 6:146-154)
    quote: Though human involvement incurs high costs [401], some prominent knowledge
      graphs have been primarily based on direct contributions from human editors
  description: Creating and enriching knowledge graphs through direct human contributions
    is costly. Additionally, human-based approaches face drawbacks including human
    error, disagreement, bias, and vandalism. Successful collaborative creation raises
    challenges concerning licensing, tooling, and culture.
- name: Text Extraction Precision-Recall Tradeoff
  sources:
  - chunk_ref: 02-Knowledge_Graphs (Chunk 6:170-176)
    quote: extracting such information with high precision and recall for the purposes
      of creating or enriching a knowledge graph is a non-trivial challenge
  description: Extracting knowledge from text corpora (newspapers, books, articles,
    social media, etc.) with both high precision and high recall simultaneously is
    inherently difficult. Techniques from NLP and Information Extraction can be applied
    but do not fully solve this limitation.
- name: Named Entity Recognition Requires Manual Labeling or Seeds
  sources:
  - chunk_ref: 02-Knowledge_Graphs (Chunk 6:214-220)
    quote: Supervised methods [46, 160, 307] require manually labelling all entity
      mentions in a training corpus, whereas bootstrapping-based approaches...require
      a small set of seed examples
  description: 'NER approaches have inherent limitations: supervised methods require
    expensive manual labeling of training data, while bootstrapping approaches require
    seed examples. Distant supervision uses known entities as seeds but may introduce
    noise from imprecise matching.'
- name: Entity Linking Ambiguity Challenge
  sources:
  - chunk_ref: 02-Knowledge_Graphs (Chunk 6:241-257)
    quote: First, there may be multiple ways to mention the same entity...Secondly,
      the same mention in different contexts can refer to distinct entities
  description: 'Entity linking faces two fundamental challenges: (1) the same entity
    can be mentioned in multiple ways (aliases, multilingual labels), and (2) the
    same mention can refer to different entities in different contexts. Both require
    disambiguation which may not always be accurate.'
- name: Wrapper-Based Extraction Brittleness
  sources:
  - chunk_ref: 02-Knowledge_Graphs (Chunk 6:457-464)
    quote: the traditional approach was to define such wrappers manually - a task
      for which a variety of declarative languages and tools have been defined - such
      approaches are brittle to changes in a website's layout
  description: Manual wrapper-based extraction from markup documents is fragile and
    breaks when website layouts change. Semi-automatic wrapper induction attempts
    to address this but still relies on structural patterns that may evolve.
- name: Web Tables Human vs Machine Readability Conflict
  sources:
  - chunk_ref: 02-Knowledge_Graphs (Chunk 6:515-522)
    quote: web tables are designed to enhance human readability, which often conflicts
      with machine readability. Many web tables are used for layout and page structure
  description: Web tables prioritize human readability over machine readability. Many
    tables serve layout purposes rather than containing data. Data tables may have
    column spans, row spans, inner tables, or be split vertically, requiring normalization
    before extraction.
- name: Knowledge Graph Incompleteness is Inherent
  sources:
  - chunk_ref: 02-Knowledge_Graphs (Chunk 7:92-94)
    quote: Coverage refers to avoiding the omission of domain-relevant elements, which
      otherwise may yield incomplete query results or entailments, biased models,
      etc.
  - chunk_ref: 02-Knowledge_Graphs (Chunk 15:642-654)
    quote: Knowledge graphs are generally incomplete - in fact, one of the main applications
      of hypothesis mining is to try to improve the completeness of the knowledge
      graph
  description: Merged from 2 sources. Knowledge graphs suffer from inherent incompleteness,
    which is a fundamental limitation. The Closed World Assumption is unsuitable because
    it assumes any edge not entailed is false. Even the Partial Completeness Assumption
    is only a heuristic to address this limitation.
- name: Completeness Measurement Requires Ideal Reference
  sources:
  - chunk_ref: 02-Knowledge_Graphs (Chunk 7:109-117)
    quote: Measuring completeness directly is non-trivial as it requires knowledge
      of a hypothetical ideal knowledge graph that contains all the elements that
      the knowledge graph should ideally represent
  description: Assessing completeness of a knowledge graph requires comparison against
    an ideal complete graph that does not exist. Practical strategies involve gold
    standards or measuring extraction recall, but these are approximations.
- name: Representativeness Bias in Knowledge Graphs
  sources:
  - chunk_ref: 02-Knowledge_Graphs (Chunk 7:120-148)
    quote: Representativeness...focuses on assessing high-level biases in what is
      included/excluded from the knowledge graph...Examples of data biases include
      geographic biases
  description: 'Knowledge graphs contain systematic biases: geographic biases under-representing
    certain regions, linguistic biases under-representing certain languages, social
    biases under-representing certain demographics. Schema biases may result from
    biased data. These biases can have adverse real-world effects.'
- name: Timeliness vs Accuracy Tradeoff
  sources:
  - chunk_ref: 02-Knowledge_Graphs (Chunk 7:68-86)
    quote: a knowledge graph may be semantically accurate now, but may quickly become
      inaccurate (outdated) if no procedures are in place to keep it up-to-date in
      a timely manner
  description: Knowledge graphs face a timeliness limitation where data becomes stale
    relative to underlying sources. Update frequency may lag behind real-world changes,
    causing temporal accuracy issues.
- name: Semantic Accuracy Assessment is Challenging
  sources:
  - chunk_ref: 02-Knowledge_Graphs (Chunk 7:50-65)
    quote: Assessing the level of semantic inaccuracies is challenging. While one
      option is to apply manual verification, an automatic option may be to check
      the stated relation against several sources
  description: Determining semantic accuracy (whether data correctly represents real-world
    phenomena) is difficult. Manual verification is expensive; automatic cross-source
    validation may not always be feasible or reliable.
- name: Inconsistency Repair is Non-Trivial
  sources:
  - chunk_ref: 02-Knowledge_Graphs (Chunk 7:526-545)
    quote: techniques are also required to repair such inconsistencies, which itself
      is not a trivial task. In the simplest case, we may have an instance of two
      disjoint classes
  description: Repairing logical inconsistencies in knowledge graphs is complex because
    determining which conflicting edge to remove requires additional knowledge about
    correctness. One edge can be involved in many inconsistencies, and one inconsistency
    can involve many edges, complicating automated repair.
- name: Complex Graph Pattern Query Intractability
  sources:
  - chunk_ref: 02-Knowledge_Graphs (Chunk 8:120-134)
    quote: the problem of evaluating such queries is known to be intractable [16].
      Perhaps for this reason, public services offering such a protocol...have been
      found to often exhibit downtimes, timeouts, partial results, slow performance
  description: Evaluating complex graph patterns is computationally intractable in
    the worst case. Public SPARQL endpoints frequently exhibit downtimes, timeouts,
    partial results, and slow performance due to this complexity.
- name: Edge Pattern Queries Require Statistics for Optimization
  sources:
  - chunk_ref: 02-Knowledge_Graphs (Chunk 8:88-98)
    quote: This issue is further aggravated if the client does not have access to
      statistics about the knowledge graph in order to plan how to best perform the
      join
  description: Edge pattern queries may transfer irrelevant intermediate results.
    Without statistics about the knowledge graph, clients cannot optimize query planning,
    leading to inefficient joins and excessive data transfer.
- name: Node Lookup Protocol Limitations
  sources:
  - chunk_ref: 02-Knowledge_Graphs (Chunk 8:53-62)
    quote: such an approach may not be feasible if no starting node is declared (e.g.,
      if all nodes are variables), if the node lookup service does not return incoming
      edges, etc.
  description: 'Node lookup protocols for accessing knowledge graphs have structural
    limitations: they require known starting nodes, may not return incoming edges,
    and often transfer irrelevant data. This affects query efficiency and flexibility.'
- name: Privacy-Utility Tradeoff in Anonymization
  sources:
  - chunk_ref: 02-Knowledge_Graphs (Chunk 8:409-417)
    quote: These approaches require information loss for stronger guarantees of privacy;
      which to choose is thus heavily application dependent
  description: Anonymization techniques for knowledge graphs (k-anonymity, l-diversity,
    differential privacy) inherently require information loss. Stronger privacy guarantees
    mean greater utility loss. No single approach works for all applications.
- name: Neighborhood Attacks on Graph Anonymization
  sources:
  - chunk_ref: 02-Knowledge_Graphs (Chunk 8:336-352)
    quote: neighbourhood attacks [581] - using information about neighbours - can
      also break k-anonymity, where we also suppress the day and time of the flight
  description: Graph structure enables neighborhood attacks that can break traditional
    anonymization. Attackers can use information about connected nodes to deanonymize
    individuals, requiring graph-specific anonymization guarantees like k-isomorphic
    neighbor anonymity or k-automorphism.
- name: Encryption-Based Access Control Efficiency
  sources:
  - chunk_ref: 02-Knowledge_Graphs (Chunk 8:299-305)
    quote: A key limitation of this approach, however, is that it requires attempting
      to decrypt all edges to find all possible solutions
  description: Fine-grained encryption of knowledge graph edges for access control
    has efficiency limitations. Finding query solutions requires attempting decryption
    on all edges, which does not scale well for large graphs.
- name: Scalability Challenge for Knowledge Graph Techniques
  sources:
  - chunk_ref: 02-Knowledge_Graphs (Chunk 9:19-27)
    quote: more general challenges for knowledge graphs include scalability, particularly
      for deductive and inductive reasoning; quality, not only in terms of data, but
      also the models induced
  description: Scalability remains a fundamental challenge for knowledge graphs, especially
    for reasoning tasks. Quality issues extend beyond data to include the quality
    of induced models. Diversity (multi-modal, contextual data), dynamicity (temporal,
    streaming), and usability are additional ongoing challenges.
- name: Unsolved Challenges as Ongoing Dimensions
  sources:
  - chunk_ref: 02-Knowledge_Graphs (Chunk 9:27-31)
    quote: Though techniques are continuously being proposed to address precisely
      these challenges, they are unlikely to ever be completely solved; rather they
      serve as dimensions along which knowledge graphs, and their techniques, tools,
      etc., will continue to mature
  description: Fundamental challenges like scalability, quality, diversity, dynamicity,
    and usability are recognized as perpetual rather than solvable problems. They
    represent ongoing dimensions of improvement rather than problems with definitive
    solutions.
- name: Knowledge Graph Definition Ambiguity
  sources:
  - chunk_ref: 02-Knowledge_Graphs (Chunk 13:116-124)
    quote: Given that knowledge graphs were gaining more and more attention in the
      academic literature, formal definitions were becoming a necessity in order to
      precisely characterise what they were, how they were structured, how they could
      be used
  description: The term 'knowledge graph' lacks a universally accepted formal definition.
    At least four distinct categories of definitions exist in academic literature,
    creating confusion and limiting precise scientific discourse about knowledge graphs.
- name: Category I Definition Fails to Distinguish from Graph Databases
  sources:
  - chunk_ref: 02-Knowledge_Graphs (Chunk 13:140-142)
    quote: 'Though simple, the Category I definition raises some doubts: How is a
      knowledge graph different from a graph (database)? Where does knowledge come
      into play?'
  description: Simple definitions of knowledge graphs (as graphs with entities and
    relationships) fail to distinguish them from ordinary graph databases or explain
    what makes them 'knowledge' graphs, limiting their conceptual utility.
- name: Category III Definitions May Not Match Practice
  sources:
  - chunk_ref: 02-Knowledge_Graphs (Chunk 13:236-244)
    quote: While having a specific, technical definition for knowledge graphs provides
      a more solid grounding for their study, as Bergman remarks, many of these definitions
      do not seem to fit the current practice of knowledge graphs
  description: Highly technical Category III definitions provide precision but may
    not reflect actual practice. It remains unclear which definitions the Google Knowledge
    Graph itself would meet, and many criteria are orthogonal to knowledge graph embedding
    work.
- name: Early Knowledge Graphs Limited by Computational Resources
  sources:
  - chunk_ref: 02-Knowledge_Graphs (Chunk 13:87-95)
    quote: 'papers from 1970-2000 tend to have worked with small graphs, which contrasts
      with modern practice where knowledge graphs can reach scales of millions or
      billions of nodes: during this period, computational resources were more limited'
  description: Historical knowledge graph research was constrained to small-scale
    graphs due to computational limitations. This restricted both practical applicability
    and development of techniques now needed for modern billion-node knowledge graphs.
- name: DL Entailment Undecidability with Full Expressiveness
  sources:
  - chunk_ref: 02-Knowledge_Graphs (Chunk 14:410-418)
    quote: the problem of deciding entailment for knowledge bases expressed in the
      DL composed of the unrestricted use of all of the axioms of Table 7 combined
      is undecidable
  description: Description Logic entailment using all available axiom types unrestricted
    is undecidable (reducible from Halting Problem). Practical DLs must restrict feature
    usage, creating tradeoffs between expressivity and computational complexity of
    reasoning.
- name: OWL Translation Limitations
  sources:
  - chunk_ref: 02-Knowledge_Graphs (Chunk 14:471-473)
    quote: Hence only a subset of graphs can be translated into well-formed DL ontologies
  description: Not all knowledge graphs can be translated into well-formed Description
    Logic or OWL ontologies. Some graph patterns (like property hierarchies) cannot
    be expressed in DL syntax, limiting interoperability between graph models and
    formal ontology languages.
- name: Knowledge Graph Embedding Full Expressiveness Requirements
  sources:
  - chunk_ref: 02-Knowledge_Graphs (Chunk 14:878-886)
    quote: TuckER with dimensions dr = |L| and de = |V| trivially satisfies full expressivity
      since its core tensor then has sufficient capacity to store the full one-hot
      encoding of any graph
  description: Achieving full expressiveness in knowledge graph embeddings (correctly
    partitioning any positive/negative edge sets) often requires impractically high
    dimensions. While useful as a theoretical property, the dimensions needed for
    full expressivity are often impractical or undesirable in practice.
- name: GNN Expressivity Limitation for Node Distinction
  sources:
  - chunk_ref: 02-Knowledge_Graphs (Chunk 15:517-527)
    quote: GNNs relying solely on the neighbourhood of each node have limited expressivity
      in terms of their ability to distinguish nodes and graphs
  description: Graph Neural Networks that rely only on local neighborhood information
    have fundamental expressivity limitations. They cannot distinguish certain nodes
    or graph structures that share identical local neighborhoods, limiting their applicability
    for fine-grained classification tasks. NRecGNNs have expressiveness similar to
    ALCQ Description Logic.
- name: Agent Hallucination and Error Propagation
  sources:
  - chunk_ref: 03-PROV-AGENT (Chunk 1:21-25)
    quote: agents can hallucinate or reason incorrectly, propagating errors when one
      agent's output becomes another's input
  description: AI agents, especially those based on LLMs, can generate hallucinated
    or incorrect outputs. These errors propagate through agentic workflows when one
    agent's output feeds into another agent's input, compounding errors and making
    it difficult to assess result correctness.
- name: Non-Deterministic Agent Behavior
  sources:
  - chunk_ref: 03-PROV-AGENT (Chunk 1:81-84)
    quote: agentic workflows are non-deterministic, shaped by near real-time data,
      adaptive decisions, and evolving interactions
  description: Unlike traditional workflows with static, deterministic paths, agentic
    workflows exhibit non-deterministic behavior. This is shaped by real-time data,
    adaptive decisions, and evolving interactions, making them harder to trace, reproduce,
    and validate.
- name: Traditional Provenance Cannot Capture Agent Dynamics
  sources:
  - chunk_ref: 03-PROV-AGENT (Chunk 1:93-98)
    quote: traditional provenance approaches are not designed to capture the intrinsic
      dynamics of modern AI agents
  description: Existing provenance methods fail to capture agent-centric metadata
    such as prompts, responses, and decisions, and cannot relate them to broader workflow
    context and downstream outcomes. They model workflows as static graphs, missing
    semantics for agentic behavior and model-driven reasoning.
- name: Isolated Agent Interaction Data
  sources:
  - chunk_ref: 03-PROV-AGENT (Chunk 1:160-167)
    quote: MCP-based agent frameworks record prompts, responses, and AI service invocations,
      these data are typically isolated from the rest of the workflow
  description: While MCP-based agent frameworks capture agent interactions, this data
    remains disconnected from the broader workflow. This isolation hinders contextualization
    of agent interactions and understanding their downstream impact.
- name: Ontology Silo Problem
  sources:
  - chunk_ref: 04-PROV-O_to_BFO (Chunk 1:44-48)
    quote: the proliferation of ontologies in a domain may lead to larger ontology
      silos...data organized by independent ontologies are not mutually interoperable
  description: While ontologies solve the data silo problem, the proliferation of
    independent ontologies creates an 'ontology silo problem' where data sets organized
    by different ontologies remain non-interoperable, requiring explicit mappings
    between terms.
- name: BFO Cannot Represent All PROV-O Domains
  sources:
  - chunk_ref: 04-PROV-O_to_BFO (Chunk 1:358-365)
    quote: BFO represents domains that are not represented in PROV-O, such as those
      covering spatial and temporal regions. Thus, not every term in BFO can be mapped
      to some term in PROV-O
  description: A total alignment of BFO to PROV-O is not possible because BFO includes
    domain-specific concepts (spatial/temporal regions) that have no PROV-O counterparts.
    This limits the achievable level of semantic interoperability to a one-way total
    alignment.
- name: OWL Data Property Mapping Limitations
  sources:
  - chunk_ref: 04-PROV-O_to_BFO (Chunk 2:191-200)
    quote: While we considered complex mappings for PROV-O data properties, we were
      unable to encode these in a computable format due to representational limits
      of OWL, RDF, and SWRL
  description: Complex mappings for data properties cannot be encoded computationally
    due to representational limitations of OWL, RDF, and SWRL. Time-related properties
    like startedAtTime require multiple fine-grained BFO relationships that cannot
    be directly represented.
- name: Conservativity Introduces New Disjoint Relations
  sources:
  - chunk_ref: 04-PROV-O_to_BFO (Chunk 1:457-458)
    quote: Although this level of conservativity was achieved, our alignment does
      entail new disjoint relations between PROV-O terms
  description: Even with conservativity achieved for subsumption hierarchies, the
    PROV-BFO alignment entails new disjoint relations between PROV-O terms (e.g.,
    PROV Agent becomes disjoint from PROV Activity), changing the semantics beyond
    the original ontology intent.
- name: Foundational Ontology Non-Computability
  sources:
  - chunk_ref: 05-DOLCE (Chunk 1:59-62)
    quote: Such richness greatly enhances expressiveness but, on the other hand, it
      makes foundational ontologies non computable, due to the well-known trade-off
      between formal expressiveness and computability
  description: DOLCE's rich axiomatization in first-order modal logic provides semantic
    transparency but sacrifices computability. This fundamental trade-off means foundational
    ontologies cannot be directly used for applications - only approximated partial
    translations in application-oriented languages are possible.
- name: DOLCE Requires Possibilist Domain Assumption
  sources:
  - chunk_ref: 05-DOLCE (Chunk 1:227-230)
    quote: 'These assumptions entail a possibilistic view of the entities: the domain
      of quantification contains all possible entities, regardless of their actual
      existence'
  description: DOLCE's formal theory in QS5 modal logic with Barcan formulas requires
    quantifying over all possible entities, not just actual ones. This philosophical
    commitment may not align with all application requirements or realist interpretations.
- name: DOLCE Does Not Formalize Functions and Roles Natively
  sources:
  - chunk_ref: 05-DOLCE (Chunk 1:184-189)
    quote: DOLCE does not formalize functions and roles, although these have been
      widely investigated and represented in DOLCE-driven approaches
  description: Functions and roles are not part of DOLCE's core formalization - they
    require extensions. Roles are represented as social concepts with anti-rigidity
    and foundation properties, requiring additional theoretical apparatus beyond the
    base ontology.
- name: Role vs Function Terminological Confusion
  sources:
  - chunk_ref: 06-BFO_Function_Role (Chunk 1:142-146)
    quote: 'The term ''function'' is sometimes used interchangeably with the term
      ''role''... Hunter defines function as: ''the role that a structure plays in
      the processes of a living thing'''
  description: There is widespread terminological confusion between 'function', 'role',
    'disposition', 'tendency', and 'capability' in the literature. This inconsistent
    thinking about biological and clinical phenomena stands in the way of coherent
    computer representations and understanding of ontologies like GO.
- name: BFO Biological Functions Limited to Parts of Organisms
  sources:
  - chunk_ref: 06-BFO_Function_Role (Chunk 1:469-474)
    quote: Biological functions are, according to the proposed definition, attributed
      to parts of organisms and not to whole organisms themselves
  description: BFO's definition restricts biological functions to parts of organisms
    (heart, liver, organs) - whole organisms cannot have functions, only roles. This
    creates a limitation when modeling entities like queen bees that appear to have
    'functions' in a colony context.
- name: BFO Has No Qualities for Occurrents
  sources:
  - chunk_ref: 07-Classifying_Processes (Chunk 1:676-678)
    quote: For item (5), on the other hand, there is no candidate category in the
      BFO ontology, since there is no counterpart on the occurrent side for BFO's
      qualities of independent continuants
  description: BFO lacks a mechanism for representing qualities of processes (occurrents).
    When measuring process attributes like speed, there is no BFO category to represent
    these - forcing a workaround where process attributions are treated as instantiation
    of process type universals rather than qualities.
- name: Processes Cannot Change in BFO Four-Dimensionalism
  sources:
  - chunk_ref: 07-Classifying_Processes (Chunk 1:689-691)
    quote: Processes, in particular, cannot change on the four-dimensionalist view,
      because processes are changes (they are changes in those independent continuant
      entities which are their participants)
  description: BFO's four-dimensionalist account of occurrents means processes cannot
    change - they ARE changes. This limits how we can describe process evolution.
    Saying 'speed up this process' must be reinterpreted as ensuring an ongoing process
    will be quicker than the alternative counterfactual process.
- name: BFO Process Universals Are Always Rigid
  sources:
  - chunk_ref: 07-Classifying_Processes (Chunk 1:735-738)
    quote: Universals on the side of occurrents, in contrast, are always rigid, so
      that if an occurrent instantiates a universal at some time, then it instantiates
      this universal at all times
  description: Unlike continuant universals (like 'larva' or 'fetus') which can be
    non-rigid, all occurrent universals in BFO are rigid. A process cannot change
    which universal it instantiates over time - this must be modeled as temporal parts
    instantiating different universals.
- name: Case ID Not Always Available
  sources:
  - chunk_ref: 12-Foundations_of_Process_Event_Data (Chunk 1:76-85)
    quote: Case IDs are not always straightforwardly available. This problem has been
      addressed in both process mining literature, as well as in practice, and is
      often referred to as event correlation
  description: Process mining requires Case IDs to correlate events, but in practice,
    Case IDs are often missing or not directly available in source systems. This is
    especially problematic in event streams where the notion of CaseID is even more
    complicated. This represents a fundamental data availability limitation for process
    mining applications.
- name: Granularity Mismatch Problem
  sources:
  - chunk_ref: 12-Foundations_of_Process_Event_Data (Chunk 1:103-109)
    quote: natural log data is stored at lower levels of granularity than desired
      for analysis purposes. Typically, one would prefer that the granularity level
      of activities is such that they can be understood
  description: Event data is often logged at much finer granularity than what is useful
    for business process analysis. The data needs abstraction to bring it to a business-understandable
    level, requiring additional event abstraction techniques. This granularity mismatch
    limits direct applicability of process mining.
- name: IoT Granularity Gap Challenge
  sources:
  - chunk_ref: 12-Foundations_of_Process_Event_Data (Chunk 1:317-322)
    quote: Sensors and actuators have been deployed widely for all kinds of purposes.
      Although the granularity gap between typical IoT data (sensor readings) and
      event data is sometimes challenging to bridge
  description: IoT sensor data operates at a fundamentally different granularity level
    than process events. The gap between raw sensor readings and meaningful business
    activities is often difficult to bridge, limiting IoT data's direct use in process
    mining despite its high availability.
- name: IID Assumption Violation
  sources:
  - chunk_ref: 12-Foundations_of_Process_Event_Data (Chunk 1:364-369)
    quote: The typical data format of event logs, consisting of events pertaining
      to cases, make that the rows in event log are intrinsically correlated. This
      invalidates the assumption of data being independent and identically distributed
      (IID)
  description: Event log data fundamentally violates the IID assumption that underpins
    most machine learning techniques. Events are by definition correlated within cases,
    which means standard data science techniques for cleaning and feature transformation
    cannot be directly applied to event data.
- name: Data Quality as Primary Limitation
  sources:
  - chunk_ref: 12-Foundations_of_Process_Event_Data (Chunk 1:529-535)
    quote: Garbage in, garbage out. It is by far the most mentioned quote in data
      science and far beyond. In process mining, while the problem has been acknowledged
      in both scientific literature and in practice
  description: Data quality remains the most critical limitation in process mining.
    Despite widespread acknowledgment of the problem, there is still a need for comprehensive
    frameworks to address bad quality data leading to incorrect analysis results.
- name: Four Data Quality Dimensions
  sources:
  - chunk_ref: 12-Foundations_of_Process_Event_Data (Chunk 1:549-552)
    quote: 'four broad data quality dimensions are identified for event logs: missing
      data, incorrect data, imprecise data and irrelevant data. Among these four dimensions,
      incorrect data and imprecise data'
  description: 'Event logs suffer from four fundamental data quality issues: missing
    data, incorrect data, imprecise data, and irrelevant data. Incorrect and imprecise
    data for key attributes like activity labels and timestamps can have significant
    consequences for all process mining techniques.'
- name: Preprocessing Dominates Project Time
  sources:
  - chunk_ref: 12-Foundations_of_Process_Event_Data (Chunk 1:576-578)
    quote: The data pre-processing task is recognized to be one of the most time-consuming
      aspects of a process mining study with many spending 60-80% of their efforts
      while some spending up to 90%
  description: Data preprocessing consumes 60-90% of total project effort in process
    mining studies. This extreme overhead limits the practical scalability and adoption
    of process mining approaches, as most resources go to data preparation rather
    than analysis.
- name: Algorithms Ignore Data Quality
  sources:
  - chunk_ref: 12-Foundations_of_Process_Event_Data (Chunk 1:599-602)
    quote: most of the existing process mining algorithms do not explicitly take the
      potential presence of data quality issues. The algorithms also typically treat
      an event log as the whole truth without considering
  description: Process mining algorithms typically do not account for data quality
    issues and treat event logs as ground truth. This can lead to misleading or inaccurate
    conclusions about processes, as the algorithms are not designed to handle imperfect
    data.
- name: Fabrication Process Complexity
  sources:
  - chunk_ref: 15-SciAgents_Multi-Agent_Graph_Reasoning (Chunk 2:62-64)
    quote: weaknesses include the complexity of the fabrication process, a lack of
      preliminary data, and concerns about long-term biocompatibility
  description: Complex multi-step fabrication processes identified as weakness in
    research proposals. The complexity limits reproducibility and scalability, and
    lack of preliminary data makes feasibility uncertain. Long-term validation remains
    a gap.
- name: Foundation Model Capability Dependency
  sources:
  - chunk_ref: 15-SciAgents_Multi-Agent_Graph_Reasoning (Chunk 2:152-154)
    quote: These types of results are expected to improve as more powerful foundation
      models become available, especially with better long-term planning and reasoning
      capabilities
  description: Multi-agent scientific discovery systems are fundamentally limited
    by the capabilities of underlying foundation models. Current models have limitations
    in long-term planning and reasoning that constrain the sophistication of generated
    research hypotheses.
- name: Complexity of Uniform Functionalization
  sources:
  - chunk_ref: 15-SciAgents_Multi-Agent_Graph_Reasoning (Chunk 4:911-913)
    quote: The process of uniformly functionalizing silk with nanoscale pigments may
      be challenging and requires precise control over the concentration and distribution
      of pigments
  description: Achieving uniform distribution of functional elements at nanoscale
    requires precise control that is technically challenging. This limits the practical
    implementation of proposed biomimetic material designs.
- name: Scalability Issues in Production
  sources:
  - chunk_ref: 15-SciAgents_Multi-Agent_Graph_Reasoning (Chunk 4:914-915)
    quote: The proposed methods, such as electrospinning and freeze-drying, may face
      scalability issues when transitioning from laboratory to industrial production
  description: Laboratory fabrication techniques like electrospinning and freeze-drying
    face significant scalability challenges when transitioning to industrial production.
    This gap between lab-scale and production-scale limits practical application of
    novel materials.
- name: Validation of Predicted Properties
  sources:
  - chunk_ref: 15-SciAgents_Multi-Agent_Graph_Reasoning (Chunk 4:915-918)
    quote: While the proposal suggests potential self-healing and adaptive optical
      properties, these need to be rigorously tested and validated, which may require
      additional time and resources
  description: AI-generated research hypotheses predict novel properties like self-healing
    and adaptive behavior, but these predictions require rigorous experimental validation.
    The gap between predicted and validated properties represents a significant limitation
    in AI-driven scientific discovery.
- name: Novelty Assessment Subjectivity
  sources:
  - chunk_ref: 15-SciAgents_Multi-Agent_Graph_Reasoning (Chunk 4:985-986)
    quote: 'Novelty: 7/10'
  description: The novelty assessment of AI-generated research ideas is inherently
    subjective and depends on the completeness of literature coverage. A score of
    7/10 indicates that while novel combinations exist, core concepts are already
    well-documented in literature, limiting true innovation.
- name: Complex Fabrication Reproducibility
  sources:
  - chunk_ref: 15-SciAgents_Multi-Agent_Graph_Reasoning (Chunk 5:14-17)
    quote: 'Complexity of Functionalization: The process of uniformly functionalizing
      silk with nanoscale pigments may be challenging and requires precise control
      over the concentration and distribution'
  description: Complex fabrication processes requiring precise control over nanoscale
    distributions present reproducibility challenges. This limits the ability to consistently
    produce materials with predicted properties across different laboratories.
- name: Scalability for Industrial Production
  sources:
  - chunk_ref: 15-SciAgents_Multi-Agent_Graph_Reasoning (Chunk 5:17-18)
    quote: 'Scalability: The proposed methods, such as electrospinning and freeze-drying,
      may face scalability issues when transitioning from laboratory to industrial
      production'
  description: Laboratory fabrication methods often cannot scale to industrial production
    without significant process modifications. This scalability limitation constrains
    the practical impact of research discoveries.
- name: Lack of Preliminary Data
  sources:
  - chunk_ref: 15-SciAgents_Multi-Agent_Graph_Reasoning (Chunk 5:969-970)
    quote: 'Limited Preliminary Data: The proposal lacks preliminary data to support
      the feasibility of achieving the proposed improvements in heat transfer and
      mechanical stability'
  description: AI-generated research proposals often lack preliminary experimental
    data to validate feasibility claims. This represents a fundamental limitation
    in AI-driven hypothesis generation, as predictions remain theoretical until experimentally
    validated.
- name: Long-term Biocompatibility Unknowns
  sources:
  - chunk_ref: 15-SciAgents_Multi-Agent_Graph_Reasoning (Chunk 5:970-973)
    quote: 'Potential Biocompatibility Issues: While the proposal emphasizes biocompatibility,
      it does not address potential issues related to long-term implantation and interaction
      with biological tissues'
  description: Short-term biocompatibility testing may not reveal long-term issues
    in biological applications. The gap between initial biocompatibility assessment
    and long-term performance represents a significant limitation for biomedical material
    applications.
- name: Complex Fabrication Process Reproducibility Limitation
  sources:
  - chunk_ref: 15-SciAgents (Chunk 6:68)
    quote: 'Complex Fabrication Process: The use of soft lithography to create intricate
      lamellar structures may present challenges in reproducibility and scalability.'
  description: Multi-agent generated research proposals identify fabrication complexity
    as a fundamental limitation. Soft lithography techniques for creating biomimetic
    lamellar structures face reproducibility challenges, limiting the practical application
    of novel material designs in real-world settings.
- name: Lack of Preliminary Data for Feasibility Validation
  sources:
  - chunk_ref: 15-SciAgents (Chunk 6:69)
    quote: 'Limited Preliminary Data: The proposal lacks preliminary data to support
      the feasibility of achieving the proposed improvements in heat transfer and
      mechanical stability.'
  description: AI-generated research proposals often lack empirical grounding. The
    multi-agent system identifies this as a weakness - proposals make theoretical
    claims about enhanced performance without supporting experimental validation data.
- name: Long-term Biocompatibility Issues Not Addressed
  sources:
  - chunk_ref: 15-SciAgents (Chunk 6:70-72)
    quote: 'Potential Biocompatibility Issues: While the proposal emphasizes biocompatibility,
      it does not address potential issues related to long-term implantation and interaction
      with biological tissues.'
  description: AI agent systems generate proposals that overlook long-term biological
    integration concerns. The limitation reveals a gap between theoretical biocompatibility
    claims and practical validation of material behavior in biological environments
    over extended periods.
- name: Multi-Scale Integration Complexity
  sources:
  - chunk_ref: 15-SciAgents (Chunk 7:310-311)
    quote: 'Complexity: The integration of multiple scales and principles may complicate
      the fabrication and characterization processes.'
  description: Multi-agent research proposal generation faces inherent limitations
    when integrating molecular, cellular, and macroscale mechanisms. The complexity
    of spanning multiple scales creates practical challenges for implementation and
    validation.
- name: In Vitro Validation Gap for Real-World Applications
  sources:
  - chunk_ref: 15-SciAgents (Chunk 7:311-312)
    quote: 'Validation: The proposal relies heavily on computational modeling and
      in vitro experiments, which may not fully capture the material''s behavior in
      real-world applications.'
  description: AI-generated research proposals over-rely on computational and in vitro
    validation, creating a gap between laboratory conditions and real-world deployment.
    This limitation affects the practical applicability of proposed materials and
    methods.
- name: Scalability Constraints for Large-Scale Production
  sources:
  - chunk_ref: 15-SciAgents (Chunk 7:312-313)
    quote: 'Scalability: Advanced manufacturing techniques like 3D printing and electrospinning
      may face scalability issues for large-scale production.'
  description: Multi-agent systems identify scalability as a recurring limitation.
    Novel fabrication techniques that work in laboratory settings may not translate
    to industrial-scale production, limiting commercial viability of proposed innovations.
- name: Biological Integration Complexity in Material Design
  sources:
  - chunk_ref: 15-SciAgents (Chunk 7:401-402)
    quote: 'Feasibility: Medium - While the integration is innovative, it adds complexity
      to the fabrication and characterization processes. The reliance on in vitro
      experiments may not fully capture real-world behavior.'
  description: Combining biological and mechanical principles introduces additional
    complexity that limits practical feasibility. The gap between controlled laboratory
    experiments and real-world biological interactions remains a fundamental constraint.
- name: Precise Microstructure Control Requirements
  sources:
  - chunk_ref: 15-SciAgents (Chunk 7:416-417)
    quote: 'Feasibility: Medium - Achieving stiffness memory through reversible deformation
      mechanisms is feasible but requires precise control over the material''s microstructure
      and interactions at multiple scales.'
  description: Advanced material properties like stiffness memory require extremely
    precise microstructural control. This precision requirement limits reproducibility
    and scalability of novel material designs across different manufacturing contexts.
- name: Controlled to Real-World Translation Barrier
  sources:
  - chunk_ref: 15-SciAgents (Chunk 7:424-425)
    quote: Dynamic adaptability through cell signaling and mechanical stimuli is feasible
      in controlled environments (e.g., in vitro). However, translating this adaptability
      to real-world applications may pose challenges.
  description: Properties demonstrated in controlled laboratory conditions may not
    persist in uncontrolled real-world environments. This translation gap is a fundamental
    limitation for biologically-integrated material designs.
- name: Nanocomposite Biocompatibility Concerns
  sources:
  - chunk_ref: 15-SciAgents (Chunk 8:566-567)
    quote: 'Biocompatibility Concerns: The proposal could benefit from more detailed
      studies on the biocompatibility of the nanocomposites used.'
  description: Integration of nanocomposites into biological scaffolds raises biocompatibility
    questions that are not adequately addressed. This limitation highlights the need
    for comprehensive safety studies before clinical translation.
- name: Long-Term Stability Under Real Conditions
  sources:
  - chunk_ref: 15-SciAgents (Chunk 8:566-567)
    quote: 'Long-Term Stability: The long-term stability and durability of the enhanced
      scaffolds need to be thoroughly investigated.'
  description: AI-generated research proposals often lack consideration of material
    behavior over extended time periods. Long-term stability testing is essential
    but frequently omitted from initial proposals.
- name: Scalability Analysis Omission
  sources:
  - chunk_ref: 15-SciAgents (Chunk 8:567)
    quote: 'Scalability: The scalability of the proposed methods for large-scale production
      is not addressed.'
  description: Multi-agent research proposal systems frequently omit scalability considerations.
    This represents a systematic limitation in AI-generated scientific proposals that
    focus on innovation over practical implementation.
- name: Moderate Novelty in Well-Explored Research Areas
  sources:
  - chunk_ref: 15-SciAgents (Chunk 8:666-668)
    quote: 'Novelty: 6/10 - The use of electrospun collagen nanofibers and the optimization
      of their mechanical properties through various techniques is a well-explored
      area.'
  description: The multi-agent system's literature search reveals that some generated
    research directions have limited novelty due to extensive prior work. This demonstrates
    both the value and limitation of AI-assisted hypothesis generation - it can identify
    this overlap but still generates proposals in saturated areas.
- name: Achieving Comparable Natural Material Properties Challenge
  sources:
  - chunk_ref: 15-SciAgents (Chunk 8:675-676)
    quote: Achieving mechanical properties comparable to spider silk and vanadium(V)
      may be challenging due to the inherent differences in material properties.
  description: Setting benchmarks based on natural high-performance materials may
    create unrealistic expectations. The fundamental differences between synthetic
    and natural materials impose constraints on achievable performance levels.
- name: Design and Fabrication Complexity
  sources:
  - chunk_ref: 15-SciAgents (Chunk 9:531-533)
    quote: 'Complexity: The proposed material''s design and fabrication may be complex
      and challenging to implement, requiring precise control over the hierarchical
      structure and the integration of amyloid fibrils.'
  description: Novel biomimetic materials combining hierarchical structures with specialized
    proteins face significant implementation complexity. Precise control requirements
    limit practical realization of theoretically sound designs.
- name: Synthesis and Assembly Scalability for Industry
  sources:
  - chunk_ref: 15-SciAgents (Chunk 9:534)
    quote: 'Scalability: The scalability of the proposed material for industrial applications
      may be a concern, as the synthesis and assembly processes may be time-consuming
      and costly.'
  description: Industrial scale-up of complex synthesis and assembly processes presents
    cost and time barriers. This fundamental limitation affects the translation of
    laboratory innovations to commercial products.
- name: Resource-Intensive Validation Requirements
  sources:
  - chunk_ref: 15-SciAgents (Chunk 9:535)
    quote: 'Validation: The proposal relies heavily on computational modeling and
      experimental validation, which may require significant resources and time to
      achieve conclusive results.'
  description: Comprehensive validation of novel materials requires substantial investment
    in both computational and experimental resources. This resource requirement limits
    the pace of innovation and verification in materials science.
- name: Gene Circuit Design Complexity
  sources:
  - chunk_ref: 15-SciAgents (Chunk 10:454)
    quote: 'Complexity of Gene Circuits: The design and implementation of gene circuits
      for protein expression and assembly can be complex and may require extensive
      optimization.'
  description: When multi-agent systems propose synthetic biology integration, they
    encounter the limitation of gene circuit complexity. Precise control over protein
    expression and assembly requires iterative optimization that is difficult to predict
    computationally.
- name: Synthesis Process Scalability Not Detailed
  sources:
  - chunk_ref: 15-SciAgents (Chunk 10:455)
    quote: 'Scalability: The scalability of the synthesis process for large-scale
      production of the composite materials is not addressed in detail.'
  description: AI-generated research proposals systematically underspecify scalability
    pathways. This represents a gap between innovation-focused proposal generation
    and practical implementation planning.
- name: Environmental and Biological Interaction Long-Term Safety
  sources:
  - chunk_ref: 15-SciAgents (Chunk 10:456-458)
    quote: 'Environmental and Biological Interactions: The long-term environmental
      and biological interactions of the composite materials need further investigation
      to ensure safety and stability.'
  description: Novel composite materials require extensive long-term safety studies
    that are often beyond the scope of initial research proposals. This gap between
    innovation and safety validation is a fundamental limitation of rapid research
    iteration.
- name: Graphene Cytotoxicity Risk
  sources:
  - chunk_ref: 15-SciAgents (Chunk 10:340-341)
    quote: Pure graphene can exhibit cytotoxicity due to its sharp edges and high
      surface area, while amyloid fibrils, being protein-based, are generally more
      biocompatible.
  description: Graphene-based composites face inherent biocompatibility limitations
    due to cytotoxicity concerns. Sharp edges and high surface area of graphene introduce
    safety constraints that must be mitigated through composite design.
- name: Graphene Environmental Degradation Susceptibility
  sources:
  - chunk_ref: 15-SciAgents (Chunk 10:350)
    quote: Graphene is susceptible to oxidation and degradation under harsh environmental
      conditions, while amyloid fibrils are relatively stable.
  description: Pure graphene faces environmental stability limitations including oxidation
    susceptibility. This constraint necessitates protective strategies or composite
    approaches for real-world applications in harsh conditions.
- name: Moderate to High Feasibility with Implementation Challenges
  sources:
  - chunk_ref: 15-SciAgents (Chunk 10:482-484)
    quote: 'Rating: Moderate to High. Reasoning: While the proposal is well-structured
      and comprehensive, the complexity of gene circuit design and the scalability
      of the synthesis process present challenges that need to be addressed.'
  description: Even well-structured multi-agent generated proposals face feasibility
    limitations related to implementation complexity. The gap between theoretical
    innovation and practical realization remains a persistent constraint across diverse
    research domains.
- name: ToT Tree Structure Rigidity Limitation
  sources:
  - chunk_ref: 19-Graph_of_Thoughts_LLM_Reasoning (Chunk 1:59-61)
    quote: the ToT approaches still fundamentally limit the reasoning abilities within
      a prompt by imposing the rigid tree structure on the thought process
  description: Tree of Thoughts approaches impose a rigid tree structure that fundamentally
    limits LLM reasoning capabilities. This architectural constraint prevents more
    flexible, graph-based thought patterns that could enable backtracking and merging
    of reasoning paths.
- name: LLM Inability to Sort Long Sequences Consistently
  sources:
  - chunk_ref: 19-Graph_of_Thoughts_LLM_Reasoning (Chunk 1:475-476)
    quote: The considered LLMs are unable to sort a sequence of such numbers correctly
      beyond a certain length consistently because duplicate counts do not match
  description: LLMs have difficulty sorting sequences of numbers with duplicates beyond
    a certain length. This is a specific task-related limitation showing that LLMs
    struggle with precise counting and ordering operations at scale.
- name: Budget Restrictions Limiting Model Evaluation
  sources:
  - chunk_ref: 19-Graph_of_Thoughts_LLM_Reasoning (Chunk 2:74-76)
    quote: Due to budget restrictions, we focus on GPT-3.5. We also experimented with
      Llama-2, but it was usually worse than GPT-3.5 and also much slower to run
  description: Research limitations due to budget constraints affect which LLMs can
    be evaluated. Llama-2 performed worse than GPT-3.5 and was significantly slower,
    making comprehensive evaluation infeasible.
- name: Few-Shot Examples Overhead in Task Decomposition
  sources:
  - chunk_ref: 19-Graph_of_Thoughts_LLM_Reasoning (Chunk 2:130-136)
    quote: the 'static' part of the prompt (i.e., few-shot examples) may become a
      significant overhead... these few-shot examples can usually also be reduced
      in size
  description: When decomposing tasks into subtasks, the static few-shot examples
    in prompts can become significant overhead that increases costs. This represents
    a trade-off between providing context and computational efficiency.
- name: GoT Single Context Focus Limitation
  sources:
  - chunk_ref: 19-Graph_of_Thoughts_LLM_Reasoning (Chunk 2:523-525)
    quote: GoT is orthogonal to this class of schemes, as it focuses on a single context
      capabilities
  description: Graph of Thoughts focuses on single context capabilities, making it
    orthogonal to prompt chaining approaches that cascade different LLMs via different
    contexts. This limits GoT to within-prompt reasoning rather than cross-context
    orchestration.
- name: Multi-Agent Collaboration Adaptability Limitation
  sources:
  - chunk_ref: 18-Multi-Agent_Architecture_Taxonomy_LLM (Chunk 4:118-119)
    quote: Among the systems analyzed, we especially observe limitations regarding
      collaboration modes and role-playing capabilities, as well as risks tied to
      prompt-driven collaboration
  description: Current multi-agent LLM systems have significant limitations in agent
    collaboration adaptability, including restricted communication protocols, limited
    role-playing capabilities, and vulnerabilities in prompt-driven collaboration.
- name: Restricted Communication Protocols Between Agents
  sources:
  - chunk_ref: 18-Multi-Agent_Architecture_Taxonomy_LLM (Chunk 4:122-126)
    quote: the collaboration between agents is mainly characterized by restricted
      communication protocols between predefined task-execution agents, such as instructor-and-executor
      relationships
  description: Agent collaboration is limited by predefined, restricted communication
    protocols that follow fixed patterns like instructor-executor relationships. This
    restricts flexible, adaptive problem-solving approaches.
- name: Dynamic Role-Playing Potential Underexplored
  sources:
  - chunk_ref: 18-Multi-Agent_Architecture_Taxonomy_LLM (Chunk 4:129-131)
    quote: the potential of engaging multiple perspectives through different roles
      and standpoints has not yet been fully sounded
  description: Multi-agent systems have not fully explored the potential for flexible
    collaboration between self-organizing role agents. The capability for simulating
    complex multi-perspective interactions remains underutilized.
- name: Prompt-Driven Collaboration Robustness Issues
  sources:
  - chunk_ref: 18-Multi-Agent_Architecture_Taxonomy_LLM (Chunk 4:134-139)
    quote: This communication mechanism, founded on a sequence of prompts, heavily
      relies on the quality of LLM responses, which are susceptible to errors in terms
      of incorrect or hallucinated results
  description: Prompt-driven inter-agent communication is vulnerable to LLM response
    quality issues including hallucinations. Without comprehensive control mechanisms,
    systems are susceptible to inaccuracies and inefficiencies.
- name: Lack of User-Centric Alignment Options
  sources:
  - chunk_ref: 18-Multi-Agent_Architecture_Taxonomy_LLM (Chunk 4:142-144)
    quote: Within the scope of analyzed systems, user-centric alignment options are
      very rare. Alignment mechanisms are predominantly integrated into the system
      architecture
  description: Multi-agent LLM systems severely lack user-centric alignment options.
    Most alignment is hard-coded into system architecture rather than allowing real-time
    user guidance or modification.
- name: Opaque Internal Agent Composition
  sources:
  - chunk_ref: 18-Multi-Agent_Architecture_Taxonomy_LLM (Chunk 4:147-151)
    quote: The options for users to access and influence the internal workings of
      the system are very limited. The internal composition and collaboration of the
      agents are mostly opaque to the user
  description: Users have very limited ability to access, understand, or influence
    internal agent operations. The lack of transparency in agent composition and collaboration
    reduces system trustworthiness.
- name: Missing Real-Time Adjustment Capabilities
  sources:
  - chunk_ref: 18-Multi-Agent_Architecture_Taxonomy_LLM (Chunk 4:158-163)
    quote: The obvious lack of real-time adjustment capabilities... The absence of
      user interaction and control during runtime restricts the potential for dynamic
      alignment
  description: Autonomous agent systems lack real-time adjustment capabilities, preventing
    dynamic alignment during runtime. This limits system flexibility in responding
    to changing operational contexts.
- name: High-Autonomy Aspects Inadequately Controlled
  sources:
  - chunk_ref: 18-Multi-Agent_Architecture_Taxonomy_LLM (Chunk 4:175-185)
    quote: non-terminating activities, where the system falls into infinite loops...
      system operations might terminate in a dead end when encountering a task that
      requires competencies or resources that are either unavailable or inaccessible
  description: Multi-agent systems exhibit operational failures including infinite
    loops and dead-end terminations. Control mechanisms are inadequate for managing
    high-autonomy aspects, leading to reliability and effectiveness concerns.
- name: Autonomy Scope Beyond Self-Organization
  sources:
  - chunk_ref: 18-Multi-Agent_Architecture_Taxonomy_LLM (Chunk 4:206-208)
    quote: autonomy can span beyond this definition, encompassing facets like an agent's
      ability for self-enhancement and proactive agency
  description: The taxonomy's scope of autonomy is limited to self-organization for
    decision-making, but does not encompass broader autonomy aspects like self-enhancement
    and proactive agency capabilities.
- name: Alignment Quality and Efficacy Not Measured
  sources:
  - chunk_ref: 18-Multi-Agent_Architecture_Taxonomy_LLM (Chunk 4:214-215)
    quote: this dimension does not reflect the quality, efficacy, or depth of the
      applied techniques
  description: The taxonomic alignment dimension captures origin and timing of alignment
    but does not measure the quality, efficacy, or depth of alignment techniques,
    limiting the taxonomy's analytical power.
- name: Traditional RAG Static Workflow Limitation
  sources:
  - chunk_ref: 20-Agentic_RAG_Survey (Chunk 1:51-52)
    quote: traditional RAG systems are constrained by static workflows and lack the
      adaptability required for multi-step reasoning and complex task management
  description: Traditional RAG systems suffer from static workflows that cannot adapt
    to dynamic scenarios. They lack capabilities for multi-step reasoning and complex
    task management that real-world applications require.
- name: LLM Static Pre-Training Data Reliance
  sources:
  - chunk_ref: 20-Agentic_RAG_Survey (Chunk 1:90-93)
    quote: LLMs face significant limitations due to their reliance on static pre-training
      data. This reliance often results in outdated information, hallucinated responses,
      and an inability to adapt to dynamic, real-world scenarios
  description: LLMs are fundamentally limited by static pre-training data, causing
    outdated information, hallucinations, and inability to adapt to dynamic scenarios.
    This creates critical gaps in real-time, contextually accurate applications.
- name: Naive RAG Lack of Contextual Awareness
  sources:
  - chunk_ref: 20-Agentic_RAG_Survey (Chunk 1:212-213)
    quote: Retrieved documents often fail to capture the semantic nuances of the query
      due to reliance on lexical matching rather than semantic understanding
  description: Naive RAG systems fail to capture semantic nuances due to reliance
    on lexical keyword matching instead of semantic understanding, leading to contextually
    inappropriate retrievals.
- name: Graph RAG Limited Scalability
  sources:
  - chunk_ref: 20-Agentic_RAG_Survey (Chunk 1:323-324)
    quote: The reliance on graph structures can restrict scalability, especially with
      extensive data sources
  description: Graph RAG systems face scalability limitations due to their dependence
    on graph structures, particularly when dealing with extensive data sources that
    exceed graph processing capabilities.
- name: Agentic RAG Coordination Complexity
  sources:
  - chunk_ref: 20-Agentic_RAG_Survey (Chunk 1:370-371)
    quote: Managing interactions between agents requires sophisticated orchestration
      mechanisms
  description: Agentic RAG systems face coordination complexity challenges. Managing
    inter-agent interactions requires sophisticated orchestration mechanisms that
    increase system complexity.
- name: Agentic RAG Computational Overhead
  sources:
  - chunk_ref: 20-Agentic_RAG_Survey (Chunk 1:374)
    quote: The use of multiple agents increases resource requirements for complex
      workflows
  description: Multi-agent architectures in Agentic RAG increase computational overhead
    and resource requirements, particularly for complex workflows requiring multiple
    specialized agents.
- name: Multi-Agent Coordination Complexity Challenge
  sources:
  - chunk_ref: 20-Agentic_RAG_Survey (Chunk 2:61-62)
    quote: Managing inter-agent communication and task delegation requires sophisticated
      orchestration mechanisms
  description: Multi-agent RAG systems face significant challenges in coordinating
    inter-agent communication and task delegation, requiring sophisticated orchestration
    that adds complexity and potential failure points.
- name: Multi-Agent Data Integration Challenge
  sources:
  - chunk_ref: 20-Agentic_RAG_Survey (Chunk 2:68-69)
    quote: Synthesizing outputs from diverse sources into a cohesive response is non-trivial
      and requires advanced LLM capabilities
  description: Integrating and synthesizing outputs from multiple diverse agent sources
    into coherent responses is a non-trivial challenge that requires advanced LLM
    capabilities beyond current standard implementations.
- name: Multi-Agent Collaboration Unpredictability
  sources:
  - chunk_ref: 20-Agentic_RAG_Survey (Chunk 1:595-596)
    quote: While multi-agent collaboration offers significant potential, it is a less
      predictable design pattern compared to more mature workflows like Reflection
      and Tool Use
  description: Multi-agent collaboration is inherently less predictable than more
    mature design patterns like Reflection and Tool Use, making system behavior harder
    to guarantee and debug.
- name: Agentic RAG Scaling and Ethics Challenges
  sources:
  - chunk_ref: 20-Agentic_RAG_Survey (Chunk 3:352-357)
    quote: Coordination complexity in multi-agent architectures, scalability, and
      latency issues, as well as ethical considerations, must be addressed... the
      lack of specialized benchmarks and datasets tailored to evaluate agentic capabilities
      poses a significant hurdle
  description: 'Agentic RAG systems face multiple unresolved challenges: coordination
    complexity, scalability issues, latency problems, ethical considerations, and
    lack of specialized evaluation benchmarks and datasets.'
- name: LLM Non-Determinism for Smart Contracts
  sources:
  - chunk_ref: 21-LLM_Smart_Contracts_from_BPMN (Chunk 1:67-68)
    quote: LLM outputs are inherently non-deterministic, making them unreliable for
      consistent behaviour
  description: LLM outputs are inherently non-deterministic, making them fundamentally
    unreliable for applications requiring consistent, reproducible behavior like smart
    contract generation.
- name: LLM Security Vulnerabilities in Code Generation
  sources:
  - chunk_ref: 21-LLM_Smart_Contracts_from_BPMN (Chunk 1:66-67)
    quote: GitHub Copilot can introduce numerous security vulnerabilities into generated
      code
  description: LLM-based code generation tools like GitHub Copilot can introduce numerous
    security vulnerabilities into generated code, posing significant risks for security-critical
    applications like smart contracts.
- name: Smart Contract LLM Generation Imperfect Reliability
  sources:
  - chunk_ref: 21-LLM_Smart_Contracts_from_BPMN (Chunk 1:638-649)
    quote: F1 scores that do not reliably achieve 100% would not be suitable for this
      context... even such a 2% error rate could lead to significant vulnerabilities
      or losses
  description: LLM-generated smart contracts cannot achieve the perfect reliability
    required for blockchain environments. Even a 2% error rate is unacceptable given
    financial risks and immutable transaction nature. This fundamental limitation
    cannot be overcome with current LLM architectures.
- name: Single Dataset Validation Limitation
  sources:
  - chunk_ref: 22-RPA_Framework_BPM_Activities (Chunk 1:595-598)
    quote: Despite the demonstration and application of the framework, it is tested
      only with one data set and process. The evaluation has shown that not all criteria
      can be tested against this data set
  description: The RPA viability assessment framework was only validated against a
    single P2P process dataset. Authors acknowledge this limits generalizability and
    the framework must be validated through application to multiple and different
    kinds of processes.
- name: Missing Event Log Attributes Limitation
  sources:
  - chunk_ref: 22-RPA_Framework_BPM_Activities (Chunk 1:562-565)
    quote: First, missing attributes such as starting timestamps in the event log
      impede the possibility to assess typically easy to evaluate criteria like the
      execution time or execution urgency
  description: Event logs often lack essential attributes needed for RPA assessment.
    Missing starting timestamps prevent evaluation of execution time and urgency criteria,
    limiting the framework's applicability to incomplete data.
- name: Exception Source Indistinguishability
  sources:
  - chunk_ref: 22-RPA_Framework_BPM_Activities (Chunk 1:564-566)
    quote: the possible lack of information about exceptions in the event log inhibits
      the ability to distinguish between a system-related stability or human error
      caused issue
  description: Event logs may not contain information needed to differentiate between
    system stability issues and human errors. This prevents accurate assessment of
    failure causes and appropriate automation decisions.
- name: Missing User Interface Interaction Data
  sources:
  - chunk_ref: 22-RPA_Framework_BPM_Activities (Chunk 1:566-572)
    quote: crucial information about the interaction on the user interface is missing
      and prevents the examination of the criteria determinism, structuredness of
      data, interfaces and number of systems
  description: Event logs typically capture back-end events but not front-end user
    interactions. This gap prevents assessment of determinism (whether tasks follow
    rule-based logic), data structuredness, interface complexity, and system count.
    Authors suggest user interaction loggers could bridge this gap.
- name: Determinism Cannot Be Evaluated From Event Logs
  sources:
  - chunk_ref: 22-RPA_Framework_BPM_Activities (Chunk 1:497-500)
    quote: The event log does not include information about the performance of the
      activity 'Change Quantity' on the presentation layer. Therefore, the criterion
      can not be evaluated for this data set
  description: Determinism is a distinctive criterion for RPA viability that requires
    understanding presentation layer interactions. Standard event logs lack this information,
    making it impossible to assess whether tasks follow rule-based, deterministic
    flows suitable for automation.
- name: Anonymized Data Limiting Evaluation
  sources:
  - chunk_ref: 22-RPA_Framework_BPM_Activities (Chunk 1:605-608)
    quote: Another important aspect is that the data set was anonymized and modified
      before publishing, limiting the accessible information stored in the event logs
  description: Data anonymization required for public datasets reduces the information
    available for comprehensive framework evaluation. This is a practical limitation
    when validating assessment frameworks against real-world data.
- name: Qualitative Criteria Not Testable With Process Mining
  sources:
  - chunk_ref: 22-RPA_Framework_BPM_Activities (Chunk 1:598-601)
    quote: the framework contains qualitative criteria that could not be tested with
      the data set. Further evaluation of these criteria - e.g. through case studies
      - is necessary
  description: Some RPA viability criteria are qualitative and cannot be assessed
    through process mining alone. These require alternative evaluation methods such
    as case studies or expert interviews.
- name: Cognitive Task Assessment Limitation
  sources:
  - chunk_ref: 22-RPA_Framework_BPM_Activities (Chunk 1:106-107)
    quote: Although RPA typically favors less complex and cognitive tasks, advances
      in machine learning can extend the range of RPA application in the future
  description: Current RPA technology cannot handle tasks requiring cognitive assessment
    or subjective judgment. The framework acknowledges this inherent limitation of
    RPA, noting machine learning advances may eventually address this gap.
- name: Subjective Judgment Exclusion
  sources:
  - chunk_ref: 22-RPA_Framework_BPM_Activities (Chunk 1:127-129)
    quote: No or low subjective judgment or interpretation skills are required for
      decision making as the process follows a rule-based flow
  description: RPA is limited to rule-based processes and cannot handle activities
    requiring subjective interpretation or judgment. This is a fundamental constraint
    defining the boundary of what can be automated with RPA.
- name: BWW Ontology Inadequate for Conceptual Modeling
  sources:
  - chunk_ref: 23-UFO_Story_Ontological_Foundations (Chunk 1:112-121)
    quote: Bunge's objective was doing philosophy of science and, being a physicist,
      chiefly, philosophy of the hard sciences. Conceptual Modeling, in contrast,
      is about representing aspects of the physical and social world
  description: The Bunge-Wand-Weber (BWW) ontology was designed for philosophy of
    hard sciences, not conceptual modeling. It fails to account for human cognition,
    linguistic competence, and descriptive metaphysics needed for conceptual modeling,
    leading to predictions that conflict with modeler intuitions.
- name: BWW Relationship Modeling Constraint
  sources:
  - chunk_ref: 23-UFO_Story_Ontological_Foundations (Chunk 1:121-125)
    quote: the BWW dictum that 'mutual properties (or relations) should never be modeled
      as classes as they should not be allowed to have properties' was in conflict
      with modeling predictions made by other foundational theories
  description: BWW ontology prohibits reified relationships (modeling relations as
    classes with properties), but this conflicts with practitioner intuitions and
    other theories. Recorded modeling sessions showed practitioners frequently model
    reified relationships, exposing BWW's limitations.
- name: GFO Bradley Regress Problem
  sources:
  - chunk_ref: 23-UFO_Story_Ontological_Foundations (Chunk 1:158-160)
    quote: the GFO theory of relations is subject to the so-called Bradley Regress
      and, hence, it can only be instantiated by infinite (logical) models. This feature
      makes it unsuitable for conceptual modeling applications
  description: GFO (General Formalized Ontology) has a theory of relations that leads
    to infinite logical models due to Bradley Regress. This philosophical problem
    makes GFO unsuitable for practical conceptual modeling applications.
- name: OntoUML Limited to Structural/Endurant Modeling
  sources:
  - chunk_ref: 23-UFO_Story_Ontological_Foundations (Chunk 1:353-358)
    quote: the language was created to represent structural conceptual models expressing
      endurantistic aspects of reality. However, systematically, a number of authors...started
      to produce OntoUML models in which perdurants would appear
  description: OntoUML was designed for structural conceptual modeling (endurants
    only), not temporal/process aspects (perdurants). Users systematically subverted
    the language to represent events and processes, revealing a limitation that requires
    extending the metamodel with UFO-B.
- name: Diagram Complexity Management Gap
  sources:
  - chunk_ref: 23-UFO_Story_Ontological_Foundations (Chunk 1:361-364)
    quote: given that the introduction of this new perspective substantially increases
      the complexity of the resulting diagrams, new complexity management theories
      and tools for OntoUML diagrams need to be developed
  description: Adding perdurant (event/process) modeling to OntoUML substantially
    increases diagram complexity. Current tools lack adequate support for filtering,
    modularization, and viewpoint selection needed to manage this complexity.
- name: BPMN Lacks Resource Specification
  sources:
  - chunk_ref: 31-BBO_BPMN_Ontology (Chunk 1:83-88)
    quote: In spite of its industrial maturity, BPMN does not support the representation
      of some process specifications such as the material resources required to carry
      out a given task, or the workstation where a given task should be performed
  description: BPMN 2.0, despite being the industry standard for business process
    modeling, cannot specify material resources required for tasks or physical locations
    where tasks are performed. BBO ontology was created to address these gaps.
- name: BPMN Resource Semantics Ambiguity
  sources:
  - chunk_ref: 31-BBO_BPMN_Ontology (Chunk 1:290-294)
    quote: The Resource concept exists in the BPMN meta-model. However, its semantics
      and definition are ambiguous. Indeed, on p. 95 of BPMN specification, the Resource
      class is supposed to cover all resource types. However, the definition of the
      relation that assigns resources...limits the set of resources to the agents
  description: BPMN has conflicting definitions of Resource - one suggests all resource
    types, another limits it to agents performing work. This semantic ambiguity required
    BBO to explicitly define a resource taxonomy covering all resource types.
- name: BPMN Natural Language Specifications Not Machine Processable
  sources:
  - chunk_ref: 31-BBO_BPMN_Ontology (Chunk 1:398-404)
    quote: the diagrams and XML schema do not reflect the whole specification and
      miss a part of its semantics...a large part of the specifications is in natural
      language. Therefore, we have tried to manually conceptualize the natural language
      specifications
  description: BPMN 2.0 specification has substantial semantic content only expressed
    in natural language text, not captured in UML diagrams or XML schemas. This prevents
    automated consistency checking and required manual formalization effort in BBO.
