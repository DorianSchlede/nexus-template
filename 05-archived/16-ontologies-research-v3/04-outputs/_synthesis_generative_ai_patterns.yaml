field: generative_ai_patterns
aggregated_at: '2026-01-01T16:22:26.835369'
batches_merged: 5
patterns_input: 144
patterns_output: 139
patterns:
- name: Prompt-Response-Decision Provenance Pattern
  sources:
  - chunk_ref: 03-PROV-AGENT (Chunk 1:27-29)
    quote: existing methods fail to capture and relate agent-centric metadata such
      as prompts, responses, and decisions with the broader workflow context
  description: Pattern for tracking the complete chain of LLM interactions including
    prompts sent, responses received, and decisions made. Enables traceability from
    final outputs back through the reasoning process for debugging and reliability
    analysis.
- name: Retrieval-Augmented Generation (RAG) for Prompt Enhancement
  sources:
  - chunk_ref: 03-PROV-AGENT (Chunk 1:148-150)
    quote: Retrieval-Augmented Generation (RAG) to dynamically augment prompts
  description: Pattern where agents use external knowledge bases or web pages to dynamically
    enhance prompts with contextual information before sending to LLM. Improves response
    accuracy by grounding generation in retrieved knowledge.
- name: Model Context Protocol (MCP) Tool Integration
  sources:
  - chunk_ref: 03-PROV-AGENT (Chunk 1:144-147)
    quote: MCP defines core agentic AI development concepts, including tools, prompts,
      resources, context management, and agent-client architecture
  description: Standardized pattern for LLM agents to interact with external tools,
    manage prompts, handle resources, and maintain context. Provides structured interface
    for function calling and tool use in agentic workflows.
- name: LLM Invocation Wrapper Pattern
  sources:
  - chunk_ref: 03-PROV-AGENT (Chunk 1:356-364)
    quote: a generic wrapper for abstract LLM objects, compatible with models from
      popular LLM interfaces, including CrewAI, LangChain, and OpenAI
  description: Pattern of wrapping LLM calls (FlowceptLLM) to capture prompts, responses,
    model metadata, and telemetry automatically. Enables standardized instrumentation
    across different LLM providers.
- name: Agent Tool Decorator Pattern
  sources:
  - chunk_ref: 03-PROV-AGENT (Chunk 1:344-351)
    quote: '@flowcept_agent_tool decorator... creates a corresponding AgentTool execution
      activity for each tool execution'
  description: Pattern using decorators to instrument agent tool functions, automatically
    capturing inputs, outputs, and linking to agent identity. Enables automatic provenance
    capture during function calling.
- name: Hallucination Detection via Lineage Tracing
  sources:
  - chunk_ref: 03-PROV-AGENT (Chunk 1:486-492)
    quote: What was the LLM prompt and response when a surprising agent decision was
      identified?
  description: Pattern for detecting and diagnosing LLM hallucinations by tracing
    back through the provenance graph to find the exact prompt and response that led
    to an erroneous output. Enables root cause analysis.
- name: Decision Propagation Tracking
  sources:
  - chunk_ref: 03-PROV-AGENT (Chunk 1:523-529)
    quote: How did an agent decision influence subsequent workflow activities?...
      the query recursively navigates on the used/wasGeneratedBy relationships
  description: Pattern for tracking how LLM decisions cascade through downstream tasks
    using provenance relationships. Enables understanding the impact of individual
    LLM outputs on final results.
- name: Iterative Agent Learning Loop
  sources:
  - chunk_ref: 03-PROV-AGENT (Chunk 1:438-445)
    quote: the decision made for each layer informs the decision logic in the next,
      enabling the system to learn over the course of a print
  description: Pattern where LLM agent decisions in iteration i influence the context
    and decision logic for iteration i+1, creating a learning feedback loop. Enables
    adaptive behavior through accumulated context.
- name: In-Context Learning from Knowledge Graph
  sources:
  - chunk_ref: 15-SciAgents (Chunk 1:95-102)
    quote: in-context learning emerges as a compelling strategy to enhance the performance
      of LLMs without the need for costly and time-intensive fine-tuning
  description: Pattern where LLMs adapt responses based on context embedded within
    prompts derived from knowledge graphs. Avoids fine-tuning by providing relevant
    context dynamically during inference.
- name: Knowledge Graph Path Sampling for Context
  sources:
  - chunk_ref: 15-SciAgents (Chunk 1:133-148)
    quote: We implemented a novel sampling strategy to extract relevant sub-graphs
      from this comprehensive knowledge graph, allowing us to identify and understand
      the key concepts
  description: Pattern of sampling paths between concepts in a knowledge graph to
    create contextual substrate for LLM reasoning. Random path approach enriches context
    with broader conceptual relationships.
- name: Multi-Agent Role Specialization
  sources:
  - chunk_ref: 15-SciAgents (Chunk 1:126-130)
    quote: Each agent in the system is assigned a distinct role, optimized through
      complex prompting strategies to ensure that every subtask is tackled with targeted
      expertise
  - chunk_ref: 15-SciAgents (Chunk 4:1-6)
    quote: 'design_principle agent...unexpected_properties_agent...comparison_agent...novelty_agent...critic_agent:
      Summarizes, critiques, and suggests improvements'
  description: Merged from 2 sources. Pattern where multiple specialized LLM agents
    are assigned distinct roles (hypothesis, outcome, mechanism, design principles,
    unexpected properties, comparison, novelty, critic) to collaboratively expand
    different aspects of a research proposal. Each agent has a focused responsibility
    contributing to a comprehensive output.
- name: Hierarchical Expansion Strategy
  sources:
  - chunk_ref: 15-SciAgents (Chunk 1:200-210)
    quote: We employ a hierarchical expansion strategy where answers are successively
      refined and improved, enriched with retrieved data, critiqued and amended
  description: 'Pattern of iteratively expanding and refining LLM outputs through
    multiple passes: initial generation, expansion with details, critique, and amendment.
    Produces comprehensive outputs from structured iteration.'
- name: Structured JSON Output Generation
  sources:
  - chunk_ref: 15-SciAgents (Chunk 1:205-206)
    quote: generating structured output in JSON following a specific set of aspects
      that the model is tasked to develop
  description: Pattern of prompting LLMs to generate structured JSON outputs with
    predefined fields (hypothesis, outcome, mechanisms, design principles, novelty).
    Enables downstream parsing and systematic expansion.
- name: Ontologist Agent for Concept Definition
  sources:
  - chunk_ref: 15-SciAgents (Chunk 1:289-296)
    quote: the ontologist agent... applies advanced reasoning and inference techniques
      to synthesize and interpret the complex web of data
  description: Pattern of using a dedicated LLM agent to analyze knowledge graph paths,
    define concepts, explain relationships, and prepare semantic context for downstream
    hypothesis generation.
- name: Scientist Agent for Hypothesis Generation
  sources:
  - chunk_ref: 15-SciAgents (Chunk 1:356-365)
    quote: The scientist agent harnesses the extensive knowledge parsed from the knowledge
      graph... to propose novel research ideas. Through complex prompting
  description: Pattern of using specialized LLM agent with structured prompts to synthesize
    novel hypotheses from ontological context. Agent generates proposals addressing
    hypothesis, mechanisms, outcomes, and novelty.
- name: Critic Agent for Hypothesis Review
  sources:
  - chunk_ref: 15-SciAgents (Chunk 1:511-516)
    quote: the Critic agent, responsible for thoroughly reviewing the research proposal,
      summarizing its key points, and recommending improvements
  description: Pattern of adversarial review by dedicated LLM agent that evaluates
    generated hypotheses for strengths, weaknesses, and suggests improvements. Implements
    peer-review-like quality control.
- name: Pre-programmed vs Autonomous Agent Interactions
  sources:
  - chunk_ref: 15-SciAgents (Chunk 1:186-197)
    quote: In the first approach... interactions between agents are pre-programmed
      and follow a predefined sequence... In contrast, the second approach features
      fully automated agent interactions
  description: 'Two patterns for multi-agent orchestration: (1) pre-programmed sequence
    ensuring consistency, (2) autonomous dynamic interactions adapting to evolving
    context. Second approach allows human-in-the-loop intervention.'
- name: Novelty Assessment via Literature Search
  sources:
  - chunk_ref: 15-SciAgents (Chunk 1:195-197)
    quote: empowered our automated multi-agent model with the Semantic Scholar API
      as a tool that provides it with an ability to check the novelty of the generated
      hypothesis against existing literature
  description: Pattern of using external API tools (Semantic Scholar) to validate
    novelty of LLM-generated hypotheses by searching existing literature. Prevents
    redundant or unoriginal outputs.
- name: Swarm Intelligence for Scientific Discovery
  sources:
  - chunk_ref: 15-SciAgents (Chunk 2:138-142)
    quote: harnessing a modular, hierarchically organized swarm of intelligence similar
      to biological systems with multiple iterations to model the process of negotiation
  description: Pattern of using multiple specialized LLM agents working together in
    iterative cycles, mimicking biological swarm intelligence. Enables more nuanced
    reasoning than single-agent zero-shot approaches.
- name: Graph-Based Context Retrieval for Reasoning
  sources:
  - chunk_ref: 15-SciAgents (Chunk 2:145-149)
    quote: The ontological knowledge graph representation of data plays a crucial
      role... ensuring that the hypotheses proposed by the AI agents are both informed
      by and rooted in a vast network
  description: Pattern of using knowledge graphs as foundational context for LLM reasoning.
    Graph structure ensures generated outputs are grounded in interconnected scientific
    concepts rather than pure hallucination.
- name: Heuristic Pathfinding with Random Waypoints
  sources:
  - chunk_ref: 15-SciAgents (Chunk 2:245-260)
    quote: The algorithm... combines heuristic-based pathfinding with node embeddings
      and randomized waypoints to discover diverse paths in a graph
  description: Pattern for sampling knowledge graph paths using embeddings and randomization
    to create diverse context for LLM prompts. Balances heuristic search with stochastic
    exploration.
- name: Prompt-Driven Aspect Expansion
  sources:
  - chunk_ref: 15-SciAgents (Chunk 2:343-376)
    quote: systematically expanding specific aspects of the hypothesis using a series
      of targeted prompts... asking the model to expand upon the original content
      by adding quantitative details
  description: Pattern of iterating through structured JSON fields and using targeted
    prompts to expand each aspect with specific details (chemical formulas, sequences,
    methods). Enables systematic depth enhancement.
- name: Scientific Critique Prompt Pattern
  sources:
  - chunk_ref: 15-SciAgents (Chunk 2:410-421)
    quote: a prompt is issued to the model to critically review the entire document.
      The review is designed to evaluate both the strengths and weaknesses
  description: Pattern of prompting LLM to perform critical scientific review identifying
    strengths, weaknesses, and improvements. Implements automated peer review quality
    control.
- name: Modeling Priority Identification
  sources:
  - chunk_ref: 15-SciAgents (Chunk 2:427-452)
    quote: the model is prompted to identify the most impactful scientific questions
      related to molecular modeling and synthetic biology
  description: Pattern of prompting LLM to identify key actionable research questions
    and outline experimental or simulation steps. Translates hypotheses into concrete
    research plans.
- name: AutoGen Multi-Agent Framework Integration
  sources:
  - chunk_ref: 15-SciAgents (Chunk 2:480-492)
    quote: The automated multi-agent collaboration is implemented in the AutoGen framework...
      using UserProxyAgent, AssistantAgent, and GroupChatManager classes
  description: Pattern of implementing multi-agent LLM systems using AutoGen framework
    with specialized agent types (UserProxy, Assistant) and group chat management
    for coordinated reasoning.
- name: Novelty Assistant with Multi-Query Search
  sources:
  - chunk_ref: 15-SciAgents (Chunk 2:505-510)
    quote: an AI agent named the novelty assistant, which calls the Semantic Scholar
      API three times using different combinations of keywords
  description: Pattern of using specialized LLM agent to perform multiple literature
    searches with varied keywords, then analyzing abstracts to assess novelty. Implements
    robust novelty verification.
- name: Molecular Dynamics Simulation Planning
  sources:
  - chunk_ref: 15-SciAgents (Chunk 3:101-108)
    quote: 'Molecular Dynamics (MD) Simulations: Use MD simulations to model the interactions...
      Software such as GROMACS or AMBER can be employed to study the self-assembly
      process'
  description: Pattern of LLM generating detailed molecular dynamics simulation plans
    including software recommendations (GROMACS, AMBER), force field selection, and
    analysis protocols. Bridges AI reasoning with computational chemistry.
- name: Step-by-Step Rationale Generation
  sources:
  - chunk_ref: 15-SciAgents (Chunk 3:28-41)
    quote: Rationale and Step-by-Step Reasoning... Extract silk fibroin from Bombyx
      mori cocoons using a degumming process
  description: Pattern of LLM generating explicit step-by-step rationale with specific
    procedural details (chemicals, temperatures, durations). Implements chain-of-thought
    reasoning with domain-specific expertise.
- name: Multi-Agent Research Proposal Pipeline
  sources:
  - chunk_ref: 15-SciAgents (Chunk 3:909-918)
    quote: ontologist will define each term... scientist will craft a research proposal...
      specialized agents... will expand on their respective sections... critic_agent
      will summarize, critique
  description: 'Complete pipeline pattern for automated scientific discovery: ontologist
    defines concepts, scientist drafts proposal, specialized agents expand aspects
    (hypothesis, mechanism, novelty), critic reviews, assistant rates feasibility.'
- name: Ontologist-First Knowledge Grounding
  sources:
  - chunk_ref: 15-SciAgents (Chunk 4:12-13)
    quote: 'Define Terms and Relationships: The ontologist will define each term in
      the knowledge path and discuss the relationships between them'
  description: Pattern where an ontologist agent first defines terms and their relationships
    before any creative generation occurs. This grounds subsequent LLM reasoning in
    explicit semantic definitions, preventing hallucination and ensuring conceptual
    consistency.
- name: Sequential Plan Execution with Reasoning
  sources:
  - chunk_ref: 15-SciAgents (Chunk 4:9-18)
    quote: 'Overview of the Plan: 1. Define Terms and Relationships...2. Craft the
      Research Proposal...3. Expand Key Aspects...4. Critique and Improve...5. Rate
      Novelty'
  description: Pattern of structured multi-step execution where each step has explicit
    reasoning (why it matters) and actions (what to do). The LLM follows a predetermined
    plan with clear phase transitions and role handoffs between specialized agents.
- name: Function Calling for Structured Outputs
  sources:
  - chunk_ref: 15-SciAgents (Chunk 4:73)
    quote: The assistant will call the functions.rate_novelty_feasibility function
      to rate the research idea
  description: Pattern where LLM agents invoke structured function calls (tools) to
    produce standardized outputs like novelty/feasibility ratings. This constrains
    the output format and enables programmatic processing of agent decisions.
- name: Critic Agent Review Pattern
  sources:
  - chunk_ref: 15-SciAgents (Chunk 4:5-6)
    quote: 'critic_agent: Summarizes, critiques, and suggests improvements after all
      seven aspects of the proposal have been expanded by the agents'
  description: Pattern where a dedicated critic agent reviews all outputs from specialist
    agents, providing meta-level evaluation, identifying weaknesses, and suggesting
    improvements. This creates a quality control layer in multi-agent systems.
- name: Knowledge Graph Path-Guided Generation
  sources:
  - chunk_ref: 15-SciAgents (Chunk 5:145-146)
    quote: Use the generate_path function to generate a knowledge path between two
      randomly selected keywords
  description: Pattern where LLM generation is guided by paths extracted from a knowledge
    graph. Random or intentional keyword selection seeds the generation, and relationships
    between concepts structure the creative output.
- name: Structured Research Proposal Template
  sources:
  - chunk_ref: 15-SciAgents (Chunk 4:148-225)
    quote: 'Research Proposal: 1- Hypothesis...2- Outcome...3- Mechanisms...4- Design
      Principles...5- Unexpected Properties...6- Comparison...7- Novelty'
  - chunk_ref: 15-SciAgents (Chunk 9:497-511)
    quote: 'The key aspects of the proposal include: 1. Hypothesis... 2. Outcome...
      3. Mechanisms... 4. Design Principles... 5. Unexpected Properties... 6. Comparison...
      7. Novelty...'
  description: Merged from 2 sources. Pattern of using a fixed 7-section template
    (Hypothesis, Outcome, Mechanisms, Design Principles, Unexpected Properties, Comparison,
    Novelty) to structure LLM-generated research proposals. Each section has specific
    semantic expectations and is expanded by specialized agents.
- name: Quantitative Goal Setting in Prompts
  sources:
  - chunk_ref: 15-SciAgents (Chunk 4:162-164)
    quote: 'Mechanical Strength: Enhanced tensile strength of up to 1.5 GPa...Energy
      Efficiency: Reduction in energy consumption by 30%'
  description: Pattern where prompts include specific quantitative targets and metrics
    that the LLM must address. This constrains outputs to include measurable, verifiable
    claims rather than vague qualitative statements.
- name: Modeling and Simulation Technique Specification
  sources:
  - chunk_ref: 15-SciAgents (Chunk 4:298-311)
    quote: 'Molecular Dynamics (MD) Simulations: To understand the interaction...Finite
      Element Analysis (FEA): FEA will be used to model the mechanical behavior'
  description: Pattern where LLM outputs must specify concrete computational methods
    (MD simulations, FEA) with parameters. Forces the LLM to ground abstract claims
    in specific, executable scientific methodologies.
- name: Experimental Validation Method Specification
  sources:
  - chunk_ref: 15-SciAgents (Chunk 4:311-319)
    quote: 'Electrospinning: Parameters such as voltage, flow rate, and collector
      distance will be optimized...Tensile Testing...Cell Culture Studies'
  description: Pattern requiring LLM to specify concrete experimental methods with
    parameters for validating proposed mechanisms. Prevents purely theoretical outputs
    by demanding actionable experimental protocols.
- name: Novelty and Feasibility Rating Pattern
  sources:
  - chunk_ref: 15-SciAgents (Chunk 4:985-998)
    quote: 'Novelty: 7/10...Multi-scale Organizational Properties of Silk: The concept...is
      well-documented...Feasibility: 8/10'
  - chunk_ref: 15-SciAgents (Chunk 9:639-646)
    quote: 'Novelty: 7/10 - The idea of mimicking nacre''s hierarchical structure
      to enhance mechanical properties is well-explored... Feasibility: 8/10 - The
      feasibility of creating nacre-like structures using modern fabrication techniques'
  description: Merged from 2 sources. Pattern of explicitly rating research ideas
    on numeric scales for novelty and feasibility, with detailed justification citing
    literature support and technical challenges. Enables quantitative comparison of
    generated ideas.
- name: Literature-Grounded Validation
  sources:
  - chunk_ref: 15-SciAgents (Chunk 6:144-165)
    quote: 'Query 1: biomimetic materials microfluidic chips...Total Results: 36...Relevant
      Papers: Surface treatments for microfluidic biocompatibility'
  description: Pattern where LLM-generated hypotheses are validated against literature
    search results. The system queries academic databases and uses the presence or
    absence of prior work to assess novelty claims.
- name: Strengths-Weaknesses-Improvements Review Structure
  sources:
  - chunk_ref: 15-SciAgents (Chunk 6:54-82)
    quote: 'Strengths: 1. Innovative Approach...Weaknesses: 1. Complex Fabrication
      Process...Suggested Improvements: 1. Pilot Studies'
  description: Pattern of structured critique using explicit Strengths, Weaknesses,
    and Suggested Improvements sections. Provides balanced evaluation and actionable
    feedback rather than unstructured commentary.
- name: Multi-Scale Mechanism Decomposition
  sources:
  - chunk_ref: 15-SciAgents (Chunk 6:636-748)
    quote: Molecular Scale Mechanisms...Cellular Scale Mechanisms...Macroscale Mechanisms
  description: Pattern where LLM must decompose mechanisms across multiple spatial
    scales (molecular, cellular, macroscale). Forces hierarchical thinking about cause-effect
    relationships at different levels of organization.
- name: Impactful Scientific Question Generation
  sources:
  - chunk_ref: 15-SciAgents (Chunk 6:85-115)
    quote: 'Most Impactful Scientific Question for Molecular Modeling: How does the
      lamellar structure...Key Steps for Molecular Modeling: 1. Model Development...2.
      Parameterization'
  description: Pattern where LLM generates specific scientific questions identified
    as most impactful, along with concrete key steps for addressing them through molecular
    modeling or synthetic biology experiments.
- name: Agent Team Assembly Declaration
  sources:
  - chunk_ref: 15-SciAgents (Chunk 5:127-139)
    quote: 'Hello everyone. We have assembled a great team today...user: An attentive
      HUMAN user...planner...assistant...ontologist...scientist...hypothesis_agent'
  description: Pattern of explicitly declaring the multi-agent team composition at
    the start of a collaborative task. Each agent's role and capabilities are stated
    upfront, establishing clear responsibilities and coordination structure.
- name: Random Keyword Path Generation
  sources:
  - chunk_ref: 15-SciAgents (Chunk 5:163-165)
    quote: The assistant will call the generate_path function with keyword_1 and keyword_2
      set to None to generate a path between randomly selected nodes
  description: Pattern of using random keyword selection to seed novel research idea
    generation. By not specifying keywords, the system explores unexpected concept
    combinations from the knowledge graph.
- name: Comparative Benchmark Analysis
  sources:
  - chunk_ref: 15-SciAgents (Chunk 7:88-132)
    quote: 'Traditional Materials: Typical Performance...Quantitative Benchmark...Proposed
      Material: Expected Performance...Quantitative Goal...Rationale'
  description: Pattern of structured comparison between traditional/baseline approaches
    and proposed innovations. Each comparison includes typical performance benchmarks,
    expected improvements with quantitative goals, and explicit rationale.
- name: Sequential Agent Calling Pattern
  sources:
  - chunk_ref: 15-SciAgents (Chunk 9:34-37)
    quote: Agent caller, please proceed by calling the hypothesis_agent to expand
      on the 'hypothesis' aspect of the research proposal.
  description: A structured multi-agent workflow where a caller agent orchestrates
    sequential invocations of specialized agents (hypothesis_agent, outcome_agent,
    mechanism_agent, design_principles_agent, etc.) to iteratively expand and refine
    outputs. Each agent has a specific role in the generation pipeline.
- name: Role-Based Agent Specialization
  sources:
  - chunk_ref: 15-SciAgents (Chunk 9:673-679)
    quote: 'planner: Who can suggest a step-by-step plan... ontologist: I can define
      each of the terms... scientist: I can craft the research proposal... critic_agent:
      I can summarize, critique'
  description: LLM agents are assigned distinct personas and specialized roles (planner,
    ontologist, scientist, critic, hypothesis_agent, outcome_agent, etc.) that constrain
    and guide their outputs. Each role has defined responsibilities and output expectations.
- name: Literature Review Integration
  sources:
  - chunk_ref: 15-SciAgents (Chunk 9:562-635)
    quote: 'Literature Review Summary: 1. Fusion of Seashell Nacre and Marine Bioadhesive
      Analogs... 2. Peptide-Polymer Conjugates... 3. Materiomics: Biological Protein
      Materials'
  description: Integration of structured literature review into the generation pipeline,
    where the LLM synthesizes existing research to inform and validate novel hypothesis
    generation, providing evidence-based grounding.
- name: Critical Scientific Review Pattern
  sources:
  - chunk_ref: 15-SciAgents (Chunk 9:517-546)
    quote: 'Strengths: 1. Innovative Approach... 2. Comprehensive Design... Weaknesses:
      1. Complexity... 2. Scalability... Suggested Improvements: 1. Simplification...'
  description: A structured critique pattern where the critic_agent evaluates generated
    content through systematic analysis of strengths, weaknesses, and suggested improvements,
    enabling iterative refinement.
- name: Knowledge Path Generation
  sources:
  - chunk_ref: 15-SciAgents (Chunk 9:688-703)
    quote: 'Generate Knowledge Path: Identify the key concepts and relationships between
      graphene and proteins... Assistant will call the functions.generate_path function
      with ''graphene'' and ''proteins'' as keywords'
  description: A pattern for generating conceptual knowledge paths between entities
    using function calling, establishing relationships that inform subsequent research
    proposal generation.
- name: Ontologist Definition Pattern
  sources:
  - chunk_ref: 15-SciAgents (Chunk 9:759-789)
    quote: 'Definitions: Graphene: A single layer of carbon atoms... Relationships:
      Graphene -- bind -- Amyloid Fibrils: Graphene can interact with amyloid fibrils'
  description: Structured extraction of definitions and relationships from knowledge
    graphs, where an ontologist agent provides formal semantic grounding for terms
    used in the generation process.
- name: Quantitative Hypothesis Specification
  sources:
  - chunk_ref: 15-SciAgents (Chunk 10:22-29)
    quote: 'Conductivity Mechanism: The hypothesis posits that the incorporation of
      amyloid fibrils into graphene will create conductive pathways... Quantitative
      Conductivity Improvement: We anticipate that the electrical conductivity...
      will be significantly higher'
  description: A pattern for generating specific, quantitative predictions within
    hypotheses, transforming vague claims into measurable expectations with numerical
    targets.
- name: Molecular Modeling Integration Pattern
  sources:
  - chunk_ref: 15-SciAgents (Chunk 10:505-566)
    quote: 'Select Appropriate Modeling Techniques: Molecular Dynamics (MD) Simulations...
      Density Functional Theory (DFT)... Monte Carlo Simulations... Set Up the Simulation
      Environment: Force Fields: Choose suitable force fields'
  description: A structured pattern for generating computational methodology, including
    technique selection, parameter specification, and validation approaches, enabling
    LLM-guided scientific workflow generation.
- name: Emergent Properties Prediction
  sources:
  - chunk_ref: 15-SciAgents (Chunk 10:253-274)
    quote: 'Expanded Unexpected Properties: 1. Emergent Electrical Properties: Quantum
      Effects... 2. Enhanced Mechanical Properties: Synergistic Reinforcement... Self-Healing
      Ability'
  description: A generative pattern for predicting emergent and unexpected properties
    in novel materials, encouraging creative extrapolation beyond explicitly stated
    hypotheses.
- name: Comparative Analysis Generation
  sources:
  - chunk_ref: 15-SciAgents (Chunk 10:313-364)
    quote: 'Expanded Comparison: 1. Electrical Conductivity: Graphene vs. Amyloid
      Fibrils... Graphene Composites... 2. Mechanical Properties... 3. Biocompatibility'
  description: Structured generation of multi-dimensional comparisons across relevant
    benchmarks, providing systematic evaluation of novel proposals against existing
    approaches.
- name: Novelty Assessment with Evidence
  sources:
  - chunk_ref: 15-SciAgents (Chunk 10:619-631)
    quote: 'Novelty: 8/10 - The hypothesis presents a novel integration of graphene
      and amyloid fibrils for bioelectronic applications, with an innovative approach
      of using engineered gene circuits... Feasibility: 7/10'
  description: A rating pattern that combines quantitative scores with explicit reasoning
    chains, grounding novelty and feasibility assessments in specific evidence from
    the literature review.
- name: Autonomous Tool Selection Pattern
  sources:
  - chunk_ref: 16-KG-Agent (Chunk 1:26-29)
    quote: we integrate the LLM, multifunctional toolbox, KG-based executor, and knowledge
      memory, and develop an iteration mechanism that autonomously selects the tool
      then updates the memory
  description: An autonomous agent pattern where the LLM iteratively selects appropriate
    tools from a toolbox, executes them, and updates memory state without human intervention
    in the reasoning loop.
- name: Code-Based Instruction Synthesis
  sources:
  - chunk_ref: 16-KG-Agent (Chunk 1:31-34)
    quote: we leverage program language to formulate the multi-hop reasoning process
      over the KG, and synthesize a code-based instruction dataset to fine-tune the
      base LLM
  description: A pattern for converting natural language reasoning into structured
    code-like function calls, enabling more precise and executable reasoning traces
    that can be used for LLM fine-tuning.
- name: Three-Tool-Type Architecture
  sources:
  - chunk_ref: 16-KG-Agent (Chunk 1:239-259)
    quote: Extraction tools aim to facilitate the access to information from KG...
      Logic tools aim to support basic manipulation operations... Semantic tools are
      developed by utilizing pre-trained models
  description: A systematic toolbox organization pattern with extraction tools (get_relation,
    get_entity), logic tools (count, intersect, judge, end), and semantic tools (retrieve_relation,
    disambiguate_entity) for structured KG reasoning.
- name: Knowledge Memory Pattern
  sources:
  - chunk_ref: 16-KG-Agent (Chunk 1:546-551)
    quote: 'The knowledge memory preserves the currently useful information to support
      the LLM-based planner for making decisions. It mainly contains four parts of
      information: natural language question, toolbox definition, current KG information,
      and history reasoning program'
  description: A structured memory architecture for LLM agents containing question
    context, available tools, retrieved KG information, and execution history to support
    informed decision-making.
- name: Function Call Generation Pattern
  sources:
  - chunk_ref: 16-KG-Agent (Chunk 1:437-456)
    quote: we reformulate the triples into several function calls with the code format,
      which represents the tool invocation and can be executed to obtain the corresponding
      triples... get_relation(e) function call to obtain the current candidate relations
  description: A pattern for LLMs to generate structured function calls (e.g., get_relation,
    get_tail_entity, get_entity_by_constraint) that can be programmatically executed
    against knowledge graphs.
- name: Iterative Planner-Executor Loop
  sources:
  - chunk_ref: 16-KG-Agent (Chunk 1:629-638)
    quote: The KG-Agent framework autonomously iterates the above tool selection and
      memory updation process to perform step-by-step reasoning, where the knowledge
      memory is used to maintain the accessed information
  description: An iterative reasoning pattern where planning (tool selection) and
    execution (memory update) alternate until the agent determines reasoning is complete,
    enabling multi-hop inference.
- name: Query Graph to Program Conversion
  sources:
  - chunk_ref: 16-KG-Agent (Chunk 1:419-433)
    quote: the query graph has a tree-like structure that can be directly mapped to
      a logical form... starting from the mentioned entity in the question, we adopt
      breadth-first search (BFS) to visit all the nodes on the query graph
  description: A systematic conversion pattern from structured query graphs to executable
    reasoning programs through BFS traversal, generating ordered function call sequences.
- name: Divide and Conquer Goal Decomposition
  sources:
  - chunk_ref: 18-Multi-Agent (Chunk 1:66-71)
    quote: Such systems tackle user-prompted goals by employing a divide & conquer
      strategy, by breaking them down into smaller manageable tasks. These tasks are
      then assigned to specialized agents
  description: A fundamental LLM-powered multi-agent pattern where complex goals are
    decomposed into subtasks assigned to specialized agents, mimicking Minsky's 'society
    of mind' theory for collective problem-solving.
- name: Prompt Augmentation Pattern
  sources:
  - chunk_ref: 18-Multi-Agent (Chunk 1:959-968)
    quote: Before the LLM receives the Agent Prompt, it may undergo Prompt Augmentation.
      This process can integrate additional specifics like the aspects or parts of
      the agent's Role or Memory, Context Information... or chosen Prompt Templates
  description: A systematic pattern for enriching agent prompts with role context,
    memory contents, retrieved information, and templates before LLM processing, enabling
    more informed and contextual responses.
- name: Task-Management Agent Types
  sources:
  - chunk_ref: 18-Multi-Agent (Chunk 2:2-12)
    quote: 'Task-Creation Agent: Generating new tasks, which optionally also includes
      deriving tasks by breaking down complex tasks. Task-Prioritization Agent: Assigning
      urgency... Task-Execution Agent: Ensuring efficient task completion'
  description: 'A taxonomy of specialized agent types for task management: creation
    agents (decomposition), prioritization agents (dependency resolution), and execution
    agents (task completion), each with distinct LLM-powered capabilities.'
- name: Action Type Taxonomy
  sources:
  - chunk_ref: 18-Multi-Agent (Chunk 2:38-49)
    quote: 'DecomposeTask: Breaking down a task into multiple sub-tasks... Create
      Task... DelegateTask: Delegating a task to another agent... ExecuteTask... EvaluateResult...
      MergeResult'
  description: 'A comprehensive taxonomy of agent actions enabling structured collaboration:
    task decomposition, creation, delegation, execution, evaluation, and result merging,
    forming the vocabulary of multi-agent coordination.'
- name: Communication Protocol Patterns
  sources:
  - chunk_ref: 18-Multi-Agent (Chunk 2:82-96)
    quote: Strict finite processes or execution chains with predefined action sequences...
      Dialogue cycles characterized by alternating DelegateTask and ExecuteTask actions...
      Multi-cycle process frameworks with interactions between generic agent types
  description: 'Three distinct communication protocol patterns for multi-agent LLM
    systems: (1) strict sequential chains, (2) two-agent dialogue cycles, and (3)
    flexible multi-cycle frameworks, each suited to different task complexities.'
- name: Contextual Resource Integration
  sources:
  - chunk_ref: 18-Multi-Agent (Chunk 2:104-171)
    quote: 'Tools: Search and Analysis Tools... Execution Tools... Reasoning Tools...
      Development Tools... Communication Tools... Data types: Structured Text Data...
      Unstructured Text Data... Multimodal Data... Foundation Models'
  - chunk_ref: 18-Multi-Agent_Architecture_Taxonomy_LLM (Chunk 3:264-266)
    quote: LLM-powered agents possess the autonomy to interface with a diverse pool
      of contextual resources. They can discerningly select, integrate, and harness
      these resources
  description: Merged from 2 sources. A comprehensive framework for integrating contextual
    resources into LLM agents, categorizing tools (search, execution, reasoning, development,
    communication), data types (structured, unstructured, multimodal, domain-specific),
    and foundation models.
- name: Autonomy-Alignment Balance Matrix
  sources:
  - chunk_ref: 18-Multi-Agent (Chunk 2:278-297)
    quote: 'L2: Real-time: User-Supervised Automation / User-Collaborative Adaptation
      / User-Responsive Autonomy... L1: User-Guided... L0: Integrated: Rule-Driven
      Automation / Pre-Configured Adaptation / Bounded Autonomy'
  description: A nine-cell matrix categorizing LLM multi-agent systems by autonomy
    levels (static/adaptive/self-organizing) and alignment levels (integrated/user-guided/real-time),
    providing systematic architectural classification.
- name: Slow Thinking via Multi-Agent Deliberation
  sources:
  - chunk_ref: 18-Multi-Agent (Chunk 1:56-60)
    quote: LLMs struggle with maintaining consistent logic across extended chains
      of reasoning. This deficiency hinders their ability to engage in a deliberate,
      in-depth, and iterative thought process (aka slow thinking)
  description: Multi-agent architectures address LLM limitations in extended reasoning
    by distributing cognitive load across specialized agents, enabling deliberate
    'slow thinking' through iterative collaboration and mutual feedback.
- name: Goal Decomposition Autonomy
  sources:
  - chunk_ref: 18-Multi-Agent_Architecture_Taxonomy_LLM (Chunk 3:118-124)
    quote: 'Taxonomic aspects of Goal-driven Task Management comprise: Decomposition
      (how the goal or complex task is broken down into manageable sub-tasks)'
  description: LLM-powered agents autonomously decompose complex goals into manageable
    sub-tasks. This is a core generative AI pattern where the LLM reasons about task
    structure and generates a task decomposition strategy. Most systems achieve L2
    (self-organizing) autonomy for decomposition.
- name: Task Orchestration Pattern
  sources:
  - chunk_ref: 18-Multi-Agent_Architecture_Taxonomy_LLM (Chunk 3:122-123)
    quote: Orchestration (how these tasks are distributed among the LLM-powered agents),
      and Synthesis (how the results of the tasks are finally combined)
  description: Pattern for coordinating task distribution and result synthesis across
    multiple LLM agents. Includes decomposition, orchestration, and synthesis phases.
    Most current systems use predefined orchestration (L0) with autonomous execution.
- name: Adaptive Prompt Engineering
  sources:
  - chunk_ref: 18-Multi-Agent_Architecture_Taxonomy_LLM (Chunk 3:162-165)
    quote: Prompt Engineering (how prompts are applied during collaboration and executing
      the actions), and Action Management (how the different kinds of action performed
      by the agents are managed)
  description: Pattern where LLM agents adapt prompt templates during execution based
    on scenario requirements. Systems typically use predefined but adaptable prompt
    templates (L1 autonomy), allowing agents to modify prompts within a defined framework.
- name: Self-Organizing Collaboration Protocol
  sources:
  - chunk_ref: 18-Multi-Agent_Architecture_Taxonomy_LLM (Chunk 3:189-193)
    quote: Agents operating at this level showcase the capability to independently
      strategize their collaboration for task execution...LLM-powered agents can self-organize
      protocols for collaboration
  description: Pattern where LLM agents autonomously plan and execute collaboration
    strategies, self-organize communication protocols, negotiate action execution
    among the agent network. Represents L2 self-organizing autonomy in multi-agent
    collaboration.
- name: Dynamic Role Definition
  sources:
  - chunk_ref: 18-Multi-Agent_Architecture_Taxonomy_LLM (Chunk 3:204-208)
    quote: Agent Generation (how the agents are created), Role Definition (how agents'
      roles are specified), Memory Usage (how agents utilize their memory), and Network
      Management
  description: Pattern for dynamically defining and adapting agent roles during execution.
    Agents can modify or extend their competencies, roles, and relationships based
    on scenario requirements. Some systems achieve L2 autonomy for role definition.
- name: Central LLM Controller Pattern
  sources:
  - chunk_ref: 18-Multi-Agent_Architecture_Taxonomy_LLM (Chunk 3:699-705)
    quote: HUGGINGGPT follows a different strategy by leveraging the LLM as an autonomous
      controller that combines various multi-modal AI models to solve complex tasks
  description: Pattern where a single central LLM acts as controller to orchestrate
    multiple specialized foundation models. The LLM breaks down tasks, selects appropriate
    models, and coordinates execution through prompting. Achieves L2 autonomy for
    most aspects.
- name: Role-Agent Collaboration Pattern
  sources:
  - chunk_ref: 18-Multi-Agent_Architecture_Taxonomy_LLM (Chunk 4:51-58)
    quote: Role-Agent Systems employ an interplay or simulation between multiple dedicated
      roles agents. This collaboration can serve different purposes, such as simulating
      a discussion
  description: Pattern where multiple LLM agents take on dedicated roles with specific
    responsibilities and collaborate through dynamic exchange. Enables multi-perspective
    collaboration and domain simulation (e.g., software development project with different
    roles).
- name: Instructor-Executor Pattern
  sources:
  - chunk_ref: 18-Multi-Agent_Architecture_Taxonomy_LLM (Chunk 4:58-62)
    quote: This collaboration is realized by communication protocols employing a dynamic
      exchange between agents with instructor and executor roles
  description: Multi-agent pattern where one agent (AI-user/instructor) provides directives
    and another agent (AI-assistant/executor) performs execution. Used in CAMEL and
    similar systems for structured task collaboration between LLM agents.
- name: Bounded Autonomy Pattern
  sources:
  - chunk_ref: 18-Multi-Agent_Architecture_Taxonomy_LLM (Chunk 4:76-78)
    quote: these high-autonomy aspects are mostly combined with low alignment levels,
      resulting in bounded autonomy aspects
  description: Architectural pattern where LLM agents have high autonomy (L2) in certain
    aspects (decomposition, action execution, resource utilization) but are constrained
    by low-autonomy predefined mechanisms in other aspects, creating a balance between
    agency and control.
- name: Intertwined Dependencies Pattern
  sources:
  - chunk_ref: 18-Multi-Agent_Architecture_Taxonomy_LLM (Chunk 4:82-90)
    quote: Autonomous decomposition directly depends on the user-prompted goal. Autonomous
      action management depends on strict or predefined communication protocol. Autonomous
      resource utilization depends on strict or predefined resource integration
  description: Pattern describing how autonomous LLM behaviors depend on predefined
    mechanisms as constraints. High-autonomy aspects are balanced by low-autonomy
    aspects that provide integrated alignment and control, ensuring accurate operation.
- name: Prompt-Driven Agent Communication
  sources:
  - chunk_ref: 18-Multi-Agent_Architecture_Taxonomy_LLM (Chunk 4:134-139)
    quote: Collaboration between LLM-powered agents basically relies on prompt-driven
      message exchange, such as by delegating tasks, asking questions, or evaluating
      task results
  description: Pattern where multi-agent collaboration is achieved through sequences
    of prompts for task delegation, questioning, and result evaluation. Susceptible
    to errors and hallucinations, requiring robust control mechanisms for quality
    checking.
- name: Real-Time Responsive Alignment
  sources:
  - chunk_ref: 18-Multi-Agent_Architecture_Taxonomy_LLM (Chunk 4:169-172)
    quote: the interaction layer allows the integration of interceptor mechanisms.
      This not only allows real-time monitoring...but also to implement effective
      feedback and intervention options
  description: Pattern for enabling dynamic realignment of LLM agents during runtime
    through interceptor mechanisms. Supports explainable AI, feedback loops, and human
    intervention. Key for hybrid teamwork between autonomous agents and human co-workers.
- name: Self-Reflection and Planning Pattern
  sources:
  - chunk_ref: 18-Multi-Agent_Architecture_Taxonomy_LLM (Chunk 3:647-654)
    quote: the agent evaluates the intermediate results, engaging in self-criticism.
      The tasks are optionally re-prioritized. The final result represents an aggregate
      of all partial results
  description: Pattern where LLM agents evaluate their own outputs, engage in self-criticism,
    and adjust task prioritization based on intermediate results. Enables iterative
    improvement through autonomous reflection on execution outcomes.
- name: Graph of Thoughts (GoT)
  sources:
  - chunk_ref: 19-Graph_of_Thoughts_LLM_Reasoning (Chunk 1:17-28)
    quote: The key idea and primary advantage of GoT is the ability to model the information
      generated by an LLM as an arbitrary graph, where units of information ('LLM
      thoughts') are vertices
  description: Framework that advances prompting capabilities by modeling LLM reasoning
    as an arbitrary graph. Thoughts are vertices, edges are dependencies. Enables
    combining arbitrary thoughts into synergistic outcomes, distilling networks of
    thoughts, and enhancing thoughts using feedback loops.
- name: Chain-of-Thought (CoT) Prompting
  sources:
  - chunk_ref: 19-Graph_of_Thoughts_LLM_Reasoning (Chunk 1:47-48)
    quote: Chain-of-Thought (CoT) is an approach for prompting, in which one includes
      the intermediate steps of reasoning within the prompt (intermediate 'thoughts')
  description: Foundational prompting pattern that includes intermediate reasoning
    steps in the prompt to improve LLM problem-solving. Significantly enhances mathematical,
    commonsense, and symbolic reasoning without model updates.
- name: Self-Consistency with CoT (CoT-SC)
  sources:
  - chunk_ref: 19-Graph_of_Thoughts_LLM_Reasoning (Chunk 1:55-57)
    quote: One major improvement over CoT, Self-Consistency with CoT (CoT-SC), is
      a scheme where multiple CoTs are generated, and then the best one is selected
      as the outcome
  description: Extension of Chain-of-Thought that generates multiple independent reasoning
    chains and selects the best outcome. Offers opportunity to explore different reasoning
    paths but lacks local exploration within a path.
- name: Tree of Thoughts (ToT)
  sources:
  - chunk_ref: 19-Graph_of_Thoughts_LLM_Reasoning (Chunk 1:58-61)
    quote: Tree of Thoughts (ToT) models the LLM reasoning process with a tree. This
      facilitates using different paths of thoughts, and offers novel capabilities
      such as backtracking
  description: Prompting paradigm that models LLM reasoning as a tree structure. Each
    node represents a partial solution. Enables thought generation, state evaluation,
    and search algorithms (BFS/DFS) for exploring reasoning paths with backtracking
    capability.
- name: Thought Aggregation Transformation
  sources:
  - chunk_ref: 19-Graph_of_Thoughts_LLM_Reasoning (Chunk 1:344-352)
    quote: with GoT, one can aggregate arbitrary thoughts into new ones, to combine
      and reinforce the advantages of these thoughts, while eliminating their disadvantages
  description: Graph-enabled transformation that merges multiple LLM thoughts into
    synergistic new thoughts. Creates vertices with multiple incoming edges to aggregate
    reasoning paths. Enables combining most promising partial solutions while eliminating
    weaknesses.
- name: Thought Refining Transformation
  sources:
  - chunk_ref: 19-Graph_of_Thoughts_LLM_Reasoning (Chunk 1:355-357)
    quote: 'refining of a current thought v by modifying its content: V+ = {} and
      E+ = {(v, v)}. This loop in the graph indicates an iterated thought'
  description: Transformation pattern where an LLM thought is iteratively refined
    through self-loops in the reasoning graph. Enables improvement of thoughts through
    repeated modification while maintaining the same thought connections.
- name: Thought Generation Transformation
  sources:
  - chunk_ref: 19-Graph_of_Thoughts_LLM_Reasoning (Chunk 1:360-364)
    quote: one can generate one or more new thoughts based on an existing single thought
      v. This class embraces analogous reasoning steps from earlier schemes, such
      as ToT or CoT-SC
  description: Transformation that generates k new thoughts from an existing thought.
    Foundation for branching in reasoning graphs. Creates multiple candidate solutions
    from a single state for parallel exploration and selection.
- name: Graph of Operations (GoO)
  sources:
  - chunk_ref: 19-Graph_of_Thoughts_LLM_Reasoning (Chunk 1:390-393)
    quote: the Graph of Operations (GoO)...is a static structure that specifies the
      graph decomposition of a given task, i.e., it prescribes transformations to
      be applied to LLM thoughts
  description: Static execution plan that prescribes the sequence of thought transformations
    (Generate, Aggregate, Score, KeepBest) to be applied for solving a task. Defines
    the graph decomposition strategy before execution.
- name: Graph Reasoning State (GRS)
  sources:
  - chunk_ref: 19-Graph_of_Thoughts_LLM_Reasoning (Chunk 1:394-395)
    quote: GRS is a dynamic structure that maintains the state of the ongoing LLM
      reasoning process (the history of its thoughts and their states)
  description: Dynamic data structure that tracks the evolving state of LLM reasoning
    during execution. Maintains thought history, validity scores, and other relevant
    information for controlling the reasoning process.
- name: Thought Scoring Pattern
  sources:
  - chunk_ref: 19-Graph_of_Thoughts_LLM_Reasoning (Chunk 1:370-378)
    quote: Thoughts are scored to understand whether the current solution is good
      enough. A score is modeled as a general function E(v, G, p_theta)
  - chunk_ref: 19-Graph_of_Thoughts_LLM_Reasoning (Chunk 1:373-374)
    quote: GoT can also rank thoughts. We model this with a function R(G, p_theta,
      h) where h specifies the number of highest-ranking thoughts in G to be returned
  description: Merged from 2 sources. Pattern for evaluating LLM thoughts to determine
    solution quality. Scoring can be done by the LLM itself, by human evaluators,
    or by local scoring functions. Enables selection of best thoughts for further
    processing.
- name: Decompose-Solve-Merge Pattern
  sources:
  - chunk_ref: 19-Graph_of_Thoughts_LLM_Reasoning (Chunk 1:477-478)
    quote: First, one decomposes the input sequence of numbers into subarrays. Then,
      one sorts these subarrays individually, and then respectively merges them into
      a final solution
  description: Task decomposition pattern where complex problems are split into smaller
    subtasks, solved individually, and merged. Analogous to divide-and-conquer algorithms.
    Particularly effective for sorting, set operations, and document merging.
- name: Volume of Thought Metric
  sources:
  - chunk_ref: 19-Graph_of_Thoughts_LLM_Reasoning (Chunk 1:129-133)
    quote: the volume of v is the number of LLM thoughts, from which one can reach
      v using directed edges. Intuitively, these are all the LLM thoughts that have
      had the potential to contribute to v
  description: Novel metric for evaluating prompting strategies. Measures how many
    preceding thoughts could have influenced a given thought. GoT achieves high volume
    (N) with low latency (log_k N), superior to CoT, CoT-SC, and ToT.
- name: Latency-Volume Tradeoff
  sources:
  - chunk_ref: 19-Graph_of_Thoughts_LLM_Reasoning (Chunk 1:743-754)
    quote: GoT is the only scheme to come with both a low latency of log_k N and a
      high volume N. This is enabled by the fact that GoT harnesses aggregations of
      thoughts
  description: Fundamental tradeoff in prompting schemes between latency (hops to
    reach final thought) and volume (information scope). GoT uniquely achieves optimal
    tradeoff through thought aggregation, combining low latency with high information
    volume.
- name: Multiple Response Generation with Selection
  sources:
  - chunk_ref: 19-Graph_of_Thoughts_LLM_Reasoning (Chunk 1:780-788)
    quote: We experiment extensively with the branching factor k and the number of
      levels L to ensure that we compare GoT to cost-effective and advantageous configurations
  description: Pattern of generating k parallel responses at each step and selecting
    best outcomes. Key parameter in ToT and GoT for balancing exploration breadth
    vs. computational cost. More responses typically improve quality but increase
    expense.
- name: Few-Shot Prompt Engineering
  sources:
  - chunk_ref: 19-Graph_of_Thoughts_LLM_Reasoning (Chunk 3:576-580)
    quote: First, we present the prompt stubs (Table 3), serving as templates to dynamically
      generate appropriate prompts at runtime. For clarity, we display their corresponding
      few-shot examples separately
  description: Pattern of using templated prompts with few-shot examples that are
    dynamically populated at runtime. Enables consistent prompt structure while adapting
    to specific inputs. Examples demonstrate expected format and reasoning approach.
- name: Structured Output Formatting
  sources:
  - chunk_ref: 19-Graph_of_Thoughts_LLM_Reasoning (Chunk 3:603-616)
    quote: 'Only output the final 2 lists in the following format without any additional
      text or thoughts!: {{ ''List 1'': [...], ''List 2'': [...] }}'
  description: Pattern of constraining LLM output to specific JSON/structured format.
    Prompts explicitly specify output schema and prohibit additional text. Enables
    reliable parsing and processing of LLM responses.
- name: Step-by-Step Approach Specification
  sources:
  - chunk_ref: 19-Graph_of_Thoughts_LLM_Reasoning (Chunk 3:633-647)
    quote: '<Approach> To fix the incorrectly sorted list follow these steps: 1. For
      each number from 0 to 9, compare the frequency... 2. Iterate through the incorrectly
      sorted list...'
  description: Pattern of including explicit algorithmic steps in prompts to guide
    LLM reasoning. Specifies procedural approach for task completion. Improves reliability
    by providing structured methodology rather than open-ended instructions.
- name: Error Correction Prompting
  sources:
  - chunk_ref: 19-Graph_of_Thoughts_LLM_Reasoning (Chunk 3:627-631)
    quote: The following two lists represent an unsorted list of numbers and a sorted
      variant of that list. The sorted variant is not correct. Fix the sorted variant
      so that it is correct
  description: Pattern for iterative improvement where LLM is given incorrect output
    and asked to fix it. Used in improve/refine operations. Leverages LLM ability
    to compare and correct errors against ground truth or constraints.
- name: Document Aggregation Pattern
  sources:
  - chunk_ref: 19-Graph_of_Thoughts_LLM_Reasoning (Chunk 5:599-608)
    quote: 'For document merging, we employ four distinct types of operations: Generate
      (merge 4 NDAs into 1), Score (score merged NDA), Aggregate (aggregate multiple
      merge attempts into one), Improve'
  description: Multi-step pattern for merging multiple documents using LLM. Combines
    generation, scoring, aggregation, and improvement operations. Goal is maximizing
    information retention while minimizing redundancy.
- name: LLM Self-Scoring Pattern
  sources:
  - chunk_ref: 19-Graph_of_Thoughts_LLM_Reasoning (Chunk 6:519-528)
    quote: Please score the merged NDA in terms of how much redundant information
      is contained...as well as how much information is retained from the original
      NDAs. A score of 10 for redundancy implies...
  description: Pattern where LLM evaluates its own outputs on specified criteria.
    Provides structured scoring rubric (0-10 scale) with clear definitions. Enables
    automated quality assessment for thought selection and comparison.
- name: Reflection Pattern
  sources:
  - chunk_ref: 20-Agentic_RAG_Survey (Chunk 1:512-531)
    quote: Reflection is a foundational design pattern in agentic workflows, enabling
      agents to iteratively evaluate and refine their outputs
  description: A core agentic pattern where agents incorporate self-feedback mechanisms
    to identify and address errors, inconsistencies, and areas for improvement. Involves
    prompting an agent to critique its outputs for correctness, style, and efficiency,
    then incorporating this feedback into subsequent iterations. External tools like
    unit tests or web searches can validate results. In multi-agent systems, distinct
    roles can be assigned where one agent generates outputs while another critiques
    them.
- name: Planning Pattern
  sources:
  - chunk_ref: 20-Agentic_RAG_Survey (Chunk 1:537-549)
    quote: Planning is a key design pattern in agentic workflows that enables agents
      to autonomously decompose complex tasks into smaller, manageable subtasks
  description: An agentic pattern essential for multi-hop reasoning and iterative
    problem-solving in dynamic and uncertain scenarios. Agents dynamically determine
    the sequence of steps needed to accomplish a larger objective. This adaptability
    allows agents to handle tasks that cannot be predefined. While powerful, Planning
    can produce less predictable outcomes compared to deterministic workflows like
    Reflection.
- name: Tool Use Pattern
  sources:
  - chunk_ref: 20-Agentic_RAG_Survey (Chunk 1:552-569)
    quote: Tool Use enables agents to extend their capabilities by interacting with
      external tools, APIs, or computational resources
  description: A pattern allowing agents to gather information, perform computations,
    and manipulate data beyond their pre-trained knowledge. Includes information retrieval,
    computational reasoning, and interfacing with external systems. Has evolved significantly
    with advancements like GPT-4's function calling capabilities. Techniques inspired
    by RAG, such as heuristic-based selection, address challenges in optimizing tool
    selection when many options are available.
- name: Multi-Agent Collaboration Pattern
  sources:
  - chunk_ref: 20-Agentic_RAG_Survey (Chunk 1:572-597)
    quote: Multi-agent collaboration is a key design pattern in agentic workflows
      that enables task specialization and parallel processing
  description: A design pattern where agents communicate and share intermediate results
    while maintaining workflow efficiency and coherence. Distributes subtasks among
    specialized agents to improve scalability and adaptability. Each agent operates
    with its own memory and workflow, which can include the use of tools, reflection,
    or planning. Frameworks such as AutoGen, Crew AI, and LangGraph provide implementations
    for multi-agent solutions.
- name: Prompt Chaining
  sources:
  - chunk_ref: 20-Agentic_RAG_Survey (Chunk 1:620-642)
    quote: Prompt chaining decomposes a complex task into multiple steps, where each
      step builds upon the previous one
  description: An agentic workflow pattern that improves accuracy by simplifying each
    subtask before moving forward. Most effective when a task can be broken down into
    fixed subtasks, each contributing to the final output. Particularly useful in
    scenarios where step-by-step reasoning enhances accuracy. May increase latency
    due to sequential processing. Example applications include generating marketing
    content and translating it, or structuring document creation through outline-verify-develop
    phases.
- name: Routing Pattern
  sources:
  - chunk_ref: 20-Agentic_RAG_Survey (Chunk 1:644-668)
    quote: Routing involves classifying an input and directing it to an appropriate
      specialized prompt or process
  description: An agentic workflow pattern ensuring distinct queries or tasks are
    handled separately, improving efficiency and response quality. Ideal for scenarios
    where different types of input require distinct handling strategies. Example applications
    include directing customer service queries into categories (technical support,
    refund requests, general inquiries) or assigning simple queries to smaller models
    for cost efficiency while routing complex requests to advanced models.
- name: Parallelization Pattern
  sources:
  - chunk_ref: 20-Agentic_RAG_Survey (Chunk 1:671-694)
    quote: Parallelization divides a task into independent processes that run simultaneously,
      reducing latency and improving throughput
  description: An agentic workflow pattern that can be categorized into sectioning
    (independent subtasks) and voting (multiple outputs for accuracy). Useful when
    tasks can be executed independently to enhance speed or when multiple outputs
    improve confidence. Example applications include splitting tasks like content
    moderation (one model screens input while another generates response) or using
    multiple models to cross-check code for vulnerabilities.
- name: Orchestrator-Workers Pattern
  sources:
  - chunk_ref: 20-Agentic_RAG_Survey (Chunk 1:696-719)
    quote: This workflow features a central orchestrator model that dynamically breaks
      tasks into subtasks, assigns them to specialized worker models, and compiles
      the results
  description: An agentic workflow pattern where unlike parallelization, it adapts
    to varying input complexity. Best suited for tasks requiring dynamic decomposition
    and real-time adaptation, where subtasks are not predefined. Example applications
    include automatically modifying multiple files in a codebase based on the nature
    of requested changes, or conducting real-time research by gathering and synthesizing
    relevant information from multiple sources.
- name: Evaluator-Optimizer Pattern
  sources:
  - chunk_ref: 20-Agentic_RAG_Survey (Chunk 1:721-741)
    quote: The evaluator-optimizer workflow iteratively improves content by generating
      an initial output and refining it based on feedback from an evaluation model
  description: An agentic workflow pattern effective when iterative refinement significantly
    enhances response quality, especially when clear evaluation criteria exist. Example
    applications include improving literary translations through multiple evaluation
    and refinement cycles, or conducting multi-round research queries where additional
    iterations refine search results.
- name: Single-Agent Router Architecture
  sources:
  - chunk_ref: 20-Agentic_RAG_Survey (Chunk 1:752-827)
    quote: A Single-Agent Agentic RAG serves as a centralized decision-making system
      where a single agent manages the retrieval, routing, and integration of information
  description: An architecture that simplifies the system by consolidating retrieval,
    routing, and integration tasks into one unified agent. Particularly effective
    for setups with limited number of tools or data sources. The coordinating agent
    evaluates queries and selects from retrieval options including structured databases
    (Text-to-SQL), semantic search, web search, and recommendation systems. Features
    centralized simplicity, efficiency, resource optimization, dynamic routing, and
    versatility across tools.
- name: Multi-Agent RAG Architecture
  sources:
  - chunk_ref: 20-Agentic_RAG_Survey (Chunk 2:1-99)
    quote: Multi-Agent RAG represents a modular and scalable evolution of single-agent
      architectures, designed to handle complex workflows and diverse query types
  description: A modular architecture that distributes responsibilities across multiple
    specialized agents instead of relying on a single agent. A coordinator agent delegates
    queries to specialized retrieval agents based on requirements. Features modularity
    (seamless addition/removal of agents), scalability (parallel processing), task
    specialization, and efficiency. Challenges include coordination complexity, computational
    overhead, and data integration from diverse sources.
- name: Hierarchical Agentic RAG
  sources:
  - chunk_ref: 20-Agentic_RAG_Survey (Chunk 2:100-188)
    quote: Hierarchical Agentic RAG systems employ a structured, multi-tiered approach
      to information retrieval and processing, enhancing both efficiency and strategic
      decision-making
  description: An architecture where agents are organized in a hierarchy with higher-level
    agents overseeing and directing lower-level agents. Top-tier agents evaluate query
    complexity and prioritize data sources. Enables multi-level decision-making, strategic
    prioritization, and scalability for complex multi-faceted queries. Challenges
    include coordination complexity across multiple levels and efficient resource
    allocation to avoid bottlenecks.
- name: Corrective RAG Pattern
  sources:
  - chunk_ref: 20-Agentic_RAG_Survey (Chunk 2:190-312)
    quote: Corrective RAG introduces mechanisms to self-correct retrieval results,
      enhancing document utilization and improving response generation quality
  description: 'A pattern embedding intelligent agents for iterative refinement of
    context documents and responses. Core principle lies in ability to evaluate retrieved
    documents dynamically, perform corrective actions, and refine queries. Built on
    five key agents: Context Retrieval Agent, Relevance Evaluation Agent, Query Refinement
    Agent, External Knowledge Retrieval Agent, and Response Synthesis Agent. Features
    iterative correction, dynamic adaptability, agentic modularity, and factuality
    assurance.'
- name: Adaptive RAG Pattern
  sources:
  - chunk_ref: 20-Agentic_RAG_Survey (Chunk 2:314-441)
    quote: Adaptive Retrieval-Augmented Generation (Adaptive RAG) enhances the flexibility
      and efficiency of large language models by dynamically adjusting query handling
      strategies
  description: A pattern employing a classifier to assess query complexity and determine
    the most appropriate approach, ranging from single-step retrieval to multi-step
    reasoning, or bypassing retrieval altogether. For straightforward queries, generates
    answers using pre-existing knowledge. For simple queries, performs single-step
    retrieval. For complex queries, employs multi-step retrieval with progressive
    refinement. Features dynamic adaptability, resource efficiency, enhanced accuracy
    through iterative refinement, and flexibility for domain-specific extensions.
- name: Agent-G Graph RAG Framework
  sources:
  - chunk_ref: 20-Agentic_RAG_Survey (Chunk 2:443-581)
    quote: Agent-G introduces a novel agentic architecture that integrates graph knowledge
      bases with unstructured document retrieval
  description: 'An agentic framework combining structured and unstructured data sources
    for improved reasoning and retrieval accuracy. Employs modular retriever banks,
    dynamic agent interaction, and feedback loops. Core components: Retriever Bank
    (modular agents for graph/unstructured data), Critic Module (validates relevance
    and quality), Dynamic Agent Interaction (task-specific collaboration), and LLM
    Integration (synthesizes validated data). Features enhanced reasoning, dynamic
    adaptability, improved accuracy, and scalable modularity.'
- name: GeAR Graph-Enhanced Agent Framework
  sources:
  - chunk_ref: 20-Agentic_RAG_Survey (Chunk 2:583-699)
    quote: GeAR introduces an agentic framework that enhances traditional Retrieval-Augmented
      Generation (RAG) systems by incorporating graph-based retrieval mechanisms
  description: A framework advancing RAG through Graph Expansion (enhances base retrievers
    by expanding to include graph-structured data) and Agent Framework (agent-based
    architecture for dynamic, autonomous decision-making in retrieval). Graph Expansion
    Module integrates graph-based data, allowing system to consider entity relationships
    during retrieval. Agent-Based Retrieval enables dynamic selection and combination
    of retrieval strategies. Features enhanced multi-hop retrieval, agentic decision-making,
    improved accuracy, and scalability.
- name: Agentic Document Workflows (ADW)
  sources:
  - chunk_ref: 20-Agentic_RAG_Survey (Chunk 2:701-803)
    quote: Agentic Document Workflows (ADW) extend traditional Retrieval-Augmented
      Generation (RAG) paradigms by enabling end-to-end knowledge work automation
  description: 'A pattern orchestrating complex document-centric processes, integrating
    document parsing, retrieval, reasoning, and structured outputs with intelligent
    agents. Addresses limitations of Intelligent Document Processing (IDP) and RAG
    by maintaining state, coordinating multi-step workflows, and applying domain-specific
    logic. Workflow: Document Parsing/Structuring, State Maintenance, Knowledge Retrieval,
    Agentic Orchestration, Actionable Output Generation. Features state maintenance,
    multi-step orchestration, domain-specific intelligence, scalability, and enhanced
    productivity.'
- name: Real-Time Adaptability in Agentic Systems
  sources:
  - chunk_ref: 20-Agentic_RAG_Survey (Chunk 3:14-21)
    quote: 'Real-Time Adaptability: Dynamically integrates evolving data, such as
      live service outages or pricing updates'
  description: A key benefit of agentic RAG systems enabling dynamic integration of
    evolving data in real-time applications. Customer support systems can incorporate
    live service outages, pricing updates, or other changing information into responses
    without requiring system reconfiguration. This enables personalized, context-aware
    replies that enhance user engagement.
- name: Patient-Specific Data Integration Pattern
  sources:
  - chunk_ref: 20-Agentic_RAG_Survey (Chunk 3:24-46)
    quote: Agentic RAG systems enable this by retrieving real-time clinical guidelines,
      medical literature, and patient history to assist clinicians
  description: A healthcare application pattern where agentic RAG integrates electronic
    health records (EHR) and up-to-date medical literature to generate comprehensive
    summaries for clinicians. Enables personalized care tailored to individual patient
    needs, time efficiency through streamlined retrieval of relevant research, and
    accuracy through recommendations based on latest evidence and patient-specific
    parameters.
- name: Legal Contract Analysis Pattern
  sources:
  - chunk_ref: 20-Agentic_RAG_Survey (Chunk 3:49-71)
    quote: A legal agentic RAG system can analyze contracts, extract critical clauses,
      and identify potential risks
  description: An application pattern combining semantic search capabilities with
    legal knowledge graphs to automate contract review. Automatically flags clauses
    that deviate from standard terms, reduces time spent on contract review processes,
    and handles large volumes of contracts simultaneously. Ensures compliance and
    mitigates risks through automated analysis.
- name: Financial Risk Analysis with Multi-Step Reasoning
  sources:
  - chunk_ref: 20-Agentic_RAG_Survey (Chunk 3:74-98)
    quote: These systems integrate live data streams, historical trends, and predictive
      modeling to generate actionable outputs
  description: A finance application pattern for investment decisions, market analysis,
    and risk management. Delivers real-time analytics based on live market data, identifies
    potential risks using predictive analysis and multi-step reasoning, and combines
    historical and live data for comprehensive strategies. In auto insurance, can
    automate claim processing by retrieving policy details and combining with accident
    data.
- name: Adaptive Learning Pattern
  sources:
  - chunk_ref: 20-Agentic_RAG_Survey (Chunk 3:101-121)
    quote: These systems enable adaptive learning by generating explanations, study
      materials, and feedback tailored to the learner's progress and preferences
  description: An education application pattern where agentic RAG assists researchers
    by synthesizing key findings from multiple sources. Provides tailored learning
    paths adapted to individual student needs and performance levels, engaging interactions
    with interactive explanations and personalized feedback, and scalability supporting
    large-scale deployments for diverse educational environments.
- name: Graph-Enhanced Multimodal Workflows
  sources:
  - chunk_ref: 20-Agentic_RAG_Survey (Chunk 3:124-150)
    quote: Graph-Enhanced Agentic RAG (GEAR) combines graph structures with retrieval
      mechanisms, making it particularly effective in multimodal workflows
  description: A pattern combining graph structures with retrieval for workflows requiring
    interconnected data sources. Enables synthesis of text, images, and videos for
    marketing campaigns. Features multi-modal capabilities integrating text, image,
    and video data; enhanced creativity generating innovative ideas and solutions;
    and dynamic adaptability to evolving market trends and customer needs.
- name: LangGraph Workflow Orchestration
  sources:
  - chunk_ref: 20-Agentic_RAG_Survey (Chunk 3:164-167)
    quote: LangGraph complements this by introducing graph-based workflows that support
      loops, state persistence, and human-in-the-loop interactions
  description: A framework providing modular components for building RAG pipelines
    with graph-based workflows. Enables sophisticated orchestration and self-correction
    mechanisms in agentic systems. Supports loops for iterative processing, state
    persistence for maintaining context across steps, and human-in-the-loop interactions
    for validation and oversight.
- name: Meta-Agent Document Architecture
  sources:
  - chunk_ref: 20-Agentic_RAG_Survey (Chunk 3:169-172)
    quote: It introduces a meta-agent architecture where sub-agents manage smaller
      document sets, coordinating through a top-level agent for tasks such as compliance
      analysis
  description: LlamaIndex's Agentic Document Workflows (ADW) pattern enabling end-to-end
    automation of document processing, retrieval, and structured reasoning. Sub-agents
    manage smaller document sets and coordinate through a top-level agent for complex
    tasks like compliance analysis and contextual understanding.
- name: Adaptive Vector Search
  sources:
  - chunk_ref: 20-Agentic_RAG_Survey (Chunk 3:174-176)
    quote: Qdrant enhances retrieval workflows with adaptive vector search capabilities,
      allowing agents to optimize performance by dynamically switching between sparse
      and dense vector methods
  description: A retrieval pattern where agents dynamically switch between sparse
    and dense vector methods to optimize performance. Combined with Hugging Face pre-trained
    models for embedding and generation tasks, enables flexible, context-aware retrieval
    optimization.
- name: Multi-Agent Orchestration Frameworks
  sources:
  - chunk_ref: 20-Agentic_RAG_Survey (Chunk 3:182-189)
    quote: CrewAI supports hierarchical and sequential processes, robust memory systems,
      and tool integrations. AG2 excels in multi-agent collaboration with advanced
      support for code generation, tool execution, and decision-making
  description: Frameworks emphasizing multi-agent architectures. CrewAI supports hierarchical
    and sequential processes with robust memory systems and tool integrations. AG2
    (formerly AutoGen) excels in multi-agent collaboration with advanced support for
    code generation, tool execution, and decision-making. OpenAI Swarm provides educational,
    lightweight multi-agent orchestration emphasizing agent autonomy and structured
    collaboration.
- name: Semantic Kernel Agentic Patterns
  sources:
  - chunk_ref: 20-Agentic_RAG_Survey (Chunk 3:198-203)
    quote: Semantic Kernel is an open-source SDK by Microsoft that integrates large
      language models (LLMs) into applications. It supports agentic patterns, enabling
      the creation of autonomous AI agents
  description: Microsoft's open-source SDK integrating LLMs into applications with
    support for agentic patterns. Enables creation of autonomous AI agents for natural
    language understanding, task automation, and decision-making. Used in scenarios
    like ServiceNow's P1 incident management to facilitate real-time collaboration,
    automate task execution, and retrieve contextual information seamlessly.
- name: Zero-Shot Prompting
  sources:
  - chunk_ref: 21-LLM_Smart_Contracts_from_BPMN (Chunk 1:145-146)
    quote: In zero-shot prompting, models complete tasks based solely on instructions
      without prior examples
  description: An LLM prompting pattern where models complete tasks based solely on
    instructions without prior examples. Demonstrates that LLMs can perform complex
    tasks beyond language understanding and generation, including analyzing and creating
    business process models and smart contracts, without task-specific training.
- name: Few-Shot Prompting
  sources:
  - chunk_ref: 21-LLM_Smart_Contracts_from_BPMN (Chunk 1:146-147)
    quote: few-shot prompting involves providing a handful of illustrative examples
      directly within the input
  description: An LLM prompting pattern providing illustrative examples within the
    input. Augmenting the generation process by a formalized model improves results
    in smart contract generation. Studies show that moving from zero-shot to one-shot
    and two-shot prompts can improve performance, though not consistently.
- name: LLM-Driven Code Generation from Process Models
  sources:
  - chunk_ref: 21-LLM_Smart_Contracts_from_BPMN (Chunk 1:54-61)
    quote: LLMs are similarly prospected to have the potential to drive automation.
      This hope can draw on seminal results from the related field of code-to-code
      translation (transpilers)
  description: A pattern using LLMs for model-driven engineering where process descriptions
    are transformed into executable artifacts. LLM-based approaches outperform traditional
    rule-based approaches in code-to-code translation. Applied to blockchain-based
    business process execution where BPMN choreographies are transformed into smart
    contracts. Challenges include non-determinism, hallucination, security vulnerabilities,
    and biases.
- name: LLM Temperature Control for Determinism
  sources:
  - chunk_ref: 21-LLM_Smart_Contracts_from_BPMN (Chunk 1:171-174)
    quote: Temperature is a setting for LLMs that controls the randomness of generated
      text; lower temperatures make outputs more predictable and focused
  description: A configuration pattern controlling LLM output randomness. Lower temperatures
    select most probable tokens for predictable, focused outputs. Higher temperatures
    select less probable tokens for creative, varied responses. For smart contract
    generation requiring reliability, temperature is set to 0 for quasi-deterministic
    inference results, though tie-breaking and floating point variability may still
    cause stochasticity.
- name: Automated Evaluation Framework for LLM Code Generation
  sources:
  - chunk_ref: 21-LLM_Smart_Contracts_from_BPMN (Chunk 1:229-241)
    quote: To assess the capabilities of LLMs for smart contract generation, we designed
      a configurable benchmarking framework
  description: A pattern for systematically evaluating LLM-generated code through
    automated testing. Replays all possible conforming traces (which the smart contract
    must accept) and non-conforming traces (which it must reject). Framework components
    include test runner, simulator (generates conforming/non-conforming traces), LLM
    provider interface, log replayer, and blockchain environment. Enables repeatable
    benchmarks as LLM capabilities evolve rapidly.
- name: LLM Integration with Verification Pipelines
  sources:
  - chunk_ref: 21-LLM_Smart_Contracts_from_BPMN (Chunk 1:650-659)
    quote: LLM integration must rely on extensive evaluation and robust verification
      of generated outcomes. This could involve using LLMs to propose code snippets
      or modifications, which are then rigorously checked
  description: A responsible integration pattern for high-security contexts like smart
    contracts. LLMs propose code snippets checked against formal specifications or
    verified using automated theorem provers. LLMs also generate test cases or identify
    potential vulnerabilities, augmenting existing verification processes. Combines
    reliability of traditional code generation tools with flexibility and natural
    language understanding of LLMs.
