#!/usr/bin/env python3
"""
Run Tests - Main Test Orchestration Script

Reads scenarios from a project's test definition file, spawns test subagents
in isolated worktrees, collects traces from Langfuse, and triggers analysis.

CRITICAL FEATURES:
- Automatic worktree creation for test isolation
- Parallel subagent execution via Task tool
- Retry logic for Langfuse trace fetching
- Structured output for test-case-analyzer

Usage:
    python run-tests.py --project 41-integrated-subagent-testing-system
    python run-tests.py --project 41 --scenario basic_flow
    python run-tests.py --scenarios-file path/to/scenarios.yaml

Output:
    - Traces saved to: 02-projects/{PROJECT}/03-working/traces/
    - Reports generated by: test-case-analyzer subagent
    - Reports saved to: 02-projects/{PROJECT}/04-outputs/validation-reports/
"""

import argparse
import json
import sys
import yaml
from datetime import datetime
from pathlib import Path
from typing import List, Dict, Any, Optional

# Import local modules using importlib to handle hyphens in filenames
script_dir = Path(__file__).parent
sys.path.insert(0, str(script_dir))

import importlib.util

def load_module(name, filename):
    """Load a module from a file with hyphens in the name."""
    spec = importlib.util.spec_from_file_location(name, script_dir / filename)
    module = importlib.util.module_from_spec(spec)
    spec.loader.exec_module(module)
    return module

worktree_manager = load_module("worktree_manager", "worktree-manager.py")
fetch_traces = load_module("fetch_traces", "fetch-traces.py")

setup_worktrees = worktree_manager.setup_worktrees
cleanup_worktrees = worktree_manager.cleanup_worktrees
get_repo_root = worktree_manager.get_repo_root
collect_traces_for_agents = fetch_traces.collect_traces_for_agents


def find_project(project_id: str, repo_root: Path) -> Path:
    """Find project folder by ID."""
    projects_dir = repo_root / "02-projects"

    # Try exact match first
    for folder in projects_dir.iterdir():
        if folder.is_dir() and folder.name == project_id:
            return folder
        # Also try partial match (e.g., "41" matches "41-integrated-...")
        if folder.is_dir() and folder.name.startswith(f"{project_id}-"):
            return folder

    raise ValueError(f"Project not found: {project_id}")


def load_scenarios(scenarios_path: Path) -> Dict[str, Any]:
    """Load scenarios from YAML file."""
    if not scenarios_path.exists():
        raise FileNotFoundError(f"Scenarios file not found: {scenarios_path}")

    with open(scenarios_path, 'r', encoding='utf-8') as f:
        return yaml.safe_load(f)


def filter_scenarios(scenarios: List[Dict], scenario_name: Optional[str] = None) -> List[Dict]:
    """Filter scenarios by name if specified."""
    if not scenario_name:
        return scenarios

    filtered = [s for s in scenarios if s.get('name') == scenario_name]
    if not filtered:
        available = [s.get('name') for s in scenarios]
        raise ValueError(f"Scenario '{scenario_name}' not found. Available: {available}")

    return filtered


def calculate_total_runs(scenarios: List[Dict]) -> int:
    """Calculate total number of test runs across all scenarios."""
    return sum(s.get('runs', 1) for s in scenarios)


def get_references_path() -> Path:
    """Get the path to the references directory."""
    # Navigate from scripts/ to references/
    return script_dir.parent / "references"


def generate_subagent_prompt(scenario: Dict, run_index: int, worktree_path: Optional[str] = None) -> Dict[str, Any]:
    """
    Generate prompt for a test subagent using the two-step pattern.

    IMPORTANT: Custom subagent_type is broken. Use general-purpose with
    a two-step prompt that tells the subagent to read the prompt file first.

    Returns:
        Dictionary with Task tool parameters
    """
    references_path = get_references_path()
    orchestrator_prompt_path = references_path / "t-orchestrator-prompt.md"

    # Build the two-step prompt
    prompt_lines = [
        "FIRST: Read the orchestrator instructions from:",
        str(orchestrator_prompt_path.resolve()),
        ""
    ]

    # Add worktree context if provided
    if worktree_path:
        prompt_lines.extend([
            f"WORKDIR: {worktree_path}",
            ""
        ])

    # Add the actual scenario request
    prompt_lines.extend([
        "THEN: Execute this user request:",
        scenario.get('prompt', ''),
        "",
        f"<!-- test_run: {scenario['name']}_run_{run_index} -->"
    ])

    return {
        "subagent_type": "general-purpose",
        "model": "sonnet",
        "prompt": "\n".join(prompt_lines),
        "description": f"Test run {run_index} for {scenario.get('name', 'unnamed')}"
    }


def prepare_analyzer_input(
    traces_data: Dict[str, Any],
    scenario: Dict,
    project_path: Path
) -> Dict[str, Any]:
    """Prepare input data for test-case-analyzer."""
    timestamp = datetime.now().strftime("%Y-%m-%d-%H%M%S")

    return {
        "traces": traces_data.get("structured_traces", []),
        "pass_criteria": scenario.get("pass_criteria", []),
        "scenario": {
            "name": scenario.get("name"),
            "description": scenario.get("description"),
            "runs": scenario.get("runs", 1)
        },
        "output_location": str(
            project_path / "04-outputs" / "validation-reports" /
            f"{timestamp}-{scenario['name']}.md"
        )
    }


def generate_analyzer_prompt(
    traces_yaml: str,
    pass_criteria: List[str],
    scenario: Dict,
    output_location: str
) -> Dict[str, Any]:
    """
    Generate prompt for test-case-analyzer using the two-step pattern.

    IMPORTANT: Custom subagent_type is broken. Use general-purpose with
    a two-step prompt that tells the subagent to read the prompt file first.

    Returns:
        Dictionary with Task tool parameters
    """
    references_path = get_references_path()
    analyzer_prompt_path = references_path / "test-case-analyzer-prompt.md"

    # Format pass criteria as YAML
    pass_criteria_yaml = yaml.dump(pass_criteria, default_flow_style=False)

    # Format scenario as YAML
    scenario_yaml = yaml.dump({
        "name": scenario.get("name"),
        "description": scenario.get("description"),
        "runs": scenario.get("runs", 1)
    }, default_flow_style=False)

    # Build the two-step prompt
    prompt = f"""FIRST: Read the analyzer instructions from:
{analyzer_prompt_path.resolve()}

THEN: Analyze the following test data:

## Traces
```yaml
{traces_yaml}
```

## Pass Criteria
```yaml
{pass_criteria_yaml}
```

## Scenario
```yaml
{scenario_yaml}
```

## Output Location
{output_location}
"""

    return {
        "subagent_type": "general-purpose",
        "model": "sonnet",
        "prompt": prompt,
        "description": f"Analyze test results for {scenario.get('name', 'unnamed')}"
    }


def run_test_scenario(
    scenario: Dict,
    project_path: Path,
    repo_root: Path,
    use_worktrees: bool = True,
    dry_run: bool = False
) -> Dict[str, Any]:
    """
    Run a single test scenario.

    Returns:
        Dictionary with test results
    """
    scenario_name = scenario.get('name', 'unnamed')
    num_runs = scenario.get('runs', 1)

    print(f"\n{'='*60}")
    print(f"Running scenario: {scenario_name}")
    print(f"Runs: {num_runs}")
    print(f"{'='*60}")

    result = {
        "scenario": scenario_name,
        "runs": num_runs,
        "worktrees": [],
        "agent_ids": [],
        "traces_collected": False,
        "analysis_requested": False
    }

    if dry_run:
        print("[DRY RUN] Would create worktrees and spawn subagents")
        print(f"  Worktrees: {num_runs} (test-{scenario_name[:20]}-0 to test-{scenario_name[:20]}-{num_runs-1})")
        return result

    # Step 1: Setup worktrees if needed
    worktree_paths = []
    if use_worktrees:
        prefix = f"test-{scenario_name[:20]}"
        worktrees = setup_worktrees(num_runs, prefix)
        worktree_paths = [wt['path'] for wt in worktrees]
        result["worktrees"] = worktree_paths
        print(f"\nCreated {len(worktree_paths)} worktrees")

    # Step 2: Generate prompts for each run using the two-step pattern
    task_configs = []
    for i in range(num_runs):
        wt_path = worktree_paths[i] if worktree_paths else None
        task_config = generate_subagent_prompt(scenario, i, wt_path)
        task_configs.append(task_config)

    # Step 3: Output prompts for Claude to execute
    # NOTE: This script prepares the data, but Claude executes the Task tool
    print("\n" + "-"*40)
    print("SUBAGENT PROMPTS READY")
    print("-"*40)
    print(f"\nScenario: {scenario_name}")
    print(f"Runs to execute: {num_runs}")
    print("\nNOTE: Use general-purpose subagent_type (custom types are broken)")
    print("Prompts saved to working directory for execution.")

    # Save task configs to working directory
    prompts_dir = project_path / "03-working" / "test-prompts"
    prompts_dir.mkdir(parents=True, exist_ok=True)

    prompts_file = prompts_dir / f"{scenario_name}_prompts.json"
    with open(prompts_file, 'w', encoding='utf-8') as f:
        json.dump({
            "scenario": scenario_name,
            "timestamp": datetime.now().isoformat(),
            "task_configs": task_configs,  # Full Task tool parameters
            "worktrees": worktree_paths,
            "pass_criteria": scenario.get('pass_criteria', []),
            "invocation_pattern": "general-purpose with two-step prompt"
        }, f, indent=2)

    print(f"Task configs saved to: {prompts_file}")

    return result


def main():
    parser = argparse.ArgumentParser(
        description="Run validation tests for a project"
    )
    parser.add_argument(
        "--project", "-p", type=str,
        help="Project ID or name"
    )
    parser.add_argument(
        "--scenarios-file", type=str,
        help="Path to scenarios YAML file (alternative to --project)"
    )
    parser.add_argument(
        "--scenario", "-s", type=str,
        help="Run only this scenario (by name)"
    )
    parser.add_argument(
        "--no-worktrees", action="store_true",
        help="Skip worktree creation (use for read-only tests)"
    )
    parser.add_argument(
        "--dry-run", action="store_true",
        help="Show what would be done without executing"
    )
    parser.add_argument(
        "--cleanup-only", action="store_true",
        help="Only cleanup worktrees, don't run tests"
    )

    args = parser.parse_args()

    # Get repo root
    repo_root = get_repo_root()

    # Cleanup mode
    if args.cleanup_only:
        print("Cleaning up test worktrees...")
        removed = cleanup_worktrees(prefix="test-", cleanup_all=True)
        print(f"Removed {removed} worktrees")
        return

    # Find scenarios file
    if args.scenarios_file:
        scenarios_path = Path(args.scenarios_file)
        if not scenarios_path.is_absolute():
            scenarios_path = repo_root / scenarios_path
        project_path = scenarios_path.parent.parent.parent  # Guess project path
    elif args.project:
        project_path = find_project(args.project, repo_root)
        scenarios_path = project_path / "02-resources" / "tests" / "scenarios.yaml"
    else:
        parser.error("Either --project or --scenarios-file is required")
        return

    print(f"Project: {project_path.name}")
    print(f"Scenarios: {scenarios_path}")

    # Load scenarios
    scenarios_data = load_scenarios(scenarios_path)
    scenarios = scenarios_data.get('scenarios', [])

    if not scenarios:
        print("No scenarios found in file")
        return

    # Filter scenarios if specified
    scenarios = filter_scenarios(scenarios, args.scenario)

    total_runs = calculate_total_runs(scenarios)
    print(f"\nScenarios to run: {len(scenarios)}")
    print(f"Total test runs: {total_runs}")

    # Run each scenario
    results = []
    for scenario in scenarios:
        result = run_test_scenario(
            scenario=scenario,
            project_path=project_path,
            repo_root=repo_root,
            use_worktrees=not args.no_worktrees,
            dry_run=args.dry_run
        )
        results.append(result)

    # Summary
    print("\n" + "="*60)
    print("TEST PREPARATION COMPLETE")
    print("="*60)
    print(f"\nScenarios prepared: {len(results)}")
    print(f"Total runs: {total_runs}")

    if not args.dry_run:
        print("\nNext steps:")
        print("1. Execute the subagent prompts via Task tool")
        print("2. Collect agent_ids from Task responses")
        print("3. Run fetch-traces.py with agent IDs")
        print("4. Spawn test-case-analyzer with traces + pass_criteria")

    # Save summary
    summary_path = project_path / "03-working" / "test-summary.json"
    with open(summary_path, 'w', encoding='utf-8') as f:
        json.dump({
            "timestamp": datetime.now().isoformat(),
            "project": project_path.name,
            "scenarios_count": len(scenarios),
            "total_runs": total_runs,
            "results": results
        }, f, indent=2)

    print(f"\nSummary saved to: {summary_path}")


if __name__ == "__main__":
    main()
